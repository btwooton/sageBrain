{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "#spacy\n",
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "#lda\n",
    "from spacy.en import English\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from stop_words import get_stop_words\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from gensim import corpora, models\n",
    "import gensim\n",
    "import pdb\n",
    "import nltk as nltk\n",
    "\n",
    "from nltk import ngrams\n",
    "from nltk import regexp_tokenize\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "#Word2Vect\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadPickle(name):\n",
    "    return pickle.load(open(name,'rb'))\n",
    "\n",
    "def savePickle(data,name):\n",
    "    pickle.dump(data,open(name,'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<__main__.Document instance at 0x14aed4cf8>, <__main__.Document instance at 0x14aed45f0>, <__main__.Document instance at 0x14aed47a0>, <__main__.Document instance at 0x14aec9758>, <__main__.Document instance at 0x14aec9050>, <__main__.Document instance at 0x1510ce950>, <__main__.Document instance at 0x1510cecf8>, <__main__.Document instance at 0x14aeb2200>, <__main__.Document instance at 0x14aedcbd8>, <__main__.Document instance at 0x14aedc200>, <__main__.Document instance at 0x14aedc170>]\n",
      "Multimodal deep learning\n",
      "Multimodal learning with deep boltzmann machines\n",
      "Deep learning: methods and applications\n",
      "Grounded compositional semantics for finding and describing images with sentences\n",
      "Unifying visual-semantic embeddings with multimodal neural language models\n",
      "Zero-shot learning through cross-modal transfer\n",
      "Recent advances in deep learning for speech research at Microsoft\n",
      "New types of deep neural network learning for speech recognition and related applications: An overview\n",
      "Deep learning for detecting robotic grasps\n",
      "Deep fragment embeddings for bidirectional image sentence mapping\n",
      "Deep canonical correlation analysis\n"
     ]
    }
   ],
   "source": [
    "#Load the session Object\n",
    "session = loadPickle(\"entry.p\")\n",
    "print session.documents\n",
    "loadSessionAnalysis(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Load Pickled Objects into session Analytics\n",
    "def loadSessionAnalysis(session):\n",
    "    session.textAnalysis = []\n",
    "    for doc in session.documents:\n",
    "        print doc.title\n",
    "        try:\n",
    "            if doc.text:\n",
    "                    session.textAnalysis.append(doc.runTextAnalysis())\n",
    "        except AttributeError:\n",
    "            if doc.metadata['Text'] != False:\n",
    "                doc.text = doc.metadata['Text'].decode('utf-8')\n",
    "                session.textAnalysis.append(doc.runTextAnalysis())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_lda(textAnalytics,parameter):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "    # create English stop words list\n",
    "    en_stop = get_stop_words('en')\n",
    "\n",
    "    # Create p_stemmer of class PorterStemmer\n",
    "    p_stemmer = SnowballStemmer('english')\n",
    "    # create sample documents\n",
    "    texts = []\n",
    "    for eachDocument in textAnalytics:\n",
    "        texts.append(eachDocument[parameter])\n",
    "\n",
    "    # turn our tokenized documents into a id <-> term dictionary\n",
    "    dictionary = corpora.Dictionary(texts)\n",
    "    # convert tokenized documents into a document-term matrix\n",
    "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "    #Store to visualize\n",
    "    pickle.dump(corpus,open('corpus.pkl','wb'))\n",
    "    dictionary.save('dictionary.gensim')\n",
    "\n",
    "    # generate LDA model\n",
    "    ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=9, id2word = dictionary, passes=1)\n",
    "    #Store to visualize\n",
    "    ldamodel.save('20top_100Tok.gensim')\n",
    "\n",
    "    return ldamodel\n",
    "\n",
    "def get_topics(lda,num_topics, num_words):\n",
    "    doc_topics = []\n",
    "    topics = lda.print_topics(num_topics=num_topics, num_words=num_words)\n",
    "    return topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session Functions Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "from collections import Counter\n",
    "def orderDocuments(parameter,bigToSmall):\n",
    "    \"\"\"Return a list of ordered documents based on a document parameter\"\"\"\n",
    "    ordered_list = sorted(session.documents, key=operator.attrgetter(parameter),reverse=bigToSmall)\n",
    "    # sorted(session.documents, key=lambda k: k['name'])\n",
    "    return ordered_list\n",
    "\n",
    "def authorParamFreq(parameter):\n",
    "    \"\"\"Returns a dictionary of the author: Name, Affiliation, frequency in the session\"\"\"\n",
    "    authors = []\n",
    "    for doc in session.documents:\n",
    "        for eachAuthor in doc.author:\n",
    "            if(eachAuthor[parameter]!=False):\n",
    "                authors.append(eachAuthor[parameter])\n",
    "    return Counter(authors)\n",
    "    \n",
    "def authorIntestsFreq(parameter):\n",
    "    \"\"\"Returns a dictionary of the Interests of the authors frequency in the session\"\"\"\n",
    "    parameter_list = []\n",
    "    for doc in session.documents:\n",
    "        for eachAuthor in doc.author:\n",
    "            if(eachAuthor[parameter]!=False):\n",
    "                for element in eachAuthor[parameter]:\n",
    "                    parameter_list.append(element)\n",
    "    return Counter(parameter_list)\n",
    "\n",
    "def documentParamFreq(parameter):\n",
    "    \"\"\"Returns a dictionary of the Document Data organizations, conference, authorHindex,year frequency in the session\"\"\"\n",
    "    parameter_list = []\n",
    "    for doc in session.documents:\n",
    "        parameter_list.append(getattr(doc, parameter))\n",
    "    return Counter(parameter_list)  \n",
    "\n",
    "def getDocuments(parameter,value,comparison):\n",
    "    document_list = []\n",
    "    for doc in session.documents:\n",
    "        if(comparison == '='):\n",
    "            if(getattr(doc, parameter) == value):\n",
    "                document_list.append(doc)\n",
    "        elif(comparison == '>'):\n",
    "             if(getattr(doc, parameter) > value):\n",
    "                document_list.append(doc)\n",
    "        elif(comparison == '<'):\n",
    "            if(getattr(doc, parameter) < value):\n",
    "                 document_list.append(doc)\n",
    "    return document_list\n",
    "\n",
    "def getDocumentsAuth(parameter,value,comparison):\n",
    "    document_list = []\n",
    "    for doc in session.documents:\n",
    "        for author in doc.author:\n",
    "            if(comparison == '='):\n",
    "                if(author[parameter] == value):\n",
    "                    document_list.append(doc)\n",
    "            elif(comparison == '>'):\n",
    "                 if(author[parameter] > value):\n",
    "                    document_list.append(doc)\n",
    "            elif(comparison == '<'):\n",
    "                if(author[parameter] < value):\n",
    "                     document_list.append(doc)\n",
    "    return document_list\n",
    "\n",
    "def topicsByDocParam(parameter):\n",
    "    \"\"\"Calculates topics grouped by an doc parameter.\"\"\"\n",
    "    yearFreq = documentParamFreq(parameter)\n",
    "    #lda functions per each year\n",
    "    lda_functions = []\n",
    "    #documents per each year\n",
    "    document_list = []\n",
    "    #Topics per year\n",
    "    topics_list = []\n",
    "    for eachYear in yearFreq.keys():\n",
    "        doc = getDocuments(parameter,eachYear,'=')\n",
    "        document_list.append(doc)\n",
    "        lda_functions.append(get_lda(session.textAnalysis,'tokens'))\n",
    "\n",
    "    for each in lda_functions:   \n",
    "        topics = get_topics(each,4,4)\n",
    "        topics_list.append(topics)\n",
    "        print \"\\n\"\n",
    "        print(\"##############\")\n",
    "        print topics\n",
    "    return topics_list\n",
    "\n",
    "def topicsByAuthorParam(function,parameter):\n",
    "    \"\"\"Calculates topics grouped by an author parameter.\"\"\"\n",
    "    \"\"\"Needs a function to search for the parameter authorIntestsFreq(\"Interests\") or authorParamFreq(\"Name\") \"\"\"\n",
    "    #Dictionary of parameters\n",
    "    paramFreq = function\n",
    "    lda_functions = []\n",
    "    document_list = []\n",
    "    #Topics per year\n",
    "    topics_list = []\n",
    "    for param in paramFreq.keys():\n",
    "        doc = getDocumentsAuth(parameter,param,'=')\n",
    "        document_list.append(doc)\n",
    "        lda_functions.append(get_lda(session.textAnalysis,'tokens'))\n",
    "\n",
    "    for each in lda_functions:   \n",
    "        topics = get_topics(each,4,4)\n",
    "        topics_list.append(topics)\n",
    "        print \"\\n\"\n",
    "        print(\"##############\")\n",
    "        print topics\n",
    "    return topics_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'Aditya Khosla', u'Juhan Nam', u'Honglak Lee', u'andrew Y ng']\n",
      "[u'Nitish Srivastava', u'Ruslan Salakhutdinov']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('doc_0', 0.10860138386487961),\n",
       " ('doc_9', 0.08349549025297165),\n",
       " ('doc_3', 0.06715176999568939),\n",
       " ('doc_5', 0.0652678906917572),\n",
       " ('doc_6', 0.062332071363925934),\n",
       " ('doc_7', 0.047797657549381256),\n",
       " ('doc_4', 0.015579424798488617),\n",
       " ('doc_1', -0.05207110568881035),\n",
       " ('doc_2', -0.07239247113466263),\n",
       " ('doc_8', -0.10687482357025146)]"
      ]
     },
     "execution_count": 397,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating Similarity Vector for the Session\n",
    "#Author, conference, title, lda, year, doc_similarity\n",
    "doc_similarity_vector = []\n",
    "\n",
    "title_initial_doc = session.documents[0].title\n",
    "authors_in_initial_doc = [author['Name'] for author in session.documents[0].author if author['Name']!= False]\n",
    "print authors_in_initial_doc\n",
    "tagged_documents = []\n",
    "#Get authors in each paper\n",
    "All_authors = []\n",
    "for doc in session.documents:\n",
    "    if doc.title != title_initial_doc: \n",
    "        All_authors.append([author['Name'] for author in doc.author if author['Name']!= False])\n",
    "print All_authors[0]\n",
    "for i, auth in enumerate(All_authors):\n",
    "    tagged_documents.append(TaggedDocument(auth,[\"doc_{}\".format(i)]))\n",
    "# print tagged_documents[0].words\n",
    "# print tagged_documents[0].tags\n",
    "\n",
    "d2v_model = gensim.models.doc2vec.Doc2Vec(tagged_documents,size=300, min_count=1)\n",
    "# print authors_in_initial_doc\n",
    "testVector = d2v_model.infer_vector(authors_in_initial_doc)\n",
    "d2v_model.docvecs.most_similar([testVector])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Aditya Khosla\n",
      "False\n",
      "Juhan Nam\n",
      "Honglak Lee\n",
      "andrew Y ng\n"
     ]
    }
   ],
   "source": [
    "for author in session.documents[0].author:\n",
    "    print author['Name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-257-43cebce864cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdocumentParamFreq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'author'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mlda_functions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdocument_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0meachYear\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetDocuments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'author'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0meachYear\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'='\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-248-df9c535d2e8e>\u001b[0m in \u001b[0;36mdocumentParamFreq\u001b[0;34m(parameter)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mparameter_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameter_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgetDocuments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcomparison\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/agonzamart/anaconda/lib/python2.7/collections.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'expected at most 1 arguments, got %d'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCounter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__missing__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/agonzamart/anaconda/lib/python2.7/collections.pyc\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    565\u001b[0m                 \u001b[0mself_get\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0melem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 567\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    568\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "a = documentParamFreq('author')\n",
    "lda_functions = []\n",
    "document_list = []\n",
    "for eachYear in a.keys():\n",
    "    doc = getDocuments('author',eachYear,'=')\n",
    "    document_list.append(doc)\n",
    "print document_list\n",
    "#     lda_functions.append(get_lda(session.textAnalysis,'tokens'))\n",
    "\n",
    "# for each in lda_functions:   \n",
    "#     topics = get_topics(each,4,4)\n",
    "#     print \"\\n\"\n",
    "#     print(\"##############\")\n",
    "#     print topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<__main__.Document instance at 0x14aed47a0>, <__main__.Document instance at 0x14aec9758>, <__main__.Document instance at 0x14aec9050>, <__main__.Document instance at 0x14aedc200>]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "year = '2014'\n",
    "for doc in session.documents:\n",
    "    if(doc.year==year):\n",
    "        document_list.append(doc)\n",
    "print document_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['GetScholarInfo',\n",
       " 'SetScholarOptions',\n",
       " '__doc__',\n",
       " '__init__',\n",
       " '__module__',\n",
       " 'abstract',\n",
       " 'author',\n",
       " 'authorHindex',\n",
       " 'calculateAuthorHindex',\n",
       " 'calculateHIndex',\n",
       " 'citationArticles',\n",
       " 'citations',\n",
       " 'citations_list',\n",
       " 'clusterID',\n",
       " 'conference',\n",
       " 'create_document_msg',\n",
       " 'excerpt',\n",
       " 'get_sections',\n",
       " 'get_text',\n",
       " 'get_toc',\n",
       " 'globalID',\n",
       " 'hindex',\n",
       " 'id',\n",
       " 'metadata',\n",
       " 'organization',\n",
       " 'pages',\n",
       " 'parseMetadata',\n",
       " 'parseScholarInfo',\n",
       " 'runTextAnalysis',\n",
       " 'scholarInfo',\n",
       " 'sections',\n",
       " 'text',\n",
       " 'title',\n",
       " 'toc',\n",
       " 'type',\n",
       " 'url',\n",
       " 'user',\n",
       " 'versions',\n",
       " 'year']"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(session.documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('doc_Multimodal deep learning', 0.8936805725097656),\n",
       " ('doc_Multimodal learning with deep boltzmann machines', 0.861474871635437),\n",
       " ('doc_Grounded compositional semantics for finding and describing images with sentences',\n",
       "  0.8581294417381287),\n",
       " ('doc_Zero-shot learning through cross-modal transfer', 0.8551200032234192),\n",
       " ('doc_Deep fragment embeddings for bidirectional image sentence mapping',\n",
       "  0.8472919464111328),\n",
       " ('doc_New types of deep neural network learning for speech recognition and related applications: An overview',\n",
       "  0.8375876545906067),\n",
       " ('doc_Deep learning for detecting robotic grasps', 0.8362963199615479),\n",
       " ('doc_Unifying visual-semantic embeddings with multimodal neural language models',\n",
       "  0.8303850293159485),\n",
       " ('doc_Recent advances in deep learning for speech research at Microsoft',\n",
       "  0.8031747937202454),\n",
       " ('doc_Deep learning: methods and applications', 0.5978598594665527)]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testVector = word2vecDoc.infer_vector(session.textAnalysis[0]['lemma'])\n",
    "word2vecDoc.docvecs.most_similar([testVector])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Interaface Class to talk with Node\n",
    "class SageBrain(object):\n",
    "    \"\"\" Sage Brain Class\"\"\"\n",
    "    def __init__(self, session_id):\n",
    "        self.id = \"brainInterface\"\n",
    "        self.sessions = []\n",
    "        self.session_counter = -1\n",
    "        self.actualSession = -1\n",
    "        self.addSession(session_id)\n",
    "\n",
    "    def DocInterface(self, fileName, doc_id):\n",
    "        \"\"\" Brain Interface \"\"\"\n",
    "        doc = Document(self.actualSession, fileName, doc_id, \"doc\", \"user\", False)\n",
    "        self.actualSession.addDoc(doc)\n",
    "        # Add citations as documents.\n",
    "        for citation in self.actualSession.documents[0].citationArticles:\n",
    "            # I need to send the object to send the information\n",
    "            doc = Document(self.actualSession, citation['Title'], citation[\"ID\"], \"doc\", \"user\", citation)\n",
    "            self.actualSession.addDoc(doc)\n",
    "        pdb.set_trace()\n",
    "        msg = doc.create_document_msg()\n",
    "        #We return the document to Node\n",
    "        return msg\n",
    "\n",
    "    def addSession(self, session_id):\n",
    "        \"\"\" Function to add new sessions \"\"\"\n",
    "        self.session_counter = self.session_counter + 1\n",
    "        self.sessions.append(Session(session_id))\n",
    "        self.actualSession = self.sessions[self.session_counter]\n",
    "\n",
    "\n",
    "#Internal Classes declarations\n",
    "#Session\n",
    "class Session:\n",
    "    \"\"\"Session Class\"\"\"\n",
    "    def __init__(self, session_id):\n",
    "        self.id = session_id\n",
    "        self.documents = []\n",
    "        self.topics = []\n",
    "        self.authorList = []\n",
    "        self.lda = False\n",
    "        self.txtAnalysis = []\n",
    "\n",
    "    def addDoc(self, doc):\n",
    "        \"\"\" Function to add a new document to the session \"\"\"\n",
    "        self.documents.append(doc)\n",
    "        self.txtAnalysis.append(doc.textAnalysis)\n",
    "        # self.addDocTopics(doc)\n",
    "        # self.lda = doc.lda;\n",
    "\n",
    "    def addDocTopics(self, doc):\n",
    "        for topic in doc.topics:\n",
    "            self.topics.append(topic)\n",
    "             \n",
    "    def addAuthor(self, author):\n",
    "        self.authorList.append(author)\n",
    "    \n",
    "    def searchAuthor(self, author):\n",
    "        \"\"\"Create function to search for authors that returns the author or False\"\"\"\n",
    "        try:\n",
    "            index = self.authorList.index(author)\n",
    "            return self.authorList[index]\n",
    "        except ValueError:\n",
    "            return \"Null\"\n",
    "\n",
    "    def get_lda(parameter):\n",
    "        \"\"\"Calculates a returns an LDA model of the Session\"\"\"\n",
    "        tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "        # create English stop words list\n",
    "        en_stop = get_stop_words('en')\n",
    "\n",
    "        # Create p_stemmer of class PorterStemmer\n",
    "        p_stemmer = SnowballStemmer('english')\n",
    "        # create sample documents\n",
    "        texts = []\n",
    "        for eachDocument in textAnalytics:\n",
    "            texts.append(eachDocument[parameter])\n",
    "\n",
    "        # turn our tokenized documents into a id <-> term dictionary\n",
    "        dictionary = corpora.Dictionary(texts)\n",
    "        # convert tokenized documents into a document-term matrix\n",
    "        corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "        #Store to visualize\n",
    "        pickle.dump(corpus,open('corpus.pkl','wb'))\n",
    "        dictionary.save('dictionary.gensim')\n",
    "\n",
    "        # generate LDA model\n",
    "        ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=9, id2word = dictionary, passes=100)\n",
    "        #Store to visualize\n",
    "        ldamodel.save('20top_100Tok.gensim')\n",
    "\n",
    "        return ldamodel\n",
    "\n",
    "    def get_topics(self, num_topics, num_words):\n",
    "        doc_topics = []\n",
    "        topics = self.lda.print_topics(num_topics=num_topics, num_words=num_words)\n",
    "        for topic in topics:\n",
    "            topicObject = Topic(self.name)\n",
    "            topicObject.extract_words(topic)\n",
    "            doc_topics.append(topicObject)\n",
    "        return doc_topics\n",
    "\n",
    "    def calculateTFidf(self,parameter):\n",
    "        #We store the tokenized text in a vector\n",
    "        tokText = []\n",
    "        for each in self.textAnalysis:\n",
    "            tokText.append(each[parameter])\n",
    "        #We map each word to a number\n",
    "        dictionary = gensim.corpora.Dictionary(tokText)\n",
    "        num_words = len(dictionary)\n",
    "        #Create a corpus: List of bag of words\n",
    "        corpus = [dictionary.doc2bow(eachtokText) for eachtokText in tokText]\n",
    "        tf_idf = gensim.models.TfidfModel(corpus)\n",
    "        #Create the simiarity measure Object\n",
    "        sims = gensim.similarities.Similarity('./similarityStorage',tf_idf[corpus],num_features=num_words)\n",
    "        return {\"sims\":sims,\"dict\":dictionary,\"tf_idf\":tf_idf}\n",
    "\n",
    "    def word2VectTraining(self,parameter):\n",
    "        \"\"\"Training Word Vectors\"\"\"\n",
    "        txt = []\n",
    "        for each in self.textAnalysis:\n",
    "            txt.append(each[parameter])\n",
    "        model = gensim.models.word2vec.Word2Vec(txt,min_count=10,size=300)\n",
    "        return model\n",
    "\n",
    "    def word2VectDocumentTraining(self,parameter):\n",
    "        \"\"\"Training Document Word Vectors\n",
    "        Needs to run with raw Text for expected results\"\"\"\n",
    "        tagged_documents = []\n",
    "        for i, doc in enumerate(self.textAnalysis):\n",
    "            tagged_documents.append(TaggedDocument(doc[parameter],[\"doc_{}\".format(self.documents[i].title)]))\n",
    "        d2v_model = gensim.models.doc2vec.Doc2Vec(tagged_documents,size=300)\n",
    "        return d2v_model\n",
    "\n",
    "#Topic Class\n",
    "class Topic:\n",
    "    \"\"\"Topic Class\"\"\"\n",
    "    def __init__(self, owner):\n",
    "        self.id = \"id\"\n",
    "        self.owner = owner\n",
    "        self.words = []\n",
    "        self.text =[]\n",
    "\n",
    "    def extract_words(self, topic):\n",
    "        \"\"\" Function to extract the words and probabilities of the topics\"\"\"\n",
    "        for wordPlusProbality in topic[1].split('+'):\n",
    "            word = {\"prob\":wordPlusProbality.split('*')[0], \"word\":wordPlusProbality.split('*')[1] }\n",
    "            self.words.append(word)\n",
    "            self.text.append(word['word'])\n",
    "\n",
    "    def create_topic_msg(self):\n",
    "        \"\"\" Function to create a topic message\"\"\"\n",
    "        topic = {\n",
    "            \"id\": self.id,\n",
    "            \"owner\": self.owner,\n",
    "            \"words\": self.words,\n",
    "            \"text\":self.text\n",
    "        }\n",
    "        return topic\n",
    "\n",
    "#Document Class\n",
    "class Document:\n",
    "    \"\"\" Document Class \"\"\"\n",
    "    def __init__(self, session, name, doc_id, doc_type, doc_user, metadata):\n",
    "        self.session = session\n",
    "        if metadata != False:\n",
    "            self.metadata = metadata\n",
    "            self.parseMetadata()\n",
    "        else:\n",
    "            self.metadata = False\n",
    "            self.title = name.split('.')[0] #Delete the extension\n",
    "            self.id = doc_id\n",
    "            self.type = doc_type\n",
    "            self.user = doc_user\n",
    "            self.text = unicode(self.get_text(name), \"utf-8\")\n",
    "            self.toc = self.get_toc(name)\n",
    "            if self.toc:\n",
    "                self.sections = self.get_sections()\n",
    "            else:\n",
    "                self.sections = False\n",
    "            self.GetScholarInfo()\n",
    "        #H-index\n",
    "        self.hindex = self.calculateHIndex()\n",
    "        self.authorHindex = self.calculateAuthorHindex()\n",
    "        if self.text:\n",
    "            self.textAnalysis = self.runTextAnalysis()\n",
    "        else:\n",
    "            self.textAnalysis = \"False\"\n",
    "        # self.topics = self.get_topics(2,4)\n",
    "\n",
    "\n",
    "    #Implement Function to Loop accross all clusterIDs.\n",
    "\n",
    "    def GetScholarInfo(self):\n",
    "        \"\"\"Get Article information from Google Scholar\"\"\"\n",
    "        def SearchScholar(options):\n",
    "            \"\"\"Send Google Scholar Query\"\"\"\n",
    "            querier = scholar.ScholarQuerier()\n",
    "            settings = scholar.ScholarSettings()\n",
    "            querier.apply_settings(settings)\n",
    "\n",
    "            if options['cluster_id']:\n",
    "                query = scholar.ClusterScholarQuery(cluster=options.cluster_id)\n",
    "            else:\n",
    "                query = scholar.SearchScholarQuery()\n",
    "                if options['author']:\n",
    "                    query.set_author(options['author'])\n",
    "                if options['allw']:\n",
    "                    query.set_words(options['allw'])\n",
    "                if options['some']:\n",
    "                    query.set_words_some(options['some'])\n",
    "                if options['none']:\n",
    "                    query.set_words_none(options['none'])\n",
    "                if options['phrase']:\n",
    "                    query.set_phrase(options['phrase'])\n",
    "                if options['title_only']:\n",
    "                    query.set_scope(True)\n",
    "                if options['pub']:\n",
    "                    query.set_pub(options['pub'])\n",
    "                if options['after'] or options['before']:\n",
    "                    query.set_timeframe(options.after, options.before)\n",
    "                if options['no_patents']:\n",
    "                    query.set_include_patents(False)\n",
    "\n",
    "            query.get_url()\n",
    "            querier.send_query(query)\n",
    "            return scholar.get_results_objects(querier)\n",
    "\n",
    "        \"\"\"Define Options for the Scholar search\"\"\"\n",
    "        self.SetScholarOptions = {\n",
    "            'author': False,\n",
    "            'cluster_id': False,\n",
    "            'allw': False,\n",
    "            'some':False,\n",
    "            'none':False,\n",
    "            'phrase': self.title,\n",
    "            'title_only': False,\n",
    "            'pub': False,\n",
    "            'after': False,\n",
    "            'before': False,\n",
    "            'no_patents': False\n",
    "        }\n",
    "        self.scholarInfo = SearchScholar(self.SetScholarOptions)[0]\n",
    "        self.parseScholarInfo()\n",
    "\n",
    "    def parseMetadata(self):\n",
    "        self.title = self.metadata[\"Title\"]\n",
    "        self.url = self.metadata[\"URL\"]\n",
    "        self.year = self.metadata[\"Year\"]\n",
    "        self.citations = self.metadata[\"Citations\"]\n",
    "        self.versions = self.metadata[\"Versions\"]\n",
    "        self.clusterID = self.metadata[\"Cluster ID\"]\n",
    "        self.citations_list = self.metadata[\"Citations list\"]\n",
    "        self.excerpt = self.metadata[\"Excerpt\"]\n",
    "        self.author = []\n",
    "        self.abstract = self.metadata[\"Abstract\"]\n",
    "        self.type = self.metadata[\"Type\"]\n",
    "        self.globalID = self.metadata[\"ID\"]\n",
    "        self.conference = self.metadata[\"Conference\"]\n",
    "        self.organization = self.metadata[\"Organization\"]\n",
    "        self.pages = self.metadata[\"Pages\"]\n",
    "        self.citationArticles = self.metadata[\"Citation articles\"]\n",
    "        self.text = unicode(self.metadata[\"Text\"], \"utf-8\")\n",
    "        ##We create the Authors here too\n",
    "        ##I might need to do this later\n",
    "        for eachAuthor in self.metadata[\"Author\"].split(\"and\"):\n",
    "            if self.session.searchAuthor(eachAuthor) == \"Null\":\n",
    "                self.author.append(scholar.get_author_data(eachAuthor))\n",
    "            else:\n",
    "                self.author.append(self.session.searchAuthor(eachAuthor))\n",
    "\n",
    "    def parseScholarInfo(self):\n",
    "        self.title = self.scholarInfo[\"Title\"]\n",
    "        self.url = self.scholarInfo[\"URL\"]\n",
    "        self.year = self.scholarInfo[\"Year\"]\n",
    "        self.citations = self.scholarInfo[\"Citations\"]\n",
    "        self.versions = self.scholarInfo[\"Versions\"]\n",
    "        self.clusterID = self.scholarInfo[\"Cluster ID\"]\n",
    "        self.citations_list = self.scholarInfo[\"Citations list\"]\n",
    "        self.excerpt = self.scholarInfo[\"Excerpt\"]\n",
    "        self.author = []\n",
    "        self.abstract = self.scholarInfo[\"Abstract\"]\n",
    "        self.type = self.scholarInfo[\"Type\"]\n",
    "        self.globalID = self.scholarInfo[\"ID\"]\n",
    "        self.conference = self.scholarInfo[\"Conference\"]\n",
    "        self.organization = self.scholarInfo[\"Organization\"]\n",
    "        self.pages = self.scholarInfo[\"Pages\"]\n",
    "        self.citationArticles = self.scholarInfo[\"Citation articles\"]\n",
    "        ##We create the Authors here too\n",
    "        for eachAuthor in self.scholarInfo[\"Author\"].split(\"and\"):\n",
    "            if self.session.searchAuthor(eachAuthor) == \"Null\":\n",
    "                self.author.append(scholar.get_author_data(eachAuthor))\n",
    "            else:\n",
    "                self.author.append(self.session.searchAuthor(eachAuthor))\n",
    "\n",
    "    def calculateHIndex(self):\n",
    "        authors_with_hIndex = 0\n",
    "        paperCitations = []\n",
    "        for eachCitation in self.citationArticles:\n",
    "            paperCitations.append(eachCitation[\"Citations\"])\n",
    "        return hindex(paperCitations)\n",
    "\n",
    "    def calculateAuthorHindex(self):\n",
    "        hIndex = 0\n",
    "        authors_with_hIndex = 0\n",
    "        for eachAuthor in self.author:\n",
    "            if eachAuthor['Hindex'] != False:\n",
    "                hIndex = hIndex + eachAuthor['Hindex']\n",
    "                authors_with_hIndex = authors_with_hIndex + 1\n",
    "        return hIndex/authors_with_hIndex\n",
    "\n",
    "    def get_sections(self):\n",
    "        sections = [];\n",
    "        temp_text = self.text\n",
    "        for section in self.toc:\n",
    "            if len(temp_text.split(section[1].upper())) > 0:\n",
    "                sections.append({\"section\":section[1],\"id\":section[0],\"text\":temp_text.split(section[1].upper())[0]})\n",
    "                if len(temp_text.split(section[1].upper())) > 1:\n",
    "                    temp_text = temp_text.split(section[1].upper())[1]\n",
    "            elif len(temp_text.split(section[1].title())) > 0:\n",
    "                sections.append({\"section\":section[1],\"id\":section[0],\"text\":temp_text.split(section[1].title())[0]})\n",
    "                if len(temp_text.split(section[1].title())) > 1:\n",
    "                    temp_text = temp_text.split(section[1].title())[1]\n",
    "            elif len(temp_text.split(section[1]) > 0):\n",
    "                sections.append({\"section\":section[1],\"id\":section[0],\"text\":temp_text.split(section[1])[0]})\n",
    "                if len(temp_text.split(section[1]) > 1):\n",
    "                    temp_text = temp_text.split(section[1])[1]\n",
    "            else:\n",
    "                sections.append(\"Could not Parse\")\n",
    "        return sections\n",
    "\n",
    "    def get_text(self,fname, pages=None):\n",
    "        if not pages:\n",
    "            pagenums = set()\n",
    "        else:\n",
    "            pagenums = set(pages)\n",
    "        output = StringIO()\n",
    "        manager = PDFResourceManager()\n",
    "        converter = TextConverter(manager, output, laparams=LAParams())\n",
    "        interpreter = PDFPageInterpreter(manager, converter)\n",
    "        infile = file(fname, 'rb')\n",
    "        for page in PDFPage.get_pages(infile, pagenums):\n",
    "            interpreter.process_page(page)\n",
    "        infile.close()\n",
    "        converter.close()\n",
    "        text = output.getvalue()\n",
    "        output.close\n",
    "        return text\n",
    "\n",
    "    def get_toc(self, pdf_path):\n",
    "        infile = open(pdf_path, 'rb')\n",
    "        parser = PDFParser(infile)\n",
    "        document = PDFDocument(parser)\n",
    "        toc = list()\n",
    "        try:\n",
    "            for (level, title, dest, a, structelem) in document.get_outlines():\n",
    "                toc.append((level, title))\n",
    "            return toc\n",
    "        except Exception:\n",
    "            return False\n",
    "\n",
    "    def runTextAnalysis(self):\n",
    "        \"\"\"Text Processing workflow for documents\"\"\"\n",
    "        def show_ents(ents):\n",
    "            \"\"\"Return an entity array for a document\"\"\"\n",
    "            entitiesArray = []\n",
    "    #         print (\"Print first entity ........\")\n",
    "            entities = list(ents)\n",
    "            for entity in entities:\n",
    "                entitiesArray.append((entity.label_,' '.join(t.orth_ for t in entity)))\n",
    "    #         if entities:\n",
    "    #             print(entities[0].label_,' '.join(t.orth_ for t in entities[0]))\n",
    "            return entitiesArray\n",
    "        def parse_ents(txtAnalysis):\n",
    "            \"\"\"Parses entities in text into entity name\"\"\"\n",
    "            all_ents = []\n",
    "            parsed_tokens = txtAnalysis['tokens'][:]\n",
    "            for entities in txtAnalysis['entities']:\n",
    "                all_ents.append(entities[0])\n",
    "            all_ents = set(all_ents)\n",
    "\n",
    "            for eachEntType in all_ents:\n",
    "                entType = set([entity[1] for entity in txtAnalysis['entities'] if entity[0] == eachEntType])\n",
    "    #             print eachEntType\n",
    "    #             print entType\n",
    "                for index, token in enumerate(txtAnalysis['tokens']):\n",
    "                    if token in entType:\n",
    "                        parsed_tokens[index] = entType\n",
    "                    else:\n",
    "                        parsed_tokens[index] = token\n",
    "            return parsed_tokens\n",
    "        def is_float(s):\n",
    "            \"\"\"Custom function needed for detecting float numbers\"\"\"\n",
    "            try:\n",
    "                float(s)\n",
    "                return True\n",
    "            except ValueError:\n",
    "                return False\n",
    "        def sentimentAnalysis(txt):\n",
    "            \"\"\"Simple function to detect sentiment of a sentence\"\"\"\n",
    "            sentimentArray = []\n",
    "            for sentence in list(txt):\n",
    "                vs = analyzer.polarity_scores(sentence.text)\n",
    "                sentimentArray.append((sentence,vs))\n",
    "            return sentimentArray\n",
    "\n",
    "        #set txt to unicode and lower case\n",
    "        txt = self.text.lower()\n",
    "        # create English stop words list\n",
    "        en_stop = get_stop_words('en')\n",
    "\n",
    "        parser = English()\n",
    "        #Convert to SPACEY text\n",
    "        doc = nlp(txt)\n",
    "        tokens = parser(txt)\n",
    "        txtAnalysis = {\n",
    "            'rawText':txt,\n",
    "            'abstract':False,\n",
    "            'conclusion': False,\n",
    "            'references': False,\n",
    "            'tokens': [token.orth_.lower() for token in tokens if not token.orth_.isspace() if not token.orth_.lower() in en_stop],\n",
    "            'sents' : list(doc.sents),\n",
    "            'lemma' : [token.lemma_.lower() for token in tokens if not token.orth_.isspace()],\n",
    "            'entities': show_ents(doc.ents),\n",
    "            'parsed_entities' : False\n",
    "        }\n",
    "        #We store the different sections if I can parse them\n",
    "        if len(txtAnalysis['rawText'].split('introduction'))>1:\n",
    "            txtAnalysis['abstract'] = {'text': nlp(''.join(txtAnalysis['rawText'].split('introduction')[0])),\"sent_analysis\":False}\n",
    "            txtAnalysis['abstract']['sent_analysis'] = sentimentAnalysis(txtAnalysis['abstract']['text'].sents)\n",
    "        if len(regexp_tokenize(txtAnalysis['rawText'], pattern=('conclusion(.*?)references')))>0:\n",
    "            txtAnalysis['conclusion'] = {'text': nlp(''.join(regexp_tokenize(txtAnalysis['rawText'], pattern=('conclusion(.*?)references')))),\"sent_analysis\":False}\n",
    "            txtAnalysis['conclusion']['sent_analysis'] = sentimentAnalysis(txtAnalysis['abstract']['text'].sents)\n",
    "        if len(txtAnalysis['rawText'].split('references'))>1:\n",
    "            txtAnalysis['references'] = {'text': nlp(''.join(txtAnalysis['rawText'].split('references')[1])),\"sent_analysis\":False}\n",
    "        txtAnalysis['parsed_entities'] = parse_ents(txtAnalysis)\n",
    "        #We delete the numbers from the tokens\n",
    "        txtAnalysis['tokens'] = [token for token in txtAnalysis['tokens'] if not token.isdigit() if not is_float(token) if not token in [',','.','[',']','-',';','(',')',':','=']]\n",
    "        return txtAnalysis\n",
    "\n",
    "    def create_document_msg(self):\n",
    "        doc_topics = []\n",
    "        doc_sections =[]\n",
    "        # for section in self.sections:\n",
    "        #     #pdb.set_trace()\n",
    "        #     doc_sections.append(json.dumps(section))\n",
    "        for topic in self.topics:\n",
    "            doc_topics.append(topic.create_topic_msg())\n",
    "        msg = {\n",
    "            'name':self.name,\n",
    "            'id':self.id,\n",
    "            'user':self.user,\n",
    "            'text': self.text,\n",
    "            'toc': self.toc,\n",
    "            #Problems parsing in Json\n",
    "            #'sections':json.loads(doc_sections),\n",
    "            'topics':doc_topics\n",
    "        }\n",
    "        #pdb.set_trace()\n",
    "        return msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
