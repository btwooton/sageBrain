{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "from cStringIO import StringIO\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import TextConverter, PDFPageAggregator\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "\n",
    "import nltk as nltk\n",
    "\n",
    "#lda\n",
    "from spacy.en import English\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from stop_words import get_stop_words\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from gensim import corpora, models\n",
    "import gensim\n",
    "import pdb\n",
    "\n",
    "#Bag of Words\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#TfIdf\n",
    "import gensim\n",
    "\n",
    "#Word2Vect\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "\n",
    "#ldaVis\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "#spacy\n",
    "import spacy\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_text(fname, pages=None):\n",
    "        if not pages:\n",
    "            pagenums = set()\n",
    "        else:\n",
    "            pagenums = set(pages)\n",
    "\n",
    "        output = StringIO()\n",
    "        manager = PDFResourceManager()\n",
    "        converter = TextConverter(manager, output, laparams=LAParams())\n",
    "        interpreter = PDFPageInterpreter(manager, converter)\n",
    "\n",
    "        infile = file(fname, 'rb')\n",
    "        for page in PDFPage.get_pages(infile, pagenums):\n",
    "            interpreter.process_page(page)\n",
    "        infile.close()\n",
    "        converter.close()\n",
    "        text = output.getvalue()\n",
    "        output.close\n",
    "        return unicode(text,'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk import ngrams\n",
    "def runTextAnalysis(txt):\n",
    "    # create English stop words list\n",
    "    en_stop = get_stop_words('en')\n",
    "    # Create p_stemmer of class PorterStemmer\n",
    "    p_stemmer = SnowballStemmer('english')\n",
    "    \n",
    "    wnl = nltk.WordNetLemmatizer()\n",
    "    sent_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    txtAnalysis = {\n",
    "        'rawText':txt,\n",
    "        'abstract': txt.lower().split('introduction')[0].split('abstract'),\n",
    "        'tokens': [i for i in nltk.word_tokenize(txt) if not i.isdigit() if len(i)>2],\n",
    "        'stopped_tokens':[i for i in nltk.word_tokenize(txt) if not i.isdigit() if not i in en_stop],\n",
    "        'stemmed_tokens':0,\n",
    "        'num_words': 0,\n",
    "        'words': 0,\n",
    "        'vocab': 0,\n",
    "        'lemmatizedVocab': 0,\n",
    "        'senteces': 0\n",
    "    }  \n",
    "    txtAnalysis['stemmed_tokens'] = [p_stemmer.stem(i) for i in txtAnalysis['stopped_tokens']]\n",
    "    txtAnalysis['num_words'] = len(txtAnalysis['tokens'])\n",
    "    txtAnalysis['words'] = [w for w in txtAnalysis['stopped_tokens']]\n",
    "    txtAnalysis['vocab'] = sorted(set(txtAnalysis['words']))\n",
    "    txtAnalysis['lemmatizedVocab'] = [wnl.lemmatize(t) for t in txtAnalysis['vocab']]\n",
    "    txtAnalysis['sentences'] = sent_tokenizer.tokenize(txt)\n",
    "    #convert to nltk Text\n",
    "    text = nltk.Text(txtAnalysis['tokens'])\n",
    "    #Collocations are very interesting but just prints text to screen need to retrieve this somehow.\n",
    "    #collocations = text.collocations()\n",
    "    return txtAnalysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def loadTextData():\n",
    "    fileList = []\n",
    "    for file in os.listdir(\".\"):\n",
    "        if file.endswith(\".pdf\"):\n",
    "#         if file.endswith('16475329441669445062.pdf'):\n",
    "          fileList.append(file)\n",
    "    return fileList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bagOfWords(documents):\n",
    "    vectorizer = CountVectorizer(min_df=5, max_df=.95)\n",
    "    print type(documents)\n",
    "    doc_term_matrix = vectorizer.fit_transform([document for document in documents])\n",
    "    vocab = np.array(vectorizer.get_feature_names())\n",
    "    print('\\nVocabulary:')\n",
    "    print vocab\n",
    "    \n",
    "    print('\\nDocument Term Matrix')\n",
    "    formatted_row = '{:>12}' * (len(documents) +1)\n",
    "#     print('\\n',formatted_row.format('Word',1),'\\n')\n",
    "    for word, item in zip(vocab, doc_term_matrix.T):\n",
    "        output = [str(x) for x in item.data]\n",
    "        print \"word: \" + word\n",
    "        print output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calculateTFidf(docAnalytics,parameter):\n",
    "     #We store the tokenized text in a vector\n",
    "        tokText = []\n",
    "        for each in docAnalytics:\n",
    "            tokText.append(each[parameter])\n",
    "        #We map each word to a number\n",
    "        dictionary = gensim.corpora.Dictionary(tokText)\n",
    "        num_words = len(dictionary)\n",
    "        #Create a corpus: List of bag of words\n",
    "        corpus = [dictionary.doc2bow(eachtokText) for eachtokText in tokText]\n",
    "        tf_idf = gensim.models.TfidfModel(corpus)\n",
    "        #Create the simiarity measure Object\n",
    "        sims = gensim.similarities.Similarity('./similarityStorage',tf_idf[corpus],num_features=num_words)\n",
    "        return {\"sims\":sims,\"dict\":dictionary,\"tf_idf\":tf_idf}\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_lda(textAnalytics,parameter):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "    # create English stop words list\n",
    "    en_stop = get_stop_words('en')\n",
    "\n",
    "    # Create p_stemmer of class PorterStemmer\n",
    "    p_stemmer = SnowballStemmer('english')\n",
    "    # create sample documents\n",
    "    texts = []\n",
    "    for eachDocument in textAnalytics:\n",
    "        texts.append(eachDocument[parameter])\n",
    "\n",
    "    # turn our tokenized documents into a id <-> term dictionary\n",
    "    dictionary = corpora.Dictionary(texts)\n",
    "    # convert tokenized documents into a document-term matrix\n",
    "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "    #Store to visualize\n",
    "    pickle.dump(corpus,open('corpus.pkl','wb'))\n",
    "    dictionary.save('dictionary.gensim')\n",
    "\n",
    "    # generate LDA model\n",
    "    ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=9, id2word = dictionary, passes=100)\n",
    "    #Store to visualize\n",
    "    ldamodel.save('20top_100Tok.gensim')\n",
    "\n",
    "    return ldamodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " def get_topics(lda,num_topics, num_words):\n",
    "        doc_topics = []\n",
    "        topics = lda.print_topics(num_topics=num_topics, num_words=num_words)\n",
    "        return topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word2VectTraining(textAnalysis,parameter):\n",
    "        \"\"\"Training Word Vectors\"\"\"\n",
    "        txt = []\n",
    "        for each in textAnalysis:\n",
    "            txt.append(each[parameter])\n",
    "        model = gensim.models.word2vec.Word2Vec(txt,min_count=10,size=300)\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word2VectDocumentTraining(textAnalysis,parameter):\n",
    "        \"\"\"Training Document Word Vectors\n",
    "        Needs to run with raw Text for expected results\"\"\"\n",
    "        tagged_documents = []\n",
    "        for i, doc in enumerate(textAnalysis):\n",
    "            tagged_documents.append(TaggedDocument(doc[parameter],[\"doc_{}\".format(i)]))\n",
    "        d2v_model = gensim.models.doc2vec.Doc2Vec(tagged_documents,size=300)\n",
    "        return d2v_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def LDAvis(model):\n",
    "    dictionary = gensim.corpora.Dictionary.load('dictionary.gensim')\n",
    "    corpus = pickle.load(open(\"corpus.pkl\",\"rb\"))\n",
    "    lda = gensim.models.ldamodel.LdaModel.load(model)\n",
    "    print dictionary \n",
    "    print lda\n",
    "    lda_display = pyLDAvis.gensim.prepare(lda,corpus,dictionary,sort_topics=False)\n",
    "    return lda_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadPickle(name):\n",
    "    return pickle.load(open(name,'rb'))\n",
    "\n",
    "def savePickle(data,name):\n",
    "    pickle.dump(data,open(name,'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk import ngrams\n",
    "from nltk import regexp_tokenize\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "def runTextAnalysisSpacey(txt):\n",
    "    \"\"\"Text Processing workflow for documents\"\"\"\n",
    "    def show_ents(ents):\n",
    "        \"\"\"Return an entity array for a document\"\"\"\n",
    "        entitiesArray = []\n",
    "#         print (\"Print first entity ........\")\n",
    "        entities = list(ents)\n",
    "        for entity in entities:\n",
    "            entitiesArray.append((entity.label_,' '.join(t.orth_ for t in entity)))\n",
    "#         if entities:\n",
    "#             print(entities[0].label_,' '.join(t.orth_ for t in entities[0]))\n",
    "        return entitiesArray\n",
    "    def parse_ents(txtAnalysis):\n",
    "        \"\"\"Parses entities in text into entity name\"\"\"\n",
    "        all_ents = []\n",
    "        parsed_tokens = txtAnalysis['tokens'][:]\n",
    "        for entities in txtAnalysis['entities']:\n",
    "            all_ents.append(entities[0])\n",
    "        all_ents = set(all_ents)\n",
    "\n",
    "        for eachEntType in all_ents:\n",
    "            entType = set([entity[1] for entity in txtAnalysis['entities'] if entity[0] == eachEntType])\n",
    "#             print eachEntType\n",
    "#             print entType\n",
    "            for index, token in enumerate(txtAnalysis['tokens']):\n",
    "                if token in entType:\n",
    "                    parsed_tokens[index] = entType\n",
    "                else:\n",
    "                    parsed_tokens[index] = token\n",
    "        return parsed_tokens\n",
    "    def is_float(s):\n",
    "        \"\"\"Custom function needed for detecting float numbers\"\"\"\n",
    "        try:\n",
    "            float(s)\n",
    "            return True\n",
    "        except ValueError:\n",
    "            return False\n",
    "    def sentimentAnalysis(txt):\n",
    "        \"\"\"Simple function to detect sentiment of a sentence\"\"\"\n",
    "        sentimentArray = []\n",
    "        for sentence in list(txt):\n",
    "            vs = analyzer.polarity_scores(sentence.text)\n",
    "            sentimentArray.append((sentence,vs))\n",
    "        return sentimentArray\n",
    "    \n",
    "    #set txt to unicode and lower case\n",
    "    txt = txt.lower()\n",
    "    # create English stop words list\n",
    "    en_stop = get_stop_words('en')\n",
    "    \n",
    "    parser = English()\n",
    "    #Convert to SPACEY text\n",
    "    doc = nlp(txt)\n",
    "    tokens = parser(txt)\n",
    "    txtAnalysis = {\n",
    "        'rawText':txt,\n",
    "        'abstract':False,\n",
    "        'conclusion': False,\n",
    "        'references': False,\n",
    "        'tokens': [token.orth_.lower() for token in tokens if not token.orth_.isspace() if not token.orth_.lower() in en_stop],\n",
    "        'sents' : list(doc.sents),\n",
    "        'lemma' : [token.lemma_.lower() for token in tokens if not token.orth_.isspace()],\n",
    "        'entities': show_ents(doc.ents),\n",
    "        'parsed_entities' : False\n",
    "    }\n",
    "    #We store the different sections if I can parse them\n",
    "    if len(a['rawText'].split('introduction'))>1:\n",
    "        txtAnalysis['abstract'] = {'text': nlp(''.join(txtAnalysis['rawText'].split('introduction')[0])),\"sent_analysis\":False}\n",
    "        txtAnalysis['abstract']['sent_analysis'] = sentimentAnalysis(txtAnalysis['abstract']['text'].sents)\n",
    "    if len(regexp_tokenize(txtAnalysis['rawText'], pattern=('conclusion(.*?)references')))>0:\n",
    "        txtAnalysis['conclusion'] = {'text': nlp(''.join(regexp_tokenize(txtAnalysis['rawText'], pattern=('conclusion(.*?)references')))),\"sent_analysis\":False}\n",
    "        txtAnalysis['conclusion']['sent_analysis'] = sentimentAnalysis(txtAnalysis['abstract']['text'].sents)\n",
    "    if len(txtAnalysis['rawText'].split('references'))>1:\n",
    "        txtAnalysis['references'] = {'text': nlp(''.join(txtAnalysis['rawText'].split('references')[1])),\"sent_analysis\":False}\n",
    "    txtAnalysis['parsed_entities'] = parse_ents(txtAnalysis)\n",
    "    #We delete the numbers from the tokens\n",
    "    txtAnalysis['tokens'] = [token for token in txtAnalysis['tokens'] if not token.isdigit() if not is_float(token) if not token in [',','.','[',']','-',';','(',')',':','=']]\n",
    "    return txtAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "def sentimentAnalysis(txt):\n",
    "    sentimentArray = []\n",
    "    for sentence in list(a['conclusion'].sents):\n",
    "        vs = analyzer.polarity_scores(sentence.text)\n",
    "        sentimentArray.append((sentence,vs))\n",
    "    return sentimentArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "{'neg': 0.0, 'neu': 0.903, 'pos': 0.097, 'compound': 0.4215}\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "{'neg': 0.0, 'neu': 0.939, 'pos': 0.061, 'compound': 0.0772}\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "{'neg': 0.09, 'neu': 0.738, 'pos': 0.172, 'compound': 0.25}\n",
      "{'neg': 0.0, 'neu': 0.828, 'pos': 0.172, 'compound': 0.6597}\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "{'neg': 0.123, 'neu': 0.636, 'pos': 0.241, 'compound': 0.3818}\n",
      "{'neg': 0.0, 'neu': 0.417, 'pos': 0.583, 'compound': 0.6369}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# list(a['conclusion'].sents)[0].text\n",
    "for sentence in list(a['conclusion'].sents):\n",
    "    vs = analyzer.polarity_scores(sentence.text)\n",
    "#     print \"\\n\"\n",
    "#     print(\"{:-<65} {}\".format(sentence, str(vs)))\n",
    "    print vs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "our ap-\n",
      "proach obtains more reasonable articulation patterns and is better\n",
      "in solving the double counting problem.\n",
      "\n",
      "\n",
      "\n",
      "{'neg': 0.123, 'neu': 0.636, 'pos': 0.241, 'compound': 0.3818}\n",
      "best viewed in color.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'neg': 0.0, 'neu': 0.417, 'pos': 0.583, 'compound': 0.6369}\n"
     ]
    }
   ],
   "source": [
    "sents = sentimentAnalysis(a['conclusion'].sents)\n",
    "for sent in sents:\n",
    "    if sent[1]['pos']>0.2:\n",
    "        print sent[0]\n",
    "        print \"\\n\\n\"\n",
    "        print sent[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = runTextAnalysisSpacey(text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(\n",
      "\n",
      "this paper has proposed a multi-source deep model for\n",
      "pose estimation., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}), (it non-linearly integrates three information\n",
      "sources: appearance score, deformation and appearance\n",
      "mixture type., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}), (these information sources are used for de-\n",
      "scribing different aspects of the single modality data, which\n",
      "is the image data in our pose estimation approach., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}), (exten-\n",
      "sive experimental comparisons on three public benchmark\n",
      "datasets show that the proposed model obviously improves\n",
      "the pose estimation accuracy and outperforms the state of\n",
      "the art., {'neg': 0.0, 'neu': 0.903, 'pos': 0.097, 'compound': 0.4215}), (since this model is a post-processing of informa-\n",
      "tion sources, it is very ﬂexible in terms of integrating with\n",
      "existing approaches that use different information sources,\n",
      "features, or articulation models., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}), (learning deep model from\n",
      "pixels for pose estimation and analyzing the inﬂuence of\n",
      "training data number will be the future work.\n",
      "\n",
      ", {'neg': 0.0, 'neu': 0.939, 'pos': 0.061, 'compound': 0.0772}), (7., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}), (acknowledgement\n",
      "\n",
      "this work is supported by the general research\n",
      "fund sponsored by the research grants council of\n",
      "hong kong (project no.\n",
      ", {'neg': 0.09, 'neu': 0.738, 'pos': 0.172, 'compound': 0.25}), (cuhk 417110, cuhk\n",
      "417011, cuhk 429412), national natural science foun-\n",
      "dation of china (91320101), shenzhen basic research\n",
      "program (jc201005270350a, jcyj20120903092050890,\n",
      "jcyj20120617114614438), and guangdong innovative\n",
      "research team program (no.201001d0104648280).\n",
      "\n",
      ", {'neg': 0.0, 'neu': 0.828, 'pos': 0.172, 'compound': 0.6597}), (figure 5., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}), (comparison between our method (left) and the approach\n",
      "in [58] (right) on the lsp, parse and uiuc dataset., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}), (our ap-\n",
      "proach obtains more reasonable articulation patterns and is better\n",
      "in solving the double counting problem., {'neg': 0.123, 'neu': 0.636, 'pos': 0.241, 'compound': 0.3818}), (best viewed in color.\n",
      "\n",
      ", {'neg': 0.0, 'neu': 0.417, 'pos': 0.583, 'compound': 0.6369})]\n"
     ]
    }
   ],
   "source": [
    "def sentimentAnalysis(txt):\n",
    "    \"\"\"Simple function to detect sentiment of a sentence\"\"\"\n",
    "    sentimentArray = []\n",
    "    for sentence in list(txt):\n",
    "        vs = analyzer.polarity_scores(sentence.text)\n",
    "        sentimentArray.append((sentence,vs))\n",
    "    return sentimentArray\n",
    "\n",
    "peyt = sentimentAnalysis(a['conclusion']['text'].sents)\n",
    "print peyt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "these information sources are used for de-\n",
       "scribing different aspects of the single modality data, which\n",
       "is the image data in our pose estimation approach."
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(a['conclusion'].sents)[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main code: We load the pdfs, run the analytics on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['10627391380307827290.pdf', '10871829039953032247.pdf', '11203799587472723180.pdf', '12474236712650393344.pdf', '12604916758840857978.pdf', '13187742643561364870.pdf', '13446248872976146902.pdf', '13911076262768119041.pdf', '14330626598474812204.pdf', '1451703473279047534.pdf', '14774739962596400760.pdf', '15677418431280773725.pdf', '16475329441669445062.pdf', '1712352881354258034.pdf', '17268489282982414918.pdf', '17554513562881542355.pdf', '2866321627490206847.pdf', '2973992380342580480.pdf', '3286245540879480275.pdf', '3905919677086105959.pdf', '394128052107362798.pdf', '4097536640488718528.pdf', '4520651819365506698.pdf', '5700939689460967442.pdf', '6558128843129001214.pdf', '6902252272651036995.pdf', '7106912543889350821.pdf', '74307944265552935.pdf', '8255440757806230750.pdf', '8535617555901469791.pdf', '9056558950588383944.pdf', 'document.pdf', 'Multimodal Deep Learning.pdf', 'test.pdf', 'test1.pdf', 'test2.pdf']\n",
      "36\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "[u'Multi-source Deep Learning for Human Pose Estimation\\n\\nWanli Ouyang Xiao Chu Xiaogang Wang\\n\\nDepartment of Electronic Engineering, The Chinese University of Hong Kong\\n\\nwlouyang@ee.cuhk.edu.hk, xgwang@ee.cuhk.edu.hk\\n\\nAbstract\\n\\nVisual appearance score, appearance mixture type and\\ndeformation are three important information sources for\\nhuman pose estimation. This paper proposes to build a\\nmulti-source deep model in order to extract non-linear\\nrepresentation from these different aspects of information\\nsources. With the deep model, the global, high-order hu-\\nman body articulation patterns in these information sources\\nare extracted for pose estimation. The task for estimat-\\ning body locations and the task for human detection are\\njointly learned using a uni\\ufb01ed deep model. The proposed\\napproach can be viewed as a post-processing of pose esti-\\nmation results and can \\ufb02exibly integrate with existing meth-\\nods by taking their information sources as input. By extract-\\ning the non-linear representation from multiple information\\nsources, the deep model outperforms state-of-the-art by up\\nto 8.6 percent on three public benchmark datasets.\\n\\n1. Introduction\\n\\nHuman pose estimation is the process of determining,\\nfrom an image, the positions of human body parts such as\\nthe head, shoulder, elbow, wrist, hip, knee, and ankle. It\\nis a fundamental problem in computer vision and has abun-\\ndant important applications such as sports, action recogni-\\ntion, character animation, clinical analysis of gait patholo-\\ngies, content-based video and image retrieval, and intelli-\\ngent video surveillance. Despite many years of research\\n[52, 54, 2, 40, 6, 57, 56], pose estimation remains a dif\\ufb01-\\ncult problem. One of the most signi\\ufb01cant challenges in pose\\nestimation is how to model the complex human articulation.\\nMany approaches have been used to handle the com-\\nplex human articulation by using three information sources:\\nmixture type, appearance score and deformation [57, 52, 54,\\n11, 58]. In\\ufb02uenced by human body articulation, clothing,\\nocclusion etc., body part appearance varies. To handle this\\nvariation, the appearance of a part is clustered into multiple\\nmixture types as shown in Fig. 1 . For each mixture type of\\na part, a part template is learned to capture its appearance.\\nThen the appearance scores (log-likelihoods) of body parts\\n\\nFigure 1. The motivation of this paper in using multi-source deep\\nmodel for constructing the non-linear representation from three in-\\nformation sources: mixture type, appearance score and deforma-\\ntion. Best viewed in color.\\nbeing at different locations are obtained by convolving the\\npart templates with the visual features of the input image,\\ne.g. HOG [7]. The appearance scores are inaccurate for\\nwell-locating body parts because the part template is imper-\\nfect. Therefore, the deformations (relative locations) among\\nbody parts are used as for encoding likely pairwise poses;\\nfor example, the head should not be far from the neck.\\n\\nExisting approaches use log-linear models with pairwise\\npotentials of these three information sources [52, 54, 40, 57,\\n56] to determine whether an estimated location is correct.\\nHowever, these information sources are not log-linearly cor-\\nrelated when choosing the correct candidate. For the exam-\\nple in Fig. 1, linear models may \\ufb01nd that the estimated\\nresult on the left and the result on the right have the same\\ndeformation score because they simply linearly add local\\ndeformation cost. While it is obvious for human to \\ufb01nd that\\nthe result on the left is not reasonable. Similar situations\\nalso occur for mixture type and appearance score. There-\\nfore, it is desirable to construct the non-linear representation\\nthat identi\\ufb01es reasonable con\\ufb01gurations of deformation, ap-\\npearance score and mixture type.\\n\\nIn order to construct useful representation from multi-\\nple information sources for pose estimation, a model should\\n\\n1\\n\\nMulti-sourcedeep modelEstimated resultType 1Type 2Non-Linearmodel?YesNoEstimated resultMixture typeHead topNeckDeformationAppearance scoreTemplateLinear model1389\\x0csatisfy certain properties. First, the model should capture\\nthe global, complex relationships among body parts. For\\nthe example in Fig. 1, the result on the left is unreasonable\\nbecause of its global con\\ufb01guration in arm, torso, and leg.\\nSecond, since reasonable con\\ufb01guration is a very abstract\\nconcept while the information sources are less abstract con-\\ncepts, the model should construct more abstract representa-\\ntion from the less abstract representation. Third, since dif-\\nferent information sources describe different aspects of hu-\\nman pose and have different statistical properties, the model\\nshould learn useful representation from these sources and\\nfuse them into a joint representation for pose estimation.\\nThe multi-source deep architecture we propose satis\\ufb01es the\\nabove requirement.\\n\\nThere are three contributions of this paper.\\n\\n1. We propose a deep architecture to construct the non-\\nlinear representation from different aspects of informa-\\ntion sources. To the best of our knowledge, this paper is\\nthe \\ufb01rst to use deep model for pose estimation.\\n\\n2. The body articulation patterns (global and more abstract\\nrepresentations) are captured by the deep model from the\\ninformation sources (local and less abstract representa-\\ntions). For each information source, more abstract repre-\\nsentation at the higher layer is composed by the less ab-\\nstract representation of all body parts in the lower layer.\\nThen representations of all information sources in the\\nhigher layer are fused for pose estimation.\\n\\n3. Both the task for detecting human and the task for esti-\\nmating body locations are jointly learned using a single\\ndeep model. Joint learning of these tasks with a shared\\nrepresentation improves pose estimation accuracy.\\n\\n2. Related work\\n\\nHuman pose estimation. Pose estimation is considered\\nas holistic recognition in [15, 33, 34]. On the other hand,\\nmany recent works use local body parts [52, 54, 11, 58, 9,\\n13, 48, 40, 2, 42, 21, 46, 55, 41, 1] in order to handle the\\nmany degrees of freedom in body part articulation. Since\\nthe \\ufb01rst work in [57], some approaches [52, 54, 11, 58, 9]\\nhave clustered part appearance into mixture types as shown\\nin Fig. 1. There are also approaches that warp the part\\ntemplate by \\ufb02exible sizes and orientations [13, 48, 40, 2,\\n42, 21, 46, 55]. The appearance score, rotation, size, and\\nlocation used in these approaches can be treated as multiple\\ninformation sources and used by our deep model for pose\\nestimation.\\n\\nIn existing pose estimation approaches, the pair-wise\\npart deformation relationships are arranged in tree models\\n[52, 54, 2, 40, 57], multi-tree model [55], or loopy mod-\\nels [56, 53, 10]. Tree models allow for ef\\ufb01cient and exact\\ninference but are insuf\\ufb01cient in modeling the complex re-\\nlationships among body parts. Hence, tree models often\\nsuffer from double counting; for example, given the posi-\\n\\ntion of a torso, the positions of two legs are independent\\nand often respond to the same visual cue. Loopy models\\nallow more complex relationships among parts, but require\\napproximate inference. Our deep architecture models the\\ncomplex relationships among parts and is computationally\\nef\\ufb01cient in both training and testing.\\n\\nDeep learning. Since the breakthrough in deep learning\\ninitiated by G. Hinton in [18, 19], deep learning is gain-\\ning more and more attention. Bengio [3] proved that exist-\\ning commonly used machine learning tools such as SVM\\nand Boosting are shallow models, and they may require\\nmany more computational elements, potentially exponen-\\ntially more (with respect to input size), than deep models\\nwhose depth is matched to the task. Deep architecture is\\nfound to yield better data representation, for example, in\\nterms of classi\\ufb01cation error [25], invariance to input trans-\\nformations [16], or modeling multi-modal data [35]. Deep\\nlearning has achieved spectacular progress in computer vi-\\nsion [45, 20, 26, 36, 23, 59, 12, 43, 39, 50, 49, 60, 38,\\n37, 30, 32, 31, 29, 61, 28, 51]. Recent progress on deep\\nlearning is reviewed in [4]. Krizhevsky et al.\\n[23] pro-\\nposed a large-scale deep convolutional network [27] with\\nbreakthrough on the large-scale ImageNet object recogni-\\ntion dataset [8], attaining a signi\\ufb01cant gap compared with\\nexisting approaches that use shallow models, and bringing\\nhigh impact to research in computer vision. Our approaches\\nin [38, 39, 37, 29] learns feature learning, translational de-\\nformation, and occlusion relationship in pedestrian detec-\\ntion; the approach in [50] learns relational \\ufb01lter pairs in face\\nveri\\ufb01cation. To the best of our knowledge, however, deep\\nmodel for human pose estimation has not yet been explored.\\nOur work is inspired by multi-modality models that learn\\nfrom multiple modalities such as audio, visual, text data [35,\\n47, 17]. In contrast to these works, we investigate multi-\\nsource learning from single modality, which is image data\\nin pose estimation.\\n\\n3. Pictorial structure model for pose estimation\\nThe model introduced in this section is used to provide\\nour deep model with information sources. Pictorial struc-\\nture model considers human body parts as nodes tied to-\\ngether in a conditional random \\ufb01eld. Let lp for p = 1, . . . , P\\nbe the con\\ufb01guration of the pth part. The posterior of a con-\\n\\ufb01guration of parts L = {lp|p = 1 . . . P} given an image I\\nis:\\n\\nP (L|I) \\u221d exp(cid:0) P(cid:88)\\n\\n(cid:88)\\n\\n\\u03c8(lp, lq)(cid:1).\\n\\n(1)\\n\\n\\u03c6(I|lp)) +\\n\\np=1\\n\\n(p,q)\\u2208E\\n\\n\\u03c8(lp, lq) is the pair-wise term that models the geometric\\ndeformation constraint on the pth and qth parts; for exam-\\nple, head shall not be too far from torso. The edge set de-\\nnoted by E is arranged in tree models [52, 54, 2, 40, 57, 11]\\n\\n\\x0cor loopy models [56, 10, 53].\\n\\n\\u03c6(I|lp) is the unary term that models the appearance of\\nlp. The appearance varies as body articulates. To model\\nthis variation, lp = {sp, \\u03b8p, zp} and \\u03c6(I|lp) speci\\ufb01es the\\npart appearance warped by size sp, orientation \\u03b8p at loca-\\ntion zp in [2, 40, 13]. Alternatively, Yang and Ramanan\\npropose to use appearance mixture type tp for approximat-\\ning the variation in rotation \\u03b8p and size sp in [57]. In this\\nmodel, lp = {tp, zp} and \\u03c6(I|lp) speci\\ufb01es the part appear-\\nance with mixture type tp at location zp. The appearance of\\na part is clustered into multiple appearance mixture types as\\nshown in Fig. 1. The overall model in [57, 58] is as follows:\\n\\nP (L|I) \\u221d exp(cid:0)S(I, t, z)(cid:1),\\n(cid:88)\\n(cid:88)\\n\\n(cid:88)\\n\\np,q\\n\\nSc(t) =\\n\\nbtp\\np +\\n\\nbtp,tq\\np,q\\n\\nwhere S(I, t, z) = Sc(t) +\\n\\nSd(t, z, p, q) +\\n\\n(cid:88)\\n\\n(2)\\n\\nSa(I, tp, zp),\\n\\n(3)\\n\\n(4)\\n\\np\\n\\n,\\n\\np\\n\\np,q\\n\\nT\\n\\nd(zp \\u2212 zq),\\n\\nSd(t, z, p, q) = wtp,tq\\np,q\\nT\\nSa(I, tp, zp) = wtp\\n\\np\\n\\nf (I, zp).\\n\\ncompatibility/co-occurrence of mixture types.\\n\\n(5)\\n\\u2022 Sc(t) is the pair-wise compatibility term that models the\\n\\u2022 Sd(t, z, p, q) is the pair-wise deformation term that mod-\\nels the geometric deformation constraints on the pth and\\nqth parts. d(zp \\u2212 zq) = [dx, dy, dx2dy2]\\nT.\\n\\u2022 Sa(I, tp, zp) is the unary appearance term that computes\\np at location zp of the\\n\\nthe score of placing a template wtp\\nHOG feature map for image I, denoted by f (I, zp).\\n\\np , wtp,tq\\nLinear SVM is used to learn the linear weights wtp\\np,q\\nand compatibility biases btp\\np,q . The model in Eq.(2)-(5)\\nis used in many approaches, with different implementations\\non edge set, part size, and part locations [52, 54, 57, 10, 11,\\n58].\\n\\np , btp,tq\\n\\n4. The multi-source deep model\\n\\nAn overview of our framework in the testing stage is\\nshown in Fig. 2. In this framework, an existing approach\\nis used to generate candidate body locations with conserva-\\ntive thresholding. In the experiment, the existing approach\\nis the off-the-shelf approach in [58]. A multi-source deep\\nmodel is then applied to a candidate of all body locations\\nin order to determine whether its body locations are correct.\\nSimultaneously, the body locations of this candidate is esti-\\nmated.\\n\\nOne direct approach with which to train a multi-source\\nmodel is to train a deep model over the concatenated infor-\\nmation sources as shown in Fig. 3(a). This approach is lim-\\nited because information sources with different statistical\\nproperties are mixed in the \\ufb01rst hidden layer. A better so-\\nlution is to have their high-level representations constructed\\nbefore they are mixed. Therefore, we use the architecture\\nas shown in Fig. 3(b), in which each information source\\n\\nFigure 2. Framework in the testing stage. The existing approach is\\nused to generate multiple candidate locations. A candidate is used\\nas the input to a deep model to determine whether the candidate is\\ncorrect and estimate body locations. Best viewed in color.\\n\\nFigure 3. Direct use of deep model (a) and the deep architecture\\nwe propose (b) for part score s, deformation d and mixture type t.\\nBest viewed in color.\\n\\nis connected to two layers for constructing high level rep-\\nresentation individually. High-level representations of dif-\\nferent information sources are then fused using other two\\nlayers for pose estimation.\\n4.1. Inference\\n\\nThe mixture type information t in Fig. 3 is taken from\\nthe t in (3). The relative positions among parts, denoted\\nby d, comes from the deformation information d(zp \\u2212 zq)\\nin (4). The appearance scores, denoted by s, is obtained\\nfrom the unary appearance term in (5). In our experiment,\\ns, t, and d are obtained using the approach in [58]. At the\\ninference stage, the model is as follows:\\n\\nh1,1 = a(sTW1,1 + b1,1),\\nh1,2 = a(dTW1,2 + b1,2),\\nh1,3 = a(tTW1,3 + b1,3),\\nh2,u = a(h1,uT\\nh2 = [h2,1T\\nh3 = a(h2T\\n\\u02dcycls = \\u03c3(h3T\\n\\u02dcypst = h3T\\n\\nWpst + bpst.\\n\\nwcls + bcls),\\n\\nW2,u + b2,u), u = 1, 2, 3,\\nh2,2T\\nh2,3T\\nW3 + b2),\\n\\n]T,\\n\\n(6)\\n(7)\\n(8)\\n\\n(9)\\n\\n(10)\\n\\n(11)\\n\\n(12)\\n\\n(13)\\n\\n\\u2022 \\u03c3(x) = (1 + exp(\\u2212x))\\u22121 is the sigmoid function.\\n\\u2022 a(\\u2217) is the point-wise non-linear activation function, for\\n\\u2022 W\\u2217 and wcls connect nodes between adjacent layers.\\n\\u2022 b\\u2217 and bcls are biases.\\n\\nwhich sigmoid function can be used.\\n\\nOur deep modelExisting approachInputCandidatesResultsExisting approachDeep model.........(d)...h3h1,3......sh2,3........................dt......ypstW1, 1W1, 2W1, 3W2, 1W2, 2W2, 3W3Wpstyclswclsh3h1...sh2............dt......ypstycls..................(a)(b)h1,1h1,2h2,2h2,1\\x0cing non-linear representations from s, d, and t.\\n\\n\\u2022 h\\u2217 are hidden nodes in different layers used for extract-\\n\\u2022 \\u02dcycls is the estimated label indicating whether the candi-\\ndate of body locations is correct. For pose estimatio of\\nsingle human, the candidate with the largest \\u02dcycls is used\\nas the \\ufb01nal output in our experiments.\\n\\n\\u2022 \\u02dcypst contains the estimated part locations.\\nThrough the \\ufb01rst two separate layers in Eq. (6)-(9), each\\ninformation source has its individual representation con-\\nstructed. Then all high-order representations are combined\\nby two layers in Eq. (11)-(13).\\n4.2. Training method\\n\\nDenote the parameter set for the model in Eq. (6)-(13) by\\n\\u03bb, \\u03bb = {W\\u2217, wcls, b\\u2217, bcls}. The objective function J(\\u03bb)\\nfor backpropagating error derivatives is as follows:\\n\\n(cid:88)\\n\\n(cid:16)\\n\\nJ(\\u03bb) =\\n\\nJ1(ycls\\n\\nn , \\u02dcycls\\n+ J3(W\\u2217, wcls),\\n\\nn\\n\\nn ) + ycls\\n\\nn J2(ypst\\n\\nn , \\u02dcypst\\nn )\\n\\n(14)\\nn ) log(1 \\u2212 \\u02dcycls\\nn ),\\n\\nn , \\u02dcycls\\nn , \\u02dcypst\\n\\nJ1(ycls\\nJ2(ypst\\nJ3(W\\u2217, wcls) =\\n\\nn ) = \\u2212 ycls\\nn ) \\u2212 (1 \\u2212 ycls\\nn log(\\u02dcycls\\n(cid:88)\\n(cid:88)\\nn \\u2212 \\u02dcypst\\nn ) =||ypst\\nn ||2,\\n|w\\u2217\\ni,j| +\\n\\n|wcls\\n\\n|,\\n\\ni\\n\\n(cid:17)\\n\\n(15)\\n\\ni,j\\n\\ni\\n\\nwhere \\u2217n denotes the nth sample, n = 1, 2, . . . N.\\n\\u2022 \\u02dcycls\\nn are computed using Eq. (6)-(13).\\n\\u2022 ycls\\n\\nn and \\u02dcypst\\nn \\u2208 {0, 1} is the ground truth classi\\ufb01cation label in-\\ndicating whether the current body location estimation is\\ncorrect or not. Positive training samples have their part\\ntemplates placed around annotated body locations. As in\\n[58], negative training samples have their part templates\\nplaced on images without human. Therefore, ycls can be\\nused for human detection by considering it as an indi-\\ncator on whether the rectangle covering body locations\\ncontains a human.\\n\\n\\u2022 J1(ycls\\n\\u2022 ypst\\n\\u2022 J2(ypst\\n\\nn , \\u02dcycls\\n\\nn ) is the cross-entropy error on classi\\ufb01cation.\\n\\nn contains the ground truth body locations.\\n\\nn , \\u02dcypst\\n\\nn ) is the sum of square error on body loca-\\ntion estimation. Since negative background samples do\\nnot have ground truth body location, the ycls\\nn is multi-\\nplied by J2 in (14) to ensure that only positive samples\\nare used to learn location estimation.\\n\\u2022 J3(W\\u2217, wcls) is the L1 norm regularization term. w\\u2217\\ni,j\\nis the (i, j)th element in W\\u2217 and wcls\\nis the ith and\\nelement in wcls. The information sources and hidden\\nnodes may have different purpose. For example, a node\\nin h3 may use the information source mixture type.\\nHence, J3(W\\u2217, wcls) is used to encourage sparsity in\\nthe weights.\\n\\ni\\n\\nBody part location estimation and human detection are both\\nlearned through shared representation in this model. They\\nare jointly learned because they are dependent tasks.\\n\\n4.3. Analysis\\n\\nThe mixture type t is used as an example for analysis. In\\nthe layer-wise pre-training stage [18], t and hidden vector\\nh1,3 are considered as a restricted Boltzmann machine with\\nthe following distribution:\\np(t, h1,3) \\u221d exp(tTW1,3h1,3 + b1,3T\\n\\nh1,3 + cTt).\\n\\n(16)\\n\\nDenote the jth column of W1,3 by w1,3\\u2217,j . Denote the jth\\nelement of b1,3 by bj. The marginal distribution p(t) can\\nbe obtained as follows:\\n\\np(t) =\\n\\np(t, h1,3)\\n\\n(cid:88)\\n\\u221d(cid:88)\\n\\nh1,3\\n\\nh1,3\\n\\n\\u221d exp(cTt)\\n\\n(cid:89)\\n\\n(cid:16)\\n\\nj\\n\\nexp(tTW1,3h1,3 + b1,3T\\n\\nh1,3 + cTt)\\n\\n1 + exp(tTw1,3\\u2217,j + bj)\\n\\n=\\n\\n(cid:17)\\n\\n(17)\\n\\n\\u03c6j(t),\\n\\n(cid:89)\\n\\nj\\n\\nwhere \\u03c6j(t) = 1 + exp(tTw1,3\\u2217,j + bj) and \\u03c6j(t) is a fully\\nconnected graphical model because it cannot be factorized.\\n\\u03c6j(t) can be considered as a factor that explains t in fac-\\ntor graph [5, 24].\\nIn pose estimation, \\u03c6j(t) can be con-\\nsidered as a global pattern explaining the mixture type t\\nfor all parts. In both training and inference stages, every\\nnode in h1,3 is connected to the mixture types of all parts.\\nTherefore, h1,3 nonlinearly extracts the global representa-\\ntion from t1,3. Similarly, the h2,3 extracts higher-level rep-\\nresentation from h1,3. Therefore, the stack of hidden layers\\nextracts global, high-level representation from the informa-\\ntion source t. The analysis to mixture type t is applica-\\nble to deformation and appearance score. As shown in Fig.\\n4, h2,3 captures the global articulation patterns of human\\nbody. One of the nodes in h2,3 has high response to people\\nsquat. Another node has high response to people standing\\nupright. Yet another node concisely captures two clusters of\\npose patterns.\\n\\nIn our deep model, the \\ufb01rst hidden layer has 200 hidden\\nnodes, the second layer, i.e. h2 in Eq. (10) has 150 hidden\\nnodes and the third layer, i.e. h3 in Eq. (11), has 100 hidden\\nnodes. Since the dimensions of s, d, and t are small, train-\\ning of the deep model is fast. Unlike loopy graphical mod-\\nels, the deep model is fast in the inference stage because it\\ndoes not require loopy belief propagation or sampling. The\\nextra testing time required by our deep model is less than 10\\npercent of the testing time required by the approach in [58].\\n\\n5. Experimental results\\n\\nThe proposed approach is evaluated on three datasets:\\nLSP [21], PARSE [44] and UIUC people [53]. The train-\\ning procedure and training set are the same as [58]. Positive\\n\\n\\x0cpose estimation for future applications. For example, in\\ncharacter animation, the rendering of a limb is possible\\nonly when both end points of the limb are correct.\\nWe follow [11, 40] and use the observer-centric anno-\\ntations for all approaches when we evaluate on the LSP\\ndataset.\\n5.2. Overall experimental results\\n\\nTable 1 shows the experimental results from the three\\n\\ndatasets.\\n\\nPishchulin\\u2019s approach in [40] used the LSP+PARSE\\ntraining set when evaluated on the PARSE dataset and used\\nthe UIUC+LSP training set when evaluated on the UIUC\\ndataset. To evaluate on the PARSE dataset, Pishchulin\\u2019s\\napproach [40] + [42] included LSP+PARSE and 2744 ex-\\ntra animated samples for training. Johnson\\u2019s approach in\\n[22] included 10,000 extra training samples when evaluated\\non the PARSE dataset. In all experiments, Andriluka\\u2019s ap-\\nproach in [2], Yang and Ramanan\\u2019s approach in [57, 58] and\\nour approach are trained on the 1000 training images of the\\nLSP dataset [21].\\n\\nAs shown in Table 1, our deep model obviously improves\\nthe pose estimation accuracy and outperforms all the state-\\nof-the-art on these three datasets. Speci\\ufb01cally, our approach\\nis better in detecting legs, arms and head compared with ex-\\nisting approaches. The approach of Pishchulin et al. [42] is\\nbetter than our approach in locating torso, possibly because\\nthe torso region is included in many poslets, which helps to\\nincrease the accuracy of their approach in locating torso.\\n\\nOur approach is complementary to existing approaches\\nbecause the information sources provided by these ap-\\nproaches can be used by our model to improve their re-\\nsults. Currently, our model uses the approach in [58] to\\nobtain information sources. Compared with the approach in\\n[58], our approach improves the pose estimation accuracy\\nby 5.8% (62.8% vs. 68.6% PCP), 7.4% (63.6% vs. 71.0%\\nPCP) and 8.6% (57.0% vs. 65.6% PCP) respectively on the\\nLSP, PARSE and UIUC datasets. Fig. 5 shows the compar-\\nison between our approach (left) and the approach in [58]\\n(right).\\n5.3. Results on different designs of deep models\\n\\nIn this section, we evaluate different designs of deep\\nmodels. Yang and Ramanan\\u2019s approach in [58] is used as\\nthe baseline because this approach is used by our model for\\nobtaining information sources. To be concise, we only refer\\nto the PCP results on the LSP dataset.\\n\\nDepth of model is investigated in Table 2. The approach\\nin [58] uses linear-SVM for combining information sources.\\nWe also trained a Kernel-SVM with RBF kernel for learn-\\ning a non-linear model using the off-the-shelf tool Libsvm.\\nThe difference in PCP between Linear SVM and kernel-\\nSVM is within 2% (62.8% vs. 64.2% on LSP). Bengio [3]\\n\\nFigure 4. Visualization of mixture-type patterns extracted by hid-\\nden nodes in h2,3. We use the approach in [26] and visualize train-\\ning samples with the largest responses on each hidden node. Sam-\\nples with the highest responses are placed at the upper-left corner.\\nHidden node 1 has high response to people squat. Node 2 has high\\nresponse to standing people. Node 3 has high response to two\\nclusters of pose patterns. Best viewed in color.\\n\\ntraining samples are constrained to have estimated part lo-\\ncations near the ground truth. Part of the training data is\\nused for validation.\\n5.1. Evaluation criteria\\n\\nIn all experiments, we use the most popular criterion,\\nwhich is the percentage of correctly localized parts (PCP)\\nintroduced in [14]. As stated in [42, 58], the PCP scoring\\nmetric has been implemented in different ways in different\\npapers. These differences have two dimensions.\\n1. There are two ways to compute the \\ufb01nal PCP score\\nacross the dataset. In the single way, only a single candi-\\ndate (given by the maximum scoring candidate of an al-\\ngorithm) for one image is used. The match way matches\\nmultiple candidates without penalizing false positives.\\n\\n2. There are two de\\ufb01nitions of a correct part localization.\\nFor the de\\ufb01nition both, it requires both end points of a\\npart (for example, end points wrist and elbow for the\\npart lower arm) to be correct. For the de\\ufb01nition avg, it\\nrequires only the average of the endpoints to be correct.\\nThe paper in [54, 57, 52] used \\u2018match+avg\\u2019. The paper in\\n[40, 2, 42] used \\u2018single+both\\u2019, which is the strictest case\\nand generally has lower PCP value. The paper in [10] pro-\\nvides results for \\u2018match+both\\u2019 and \\u2018match+avg\\u2019. We follow\\n[42, 40] and evaluate all approaches using the strictest \\u2018sin-\\ngle+both\\u2019 criterion. This is used because of the following\\nreasons:\\n1. For \\u2018single\\u2019 and \\u2018match\\u2019, as discussed in [58],\\n\\nthe\\n\\u2018match\\u2019 way gives unfair advantage to approaches that\\nproduce a large number of candidates because mis-\\nmatched candidates (false positives) are not penalized.\\n\\n2. For \\u2018both\\u2019 and \\u2018avg\\u2019, \\u2018both\\u2019 is better at describing the\\norientation of body parts and will facilitate the use of\\n\\n\\x0cTable 1. Pose estimation results (PCP) on LSP [21], UIUC people\\n[53] and PARSE [44].\\n\\nMethod\\n\\nTorso U.leg L.leg U.arm L.arm head Total\\n\\nLSP\\n\\nAndriluka et al. [2] 80.9 67.1 60.7 46.5\\nYang&Ramanan [57] 81.0 69.5 65.9 53.5\\nYang&Ramanan [58] 82.9 70.3 67.0 56.0\\nPishchulin et al. [40] 87.5 75.7 68.0 54.2\\n\\n26.4 74.9 55.7\\n35.8 76.8 60.7\\n39.8 79.3 62.8\\n33.9 78.1 62.9\\n\\nEichner&\\nFerrari [11]\\n\\nOurs\\n\\n86.2 74.3 69.3 56.5\\n85.8 76.5 72.2 63.3\\n\\n37.4 80.1 64.3\\n46.6 83.1 68.6\\n\\nPARSE\\n\\nAndriluka et al. [2] 86.3 66.3 60.0 54.6\\nYang&Ramanan [57] 83.4 68.8 60.7 59.8\\nYang&Ramanan [58] 82.9 68.8 60.5 63.4\\nPishchulin et al. [42] 88.8 77.3 67.1 53.7\\nPishchulin et al. [40] 92.2 74.6 63.7 54.9\\n90.7 80.0 70.0 59.3\\n\\n[40]+[42]\\nJohnson&\\n\\n35.6 72.7 59.2\\n40.7 83.4 62.7\\n42.4 82.4 63.6\\n36.1 73.7 63.1\\n39.8 70.7 62.9\\n37.1 77.6 66.1\\n\\nEveringham [22]\\n\\nOurs\\n\\n87.6 74.7 67.1 67.3\\n89.3 78.0 72.0 67.8\\n\\n45.8 76.8 67.4\\n47.8 89.3 71.0\\n\\nUIUC People\\n\\nAndriluka et al. [2] 88.3 64.0 50.6 42.3\\nYang&Ramanan [57] 78.1 60.9 53.2 41.3\\nYang&Ramanan [58] 81.8 65.0 55.1 46.8\\nPishchulin et al. [40] 91.5 66.8 54.7 38.3\\n86.8 56.3 50.2 30.8\\n89.1 72.9 62.4 56.3\\n\\nWang et al. [56]\\n\\nOurs\\n\\n21.3 81.8 52.6\\n32.2 76.1 53.0\\n37.7 79.8 57.0\\n23.9 85.0 54.4\\n20.3 68.8 47.0\\n47.6 89.1 65.6\\n\\nTable 2. Results (PCP) on investigating model depth.\\n\\nMethod\\n\\nTorso U.leg L.leg U.arm L.arm Head Total\\n\\nLSP\\n[58]\\n82.9 70.3 67.0\\nKernel SVM 81.9 72.2 67.6\\n1 hidden layer 84.9 73.9 69.5\\n2 hidden layers 85.0 74.6 70.7\\nOurs\\n85.8 76.5 72.2\\n\\nPARSE\\n\\n[58]\\n82.9 68.8 60.5\\nKernel SVM 81.0 67.8 61.2\\n1 hidden layer 84.4 71.2 63.2\\n2 hidden layers 85.9 74.4 68.3\\nOurs\\n89.3 78.0 72.0\\n\\nUIUC\\n\\n[58]\\n81.8 65.0 55.1\\nKernel SVM 82.2 65.0 54.9\\n1 hidden layer 83.0 65.6 55.9\\n2 hidden layers 84.2 68.4 59.3\\nOurs\\n89.1 72.9 62.3\\n\\n56\\n58.8\\n57.5\\n61.2\\n63.3\\n\\n63.4\\n63.2\\n62.4\\n64.6\\n67.8\\n\\n46.8\\n50.2\\n50.6\\n53.0\\n56.3\\n\\n39.8 79.3 62.8\\n42.8 77.5 64.2\\n42.9 50.7 62.3\\n45.2 82.2 67.1\\n46.6 83.1 68.6\\n\\n42.4 82.4 63.6\\n44.1 78.0 63.2\\n44.4 70.2 63.7\\n46.3 85.4 67.9\\n47.8 89.3 71.0\\n\\n37.7 79.8 57.0\\n43.1 80.6 58.9\\n42.3 79.8 59.2\\n45.3 83.4 62.0\\n47.6 89.1 65.6\\n\\nproved that linear-SVM and kernel-SVM are shallow mod-\\nels. With the deep model, our approach performs better. As\\nthe number of hidden layers increases from 1 hidden layer\\nto 2 hidden layers, the estimation accuracy increases from\\n62.3% to 67.1%. With PCP 68.6%, our \\ufb01nal model in Fig.\\n3(b) uses three hidden layers and is better than SVM and\\ndeep models with fewer layers.\\n\\nTable 3. Results (PCP) on investigating deep model structures.\\nMethod\\n\\nTorso U.leg L.leg U.arm L.arm Head Total\\n\\nLSP\\nDBN in Fig. 3(a) 82.9 73.2 69.5\\nOurs\\n85.8 76.5 72.2\\n\\nPARSE\\n\\nDBN in Fig. 3(a) 82.0 70.0 64.6\\nOurs\\n89.3 78.0 72.0\\n\\nDBN in Fig. 3(a) 87.4 68.4 58.3\\nOurs\\n89.1 72.9 62.3\\n\\nUIUC\\n\\n59.8\\n63.3\\n\\n62.9\\n67.8\\n\\n52.2\\n56.3\\n\\n43.8 79.2 65.5\\n46.6 83.1 68.6\\n\\n46.3 80.5 65.0\\n47.8 89.3 71.0\\n\\n44.3 84.6 61.8\\n47.6 89.1 65.6\\n\\nDeep model structure design is investigated in Table 3.\\nThe DBN in Fig. 3(a) trains a three-layer deep model over\\nthe concatenated informations with three hidden layers. The\\nmodel in 3(b) learns high-order representations individu-\\nally. The model in 3(b) with PCP 68.6% is better in con-\\nstructing the high-order representations and therefore has\\nhigher estimation accuracy compared with the DBN in Fig.\\n3(a) with PCP 65.5%.\\n\\nClassi\\ufb01cation label and location learning is investigated\\nin Table 4. There are two sets of labels to be estimated\\nin our deep model: classi\\ufb01cation label ycls and part posi-\\ntions ypst. In the experiments, we evaluate different ways\\nof estimating these labels. The Only ycls in Table 4, with\\nPCP 63.7%, only estimates class label, with part location\\ndirectly obtained by the approach in [58]. The Only ypst,\\nwith PCP 64.1%, only re\\ufb01nes the part location, with class\\nlabel directly obtained by the approach in [58]. Separate\\nycls+ypst, with PCP 64.7%, uses two deep models for esti-\\nmating ycls and ypst separately. It can be seen that both ycls\\nand ypst are helpful for improving accuracy. Our model\\nuses the single deep model to jointly learn both ycls and\\nypst (PCP 68.6%) and performs better than using two mod-\\nels to learn them separately (PCP 64.7%) because body lo-\\ncation and the correctness of candidate body location are\\ndependent.\\n\\nAnalysis. Our model extracts high-order representations\\nof appearance, deformation and mixture types and better\\nmodels their dependence at the top layer. For example, if\\nthe mixture types are upright upper- and lower-arms, the\\nweighted combination of the locations of wrist and shoul-\\nder is a good estimation on the location of elbow.\\nIf the\\nmixture types change, such estimation should change cor-\\nrespondingly. Such complex dependence cannot be mod-\\neled linearly and deep model is a better solution. When\\ndifferent information sources are extracted separately with\\nthe \\ufb01rst several layers, the connections across sources are\\nremoved and the number of parameters is reduced. It helps\\nto regularize optimization when training samples are lim-\\nited. Existing methods only use ycls for supervision, while\\nwe use both ypts and ycls. As shown in Fig. 4, re\\ufb01ning\\nypts does help to rectify incorrect part locations based on\\nthe high order prior model of body pose. Jointly learning\\n\\n\\x0cTable 4. PCP results on classi\\ufb01cation label and location learning.\\n\\nMethod\\n\\nTorso U.leg L.leg U.arm L.arm Head Total\\n\\nLSP\\n[58]\\n82.9 70.3 67.0\\nOnly ycls 82.0 71.5 68.0\\nOnly ypst 80.4 72.0 68.0\\nSeparate\\nycls+ypst 81.1 72.8 69.0\\nOurs\\n85.8 76.5 72.2\\n\\nPARSE\\n\\n[58]\\n82.9 68.8 60.5\\nOnly ycls 81.0 69.8 66.1\\nOnly ypst 80.5 71.2 65.4\\nSeparate\\nycls+ypst 83.4 73.7 67.6\\n89.3 78.0 72.0\\nOurs\\n\\nUIUC\\n\\n[58]\\n81.8 65.0 55.1\\nOnly ycls 85.4 68.8 59.3\\nOnly ypst 82.6 66.6 58.3\\nSeparate\\nycls+ypst 87.9 69.6 60.3\\nOurs\\n89.1 72.9 62.3\\n\\n56.0\\n57.6\\n59.2\\n\\n59.5\\n63.3\\n\\n63.4\\n60.5\\n62.2\\n\\n64.4\\n67.8\\n\\n46.8\\n49.2\\n52.2\\n\\n53.0\\n56.3\\n\\n39.8 79.3 62.8\\n42.0 77.2 63.7\\n42.8 76.8 64.1\\n\\n43.0 77.7 64.7\\n46.6 83.1 68.6\\n\\n42.4 82.4 63.6\\n43.9 76.1 63.8\\n44.4 79.5 64.6\\n\\n47.1 82.0 67.1\\n47.8 89.3 71.0\\n\\n37.7 79.8 57.0\\n40.5 83.4 60.4\\n44.7 81.8 60.8\\n\\n44.3 85.4 62.8\\n47.6 89.1 65.6\\n\\nypst and ycls helps to \\ufb01nd their shared representation under\\na multi-task learning framework, for which deep model is\\nan ideal choice.\\n\\n6. Conclusion\\n\\nThis paper has proposed a multi-source deep model for\\npose estimation. It non-linearly integrates three information\\nsources: appearance score, deformation and appearance\\nmixture type. These information sources are used for de-\\nscribing different aspects of the single modality data, which\\nis the image data in our pose estimation approach. Exten-\\nsive experimental comparisons on three public benchmark\\ndatasets show that the proposed model obviously improves\\nthe pose estimation accuracy and outperforms the state of\\nthe art. Since this model is a post-processing of informa-\\ntion sources, it is very \\ufb02exible in terms of integrating with\\nexisting approaches that use different information sources,\\nfeatures, or articulation models. Learning deep model from\\npixels for pose estimation and analyzing the in\\ufb02uence of\\ntraining data number will be the future work.\\n\\n7. Acknowledgement\\n\\nThis work is supported by the General Research\\nFund sponsored by the Research Grants Council of\\nHong Kong (Project No.\\nCUHK 417110, CUHK\\n417011, CUHK 429412), National Natural Science Foun-\\ndation of China (91320101), Shenzhen Basic Research\\nProgram (JC201005270350A, JCYJ20120903092050890,\\nJCYJ20120617114614438), and Guangdong Innovative\\nResearch Team Program (No.201001D0104648280).\\n\\nFigure 5. Comparison between our method (left) and the approach\\nin [58] (right) on the LSP, PARSE and UIUC dataset. Our ap-\\nproach obtains more reasonable articulation patterns and is better\\nin solving the double counting problem. Best viewed in color.\\n\\nReferences\\n[1] M. Andriluka, L. Pishchulin, P. Gehler, and B. Schiele. 2d\\nhuman pose estimation: New benchmark and state of the art\\nanalysis. In CVPR, 2014.\\n\\n[2] M. Andriluka, S. Roth, and B. Schiele. Pictorial structures\\nrevisited: people detection and articulated pose estimation.\\nIn CVPR, 2009.\\n\\n[3] Y. Bengio. Learning deep architectures for AI. Foundations\\n\\nand Trends in Machine Learning, 2(1):1\\u2013127, 2009.\\n\\n[4] Y. Bengio, A. Courville, and P. Vincent. Representation\\nlearning: A review and new perspectives. IEEE Trans. PAMI,\\n35(8):1798\\u20131828, 2013.\\n\\n[5] C. M. Bishop and N. M. Nasrabadi. Pattern recognition and\\n\\nmachine learning. springer, 2006.\\n\\n[6] L. Bourdev and J. Malik. Poselets: body part detectors\\n\\ntrained using 3D human pose annotations. In ICCV, 2009.\\n\\n[7] N. Dalal and B. Triggs. Histograms of oriented gradients for\\n\\nhuman detection. In CVPR, 2005.\\n\\n[8] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-\\nFei. Imagenet: a large-scale hierarchical image database. In\\nCVPR, 2009.\\n\\n[9] C. Desai and D. Ramanan. Detecting actions, poses, and\\n\\nobjects with relational phraselets. In ECCV, 2012.\\n\\n[10] K. Duan, D. Batra, and D. J. Crandall. A multi-layer com-\\n\\nposite model for human pose estimation. In BMVC, 2012.\\n\\n[11] M. Eichner and V. Ferrari. Appearance sharing for collective\\n\\nhuman pose estimation. In ACCV, 2012.\\n\\n[12] C. Farabet, C. Couprie, L. Najman, and Y. LeCun. Learning\\nhierarchical features for scene labeling. IEEE Trans. PAMI,\\n30:1915\\u20131929, 2013.\\n\\n[13] P. F. Felzenszwalb and D. P. Huttenlocher. Pictorial struc-\\n\\ntures for object recognition. IJCV, 61:55\\u201379, 2005.\\n\\n[14] V. Ferrari, M. Marin-Jimenez, and A. Zisserman. Progressive\\nsearch space reduction for human pose estimation. In CVPR,\\n2008.\\n\\nLeft-OursRight-Yang&Ramanan\\x0c[15] G. Gkioxari, P. Arbel\\xb4aez, L. Bourdev, and J. Malik. Articu-\\nlated pose estimation using discriminative armlet classi\\ufb01ers.\\nIn CVPR, 2013.\\n\\n[16] I. Goodfellow, H. Lee, Q. V. Le, A. Saxe, and A. Y. Ng.\\n\\nMeasuring invariances in deep networks. In NIPS, 2009.\\n\\n[17] M. Guillaumin, J. Verbeek, and C. Schmid. Multimodal\\nsemi-supervised learning for image classi\\ufb01cation. In CVPR,\\n2010.\\n\\n[18] G. E. Hinton, S. Osindero, and Y. Teh. A fast learning al-\\ngorithm for deep belief nets. Neural Computation, 18:1527\\u2013\\n1554, 2006.\\n\\n[19] G. E. Hinton and R. R. Salakhutdinov.\\n\\ndimensionality of data with neural networks.\\n313(5786):504 \\u2013 507, July 2006.\\n\\nReducing the\\nScience,\\n\\n[20] K. Jarrett, K. Kavukcuoglu, M. Ranzato, and Y. LeCun.\\nWhat is the best multi-stage architecture for object recog-\\nnition? In CVPR, 2009.\\n\\n[21] S. Johnson and M. Everingham. Clustered pose and nonlin-\\near appearance models for human pose estimation. In BMVC,\\n2010.\\n\\n[22] S. Johnson and M. Everingham. Learning effective human\\npose estimation from inaccurate annotation. In CVPR, 2011.\\n[23] A. Krizhevsky, I. Sutskever, and G. Hinton. Imagenet clas-\\nsi\\ufb01cation with deep convolutional neural networks. In NIPS,\\n2012.\\n\\n[24] F. R. Kschischang, B. J. Frey, and H.-A. Loeliger. Factor\\ngraphs and the sum-product algorithm. IEEE Trans. Inf. The-\\nory, 47(2):498\\u2013519, 2001.\\n\\n[25] H. Larochelle, Y. Bengio, J. Louradour, and P. Lamblin. Ex-\\nploring strategies for training deep neural networks. J. Ma-\\nchine Learning Research, 10:1\\u201340, 2009.\\n\\n[26] Q. V. Le, M. Ranzato, R. Monga, M. Devin, K. Chen, G. S.\\nCorrado, J. Dean, and A. Y. Ng. Building high-level features\\nusing large scale unsupervised learning. In ICML, 2012.\\n\\n[27] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-\\nbased learning applied to document recognition. Proceed-\\nings of the IEEE, 86(11):2278\\u20132324, 1998.\\n\\n[28] W. Li, R. Zhao, T. Xiao, and X. Wang. Deepreid: Deep \\ufb01lter\\npairing neural network for person re-identi\\ufb01cation. In CVPR,\\n2014.\\n\\n[29] P. Luo, Y. Tian, X. Wang, and X. Tang. Switchable deep\\n\\nnetwork for pedestrian detection. In CVPR, 2014.\\n\\n[30] P. Luo, X. Wang, and X. Tang. Hierarchical face parsing via\\n\\ndeep learning. In CVPR, 2012.\\n\\n[31] P. Luo, X. Wang, and X. Tang. A deep sum-product archi-\\n\\ntecture for robust facial attributes analysis. In ICCV, 2013.\\n\\n[32] P. Luo, X. Wang, and X. Tang. Pedestrian parsing via deep\\n\\ndecompositional neural network. In ICCV, 2013.\\n\\n[33] G. Mori and J. Malik. Estimating human body con\\ufb01gurations\\n\\nusing shape context matching. In ECCV, 2002.\\n\\n[34] G. Mori and J. Malik. Recovering 3D human body con\\ufb01gura-\\ntions using shape contexts. IEEE Trans. PAMI, 28(7):1052\\u2013\\n1062, 2006.\\n\\n[35] J. Ngiam, A. Khosla, M. Kim, J. Nam, H. Lee, and A. Ng.\\n\\nMultimodal deep learning. In ICML, 2011.\\n\\n[36] M. Norouzi, M. Ranjbar, and G. Mori. Stacks of convolu-\\ntional restricted boltzmann machines for shift-invariant fea-\\nture learning. In CVPR, 2009.\\n\\n[37] W. Ouyang and X. Wang. A discriminative deep model\\nfor pedestrian detection with occlusion handling. In CVPR,\\n2012.\\n\\n[38] W. Ouyang and X. Wang. Joint deep learning for pedestrian\\n\\ndetection. In ICCV, 2013.\\n\\n[39] W. Ouyang, X. Zeng, and X. Wang. Modeling mutual visi-\\n\\nbility relationship in pedestrian detection. In CVPR, 2013.\\n\\n[40] L. Pishchulin, M. Andriluka, P. Gehler, and B. Schiele. Pose-\\n\\nlet conditioned pictorial structures. In CVPR, 2013.\\n\\n[41] L. Pishchulin, M. Andriluka, P. Gehler, and B. Schiele.\\nStrong appearance and expressive spatial models for human\\npose estimation. In ICCV, December 2013.\\n\\n[42] L. Pishchulin, A. Jain, M. Andriluka, T. Thormahlen, and\\nB. Schiele. Articulated people detection and pose estimation:\\nReshaping the future. In CVPR, 2012.\\n\\n[43] H. Poon and P. Domingos. Sum-product networks: A new\\n\\ndeep architecture. In UAI, 2011.\\n\\n[44] D. Ramanan. Learning to parse images of articulated bodies.\\n\\nIn NIPS, 2007.\\n\\n[45] M. Ranzato, F. J. Huang, Y.-L. Boureau, and Y. Lecun. Un-\\nsupervised learning of invariant feature hierarchies with ap-\\nplications to object recognition. In CVPR, 2007.\\n\\n[46] B. Sapp, A. Toshev, and B. Taskar. Cascaded models for\\n\\narticulated pose estimation. In ECCV, 2010.\\n\\n[47] N. Srivastava and R. Salakhutdinov. Multimodal learning\\n\\nwith deep boltzmann machines. In NIPS, 2012.\\n\\n[48] M. Sun and S. Savarese. Articulated part-based model for\\n\\njoint object detection and pose estimation. In ICCV, 2011.\\n\\n[49] Y. Sun, X. Wang, and X. Tang. Deep convolutional network\\n\\ncascade for facial point detection. In CVPR, 2013.\\n\\n[50] Y. Sun, X. Wang, and X. Tang. Hybrid deep learning for\\n\\ncomputing face similarities. In ICCV, 2013.\\n\\n[51] Y. Sun, X. Wang, and X. Tang. Deep learning face represen-\\n\\ntation from predicting 10,000 classes. In CVPR, 2014.\\n\\n[52] Y. Tian, C. L. Zitnick, and S. G. Narasimhan. Exploring the\\nspatial hierarchy of mixture models for human pose estima-\\ntion. In ECCV, 2012.\\n\\n[53] D. Tran and D. Forsyth. Improved human parsing with a full\\n\\nrelational model. In ECCV, 2010.\\n\\n[54] F. Wang and Y. Li. Beyond physical connections: Tree mod-\\n\\nels in human pose estimation. In CVPR, 2013.\\n\\n[55] Y. Wang and G. Mori. Multiple tree models for occlusion\\nand spatial constraints in human pose estimation. In ECCV,\\n2008.\\n\\n[56] Y. Wang, D. Tran, and Z. Liao. Learning hierarchical pose-\\n\\nlets for human parsing. In CVPR, 2011.\\n\\n[57] Y. Yang and D. Ramanan. Articulated pose estimation with\\n\\n\\ufb02exible mixtures-of-parts. In CVPR, 2011.\\n\\n[58] Y. Yang and D. Ramanan. Articulated human detection with\\n\\n\\ufb02exible mixtures-of-parts. IEEE Trans. PAMI, To appear.\\n\\n[59] M. D. Zeiler, G. W. Taylor, and R. Fergus. Adaptive decon-\\nvolutional networks for mid and high level feature learning.\\nIn ICCV, 2011.\\n\\n[60] X. Zeng, W. Ouyang, and X. Wang. Multi-stage contextual\\n\\ndeep learning for pedestrian detection. In ICCV, 2013.\\n\\n[61] Z. Zhu, P. Luo, X. Wang, and X. Tang. Deep learning identity\\n\\npreserving face space. In ICCV, 2013.\\n\\n\\x0c', u'NEW TYPES OF DEEP NEURAL NETWORK LEARNING FOR SPEECH RECOGNITION \\n\\nAND RELATED APPLICATIONS: AN OVERVIEW  \\n\\n \\n\\nLi Deng1, Geoffrey Hinton2, and Brian Kingsbury3 \\n\\n1Microsoft Research, Redmond, WA, USA  \\n2University of Toronto, Ontario, Canada  \\n\\n3IBM T. J. Watson Research Center, Yorktown Heights, NY, USA \\n\\nABSTRACT \\n\\n \\nIn  this  paper,  we  provide  an  overview  of  the  invited  and \\ncontributed  papers  presented  at  the  special  session  at  ICASSP-\\n2013,  entitled  \\u201cNew  Types  of  Deep  Neural  Network  Learning  for \\nSpeech  Recognition  and  Related  Applications,\\u201d  as  organized  by \\nthe  authors.  We  also  describe  the  historical  context  in  which \\nacoustic  models  based  on  deep  neural  networks  have  been \\ndeveloped. \\n     The  technical  overview  of  the  papers  presented  in  our  special \\nsession  is  organized  into  five  ways  of  improving  deep  learning \\nmethods:  (1)  better  optimization;  (2)  better  types  of  neural \\nactivation  function  and  better  network  architectures;  (3)  better \\nways  to  determine  the  myriad  hyper-parameters  of  deep  neural \\nnetworks; (4) more appropriate ways to preprocess speech for deep \\nneural networks; and (5) ways of leveraging multiple languages or \\ndialects  that  are  more  easily  achieved  with  deep  neural  networks \\nthan with Gaussian mixture models.  \\n \\n\\nIndex Terms\\u2014 deep neural network, convolutional neural \\n\\nnetwork, recurrent neural network, optimization, spectrogram \\nfeatures, multitask, multilingual, speech recognition, music \\nprocessing \\n \\n\\n1. INTRODUCTION \\n\\n \\nIn  recent  years,  the  speech  recognition  community  has  seen  a \\nrevival  of  interest  in  neural  networks,  which  were  popular  during \\nlate 80\\u2019s and early 90\\u2019s but could not significantly outperform the \\nvery successful combination of HMMs with acoustic models based \\non Gaussian mixtures. Three main factors were responsible for the \\nrecent  emergence  of  neural  networks  as  high-quality  acoustic \\nmodels:  (1)  making  the  networks  deeper  makes  them  more \\npowerful,  hence  deep  neural  networks  (DNN);  2)  initializing  the \\nweights sensibly [24][43][16][52] and using much faster hardware \\nmakes it possible to train deep neural networks effectively, and 3) \\nusing  a \\nlarger  number  of  (context-dependent)  output  units \\n[8][10][48][49][53] greatly improves their performance. \\n     The  papers  presented  in  this  special  session  feature  both  the \\ncurrent state-of-the-art practice of DNNs and some promising new \\ndevelopments  beyond  the  standard  network  architectures  and \\nlearning  methodologies.  The  new  types  of  DNN  models  and \\nlearning techniques hold promise for creating better technology for \\nfuture-generation \\nand  possibly  other \\napplications. \\n     To  help  readers  understand  and  appreciate \\nthe  material \\npresented  in  our  special  session,  we  include  an  overview  of  the \\n\\nrecognition \\n\\nspeech \\n\\n \\n \\n\\nhistorical  context  in  which  DNN  technology  has  been  developed. \\nThe  application  areas  covered  include  speech  recognition,  music \\nprocessing, and language processing. \\n \\n\\n2. SPECIAL SESSION MOTIVATIONS \\n\\n \\nDeep  learning  has  become  increasingly  popular  [38]  since  the \\nintroduction  of  an  effective  new  way  of  learning  deep  neural \\nnetworks  in  2006  [25][26].  It  has  proved  very  successful  for \\nacoustic  modeling  in  speech  recognition  especially  for  large-scale \\ntasks,  and  this  success  has  been  based  largely  on  the  use  of  the \\nback-propagation  algorithm  with  rather  standard,  feed-forward \\nmulti-layer  neural  networks;  see  a  comprehensive  review  in  [24] \\nand  reviews  of  earlier  work  in  [6][44].  In  addition  to  improved \\nlearning  procedures,  the  main  factors  that  have  contributed  to  the \\nrecent successes of deep neural networks have been the availability \\nof  more  computing  power,  the  availability  of  more  training  data, \\nand  better  software  engineering.  The  initial  breakthrough  in \\nacoustic modeling was triggered by the use of a generative, layer-\\nby-layer  pre-training  method  for  initializing  the  weights  sensibly \\nbefore  running \\nlearning \\nprocedure,  but  subsequent  research  has  revealed  that  generative \\npre-training  is  unnecessary  when  there  is  a  very  large  amount  of \\nlabeled  training  data.  Back-propagation  can  be  started  from \\nrandom \\ntheir  scales  are  carefully \\ndetermined to prevent the initial error derivatives from being very \\nlarge or very small.  \\n     More than a year ago, four research groups (a group at Google \\nplus the three groups represented by the current organizers) wrote \\nan overview article [24] in which they presented their shared views \\non  applying  DNNs  to  acoustic  modeling  in  speech  recognition. \\nSince  then,  the  four  groups  and  other  speech  or  machine  learning \\ngroups around the world have done a lot of new  work developing \\nnew models and learning methods, and performing new evaluation \\nexperiments.  The  main  aim  of  this  special  session  is  to  highlight \\nadvances in the application of DNNs over the last year.  \\n  \\n\\nthe  discriminative  back-propagation \\n\\ninitial  weights  provided \\n\\n3. THE RECENT HISTORY OF DEEP NEURAL \\n\\nNETWORKS FOR ACOUSTIC MODELING \\n\\n \\nThe  DNNs  that  first  showed  big  improvements  over  Gaussian \\nMixture  Models  (GMMs)  for  acoustic  modeling  all  used  minor \\nvariations of the same  successful recipe, but training was so slow, \\neven  on  GPUs,  that  it  was  impossible  to  perform  the  extensive \\nexperimentation required to establish  which aspects of this recipe \\nmade  it  successful.  What  was  important  initially  was  to  find  any \\nreasonable way of training DNNs that allowed them to outperform \\n\\n\\x0cGMMs and shallow neural nets. The particular recipe was just the \\nfirst successful method to be developed and it was based on a lot of \\nintuitive  guesses  without  much  evidence  to  support  the  individual \\ndecisions.   \\n\\nThe successful recipe that was originally used in [42] presented \\nat  the  NIPS-2009  Workshop  [17]  to  train  an  acoustic  model  for \\nspeech recognition on the TIMIT database differed in several ways \\nfrom  previous  attempts  to  use  neural  networks  for  acoustic \\nmodeling.    The  nets  were  much  deeper  and  larger  than  previous \\nattempts,  having  up  to  eight  hidden  layers  with  a  few  thousand \\nhidden  units  per  layer  and  full  connectivity  between  adjacent \\nlayers.  The  final  fine-tuning  of  the  nets  used  the  standard, \\ndiscriminative  back-propagation  algorithm  to  compute  gradients \\nand  stochastic  gradient  descent  with  momentum  to  update  the \\nweights,  but  before  the  fine-tuning  started,  the  weights  were \\ninitialized by using an unsupervised learning algorithm that had no \\nknowledge  of  the  labels  used  for  fine-tuning.  The  unsupervised \\nlearning  algorithm  learned  one  hidden  layer  of  binary  stochastic \\nfeatures  at  a time  with  the  aim  of  the  learning  being  to  model  the \\nstatistical structure of the patterns of feature activations in the layer \\nbelow  (or  in  the  MFCCs    when  learning  the  first  hidden  layer).  \\nResults  were  found  to  be  only  slightly  superior  to  the  then-best-\\nperforming  single  system,  which  was  built  on  a  deep/dynamic \\ngenerative  model  called  the  hidden  trajectory  model  (HTM) \\n[14][15],  in  the  literature  and  evaluated  on  the  identical  task.  The \\nerror  patterns  produced  by  these  two  separate  systems  (DNN-\\nHMM vs. HTM) were carefully analyzed at MSR and found to be \\nvery  different,  reflecting  distinct  core  capabilities  of  the  two \\napproaches and motivating further studies on the DNN approach.  \\n\\n Over  the  following  few  years,  researchers  using  DNNs  for \\n\\nspeech recognition discovered a lot of things about this recipe:  \\n\\n \\n1)  It  works  well  for  LVCSR  and  it  works  even  better  for \\nLVCSR if the DNN\\u2019s output units correspond to context dependent \\nHMM  states,  and  importantly,  this  choice  keeps  the  decoding \\nalgorithm largely unchanged. \\n\\n \\n2) When there is a large amount of labeled data, the main effect \\nof the pre-training is just to get the initial weights to be about the \\nright  scale  so  that  back-propagation  works  well.  But  there  are \\nsimpler ways of doing this.  \\n\\n \\n3)  Even  if  we  use  layer-by-layer  pre-training,  there  are  many \\nalternatives  to  using  Restricted  Boltzmann  Machines  (RBMs)  for \\npre-training each layer. \\n\\n \\n4) DNNs work significantly better on filterbank outputs than on \\n\\nMFCCs. \\n\\n \\n5)  Speaker-dependent  methods  provide  surprisingly \\n\\nlittle \\nimprovement  over  speaker-independent  DNNs.  While  this  was \\ninitially  somewhat  disappointing,  using  speaker-independent \\nmodels  reduces  computational  expense  and  latency  when  these \\nmodels are used in applications. \\n\\n \\n6) DNNs work well for noisy speech. \\n \\n7) Using full connectivity between the early layers is simple but \\nnot sensible. DNNs work much better for acoustic modeling if we \\nuse one or more convolutional layers that do weight-sharing across \\nnearby  frequencies  and  then  pool  the  filter  responses  to  similar \\n\\nfrequencies  thus  giving  some  invariance  to  vocal  tract  differences \\nbetween speakers. \\n\\n \\n\\n     8)  Using  standard  logistic  neurons  is  sensible  but  not  optimal. \\nDNNs learn much faster if we use rectified linear units. These also \\noverfit  faster  but  a  new  regularization  method  called  \\u201cdropout\\u201d  is \\nvery effective at controlling this overfitting. \\n \\n     9)  The  same  methods  can  be  used  for  applications  other  than \\nacoustic modeling. \\n \\n    10) The DNN architecture can be used for multi-task learning in \\nseveral  different  ways  and  DNNs  are  far  more  effective  than \\nGMMs  at  leveraging  data  from  one  task  to  improve  performance \\non related tasks. \\n \\n\\nSome of these new discoveries about applying DNNs to speech \\nrecognition are described in the papers we selected for our special \\nsession and we describe those discoveries in more detail in the next \\nsection.  \\n \\n\\n4. OVERVIEW OF THE SPECIAL SESSION PAPERS \\n \\nHere  we  provide  a  technical  overview  of  the  five  papers  selected \\nfor  the  special  session.  The  technical  overview  covers  five \\npromising  ways  of  improving  deep  learning  methods:  (1)  better \\noptimization;  (2)  better  types  of  neural  activation  function  and \\nbetter network architectures; (3) better ways to optimize the myriad \\nhyper-parameters  of  DNNs;  (4)  more  appropriate  ways  to  pre-\\nprocess  speech  for  DNNs;  and  (5)  ways  of  leveraging  multiple \\nlanguages  or  dialects  that  are  more  easily  achieved  with  DNNs \\nthan with Gaussian mixture models.  \\n     Online,  stochastic  gradient  descent  has  been  the  workhorse  for \\nneural  network  training,  including  deep  learning,  for  over  25 \\nyears.   This  is  not  an  accident,  for  stochastic  gradient  descent \\nenjoys  a  number  of  advantages:   it  is  very  easy  to  implement;  it \\nmakes extremely rapid progress per training sample processed [5], \\nand  well-implemented  stochastic  gradient  descent  (where  care  is \\ntaken  in  the  randomization  of  training  samples  and  choice  of \\nlearning  rates)  frequently  converges  to  better  local  optima  than \\nother algorithms. \\n     The  main  problems  with  stochastic  gradient  descent have  been \\nthe  challenge of  scaling  to  very  large  data  sets  and  networks  with \\nmany  parameters  and  the  challenge  of  learning  very  deep  or \\nrecurrent neural network models.  For scaling up deep learning, the \\nmost  common  recent  solution  has  been \\nthe  use  of  GPU \\nhardware.   The  paper  from  Google  [22]  is  notable  because  it \\nfeatures a distributed framework for deep learning that successfully \\nuses  a  large  compute  cluster.   The  framework,  called  DistBelief \\n[11][35],  uses  an  asynchronous  version  of  stochastic  gradient \\ndescent  that  uses  many  different  replicas  of  the  neural  net  to \\ncompute  gradients  on  different  subsets  of  the  training  data  in \\nparallel. These  gradients are communicated to a central parameter \\nserver  that  updates  the  shared  weights  and  even  though  each \\nreplica  will  typically  be  computing  gradients  using  slightly  stale \\nparameter values, stochastic gradient descent is robust to the slight \\nerrors \\nthe \\nimplementation  of  each  replica  across  many  cores  which  greatly \\nincreases the degree of parallelization.  \\n     Training  very  deep  neural  networks  with  stochastic  gradient \\ndescent  is  difficult  because  the  gradients  tend  to  decrease  as  they \\nare  back-propagated  through  multiple  levels  of  nonlinearity. \\n\\nintroduces.  DistBelief \\n\\nthis \\n\\nalso \\n\\ndistributes \\n\\n\\x0cDistBelief  deals  with  this  problem  by  separately  adapting  the \\nlearning  rate  for  each  parameter.  For  recurrent  neural  networks, \\nwhich  are  typically  very  deep  in  time,  the  \\u201cvanishing  gradients\" \\nproblem  is  even  more  severe. Recently  developed  versions  of \\nsemi-online, second order optimization methods that use stochastic \\ncurvature  estimates,  such  as  Hessian-free  optimization  [39][40] \\nhave  revitalized  work  on  recurrent  network  models.  Hessian-free \\ntraining is used in the IBM paper [48] for sequence-discriminative \\ntraining.    \\n     The  paper  from  the  University  of  Montreal  [3]  explores  the \\ntraining  of  recurrent  models  using  modifications  to  stochastic \\ngradient descent, and, following the work of [51], shows that these \\nmodifications can outperform Hessian-free baselines. Optimization \\nideas that are explored in this work include clipping the gradient if \\nits  norm  exceeds  a  threshold  and  a  new  formulation  of  Nesterov \\naccelerated gradient training.  \\n     The use of recurrent neural networks for acoustic modeling was \\npioneered  by  Tony  Robinson  [47]  but  they  then  fell  out  of  favor \\nbecause  of  the  difficulty  of  training  them.  Recently,  however, \\nrecurrent  neural  networks  have  achieved  excellent  results  at \\nlanguage  modeling  [41]  and  the use  of  multiple  hidden  layers  has \\nallowed recurrent neural networks to outperform all other methods \\non TIMIT [21]. \\n     Related  to  optimization  in  deep  learning  are  problems  of \\nregularization.  Generative pre-training and standard methods such \\nas  weight  decay  (L2  regularization)  are  important,  but  beyond \\nthose  are  other  useful  ideas.   The  paper  from  the  University  of \\nToronto [9] describes \\u201cdropout,\\u201d which is a regularization method \\nthat randomly omits some fraction of the units in each hidden layer \\nduring  training.   This  procedure  discourages  brittle  co-adaptations \\nin which a hidden unit is useful only in the context of specific other \\nhidden  units.  The  dropout  method  is  easily  implemented  and \\nimproves the performance of DNNs on a wide variety of standard \\nbenchmarks  including  TIMIT  [23].  In  the  paper  submitted  to  this \\nspecial  session,  it  is  shown  that  dropout  regularization  can  be \\ncombined  with  rectified  linear  hidden  units  to  improve  speech \\nrecognition  on  a  50-hour  broadcast  news \\n Another \\nregularization method that is proving its worth is the application of \\na sparsity penalty to hidden representations in a network, which is \\nexplored in [3]. \\n     A  major  barrier  to  the  application  of  DNNs  is  that  it  currently \\nrequires  considerable  skill  and  experience  to  choose  sensible \\nvalues for hyper-parameters such as the learning rate schedule, the \\nstrength of the regularizer, the number of layers and the number of \\nunits  per  layer.    Sensible  values  for  one  hyper-parameter  may \\ndepend on the values chosen for other hyper-parameters and hyper-\\nparameter tuning in DNNs is especially expensive because testing \\na  single  setting  of  the  hyper-parameters  is  costly.     Papers  in  this \\nspecial  session  describe  two  methods  for  tackling  this  problem: \\npaper [9]    uses  an  off-the-shelf  Bayesian  optimization  procedure \\n[50],  while paper  [3]  employs  a  sampling  procedure  [4]  to  avoid \\nthe expense of a full grid search. \\n     Exploring  different  types  of  neuron  activation  function  and \\ndifferent network architectures is a theme common to many papers \\nin this session.  Both papers [9] and [3] explore the use of rectified \\nlinear \\ntanh \\nnonlinearities.   Rectified  linear  units  compute  y  =  max(x,  0),  and \\nlead to sparser gradients, less diffusion of credit and blame in deep \\nor  recurrent  networks,  and  faster  training  [54].  Paper  [3]  also \\nproposes  the  use  in  recurrent  networks  of  an  explicit  subset  of \\nleaky  integrator  units  in  the  state-to-state  map  to  better  capture \\nlong-range  dependencies,  as  well  as  the  use  of  a  powerful  output \\n\\nlogistic \\n\\ninstead \\n\\ntask. \\n\\nhidden \\n\\nunits \\n\\nof \\n\\nor \\n\\nfor  acoustic  modeling.  Spectrograms  contain \\n\\nprobability  model,  the  neural  autoregressive  distribution  estimator \\n[34].  \\n     Convolutional  neural  networks  have  been  widely  used  in \\ncomputer  vision  [36]  where  they  have  been  very  successful  [32]. \\nThey  showed  early  promise  for  acoustic  modeling  [33]  but  were \\nlater  abandoned,  probably  because  the  convolution  was  done \\nacross  time  rather  than  across  frequency.  Temporal  variation  is \\nalready  well-handled  by \\nthe  HMM  so  convolution  across \\nfrequency  is  much  more  helpful  because  it  provides  partial \\ninvariance  to  changes  in  the  properties  of  the  vocal  tract.  In  an \\nimportant  paper,  Abdel-Hamid  et.  al.  [1]  demonstrated  that \\nconvolution across frequency was very effective for TIMIT. More \\nrecent  work  described  in  the  papers  from  Microsoft  [12][2][13] \\nshows  that  designing  the  convolution  and  pooling  layers  to \\nproperly trade-off between invariance to the vocal tract length and \\ndiscrimination  among  speech  sounds  together  with  the  \\u201cdropout\\u201d \\ntechnique of regularization [27] leads to much better TIMIT phone \\nrecognition accuracy. This set of work also points to the direction \\nof  trading-off  between  trajectory  discrimination  and  invariance \\nexpressed in the whole dynamic pattern of speech defined in mixed \\ntime  and  frequency  domains  using  well  designed  weight  sharing \\nand pooling. The IBM paper [48] shows that convolutional neural \\nnetworks are also useful for LVCSR and further demonstrates that \\nmultiple  convolutional  layers  provide  even  more  improvement \\nwhen  the  convolutional  layers  use  a  large  number  of  convolution \\nkernels (i.e. feature maps).   \\n     In  light  of  the  powerful  DNN  learning  architectures  and \\nalgorithms  developed  recently,  it  is  useful  to  re-examine  some \\nlong-standing  assumptions  about  the  best  ways  to  pre-process \\nspeech \\nrich \\ninformation,  but  systems  that  use  GMMs  for  acoustic  modeling \\nwork  best  with  transformed  features  such  as  MFCCs  or  PLPs \\nwhose  elements  are  largely  de-correlated.  The  powerful  learning \\nprocedures  for  DNNs  allow  them  to  handle  correlations  between \\ninput  features  and  also  allow  them  to  transform  spectrograms  in \\nwhatever  way \\nthe \\nconventional wisdom about the kind of pre-processing that is most \\nhelpful.  The  paper  by  Microsoft  [12]  analyzes  the  fundamental \\nissue  of  what  are  effective  features  for  use  in  the  pattern \\nrecognition component of speech recognition. It reviews the use of \\nspectrograms as the input features for deep auto-encoders to extract \\nbottleneck  higher-level  features  [18]  and  the  extended  work  on \\nmulti-modal  deep  auto-encoder [45]  (see  the  most  recent  work  on \\naudio-visual  deep  learning  in  [28]).  It  also  presents  the  recent \\nresults  on  a  DNN-based,  large  vocabulary  speech  recognizer  with \\n(Mel-scaled) spectrograms as the input features which outperforms \\nthe same recognizer but with MFCCs as the input features.   \\n     The final theme that emerges from the set of selected papers is \\nthe  excellent  performance  of  DNN  acoustic  models  for  multi-task \\nlearning  [7].    Shallow  models  such  as  the  GMMs  used  in  the \\nprevious  generation  of  acoustic  models  do  not  benefit  nearly  as \\nmuch  as  DNNs  from  being  trained  on  multiple  languages \\nsimultaneously  or  from  being  trained  on  one  language  and  then \\nmodified  for  another  language  (e.g.  [37]).   Both  the  Microsoft \\npaper [12]  (also [29])  and  the  Google  paper  [22]  elaborate  such a \\nnew  capability,  sharing  the  same  example  of  multilingual  speech \\nrecognition.  In  Figure  1,  the  multi-task  learning  accomplished  by \\nDNN  is  shown  for  two  scenarios:  a)  with  high  practical  value: \\nlearning joint representation for both 16k and 8k acoustic data for \\nperforming recognition for both wideband (e.g., high quality smart \\nphone) voice search and narrowband telephony speech recognition, \\nand  b)  multilingual  or  cross-lingual  speech  recognition  that \\n\\nthey  want.  This  completely  changes \\n\\n\\x0ceffectively leverages acoustic training data across a wide range of \\nlanguages. \\n \\n \\n      \\n\\n[5]  L.  Bottou  and  Y.  LeCun.  \\u201cLarge  scale  online  learning,\\u201d \\n\\nNIPS, 2004. \\n\\n[6]  H.  Bourlard  and  N.  Morgan,  Connectionist  Speech \\nRecognition:  A  Hybrid  Approach,  Norwell,  MA:  Kluwer, \\n1993. \\n\\n[7]  R.  Caruana,  \\u201cMultitask  Learning.\\u201d  Machine  Learning,  Vol. \\n\\n28, pp. 41-75, Kluwer Academic Publishers, 1997. \\n\\n[8]  G.  Dahl,  D.  Yu,  L.  Deng,  and  A.  Acero,  \\u201cLarge  vocabulary \\ncontinuous  speech  recognition  with  context-dependent  DBN-\\nHMMs,\\u201d ICASSP, 2011. \\n\\n[9]  G.  Dahl,  T.  Sainath,  and  G.  Hinton.  \\u201cImproving  DNNs  for \\nLVCSR  using  rectified  linear  units  and  dropout,\\u201d  ICASSP, \\n2013. \\n\\n[10] G. Dahl, D. Yu, L. Deng, and A. Acero, \\u201cContext-dependent \\npre-trained deep neural networks for large vocabulary speech \\nrecognition.\\u201d  IEEE  Trans.  Speech  and  Audio  Proc.,  vol.  20, \\nno. 1, pp. 30 \\u2013 42, 2012. \\n\\n[11] J. Dean, G. Corrado, R. Monga, K. Chen, M. Devin, Q. Le, M. \\nMao, M. Ranzato, A. Senior, P. Tucker, K. Yang, and A. Ng. \\n\\u201cLarge scale distributed deep networks,\\u201d NIPS, 2012. \\n\\n \\n\\n \\n\\n \\nFigure 1: a) left: DNN training/testing with mixed-band acoustic \\ndata with16-kHz and 8-kHz sampling rates; b) right: Illustrative \\narchitecture for a multilingual DNN  \\n \\n \\n\\n5. CONCLUSIONS \\n\\nfronts  and \\n\\n \\nIn  summary,  the  articles  in  our  special  session  demonstrate  that \\nthere  continues  to  be  rapid  progress  in  acoustic  models  that  use \\nDNNs  and  that  similar  methods  are  also  applicable  in  related \\ndomains  such  as  music.  The  progress  is  occurring  on  many \\ndifferent \\nthe  already  significant \\nperformance  gap  between  acoustic  models  based  on  DNNs  and \\nthose based on GMMs. We believe that the lessons we are learning \\nin  acoustic  modeling  are  likely  to  be  relevant  to  a  wide  range  of \\nother  signal  processing,  language  processing,  machine  learning, \\nand artificial intelligence tasks.   \\n \\n \\n\\nis  widening \\n\\n6. ACKNOWLEDGEMENTS \\n\\n \\nWe thank all authors for contributing their papers to our special \\nsession  and  thank  anonymous  reviewers  who  provided  valuable \\nfeedback to us. \\n\\n \\n \\n\\n7. REFERENCES \\n\\n \\n[1]  O.  Abdel-Hamid,  A.  Mohamed,  H.  Jiang,  and  G.  Penn, \\n\\u201cApplying  convolutional  neural  networks  concepts  to  hybrid \\nNN-HMM model for speech recognition,\\u201d ICASSP, 2012. \\n\\n[2]  O.  Abdel-Hamid,  L.  Deng,  and  D.  Yu.  \\u201cExploring \\nconvolutional  neural  network  structures  and  optimization  for \\nspeech recognition,\\u201d Interspeech, 2013, submitted. \\n\\n[3]  Y.  Bengio,  N.  Boulanger,  and  R.  Pascanu.  \\u201cAdvances  in \\n\\noptimizing recurrent networks,\\u201d ICASSP, 2013.  \\n\\n[4]  J.  Bergstra  and  Y.  Bengio,  \\u201cRandom  search  for  hyper-\\nparameter  optimization,\\u201d  J.  Machine  Learning  Research,\\u201d \\nVol. 3, pp. 281-305, 2012.   \\n\\n[12] L. Deng, J. Li, J. Huang, K. Yao, D. Yu, F. Seide, M. Seltzer, \\nG.  Zweig,  X.  He,  J.  Williams,  Y.  Gong,  and  A.  Acero. \\n\\u201cRecent  advances  of  deep  learning  for  speech  research  at \\nMicrosoft,\\u201d ICASSP, 2013. \\n\\n[13] L.  Deng,  O.  Abdel-Hamid,  and  D.  Yu.  \\u201cDeep  convolutional \\nneural  networks  using  heterogeneous  pooling  for  trading-off \\nacoustic invariance with phonetic confusion,\\u201d ICASSP, 2013. \\n[14] L.  Deng,  D.  Yu,  and  A.  Acero.  \\u201cStructured  Speech \\nModeling,\\u201d  IEEE  Trans.  on  Audio,  Speech  and  Language \\nProcessing. Volume: 14 Issue: 5, Sep 2006. pp. 1492- 1504. \\n\\n[15] L.  Deng  and  D.  Yu. \\u201cUse  of  differential  cepstra  as  acoustic \\nfor  phonetic \\n\\ntrajectory  modeling \\n\\nfeatures \\nrecognition,\\u201d ICASSP, 2007. \\n\\nin  hidden \\n\\n[16] L. Deng, D. Yu, and J. Platt. \\u201cScalable stacking and learning \\n\\nfor building deep architectures,\\u201d ICASSP, 2012. \\n\\n[17] L.  Deng,  D.  Yu,  and  G.  Hinton.  \\u201cDeep  Learning  for  Speech \\nRecognition and Related Applications\\u201d NIPS Workshop, 2009 \\nhttp://nips.cc/Conferences/2009/Program/event.php?ID=1512 \\n[18] L.  Deng,  M.  Seltzer,  D.  Yu,  A.  Acero,  A.  Mohamed,  and  G. \\nHinton,  \\u201cBinary  coding  of  speech  spectrograms  using  a  deep \\nauto-encoder,\\u201d Interspeech, 2010. \\n\\n[19] L. Deng, G. Tur, X. He, and D. Hakkani-Tur, \\u201cUse of kernel \\ndeep  convex  networks  and  end-to-end  learning  for  spoken \\nlanguage understanding,\\u201d IEEE SLT, 2012. \\n\\n[20] J.  Duchi,  E.  Hazan,  and  Y.  Singer,  \\u201cAdaptive  subgradient \\nmethods  for  online  learning  and  stochastic  optimization,\\u201d  J. \\nMachine Learning Research, 2011, pp. 2121-2159. \\n\\n[21] A. Graves, A. Mohamed, and G. Hinton. \\u201cSpeech recognition \\n\\nwith deep recurrent neural networks,\\u201d ICASSP, 2013. \\n\\n[22] G.  Heigold,  V.  Vanhoucke,  A.  Senior,  P.  Nguyen,  M. \\nRanzato,  M.  Devin,  and  J.  Dean.  \\u201cMultilingual  acoustic \\nmodels  using  distributed  deep  neural  networks,\\u201d  ICASSP, \\n2013. \\n\\n[23] G.  Hinton,  N.  Srivastava,  A.  Krizhevsky,  I.  Sutskever,  and \\nSalakhutdinov. \\u201cImproving neural networks by preventing co-\\nadaptation \\n2012. \\nhttp://arxiv.org/abs/1207.0580. \\n\\ndetectors,\\u201d \\n\\nof \\n\\nfeature \\n\\n[24] G. Hinton, L. Deng, D. Yu, G. Dahl, A. Mohamed, N. Jaitly, \\nA.  Senior,  V.  Vanhoucke,  P.  Nguyen,  T.  Sainath,  and  B. \\nKingsbury,  \\u201cDeep  neural  networks  for  acoustic  modeling  in \\nspeech  recognition,\\u201d  IEEE  Signal Processing  Magazine,  Vol. \\n29 (6), pp. 82-97, 2012. \\n\\n\\x0c[25] G.  Hinton,  S.  Osindero, \\n\\nfast \\nlearning algorithm for  deep  belief  nets.  Neural  Computation, \\nVol. 18, 1527-1554, 2006. \\n\\nand  Y.  Teh. \\n\\n\\u201cA \\n\\n[26] G.  Hinton \\n\\nand  R.  Salakhutdinov. \\n\\n\\u201cReducing \\n\\nthe \\nnetworks. \\n\\ndimensionality \\nScience, Vol. 313. no. 5786, pp. 504 - 507, July 2006. \\n\\ndata  with \\n\\nneural \\n\\nof \\n\\n[27] G.  Hinton,  N.  Srivastava,  A.  Krizhevsky,  I.  Sutskever,  &  R. \\nSalakhutdinov. \\u201cImproving neural networks by preventing co-\\nadaptation of feature detectors,\\u201d arXiv: 1207.0580v1, 2012. \\n\\n[28] J.  Huang  and  B.  Kingsbury.  \\u201cAudio-visual  deep  learning  for \\n\\nnoise robust speech recognition,\\u201d ICASSP, 2013. \\n\\n[29] J.-T.  Huang,  J.  Li,  D.  Yu,  L.  Deng,  and  Y.  Gong.  \\u201cCross-\\nlanguage  knowledge  transfer  using  multilingual  deep  neural \\nnetworks with shared hidden layers,\\u201d ICASSP, 2013. \\n\\neffective for large vocabulary continuous speech recognition\\u201d, \\nProc. ASRU, pp. 30-35, 2011. \\n\\n[47] A.  Robinson,  \\u201cAn  application  to  recurrent  nets  to  phone \\nprobability estimation,\\u201d IEEE Trans. Neural Networks, vol. 5, \\nno. 2, pp. 298\\u2013305, 1994. \\n\\n[48] T.    Sainath,  A.  Mohamed,  B.  Kingsbury,  B.  Ramabhadran, \\n\\u201cConvolutional neural networks for LVCSR,\\u201d ICASSP, 2013.  \\n[49] F.  Seide,  G.  Li,  and  D.  Yu,  \\u201cConversational  speech \\ntranscription  using  context-dependent  deep  neural  networks,\\u201d \\nInterspeech, 2011, pp. 437-440, 2011.  \\n\\n[50] J.  Snoek,  H.  Larochelle,  and  R.  Adams,  \\u201cPractical  Bayesian \\n\\noptimization of machine learning algorithms,\\u201d NIPS, 2012.  \\n\\n[51] I.  Sutskever.  \\u201cTraining  Recurrent  Neural  Networks,\\u201d  Ph.D. \\n\\nThesis, University of Toronto, 2013. \\n\\n[30] N. Jaitly, P. Nguyen, and V. Vanhoucke, \\u201cApplication of pre-\\ntrained  deep  neural  networks  to  large  vocabulary  speech \\nrecognition,\\u201d Interspeech, 2012. \\n\\n[52] D.  Yu,  L.  Deng,  G.  Li,  and  Seide  F,  \\u201cDiscriminative \\npretraining of deep neural networks,\\u201d U.S. Patent Filing, Nov. \\n2011. \\n\\n[53] D. Yu, L. Deng, and G. Dahl, \\u201cRoles of pretraining and fine-\\ntuning  in  context-dependent  DNN-HMMs  for  real-world \\nspeech  recognition,\\u201d  NIPS  Workshop  on  Deep  Learning  and \\nUnsupervised Feature Learning, Dec. 2010. \\n\\n[54] M.  Zeiler,  M.  Ranzato,  R.  Monga,  M.  Mao,  K.  Yang,  Q.  Le, \\nP. Nguyen, A. Senior, V. Vanhoucke, J. Dean, and G. Hinton. \\n\\u201cOn  rectified  linear  units  for  speech  processing,\\u201d  ICASSP, \\n2013. \\n\\n[31] B.  Kingsbury,  T.  N.  Sainath,  and  H.  Soltau,  \\u201cScalable \\nminimum Bayes risk training of deep neural network acoustic \\nmodels  using  distributed  Hessian-free  optimization,\\u201d \\nInterspeech, 2012. \\n[32] A.  Krizhevsky, \\n\\nand  G.  Hinton. \\n\\u201cImageNet  classification  with  deep  convolutional  neural \\nNetworks,\\u201d NIPS 2012. \\n\\nI.  Sutskever, \\n\\nI. \\n\\n[33] K.  Lang,  A.  Waibel,  and  G.  Hinton.  \\u201cA  time-delay  neural \\nnetwork  architecture  for  isolated  word  recognition,\\u201d  Neural \\nNetworks, Vol. 3(1), pp. 23-43, 1990. \\n\\n[34] H.  Larochelle  and  I.  Murray.  \\u201cThe  neural  autoregressive \\n\\ndistributed estimator,\\u201d ICML, 2011.  \\n\\n[35] Q.  Le,  M.  Ranzato,  R.  Monga,  M.  Devin,  K.  Chen,  G. \\nCorrado,  J.  Dean,  A.  Ng,  \\u201cBuilding  high-level  features  using \\nlarge scale unsupervised learning.\\u201d ICML, 2012. \\n\\n[36] Y  LeCun,  L  Bottou,  Y  Bengio, \\n\\nP  Haffner. \\n\\u201cGradient-based  learning  applied  to  document  recognition,\\u201d \\nProceedings of the IEEE 86 (11), 2278-2324. \\n\\n[37] H. Lin, L. Deng, D. Yu, Y. Gong, A. Acero, and C-H Lee, \\u201cA \\nstudy on multilingual acoustic modeling for large  vocabulary \\nASR.\\u201d ICASSP, 2009, pp. 4333\\u20134336. \\n\\n[38] J.  Markoff.  \\u201cScientists  See  Promise \\n\\nin  Deep-Learning \\n\\nPrograms,\\u201d New York Times, Nov 24, 2012. \\n\\n[39] J.  Martens.  \\u201cDeep  learning  via  Hessian-free  optimization,\\u201d \\n\\nICML, 2010. \\n\\n[40] J.  Martens  and  I.  Sutskever.  \\u201cLearning  Recurrent  Neural \\n\\nNetworks with Hessian-Free Optimization,\\u201d ICML, 2011. \\n\\n[41] T.  Mikolov,  M.  Karafiat,  L.  Burget,  J.  Cernocky,  and  S. \\nlanguage \\n\\nKhudanpur.  \\u201cRecurrent  neural  network  based \\nmodel,\\u201d Interspeech, 2010. \\n\\n[42]   A.  Mohamed,  G.  Dahl,  and  G.  Hinton,  \\u201cDeep  belief \\nnetworks  for  phone  recognition,\\u201d  in  Proc.  NIPS  Workshop \\nDeep  Learning \\nfor  Speech  Recognition  and  Related \\nApplications, 2009. \\n\\n[43] A.  Mohamed,  G.  Dahl,  and  G.  Hinton,  \\u201cAcoustic  modeling \\nusing  deep  belief  networks,\\u201d  IEEE  Trans.  on  Audio,  Speech, \\nand Language Processing, vol. 20, no. 1, pp. 14\\u201322, 2012. \\n\\n[44] N.  Morgan.  \\u201cDeep  and  Wide:  Multiple  Layers  in  Automatic \\nSpeech  Recognition,\\u201d  IEEE  Trans.  on  Audio,  Speech,  and \\nLanguage Processing, vol. 20, no. 1, pp. 7-13, 2012. \\n\\n[45] J.  Ngiam,  A.  Khosla,  M.  Kim,  J.  Nam,  H.  Lee,  and  A.  Ng, \\n\\n\\u201cMultimodal deep learning,\\u201d ICML, 2011.  \\n\\n[46] T.  N.  Sainath,  B.  Kingsbury,  B.  Ramabhadran, P.  Fousek, P. \\nNovak,  and  A.  Mohamed,  \\u201cMaking  deep  belief  networks \\n\\n\\x0c', u'IEEE TRANSACTIONS ON BIG DATA, TBD-2015-05-0037 \\n\\n1 \\n\\nMethodologies for Cross-Domain Data \\n\\nFusion: An Overview \\n\\nYu Zheng, Senior Member \\n\\nAbstract\\u2014 Traditional data mining usually deals with data from a single domain. In the big data era, we face a diversity of datasets \\nfrom  different  sources  in  different  domains.  These  datasets  consist  of  multiple  modalities,  each  of  which  has  a  different \\nrepresentation,  distribution, scale,  and  density.  How  to  unlock  the power  of  knowledge  from multiple  disparate  (but  potentially \\nconnected) datasets is paramount in big data research, essentially distinguishing big data from traditional data mining tasks. This \\ncalls  for  advanced  techniques  that  can  fuse  the  knowledge  from  various  datasets  organically  in  a  machine  learning  and  data \\nmining task. This paper summarizes the data fusion methodologies, classifying them into three categories: stage-based, feature \\nlevel-based, and semantic meaning-based data fusion methods. The last category of data fusion methods is further divided into \\nfour groups: multi-view learning-based, similarity-based, probabilistic dependency-based, and transfer learning-based methods. \\nThese methods focus on knowledge fusion rather than schema mapping and data merging, significantly distinguishing between \\ncross-domain data fusion and traditional data fusion studied in the database community. This paper does not only introduce high-\\nlevel principles of each category of methods, but also give examples in which these techniques are used to handle real big data \\nproblems.  In  addition,  this  paper  positions  existing  works  in  a  framework,  exploring  the  relationship  and  difference  between \\ndifferent data fusion methods. This paper will help a wide range of communities find a solution for data fusion in big data projects. \\n\\nIndex Terms\\u2014 Big Data, cross-domain data mining, data fusion, multi-modality data representation, deep neural networks, \\nmulti-view learning, matrix factorization, probabilistic graphical models, transfer learning, urban computing.    \\n\\n\\u2014\\u2014\\u2014\\u2014\\u2014\\u2014\\u2014\\u2014\\u2014\\u2014   \\uf075   \\u2014\\u2014\\u2014\\u2014\\u2014\\u2014\\u2014\\u2014\\u2014\\u2014 \\n\\n1  INTRODUCTION\\n\\nI \\n\\nn the big data era, a wide array of data have been gener-\\nated in different domains, from social media to transpor-\\ntation,  from  health  care  to  wireless  communication  net-\\nworks.  When  addressing  a  problem,  we  usually  need  to \\nharness  multiple  disparate  datasets  [84].  For  example,  to \\nimprove urban planning, we need to consider the structure \\nof a road network, traffic volume, points of interests (POIs) \\nand populations in a city. To tackle air pollution, we need \\nto  explore  air  quality  data  together  with  meteorological \\ndata, emissions from vehicles and factories, as well as the \\ndispersion condition of a place. To generate a more accu-\\nrate travel recommendation for users, we shall consider the \\nuser\\u2019s behavior on the Internet and in the physical world. \\nTo  better  understand  an  image\\u2019s  semantic  meanings,  we \\ncan use its surrounding text and the features derived from \\nits pixels. So, how to unlock the power of knowledge from \\nmultiple datasets across different domains is paramount in \\nbig data research, essentially distinguishing big data from \\ntradition data mining tasks.  \\n     However,  the  data  from  different  domains  consists  of \\nmultiple  modalities,  each  of  which  has  a  different  repre-\\nsentation, distribution, scale and density. For example, text \\nis usually represented as discrete sparse word count vec-\\ntors, whereas an image is represented by pixel intensities \\nor outputs of feature extractors which are real-valued and \\ndense.  POIs  are  represented  by  spatial  points  associated \\nwith  a  static  category,  whereas  air  quality  is  represented \\nusing  a  geo-tagged  time  series.  Human  mobility  data  is \\nrepresented by trajectories [82], whereas a road network is \\n\\n\\u2014\\u2014\\u2014\\u2014\\u2014\\u2014\\u2014\\u2014\\u2014\\u2014\\u2014\\u2014\\u2014\\u2014\\u2014\\u2014 \\n\\n\\uf0b7  Yu Zheng is with Microsoft Research, Beijing 100080, China. E-mail: yu-\\n\\nzheng@ microsoft.com. \\n \\n\\nBuilding 2, No. 5 Danling Street, Haidian District, Beijing 100080, China. \\n\\ndenoted  as  a  spatial  graph.  Treating  different  datasets \\nequally or simply concatenating the features from dispar-\\nate  datasets  cannot  achieve  a  good  performance  in  data \\nmining tasks [8][46][56]. As a result, fusing data across mo-\\ndalities becomes a new challenge in big data research, call-\\ning for advanced data fusion technology. \\n     This paper summarizes three categories of methods that \\ncan fuse multiple datasets. The first category of data fusion \\nmethods use different datasets at different stages of a data \\nmining task. We call them stage-based fusion methods. For \\nexample, Zheng et al. [86] first partition a city into disjoint \\nregions by road network data, and then detect the pairs of \\nregions that are not well connected based on human mo-\\nbility data. These region pairs could denote the design that \\nis out of date in a city\\u2019s transportation network. The second \\ncategory  of  methods  learns  a  new  representation  of  the \\noriginal features extracted from different datasets by using \\ndeep neural networks (DNN). The new feature representa-\\ntion will then be fed into a model for classification or pre-\\ndiction. The third category blends data based on their se-\\nmantic meanings, which can be further classified into four \\ngroups: \\n\\uf0b7  Multi-view-based methods: This group of methods treats \\ndifferent datasets (or features from different datasets) \\nas different views on an object or an event.  Different \\nfeatures  are  fed  into  different  models,  describing  an \\nobject from different perspectives. The results are later \\nmerged together or mutually reinforce each other. Co-\\nTraining is an example of this category. \\nSimilarity-based methods: This group of methods lever-\\nages the underlying correlation (or similarity) between \\ndifferent  objects  to  fuse  different  datasets.  A  typical \\nmethod  is  coupled  collaborative  filtering  (CF),  a.k.a. \\n\\n\\uf0b7 \\n\\nxxxx-xxxx/0x/$xx.00 \\xa9 2015 IEEE        Published by the IEEE Computer Society \\n\\n \\n\\n\\x0c2 \\n\\nIEEE TRANSACTIONS ON BIG DATA, MANUSCRIPT ID \\n\\ncontext-aware  CF,  where  different  datasets  are  mod-\\neled by different matrices with common dimensions. \\nThrough decomposing these matrices (or tensors) to-\\ngether, we can achieve a better result than solely fac-\\ntorizing a single matrix (or a tensor). Manifold align-\\nment also belongs to this group.  \\n\\n\\uf0b7  Probabilistic dependency-based methods: This group mod-\\nels the probabilistic causality (or dependency) betwe-\\nen  different  datasets  using  a  graphic  representation. \\nBayesian Network and Markov Random Field are rep-\\nresentative  models,  denoting  features  extracted  from \\ndifferent datasets as graph nodes and the dependency \\nbetween two features with an edge.  \\n\\n\\uf0b7  Transfer learning-based methods: This group of methods \\ntransfers the knowledge from a source domain to an-\\nother  target  domain,  dealing  with  the  data  sparsity \\nproblems  (including  the  feature  structure  missing  or \\nobservation  missing)  in  the  target  domain.  Transfer \\nlearning can even transfer knowledge between differ-\\nent learning tasks, e.g. from book recommendation to \\ntravel recommendation.  \\n\\nThe rest of this paper goes deeper each category of meth-\\nods,  introducing  the  high-level  principle  and  representa-\\ntive examples for each category. With this paper, research-\\ners and professionals are more capable of choosing proper \\napproaches to solve real-world data fusion problems with \\nbig data. This paper also shares a collection of public da-\\ntasets that can facilitate research on big data.  \\n\\n2 RELATED WORK \\n\\n2.1 Relation to Traditional Data Integration \\nConventional data fusion [10], which is regarded as a part \\nof data integration, is a process of integration of multiple \\ndata  representing  the  same  real-world  object  into  a  con-\\nsistent, accurate, and useful representation. Fig. 1 A) pre-\\nsents the paradigm of conventional data fusion. For exam-\\nple, there are three POI datasets  for Beijing generated by \\nthree  different  data  providers.  Conventional  data  fusion \\naims to merge the three datasets into a database with a con-\\nsistent data schema, through a process of schema mapping \\nand  duplicate  detection.  The  records  (from  different  da-\\ntasets) describing the same POI, e.g. a restaurant, are gen-\\nerated in the same domain, i.e. POI.  \\n     As illustrated in Fig. 1 B), however, in the era of big data, \\nthere are multiple datasets generated in different domains, \\nwhich  are  implicitly  connected  by  a  latent  object.  For  in-\\nstance,  traffic  conditions,  POIs  and  demography  of  a  re-\\ngion  describe  the  region\\u2019s  latent  function  collectively, \\nwhile  they  are  from  three  different  domains.  Literally\\uff0c \\nrecords from the three datasets describe different objects, \\ni.e.  a  road  segment,  a  POI,  and  a  neighborhood,  respec-\\ntively. Thus, we cannot merge them straightforwardly by \\na schema mapping and duplication detection. Instead, we \\nneed to extract knowledge from each dataset by different \\nmethods, fusing the knowledge from them organically to \\nunderstand  a  region\\u2019s  function  collectively.  This  is  more \\nabout  knowledge  fusion  rather  than  schema  mapping, \\nwhich significantly differentiates between traditional data \\n\\nfusion (studied in the database community) and cross-do-\\nmain data fusion.   \\n\\nFig. 1 Paradigms of different data fusion  \\n\\n  \\n\\n2.2 Relation to Heterogeneous Information Network \\nAn  information  network  represents  an  abstraction  of  the \\nreal  world,  focusing  on  objects  and  interactions  between \\nobjects. It turns out that this level of abstraction has great \\npower in not only representing and storing essential infor-\\nmation  about  the  real-world,  but  also  providing  a  useful \\ntool to mine knowledge from it, by exploring the power of \\nlinks [57]. Departing from many existing network models \\nthat view interconnected data as homogeneous graphs or \\nnetworks,  a  heterogeneous  information  network  consists \\nof  nodes  and  relations  of  different  types.  For  example,  a \\nbibliographic  information  network  consists  of  authors, \\nconferences and papers as different types of nodes. Edges \\nbetween different nodes in this network can denote differ-\\nent semantic meanings, e.g. an author publishes a paper, a \\npaper is presented at a conference, and an author attends a \\nconference. Quite a few algorithms have been proposed to \\nmine a heterogeneous network, e.g. ranking and clustering \\n[58][59].   \\n      Heterogeneous  information  networks  can  be  con-\\nstructed in almost any domain, such as social networks, e-\\ncommerce, and online movie databases. However, it only \\nlinks the object in a single domain rather than data across \\ndifferent  domains.  For  instance,  in  a  bibliographic  infor-\\nmation  network,  people,  papers,  and  conferences  are  all \\nfrom a bibliographic domain. In a Flickr information net-\\nwork, users, images, tags, and comments are all from a so-\\ncial media domain. If we want to fuse data across totally \\ndifferent  domains,  e.g.  traffic  data,  social  media,  and  air \\nquality, such a heterogeneous network may not be able to \\nfind explicit links with semantic meanings between objects \\nof different domains. Consequencely, algorithms proposed \\nfor mining heterogeneous information networks cannot be \\napplied to cross-domain data fusion directly.   \\n\\n3 STAGE-BASED DATA FUSION METHODS \\n\\nThis category of methods uses different datasets at the dif-\\nferent  stages  of  a  data  mining  task.  So,  different  datasets \\nare loosely coupled, without any requirements on the con-\\nsistency of their modalities.  \\n     Example 1: As illustrated in Fig. 2 A), Zheng et al.  [86] \\n\\n \\n\\nDataset ADataset BDataset CSchema MappingSchema MappingSchema MappingDuplicate DetectionKnowledge ExtractionKnowledge ExtractionKnowledge ExtractionKnowledge FusionKnowledgeKnowledgeKnowledgeDataset ADataset BDataset CDomain SDomain BDomain CDomain AData MergeA) Paradigm of the conventional data fusionB) Paradigm of data fusion for (cross-domain) big dataObjectLatent Object\\x0cYU ZHENG:  METHODOLOGIES FOR CROSS-DOMAIN DATA FUSION: AN OVERVIEW 3 \\n\\nfirst  partition  a  city  into  regions  by  major  roads  using  a \\nmap  segmentation  method  [75].  The  GPS  trajectories  of \\ntaxicabs are then mapped onto the regions to formulate a \\nregion  graph,  as  depicted  in  Fig.  2  B),  where  a  node  is  a \\nregion and an edge denotes the aggregation of commutes \\n(by  taxis  in  this  case)  between  two  regions.  The  region \\ngraph actually blends knowledge from the road network \\nand taxi trajectories. By analyzing the region graph, a body \\nof research has been carried out to identify the improper \\ndesign of a road network [86], detect and diagnose traffic \\nanomalies [15][43] as well as find urban functional regions \\n[74][76]. \\n\\nthat people have posted at the locations when the anomaly \\nwas happening. From the retrieved social media, they then \\ntry to describe the detected anomaly by mining representa-\\ntive terms, e.g. \\u201cparades\\u201d and \\u201ddisasters\\u201d, which barely oc-\\ncur in normal days but become frequent when the anomaly \\nincurs. The first step scales down the scope of social media \\nto be checked, while the second step enriches the semantic \\nmeaning of the results detected by the 1st step. \\n\\nFig. 2. Map partition and graph building \\n\\n \\n\\n      Example 2: In friend recommendation, as illustrated in \\nFig. 3, Xiao et al. [67][68] first detect the stay points from \\nan individual\\u2019s location history (recorded in a form of spa-\\ntial trajectories). As different users\\u2019 location histories may \\nnot  have  any  overlaps  in  the  physical  world,  each  stay \\npoint is then converted into  a feature vector based on its \\nsurrounding POIs. For example, there are five restaurants, \\n1 shopping mall and 1 gas station around a stay point. In \\nother words, the distance between these feature vectors de-\\nnotes the similarity between the places people have visited.  \\n    Later, these stay points are hierarchically clustered into \\ngroups according to their feature vectors of POIs, formu-\\nlating  a  tree  structure,  where  a  node  is  a  cluster  of  stay \\npoints; a parent node is comprised of the stay points from \\nits  children  nodes.  By  selecting  the  nodes  (from  the  tree) \\nthat a user has at least one stay point in, we can represent \\nthe user\\u2019s location history with a partial tree. A user\\u2019s par-\\ntial tree is further converted into a hierarchical graph, by \\nconnecting two nodes (on the same layer) with an edge, if \\nthe user has two consecutive stay points occurring in the \\ntwo  nodes.  So,  the  hierarchical  graph  contains  the  infor-\\nmation  of  a  user\\u2019s  trajectories  and  the  POIs  of  the  places \\nthe user has visited.  Because different users\\u2019 hierarchical \\ngraphs are built based on the same tree structure, their lo-\\ncation histories become comparable. Finally, the similarity \\nbetween two users can be measured by the similarity be-\\ntween their hierarchical graphs. \\n     Example 3: In the third example, Pan et al. [49] first detect \\na traffic anomaly based on GPS trajectories of vehicles and \\nroad network data. An anomaly is represented by a sub-\\ngraph of a road network where drivers\\u2019 routing behaviors \\nsignificantly differ from their original patterns. Using the \\ntime span of the detected anomaly and the names of loca-\\ntions fallen in the anomaly\\u2019s geographical scope as condi-\\ntions, they retrieve the relevant social media (like tweets) \\n\\n \\nFig. 3. Estimate user similarity using trajectories and POIs \\n\\n    Stage-based  data  fusion  methods  can  be  a  meta-ap-\\nproach used together with other data fusion methods. For \\nexample, Yuan et al. [76] first use road network data and \\ntaxi trajectories to build a region graph, and then propose \\na graphical model to fuse the information of POIs and the \\nknowledge of the region graph. In the second stage, a prob-\\nabilistic-graphical-model-based  method  is  employed  in \\nthe framework of the stage-based method. \\n\\n4. FEATURE-LEVEL-BASED DATA FUSION  \\n\\n4.1 Direct Concatenation \\nStraightforward methods [66][70] in this category treat fea-\\ntures extracted from different datasets equally, concatenat-\\ning  them  sequentially  into  a  feature  vector.  The  feature \\nvector is then used in clustering and classification tasks. As \\nthe  representation,  distribution  and  scale  of  different  da-\\ntasets may be very different, quite a few studies have sug-\\ngested limitations to this kind of fusion [5] [46][56]. First, \\nthis concatenation causes over-fitting in the case of a small \\nsize training sample, and the specific statistical property of \\neach view is ignored [69]. Second, it is difficult to discover \\nhighly  non-linear  relationships  that  exist  between  low-\\nlevel features across different modalities [56]. Third, there \\nare redundancies and dependencies between features ex-\\ntracted from different datasets which may be correlated.  \\n     Advanced learning methods [4][61][62] in this sub-cate-\\ngory suggest adding a sparsity regularization in an objec-\\ntive  function  to  handle  the  feature  redundancy  problem. \\n\\n \\n\\nA) Map segmentationB) Region graphabc2\\u03b3Geographic spacesFeature spacesFeature spaceHierarchical clusteringLocation category hierarchyC1C2C3Layer 2Clusters at a layer Location history of User 1Location history of User mFeature spaceLayer 3Layer 1Feature spaceLayer 2Layer 3Layer 1User 1Feature vectors of stay regionsFeature vectors of stay regions User m2\\u03b3a stay pointa stay regiona feature vectorc32c33c30c20c21Tramc20c21c32c31c30c34c33\\x0c4 \\n\\nIEEE TRANSACTIONS ON BIG DATA, MANUSCRIPT ID \\n\\n\\U0001d45a\\n\\n\\U0001d45a\\n\\n;    (1) \\n\\n2, \\U0001d6fd2\\n\\n2, \\u2026 , \\U0001d6fd\\U0001d45a\\n\\n2 |\\U0001d44e, \\U0001d44f)\\n\\n\\u220f \\U0001d43c\\U0001d45b\\U0001d463\\U0001d452\\U0001d45f\\U0001d460\\U0001d452 \\u2212 \\U0001d43a\\U0001d44e\\U0001d45a\\U0001d45a\\U0001d44e(\\U0001d6fd\\U0001d45a\\n\\nAs a result, a machine learning model is likely to assign a \\nweight close to zero to redundant features.  \\n      Example 4: Fu et al. [20] feed \\U0001d45a features extracted from \\ndisparate datasets, such as taxi trajectories, POIs, road net-\\nworks, and online social media, into a learn-to-rank model \\nto predict the ranking (in terms of its potential investment \\nvalue)  of  a  residential  real  estate.  Equation  1  is  added  to \\nthe  learning-to-rank  objective  function  to  enforce  sparse \\nrepresentations during learning. \\n    \\U0001d443(\\u03a8|\\u03a9) = \\U0001d443(\\U0001d74e|0, \\U0001d7372)\\U0001d443(\\U0001d7372|\\U0001d44e, \\U0001d44f)  \\n2 )\\n    = \\u220f \\U0001d441(\\U0001d714\\U0001d45a|0, \\U0001d6fd\\U0001d45a\\nwhere  \\U0001d74e = (\\U0001d7141, \\U0001d7142, \\u2026 , \\U0001d714\\U0001d45a)  is  a  parameter  vector  of  fea-\\ntures, \\U0001d45a is  the  number  of  features  involved  in  a  learning \\nmodel,  \\U0001d7372 = (\\U0001d6fd1\\n2 )  is  the  variance  vector  of  the \\ncorresponding parameters. More specifically, the value of \\na parameter \\U0001d714\\U0001d45a is assumed following a Guassian distribu-\\n2 . Setting a zero mean \\ntion with a zero mean and variance \\U0001d6fd\\U0001d45a\\nfor the distribution reduces the probability of assigning \\U0001d714\\U0001d45a \\na big value. A prior distribution, e.g. an inverse gamma, is \\n2 . To strengthen \\nfurther placed to regularize the value of \\U0001d6fd\\U0001d45a\\nthe sparsity, the constants \\U0001d44e and \\U0001d44f are usually set close to \\n2  tends  to  be  small.  In  other  words,  feature \\nzero.  Thus, \\U0001d6fd\\U0001d45a\\nweight \\U0001d714\\U0001d45a has a very high probability of varying around \\nthe  Gaussian  expectation,  i.e.  zero.  Through  such  a  dual \\nregularization  (i.e.,  zero-mean  Gaussian  plus  inverse-\\ngamma),  we  can  simultaneously  regularize  most  feature \\nweights  to  be  zero  or  close  to  zero  via  a  Bayesian  sparse \\nprior, while allowing for the possibility of a model learning \\nlarge  weights  for  significant  features.  In  addition,  the \\nBayesian  sparse  prior  is  a  smooth  function,  and  thus  its \\ngradient  is  easy  to  compute.  Given  that  many  objective \\nfunctions are solved by gradient descent, the sparse regu-\\nlarization  can  be  applied  to  many  data  mining  tasks.     \\nHowever, the sparsity regularization of a Bayesian sparse \\nprior is not as strong as L1 regularization.  \\n\\n4.2 DNN-Based Data Fusion \\n     Recently, more advanced methods have been proposed \\nto learn a unified feature representation from disparate da-\\ntasets based on DNN. DNN is actually not fundamentally \\nnew in artificial intelligence. As depicted in Fig. 4 A), it is \\nbasically  a  multiple-layer  neural  network  containing  a \\nhuge number of parameters. Previously, a neural network \\nis  trained  based  on  a  back-propagation  algorithm,  which \\ndoes  not  work  well  when  the  neural  network  has  many \\nhidden  layers.  Recently,  new  learning  algorithms  (a.k.a. \\ndeep learning), such as autoencoder and Restricted Boltz-\\nmann  Machines  (RBM),  have  been  proposed  to  learn  the \\nparameters of a DNN layer by layer. Using supervised, un-\\nsupervised and semi-supervised approaches, Deep Learn-\\ning learns multiple levels of representation and abstraction \\nthat help make sense of data, such as images, sound, and \\ntext. Besides being a predictor, DNN is also used to learn \\nnew  feature  representations  [8],  which  can  be  fed  into \\nother classifiers or predictors. The new feature representa-\\ntions have proven more useful than hand-crafted features \\nin  image  recognition  [37]  and  speech  translations  [12].  A \\ntutorial on DNN can be found in [40], and a survey on fea-\\nture representation using DNN can be found in [8]. \\n\\n      The majority of DNN is applied to handle data with a \\nsingle  modality.  More  recently,  a  series  of  research  [46] \\n[52][56][55]  starts  using  DNN  to  learn  feature  presenta-\\ntions from data with different modalities. This representa-\\ntion  was  found  to  be  useful  for  classification  and  infor-\\nmation retrieval tasks.  \\n     Example 5: Ngiam et al. [46] propose a deep autoencoder \\narchitecture  to  capture  the  \\u201cmiddle-level\\u201d  feature  repre-\\nsentation between two modalities (e.g., audio and video). \\nAs shown in Table 1, three learning settings (consisting of \\ncross-modality  learning,  shared  representation  learning, \\nand multi-modal fusion) are studied.  Fig. 4 B) presents the \\nstructure  of  the  deep  autoencoder  for  the  cross-modality \\nlearning, where a single modality (e.g. video or audio) is \\nused as the input to reconstruct a better feature represen-\\ntation for video and audio respectively.  W.r.t. the shared \\nrepresentation learning and multi-modal fusion, which in-\\nvolve different modalities during training and testing, the \\npaper adopts the architecture  shown  in Fig. 4. C).  Exten-\\nsive evaluations on these proposed deep learning models \\ndemonstrate that deep learning effectively learns 1) a bet-\\nter  single  modality  representation  with  the  help  of  other \\nmodalities; and 2) the shared representations capturing the \\ncorrelations across multiple modalities. \\n      Deep  learning  using  Boltzmann  Machines  is  another \\npiece  of  work  on  multi-modality  data  fusion.  Paper  [56] \\nfirst defines three criteria for a good multi-modality learn-\\ning  model:  1)  the  learned  shared  feature  representation \\npreserves the similarity of \\u201cconcepts\\u201d; 2) the joint feature \\nrepresentation is easy to obtain in the absence of some mo-\\ndalities, and thus fills in missing modalities; 3) the new fea-\\nture  representation  facilitates  retrieval  of  one  modality \\nwhen  querying  from  the  other.  A  deep  learning  model, \\ncalled  multimodal  Deep  Boltzmann  Machine  (DBM),  is \\nproposed,  to  fuse  images  and  texts  for  classification  and \\nretrieval  problems.  The  proposed  DBM  model  also  satis-\\nfies the three criteria. \\n\\nTable 1. Multi-Modal Feature Represenation Learning [46] \\n\\nFeature learn-\\n\\nSupervised \\n\\n \\n\\nClassic deep learning \\n\\nCross modality learn-\\n\\ning \\n\\nShared representation \\n\\nlearning \\n\\nMulti-modal fusion \\n\\ning \\n\\nAudio \\n\\nVideo \\n\\nA + V \\n\\nA + V \\n\\nA + V \\n\\nA + V \\n\\nA + V \\n\\ntraining \\n\\nAudio \\n\\nVideo \\n\\nA \\n\\nV \\n\\nA \\n\\nV \\n\\nTesting \\n\\nAudio \\n\\nVideo \\n\\nA \\n\\nV \\n\\nV \\n\\nA \\n\\nA + V \\n\\nA + V \\n\\n     Example 6: As shown in Fig. 4 D), the multimodal DBM \\nutilizes  Gaussian-Bernoulli  RBM  (Restricted  Boltzmann \\nMachine) to model dense real-valued image features vec-\\ntors, while employing replicated softmax to model sparse \\nword  count  vectors.  The  multimodal  DBM  constructs  a \\nseparate two-layer DBM for each modality and then com-\\nbines  them  by  adding  a  layer  on  top  of  them.  Moreover, \\nthe multimodal DBM is a generative and undirected grap-\\nhic model with bipartite connections between adjacent lay-\\ners.  This  graphic  model  enables  a  bi-directional  (bottom-\\nup and top-down) search (denoted by the two red arrows). \\n\\n \\n\\n\\x0cYU ZHENG:  METHODOLOGIES FOR CROSS-DOMAIN DATA FUSION: AN OVERVIEW 5 \\n\\n \\n\\nFig. 4. Fusing multi-modality data using DNN\\n\\n \\n\\nArmed with a well-designed architecture, the key idea of \\nmultimodal  DBM  is  to  learn  a  joint  density  distribution \\nover texts and images, i.e. \\U0001d443(\\U0001d42f\\U0001d456\\U0001d45a\\U0001d454, \\U0001d42f\\U0001d461\\U0001d452\\U0001d465\\U0001d461; \\U0001d703) where \\U0001d703 include \\nthe  parameters,  from  a  large  number  of  user-tagged  im-\\nages. The paper performs extensive experiments on classi-\\nfication  as  well  as  retrieval  tasks.  Both  multimodal  and \\nunimodal inputs are tested, validating the effectiveness of \\nthe model in fusing multimodality data. \\n     In  practice,  the  performance  of  a  DNN-based  fusion \\nmodel usually depends on how well we can tune parame-\\nters for the DNN. Finding a set of proper parameters can \\nlead  to  a  much  better  performance  than  others.  Given  a \\nlarge  number  of  parameters  and  a  non-convex  optimiza-\\ntion setting, however, finding optimal parameters is still a \\nlabor-intensive  and  time-consuming  process  that  heavily \\nrelys on human experiences. In addition, it is hard to ex-\\nplain what the middle-level feature representation stands \\nfor. We do not really understand the way a DNN makes \\nraw features a better representation either. \\n\\n5. SEMANTIC MEANING-BASED DATA FUSION \\n\\nFeature-based data fusion methods (introduced in Section \\n4) do not care about the meaning of each feature, regarding \\na  feature  solely  as  a  real-valued  number  or  a  categorical \\nvalue.  Unlike  feature-based  fusion,  semantic  meaning-\\nbased methods understand the insight of each dataset and \\nrelations  between  features  across  different  datasets.  We \\nknow what each dataset stands for, why different datasets \\ncan be fused, and how they reinforce between one another. \\nThe process of data fusion carries a semantic meaning (and \\ninsights)  derived  from  the  ways  that  people  think  of  a \\nproblem with the help of multiple datasets. Thus, they are \\ninterpretable and meaningful. This section introduces four \\ngroups  of  semantic  meaning-based  data  fusion  methods: \\nmulti-view-based,  similarity-based,  probabilistic  depend-\\nency-based, and transfer-learning-based methods.  \\n\\n5.1 Multi-View Based Data Fusion \\nDifferent datasets or different feature subsets about an ob-\\nject  can  be  regarded  as  different  views  on  the  object.  For \\nexample, a person can be identified by the information ob-\\ntained from multiple sources, such as face, fingerprint, or \\nsignature.  An  image  can  be  represented  by  different  fea-\\nture sets like color or texture features. As these datasets de-\\nscribe the same object, there is a latent consensus among \\n\\nthem.  On  the  other  hand,  these  datasets  are  complemen-\\ntary to each other, containing knowledge that other views \\ndo not have. As a result, combining multiple views can de-\\nscribe an object comprehensively and accurately.       \\n      According to [69], the multi-view learning algorithms \\ncan be classified into three groups: 1) co-training, 2) multi-\\nple kernel learning, and 3) subspace learning. Notably, co-\\ntraining style algorithms [11] train alternately to maximize \\nthe  mutual  agreement  on  two  distinct  views  of  the  data. \\nMultiple  kernel  learning  algorithms  [23]  exploit  kernels \\nthat naturally correspond to different views and combine \\nkernels either linearly or non-linearly to improve learning. \\nSubspace  learning  algorithms  [16]  aim  to  obtain  a  latent \\nsubspace shared by multiple views, assuming that the in-\\nput views are generated from this latent subspace.  \\n\\n5.1.1. Co-Training  \\nCo-training [11] was one of the earliest schemes for multi-\\nview  learning.  Co-training  considers  a  setting  in  which \\neach  example  can  be  partitioned  into  two  distinct  views, \\nmaking three main assumptions: 1) Sufficiency: each view \\nis sufficient for classification on its own, 2) Compatibility: \\nthe target functions in both views predict the same labels \\nfor  co-occurring  features  with  high  probability,  and  3) \\nConditional independence: the views are conditionally in-\\ndependent given the class label. The conditional independ-\\nence  assumption  is  usually  too  strong  to  be  satisfied  in \\npractice.  Consequently,  several  weaker alternatives  [1][5] \\nhave thus been considered. \\n     In the original co-training algorithm [11], given a set \\U0001d43f \\nof labeled examples and a set \\U0001d448 of unlabeled examples, the \\nalgorithm first creates a smaller pool \\U0001d448\\u2032 containing \\U0001d462 unla-\\nbeled examples. It then iterates the following procedures. \\nFirst, use \\U0001d43f to train two classifiers \\U0001d4531 and \\U0001d4532 on the view \\U0001d4631 \\nand \\U0001d4632 respectively. Second, allow each of these two clas-\\nsifiers to examine the unlabeled set \\U0001d448\\u2032 and add the \\U0001d45d exam-\\nples it most confidently labels as positive, and \\U0001d45b examples \\nit most confidently labels as negative to \\U0001d43f, along with the \\nlabels assigned by the corresponding classifier. Finally, the \\npool \\U0001d448\\u2032 is replenished by drawing 2\\U0001d45d + 2\\U0001d45b examples from \\n\\U0001d448 at  random.  The  intuition  behind  the  co-training  algo-\\nrithm is that classifier \\U0001d4531 adds examples to the labeled set \\nthat classifier \\U0001d4532 will then be able to use for learning. If the \\nindependence assumption is violated, on average the add-\\ned  examples  will  be  less  informative.  Thus,  Co-training \\nmay not be that successful. Since then, many variants have \\n\\n \\n\\nA) DNNB) Video-only Deep AutoencoderVideo InputAudio ReconstructionVideo ReconstructionC) Bimodal Deep AutoencoderVideo InputAudio ReconstructionVideo ReconstructionAudio InputImage FeaturesDense and Real-valuedText FeaturesWord countsGaussian ModelReplicated SoftmaxBottom-Up + Top-DownD) Bi-direction Feature Representation LearningShared representationShared representation\\x0c6 \\n\\nIEEE TRANSACTIONS ON BIG DATA, MANUSCRIPT ID \\n\\nbeen developed.  \\n      Instead of assigning labels to the unlabeled examples, \\nNigam et al. [47] give unlabeled examples probabilistic la-\\nbels that may change from one iteration to another by run-\\nning  EM  (Expectation  and  Maximization)  in  each  view. \\nThis algorithm, called Co-EM, outperforms co-training for \\nmany problems, but requires the classifier of each view to \\ngenerate  class  probabilities.  By  reformulating  the  SVM \\n(supported vector machine) in a probabilistic way, Brefeld \\net al. [12] develop a co-EM version of SVM to close this gap. \\nZhou  et  al.  [92]  expand  the  co-training  style  algorithms \\nfrom  classification  to  regression  problems.  They  propose \\nan algorithm, called CoREG, which employs two k-nearest \\nneighbor (kNN) regressors. Each regressor labels the unla-\\nbeled  data  for  the  other  during  the  learning  process.  For \\nthe sake of choosing the appropriate unlabeled examples \\nto label, CoREG estimates the labeling confidence by con-\\nsulting the influence of the labeling of unlabeled examples \\non the labeled examples. The final prediction is made by \\naveraging  the  regression  estimates  generated  by  both  re-\\ngressors. \\n     Example  7:  Zheng  et  al.  [76][85]  propose  a  co-training-\\nbased model to infer the fine-grained air quality through-\\nout a city based on five datasets: air quality, meteorological \\ndata, traffic, POIs and road networks. Fig. 5 A) illustrates \\nthe  philosophy  of  the  model  from  multi-view  learning\\u2019s \\nperspective.  Naturally,  air  quality  has  temporal  depend-\\nency in an individual location (represented by the broken \\nblack arrows) and the spatial correlation among different \\nlocations (denoted by the red solid arrows). For example, \\nthe current air quality of a location depends on past hours. \\nIn addition, the air quality of a place could be bad if the air \\nquality of its surrounding locations is bad. So, the temporal \\ndependency and spatial correlation formulate two distinct \\nviews (a temporal view and a spatial view) on the air qual-\\nity of a location.  \\n      As  presented  in  Fig.  5  B),  a  co-training-based  frame-\\nwork  is  proposed,  consisting  of  two  classifiers.  One  is  a \\nspatial  classifier  based  on  an  artificial  neural  network \\n(ANN),  which  takes  spatially-related  features  (e.g.,  the \\ndensity of POIs and length of highways) as input to model \\nthe spatial correlation between air qualities of different lo-\\ncations. The other is a temporal classifier based on a linear-\\nchain  conditional  random  field  (CRF),  involving  tempo-\\nrally-related  features  (e.g.,  traffic  and  meteorology)  to \\nmodel the temporal dependency of air quality in a location. \\nThe  two  classifiers  are  first  trained  based  on  limited  la-\\nbeled data using non-overlapped features, and then infer \\nunlabeled  instances  respectively.  The  instances  that  are \\nconfidently  inferred  by  a  classifier  in  each  round  are \\nbrought to the training set, which will be used to re-train \\nthe two classifiers in the next round. The iteration can be \\nstopped until the unlabeled data has been consumed out \\nor  the  inference  accuracy  does  not  increase  any  more. \\nWhen inferring the label of an instance, we send different \\nfeatures to different classifiers, generating two sets of prob-\\nabilities  across  different  labels.  The  label  that  maximizes \\nthe production of the corresponding probabilities from the \\ntwo classifiers is selected as the result. \\n\\nFig. 5 Co-training-based air quality inference model \\n\\n \\n\\n     The  proposed  method  was  evaluated  based  on  data \\nfrom four cities, showing its advantages beyond four cate-\\ngories of baselines: interpolation-based methods, classical \\ndispersion  models,  well-known  classification  models  like \\ndecision tree and CRF, and ANNs. In the later two catego-\\nries of baselines, all the features are fed into a single model \\nwithout  differentiating  between  their  semantic  meanings \\nand views.  \\n\\n5.1.2. Multi-Kernel Learning \\nMultiple Kernel Learning (MKL) refers to a set of machine \\nlearning methods that uses a predefined set of kernels and \\nlearns an optimal linear or non-linear combination of ker-\\nnels as part of the algorithm. A kernel is a hypothesis on \\nthe data, which could be a similarity notion, or a classifier, \\nor a regressor. According to [23], there are two uses of MKL \\n(as shown in Fig. 6):  \\n\\n \\n\\nFig. 6. Procedure of Multi-Kernel Learning       \\n\\n    a)  Different  kernels  correspond  to  different  notions  of \\nsimilarity. A learning method picks the best kernel, or uses \\na  combination  of  these  kernels.  A  sample  of  data  is  re-\\ntrieved from the entrie set to train a kernel based on all fea-\\ntures. While using a specific kernel may be a source of bias, \\nallowing a learner to choose among a set of kernels can re-\\nsult in a better solution. For example, there are several ker-\\nnel functions, such as the linear, polynomial and Gaussian \\nkernels, successfully used in SVM. This kind of MKL was \\nnot originally designed for multi-view learning, as the en-\\ntire feature set is used for training each kernel.  b) A varia-\\ntion of the first use of MKL is to train different kernels us-\\ning inputs coming from different representations possibly \\nfrom different sources or modalities. Since these are differ-\\nent representations, they have different measures of simi-\\nlarity  corresponding  to  different  kernels.  In  such  a  case, \\ncombining kernels is one possible way to combine multiple \\ninformation sources. The reasoning is similar to combining \\ndifferent  classifiers.  Noble  [48]  calls  this  method  of  com-\\nbining  kernels  intermediate  combination,  in  contrast  with \\nearly combination (where features from different sources are \\nconcatenated and fed to a single learner) and late combina-\\ntion (where different features are fed to different classifiers \\n\\n \\n\\ns2s1s3s4ls2s1s3s4ls2s1s3s4tit1t2lTimeA) Philosophy of the inference modelA location with AQI labels A location to be inferred Temporal dependencySpatial correlationPOIs Road NetworksMeteorologyTrafficHuman MobilitySpatial ClassifierTemporal ClassifierSpatial FeaturesTemporal FeaturesTime of DayAQI Labels Co-TrainingLabeled DataUnlabeled DataB) Procedure of co-trainingKernel 1Kernel 2Kernel nKernel LearningUnified KernelDataa) a sample of data b) a view on entire dataa) Linear b) Non-linear c) Data-dependent \\x0cYU ZHENG:  METHODOLOGIES FOR CROSS-DOMAIN DATA FUSION: AN OVERVIEW 7 \\n\\nwhose decisions are then combined by a fixed or  trained \\ncombiner). \\n      There are three ways to combine the results of kernels: \\nlinear, non-linear, and data-dependent combinations. The \\nlinear combination consists of unweighted (i.e. mean) and \\nweighted  sum.  Nonlinear  combination  methods  [63]  use \\nnonlinear  functions  of  kernels,  namely,  multiplication, \\npower, and exponentiation. Data-dependent combination \\nmethods  assign  specific  kernel  weights  for  each  data  in-\\nstance. By doing this, they can identify local distributions \\nin the data and learn proper kernel combination rules for \\neach region [23]. \\n\\nExisting  MKL  algorithms  have  two  main  groups  of \\ntraining methodology: 1)  One-step methods calculate  the \\nparameters of the combination function and base learners \\nin a single pass, using a sequential approach or a simulta-\\nneous approach. In the sequential approach, the combina-\\ntion function parameters are determined first, and then a \\nkernel-based learner is trained using the combined kernel. \\nIn the simultaneous approach, both set of parameters are \\nlearned together. 2) Two-step methods use an iterative ap-\\nproach. In each iteration, we first update the parameters of \\nthe  combination  function  while  fixing  that  of  the  base \\nlearner.  We  then  update  the  parameters  of  base  learners \\nwhile  fixing  the  parameters  of  the  combination  function. \\nThese two steps are repeated until convergence. \\n       Example 8: Ensemble and boosting methods [1], such as \\nRandom Forest [13], are inspired by MKL. Random Forest \\ncombines  the  idea  of  Bootstrap  Aggregating  (also  called \\nBagging) and the random selection of features [27][28], in \\norder to construct a collection of decision trees with a con-\\ntrolled variance. More specifically, it trains multiple Deci-\\nsion Trees by selecting a portion of training data each time \\nbased  on  Bagging  and  a  portion  of  features  according  to \\nthe  principle  introduced  by  [27][28].  When  a  test  case \\ncomes, different selections of the case\\u2019s features are sent to \\ncorresponding  Decision  Trees  (i.e.  kernels)  simultane-\\nously. Each kernel generates a prediction, which is then ag-\\ngregated linearly.  \\n\\nExample 9: Zheng et al. [89] forecast air quality for the \\nnext  48  hours  of  a  location  based  on  five  datasets.  Fig.  7 \\npresents  the  architecture  of  the  predictive  model,  which \\ncontains two kernels (spatial predictor and temporal pre-\\ndictor) and a kernel learning module (i.e. the prediction ag-\\ngregator). The Temporal Predictor predicts the air quality \\nof a station in terms of the data about the station, such as \\nthe local meteorology, AQIs of the past few hours and the \\nweather forecast of the place. Instead, the Spatial Predictor \\nconsiders the spatial neighbor data, such as the AQIs and \\nthe wind speed at the other stations, to predict a station\\u2019s \\nfuture air quality. The two predictors generate their own \\npredictions  independently  for  a  station,  which  are  com-\\nbined by the Prediction Aggregator dynamically according \\nto  the  current  weather  conditions  of  the  station.  Some-\\ntimes,  local  prediction  is  more  important,  while  spatial \\nprediction should be given a higher weight on other occa-\\nsions (e.g. when wind blows strongly). The Prediction Ag-\\ngregator  is  based  on  a  Regression  Tree,  learning  the  dy-\\nnamic combination between the two kernels from the data. \\n\\n \\nFig. 7. MKL-based framework for forecasting air quality \\n\\nThe  MKL-based  framework  outperforms  a  single  ker-\\nnel-based model in the air quality forecast example, for the \\nfollowing three reasons: 1) From the feature space\\u2019s perspec-\\ntive: The features used by the spatial and temporal predic-\\ntors do not have any overlaps,  providing different views \\non a station\\u2019s air quality. 2) From the model\\u2019s perspective: The \\nspatial and temporal predictors model the local factors and \\nglobal factors respectively, which have significantly differ-\\nent properties. For example, the local is more about a re-\\ngression  problem,  while  the  global  is  more  about  a  non-\\nlinear interpolation. Thus, they should be handled by dif-\\nferent  techniques.  3)  From  the  parameter  learning\\u2019s  perspec-\\ntive: Feeding all the features into a single model results in \\na big model with many parameters to learn. However, the \\ntraining data is limited. For instance, we only have one and \\nhalf year AQI data of a city. Decomposing a big model into \\nthree  organically  coupled  small  models  scales  down  the \\nparameter spaces tremendously, leading to more accurate \\nlearning and therefore the prediction. \\n\\n5.1.3. Subspace Learning \\nSubspace learning-based approaches aim to obtain a latent \\nsubspace shared by multiple views by assuming that input \\nviews  are  generated  from  this  latent  subspace,  as  illus-\\ntrated in Fig. 8. With the subspace, we can perform subse-\\nquent tasks, such as classification and clustering. Addition-\\nally,  as  the  constructed  subspace  usually  has  a  lower  di-\\nmensionality than that of any input view, the \\u201ccurse of di-\\nmensionality\\u201d problem can be solved to some extent. \\n\\n \\n\\nFig.8. Concept of subspace learning \\n\\n     In the literature on single-view learning, principal com-\\nponent  analysis  (PCA)  is  a  widely-used  technique  to  ex-\\nploit the subspace for single-view data. Canonical correla-\\ntion analysis (CCA) [25] can be regarded as the multi-view \\nversion  of  PCA.  Through  maximizing  the  correlation  be-\\ntween two views in the subspace, CCA outputs one opti-\\nmal projection on each view. The subspace constructed by \\nCCA  is  linear,  and  thus  cannot  be  straightforwardly  ap-\\nplied to non-linearly embedded datasets. To address this \\n\\n \\n\\nTemporal PredictorInflection PredictorSpatial PredictorLocal DataShape featuresRecent MeteorologyWeather ForecastRecent AQI AQI AQIPrediction AggregatorSpatial Neighbor Data AQIRecent MeteorologySelected  factorsRecent AQIThresholdFinal AQI AQIAQIDataSpace 1Space nLatent SubSpace\\x0c8 \\n\\nIEEE TRANSACTIONS ON BIG DATA, MANUSCRIPT ID \\n\\nissue, the kernel variant of CCA, namely KCCA [38], was \\nproposed to map each (non-linear) data point to a higher \\nspace in which linear CCA operates. Both CCA and KCCA \\nexploit  the  subspace  in  an  unsupervised  way.  Motivated \\nby  the  generation  of  CCA  from  PCA,  multi-view  Fisher \\ndiscriminant analysis [33] is developed to find informative \\nprojections with label information. Lawrence [39] casts the \\nGaussian  process  as  a  tool  to  construct  a  latent  variable \\nmodel  which  could  accomplish  the  task  of  non-linear  di-\\nmensional reduction. Chen et al. [16] develop a statistical \\nframework  that  learns  a  predictive  subspace  shared  by \\nmultiple views based on a generic multi-view latent space \\nMarkov network. \\n\\n5.2 Similarity-Based Data Fusion \\nSimilarity  lies  between  different  objects.  If  we  know  two \\nobjects (X, Y) are similar in terms of some metric, the infor-\\nmation of X can be leveraged by Y when Y is lack of data. \\nWhen X and Y have multiple datasets respectively, we are \\ncan  learn  multiple  similarities  between  the  two  objects, \\neach of which is calculated based on a pair of correspond-\\ning datasets. These similarities can mutually reinforce each \\nother,  consolidating  the  correlation  between  two  objects \\ncollectively. The latter enhances each individual similarity \\nin turn. For example, the similarity learned from a dense \\ndataset can reinforce those derived from other sparse da-\\ntasets, thus helping fill in the missing values of the latter. \\nFrom another perspective, we can say we are more likely \\nto  accurately  estimate  the  similarity  between  two  objects \\nby combining multiple datasets of them. As a result, differ-\\nent datasets can be blended together based on similarities. \\nCoupled matrix factorization and manifold alignment are \\ntwo types of representative methods in this category. \\n\\n5.2.1. Coupled Matrix Factorization \\nBefore elaborating on the coupled matrix factorization, we \\nneed to introduce two concepts. One is collaborative filter-\\ning (CF); the other is matrix factorization. The latter can be \\nan efficient approach to the implementation of CF models. \\n\\n5.2.1.1 Collaborative Filtering  \\nCF  is  a  well-known  model  widely  used  in  recommender \\nsystems. The general idea behind collaborative filtering is \\nthat similar users make ratings in a similar manner for sim-\\nilar  items  [21].  Thus,  if  similarity  is  determined  between \\nusers and items, a potential prediction can be made as to \\nthe rating of a user with regards to future items. Users and \\nitems are generally organized by a matrix, where an entry \\ndenotes  a  user\\u2019s  rating  on  an  item.  The  rating  can  be  ex-\\nplicit rankings or implicit indications, such as the number \\nof visits to a place or the times that a user has browsed an \\nitem. Once formulating a matrix, the distance between two \\nrows in the matrix denotes the similarity between two us-\\ners, while the distance between two columns stands for the \\nsimilarity between two items.  \\n      Memory-based  CF  is  the  most  widely-used  algorithm \\nthat computes the value of the unknown rating for a user \\nand an item as an aggregate of the ratings of some other \\n(usually, the N most similar) users for the same item. There \\nare  two  classes  of  memory-based  CF  models:  user-based \\n[45] and item-based [42] techniques. For example, user \\U0001d45d\\u2019s \\n\\ninterest  (\\U0001d45f\\U0001d45d\\U0001d456)  in  a  location \\U0001d456 can  be  predicted  according  to \\nEquation 1, which is an implementation of user-based col-\\nlaborative filtering [45][91]: \\n\\n          \\U0001d45f\\U0001d45d\\U0001d456 =  \\U0001d445\\U0001d45d\\u0305\\u0305\\u0305\\u0305 + \\U0001d451 \\u2211\\n\\n\\U0001d462\\U0001d45e\\u2208\\U0001d448\\u2032\\n\\n\\U0001d460\\U0001d456\\U0001d45a(\\U0001d462\\U0001d45d, \\U0001d462\\U0001d45e)\\n\\n\\xd7 (\\U0001d45f\\U0001d45e\\U0001d456 \\u2212  \\U0001d445\\U0001d45e\\u0305\\u0305\\u0305\\u0305);         (2) \\n\\n                         \\U0001d451 =\\n\\n                        \\U0001d445\\U0001d45d\\u0305\\u0305\\u0305\\u0305 =\\n\\n1\\n\\n|\\U0001d448\\u2032|\\n1\\n\\n|\\U0001d446(\\U0001d445\\U0001d45d)|\\n\\n\\u2211\\n\\n\\U0001d462\\U0001d45e\\u2208\\U0001d448\\u2032\\n\\n\\U0001d460\\U0001d456\\U0001d45a(\\U0001d462\\U0001d45d, \\U0001d462\\U0001d45e)\\n\\n;                     (3) \\n\\n\\u2211\\n\\n\\U0001d456\\u2208\\U0001d446(\\U0001d445\\U0001d45d)\\n\\n\\U0001d45f\\U0001d45d\\U0001d456\\n\\n,  ;                              (4) \\n\\nwhere \\U0001d460\\U0001d456\\U0001d45a(\\U0001d462\\U0001d45d, \\U0001d462\\U0001d45e) denotes  the  similarity  between  user \\U0001d462\\U0001d45d \\nand \\U0001d462\\U0001d45e;  \\U0001d445\\U0001d45e\\u0305\\u0305\\u0305\\u0305 and  \\U0001d445\\U0001d45d\\u0305\\u0305\\u0305\\u0305 mean  the  average  rating  of \\U0001d462\\U0001d45d and \\U0001d462\\U0001d45e \\nrespectively,  denoting  their  rating  scale; \\U0001d446(\\U0001d445\\U0001d45d) represents \\nthe  collection  of  items  rated  by \\U0001d462\\U0001d45d; \\U0001d448\\u2032 is  the  collection  of \\nusers  who  are  the  most  similar  to \\U0001d462\\U0001d45e. \\U0001d45f\\U0001d45e\\U0001d456 \\u2212  \\U0001d445\\U0001d45e\\u0305\\u0305\\u0305\\u0305 is  to  avoid \\nrating biases of different users. When the number of users \\nbecomes big, computing the similarity between each pair \\nof  users  is  impractical  for  a  real  system.  Given  that  the \\nnumber of items could be smaller than that of users, item-\\nbased CF, e.g. the Slop One algorithm [42], was proposed \\nto address this issue. When the number of users and num-\\nber  of  items  are  both  huge,  matrix  factorization-based \\nmethod is employed to implement a CF model. \\n\\n5.2.1.2 Matrix Factorization  \\nMatrix  factorization  decomposes  a  (sparse)  matrix \\U0001d44b into \\nthe production of two (low-rank) matrices, which denote \\nthe  latent  variables  of  users  and  items  respectively.  The \\nproduction of the two matrices can approximate matrix \\U0001d44b, \\ntherefore helping fill the missing values in \\U0001d44b. There are two \\nwidely used matrix factorization methods: Singular Value \\nDecomposition  (SVD)  [22][35]  and  non-negative  matrix \\nfactorization (NMF) [30][41].   \\n\\n \\n\\nFig. 9. SVD matrix factorization \\n\\n      1)  SVD  factorizes  an \\U0001d45a \\xd7 \\U0001d45b matrix \\U0001d44b into  the  produc-\\ntion  of  three  matrices \\U0001d44b = \\U0001d448\\u2211\\U0001d449\\U0001d447,  where \\U0001d448 is  a \\U0001d45a \\xd7 \\U0001d45a real \\nunitary matrix (a.k.a. left singular vectors), \\u2211 is an \\U0001d45a \\xd7 \\U0001d45b \\nrectangular diagonal matrix with non-negative real num-\\nbers on the diagonal (a.k.a. singular values); \\U0001d449\\U0001d447 is a \\U0001d45b \\xd7 \\U0001d45b \\nreal unitary matrix (a.k.a. left singular vectors). In practice, \\nas shown in Fig. 9, when trying to approximate matrix \\U0001d44b \\nby \\U0001d448\\u2211\\U0001d449\\U0001d447, we only need to keep top \\U0001d458 biggest singular val-\\nues in \\u2211 and the corresponding singular vectors in \\U0001d448 and \\n\\U0001d449.  SVD  has  some  good  properties.  First, \\U0001d448 and \\U0001d449 are  or-\\nthogonal  matrices;  i.e. \\U0001d448 \\u2219 \\U0001d448\\U0001d447 = \\U0001d43c  and \\U0001d449 \\u2219 \\U0001d449\\U0001d447 = \\U0001d43c  .  Second, \\nthe value of \\U0001d458 can be determined by \\u2211. For example, select \\nthe first \\U0001d458 diagonal entries (in \\u2211) whose sum is larger than \\n90% of the entire diagonal entries\\u2019 sum.  However, SVD is \\nmore computationally expensive and harder to parallelize, \\nas compared to NFM. \\n       2)  NFM  factorizes  an  \\U0001d45a \\xd7 \\U0001d45b  matrix  \\U0001d445  (with  \\U0001d45a  users \\nand \\U0001d45b items)  into  a  production  of  an \\U0001d45a \\xd7 \\U0001d43e matrix \\U0001d443 and \\n\\n \\n\\n\\x0cYU ZHENG:  METHODOLOGIES FOR CROSS-DOMAIN DATA FUSION: AN OVERVIEW 9 \\n\\n\\U0001d43e \\xd7 \\U0001d45b matrix \\U0001d444 , \\U0001d445 = \\U0001d443 \\xd7 \\U0001d444, with the property that all three \\nmatrices  have  no  negative  elements.  This  non-negativity \\nmakes the resulting matrices easier to inspect [30]. Addi-\\ntionally, non-negativity is inherent to the data being con-\\nsidered in many applications, such as location recommen-\\ndation [2][88], traffic estimation [53], and processing of au-\\ndio spectrums. Each row of matric \\U0001d443 denotes the latent fea-\\nture of a user; each column of matrix \\U0001d444 stands for the latent \\nfeature  of  an  item. \\U0001d43e can  be  significantly  smaller  than \\U0001d45a \\nand \\U0001d45b,  denoting  the  number  of  latent  features  for  a  user \\nand an item. To predict a rating of an item \\U0001d451\\U0001d457 by \\U0001d462\\U0001d456, we can \\ncalculate the dot product of the two vectors corresponding \\nto \\U0001d462\\U0001d456 and \\U0001d451\\U0001d457 as Equation 5.  \\n\\n\\U0001d447\\U0001d45e\\U0001d457 = \\u2211\\n\\U0001d458\\n\\U0001d458=1\\n\\n\\U0001d45d\\U0001d456\\U0001d458\\U0001d45e\\U0001d458\\U0001d457\\n\\n;                         (5) \\n\\n                               \\U0001d45f\\u0302\\U0001d456\\U0001d457 = \\U0001d45d\\U0001d456\\nTo find a proper \\U0001d443 and \\U0001d444, we can first initialize the two ma-\\ntrices  with  some  values  and  calculate  the  difference  be-\\ntween their product and \\U0001d445, as shown in Equation 6. We can \\n2  iteratively using gradient descent, \\nthen try to minimize \\U0001d452\\U0001d456\\U0001d457\\nwhich finds a local minimum of the difference. \\n\\n                       \\U0001d452\\U0001d456\\U0001d457\\n\\n2 = (\\U0001d45f\\U0001d456\\U0001d457\\u2212\\U0001d45f\\u0302\\U0001d456\\U0001d457)2 = (\\U0001d45f\\U0001d456\\U0001d457 \\u2212 \\u2211\\n\\n\\U0001d43e\\n\\U0001d458=1\\n\\n\\U0001d45d\\U0001d456\\U0001d458\\U0001d45e\\U0001d458\\U0001d457\\n\\n)2;           (6) \\n\\nSpecifically, to know in which direction we have to modify \\nthe values, we differentiate Equation 6 with respect to \\U0001d45d\\U0001d456\\U0001d458 \\nand \\U0001d45e\\U0001d458\\U0001d457 separately: \\n\\n                         \\n\\n                         \\n\\n2\\n\\U0001d715\\U0001d452\\U0001d456\\U0001d457\\n\\U0001d715\\U0001d45d\\U0001d456\\U0001d458\\n2\\n\\U0001d715\\U0001d452\\U0001d456\\U0001d457\\n\\U0001d715\\U0001d45e\\U0001d458\\U0001d457\\n\\n= \\u22122(\\U0001d45f\\U0001d456\\U0001d457\\u2212\\U0001d45f\\u0302\\U0001d456\\U0001d457)(\\U0001d45e\\U0001d458\\U0001d457) = \\u22122\\U0001d452\\U0001d456\\U0001d457\\U0001d45e\\U0001d458\\U0001d457;              (7) \\n\\n= \\u22122(\\U0001d45f\\U0001d456\\U0001d457\\u2212\\U0001d45f\\u0302\\U0001d456\\U0001d457)(\\U0001d45d\\U0001d456\\U0001d458) = \\u22122\\U0001d452\\U0001d456\\U0001d457\\U0001d45d\\U0001d456\\U0001d458;               (8) \\n\\nHaving obtained the gradient, we can now formulate the \\nupdate rules for  \\U0001d45d\\U0001d456\\U0001d458 and \\U0001d45e\\U0001d458\\U0001d457 as follows: \\n\\nAdvanced methods [80][53] use coupled matrix factoriza-\\ntion  (or  called  context-aware  matrix  factorization)  [54]  to \\naccommodate  different  datasets  with  different  matrices, \\nwhich  share  a  common  dimension  between  one  another. \\nBy  decomposing  these  matrices  collaboratively,  we  can \\ntransfer  the  similarity  between  different  objects  learned \\nfrom  a  dataset  to  another  one,  therefore  complementing \\nthe missing values more accurately. \\n\\n  Example 10: Zheng et al. [80] propose a coupled matrix \\nfactorization  method  to  enable  location-activity  recom-\\nmendation. As illustrated in Fig. 10, a location-activity ma-\\ntrix \\U0001d44b is  built  based  on  many  users\\u2019  location  histories.  A \\nrow  of \\U0001d44b stands  for  a  venue  and  a  column  represents  an \\nactivity (like shopping and dinning). An entry in matrix \\U0001d44b \\ndenotes  the  frequency  that  a  particular  activity  has  been \\nperformed in a particular location. If this location-activity \\nmatrix is completely filled, we can recommend a set of lo-\\ncations for a particular activity by retrieving the top k loca-\\ntions with a relatively high frequency from the column that \\ncorresponds  to  that  activity.  Likewise,  when  performing \\nactivity recommendation for a location, the top k activities \\ncan be retrieved from the row corresponding to the loca-\\ntion.  However,  the  location-activity  matrix  is  incomplete \\nand very sparse, as we only have a portion of users\\u2019 data \\n(and  an  individual  can  visit  very  few  locations).  Accord-\\ningly, a traditional CF model does not work very well in \\ngenerating quality recommendations. Solely factorizing \\U0001d44b \\ndoes not help much either as the data are over sparse. \\n\\n \\n\\n= \\U0001d45d\\U0001d456\\U0001d458 + 2\\U0001d452\\U0001d456\\U0001d457\\U0001d45e\\U0001d458\\U0001d457;                  (9) \\n\\nFig. 10 Coupled matrix factorization for recommendation \\n\\n                         \\U0001d45d\\U0001d456\\U0001d458\\n\\n\\u2032 = \\U0001d45d\\U0001d456\\U0001d458 + \\U0001d6fc\\n\\n                         \\U0001d45e\\U0001d458\\U0001d457\\n\\n\\u2032 = \\U0001d45e\\U0001d458\\U0001d457 + \\U0001d6fc\\n\\n2\\n\\U0001d715\\U0001d452\\U0001d456\\U0001d457\\n\\U0001d715\\U0001d45d\\U0001d456\\U0001d458\\n2\\n\\U0001d715\\U0001d452\\U0001d456\\U0001d457\\n\\U0001d715\\U0001d45e\\U0001d458\\U0001d457\\n\\n= \\U0001d45e\\U0001d458\\U0001d457 + 2\\U0001d452\\U0001d456\\U0001d457\\U0001d45d\\U0001d456\\U0001d458;                 (10) \\n\\nwhere \\U0001d6fc is  a  small  value  that  determines  the  rate  of  ap-\\nproaching the minimum. When optimizing \\U0001d45d\\U0001d456\\U0001d458, NFM fixes \\n\\U0001d45e\\U0001d458\\U0001d457 ,  vice  versa;  the  gradient  descent  is  performed  itera-\\n2  converges to its minimum. \\ntively until the total error \\u2211 \\U0001d452\\U0001d456\\U0001d457\\nTo avoid over fitting, a regularization is introduced to the \\nerror function.  \\n\\n           \\U0001d452\\U0001d456\\U0001d457\\n\\n2 = (\\U0001d45f\\U0001d456\\U0001d457 \\u2212 \\u2211\\n\\U0001d43e\\n\\U0001d458=1\\n\\n\\U0001d45d\\U0001d456\\U0001d458\\U0001d45e\\U0001d458\\U0001d457\\n\\n)2 +\\n\\n\\U0001d6fd\\n\\n2\\n\\n\\u2211 (\\u2016\\U0001d443\\u20162 + \\u2016\\U0001d444\\u20162\\n\\n\\U0001d43e\\n\\U0001d458=1\\n\\n);    (11) \\n\\nAs compare to SVD, NFM is flexible and can be parallel-\\nized, but it is less precise.  \\n\\n5.2.1.3. Coupled Matrix Factorization \\n\\nDepending on applications, an item can also be a location \\n[2][88][91],  a  website,  or  a  company,  while  users  can  be \\ndrivers, or passengers, or subscribers of a service. We can \\neven generalize a user to an object and an item to a prop-\\nerty  of  the  object.  When  there  are  multiple  datasets  con-\\ncerning an object, we cannot simply deposit different prop-\\nerties from different sources into a single matrix. As differ-\\nent  datasets  have  different  distributions  and  meanings, \\nfactorizing them in a single matrix would lead to an inac-\\ncurate  complementation  of  missing  values  in  the  matrix. \\n\\nTo address this issue, the information from another two \\nmatrices (\\U0001d44c and \\U0001d44d), respectively shown in the left and right \\npart of Fig. 10, are incorporated into the matrix factoriza-\\ntion. One is a location-feature matrix; the other is an activ-\\nity-activity  matrix.  Such  kind  of  additional  matrices  are \\nusually  called  contexts,  which  can  be  learned  from  other \\ndatasets. In this example, matrix \\U0001d44c, where a row stands for \\na location and a column denotes a category of POIs (such \\nas restaurants and hotels) that fall in the location, is built \\nbased on a POI database. The distance between two rows \\nof matrix \\U0001d44c denotes the similarity between two locations in \\nterms of their geographical properties. The insight is that \\ntwo locations with a similar geographical property could \\nhave similar user behaviors. Matrix \\U0001d44d models the correla-\\ntion between two different activities, which can be learned \\nfrom the search results by sending the titles of two activi-\\nties into a search engine. The main idea is to propagate the \\ninformation among \\U0001d44b, \\U0001d44c and \\U0001d44d by requiring them to share \\nlow-rank matrices \\U0001d448 and \\U0001d449 in a collective matrix factoriza-\\ntion  model.  As  matrix \\U0001d44c and \\U0001d44d are  built  based  on  dense \\ndata, we can obtain an accurate decomposition of them, i.e. \\nmatrices \\U0001d448 and \\U0001d449.    Thus,  Matrix \\U0001d44b can  be  complemented \\nmore  accurately  by \\U0001d44b = \\U0001d448\\U0001d449\\U0001d447.  More  specifically,  an  objec-\\ntive function was formulated as Equation 12: \\n\\n\\U0001d43f(\\U0001d448, \\U0001d449, \\U0001d44a) =\\n\\n1\\n\\n2\\n\\n\\u2225 \\U0001d43c \\u2218 (\\U0001d44b \\u2212 \\U0001d448\\U0001d449\\U0001d447) \\u2225\\U0001d439\\n\\n2 +\\n\\n\\U0001d7061\\n2\\n\\n\\u2225 \\U0001d44c \\u2212 \\U0001d448\\U0001d44a\\U0001d447 \\u2225\\U0001d439\\n\\n2+\\n\\n \\n\\nLocationsFeaturesActivitiesX = UVTY = UWTZ = VVTUVActivitiesLocationsActivities\\x0c10 \\n\\nIEEE TRANSACTIONS ON BIG DATA, MANUSCRIPT ID \\n\\n         \\n\\n\\U0001d7062\\n2\\n\\n\\u2225 \\U0001d44d \\u2212 \\U0001d449\\U0001d449\\U0001d447 \\u2225\\U0001d439\\n\\n2+\\n\\n\\U0001d7063\\n2\\n\\n(\\u2225 \\U0001d448 \\u2225\\U0001d439\\n\\n2+\\u2225 \\U0001d449 \\u2225\\U0001d439\\n\\n2+\\u2225 \\U0001d44a \\u2225\\U0001d439\\n\\n2),       (12) \\n\\n\\U0001d43f(\\U0001d447, \\U0001d445, \\U0001d43a, \\U0001d439) =\\n\\n1\\n2\\n\\nwhere \\u2225\\u2219\\u2225\\U0001d439 denotes  the  Frobenius  norm.  I  is  an  indicator \\nmatrix with its entry \\U0001d43c\\U0001d456\\U0001d457 = 0 if \\U0001d44b\\U0001d456\\U0001d457 is missing, \\U0001d43c\\U0001d456\\U0001d457 = 1 other-\\nwise. The operator \\u201c\\u2218\\u201d denotes the entry-wise product. The \\nfirst three terms in the objective function control the loss in \\nmatrix factorization, and the last term controls the regular-\\nization over the factorized matrices so as to prevent over-\\nfitting. In general, this objective function is not jointly con-\\nvex  to  all  the  variables \\U0001d448, \\U0001d449 and \\U0001d44a.  Consequently,  some \\nnumerical method, such as gradient descent, was used to \\nget local optimal solutions. \\n     Example 11: Shang and Zheng et al. [53] propose a cou-\\npled-matrix factorization method to instantly estimate the \\ntravel  speed  on  each  road  segment  throughout  an  entire \\ncity,  based  on  the  GPS  trajectory  of  a  sample  of  vehicles \\n(such as taxicabs). As shown in Fig. 11 A), after map match-\\ning the GPS trajectories onto a road network, they formu-\\nlate a matrix \\U0001d440\\u2032\\U0001d45f with a row denoting a time slot (e.g., 2pm-\\n2:10pm) and a column standing for a road segment. Each \\nentry in \\U0001d440\\u2032\\U0001d45f contains the travel speed on a particular road \\nsegment and in a particular time slot, calculated based on \\nthe recently received GPS trajectories. The goal is to fill the \\nmissing values in row \\U0001d461\\U0001d457, which corresponds to the current \\ntime slot. Though we can achieve the goal by solely apply-\\ning matrix factorization to \\U0001d440\\u2032\\U0001d45f, the accurarcy of the infer-\\nence is not very high as the majority of road segments are \\nnot covered by trajectories. \\n     To address this issue, four context matrices (\\U0001d440\\U0001d45f, \\U0001d440\\U0001d43a, \\U0001d440\\u2032\\U0001d43a \\nand \\U0001d44d)  are  built.  Specifically, \\U0001d440\\U0001d45f  stands  for  the  historical \\ntraffic patterns on road segments. While the rows and col-\\numns of \\U0001d440\\U0001d45f have the same meaning as \\U0001d440\\u2032\\U0001d45f, an entry of \\U0001d440\\U0001d45f \\ndenotes  the  average  travel  speed  derived  from  historical \\ndata  over  a  long  period.  The  difference  between  the  two \\ncorresponding entries from \\U0001d440\\u2032\\U0001d45f and \\U0001d440\\U0001d45f indicates the devi-\\nation of current traffic situation (on a road segment) from \\nits average patterns. As depicted in Fig.11 B),  \\U0001d44d contains \\nthe physical features of a road segment, such as the shape \\nof a road, number of lanes, speed constraint, and the dis-\\ntribution of surrounding POIs. The general assumption is \\nthat two road segments with similar geographical proper-\\nties could have similar traffic conditions at the same time \\nof day. To capture high-level traffic conditions, as demon-\\nstrated in Fig. 11 C), a city is divided into uniform grids. \\nBy  projecting  the  recently  received  GPS  trajectories  into \\nthese grids, a matrix \\U0001d440\\u2032\\U0001d43a is built, with a column standing \\nfor a grid and a row denoting a time slot; an entry of \\U0001d440\\u2032\\U0001d43a \\nmeans the number of vehicles traveling in a particular grid \\nand at a particular time slot. Likewise, by projecting histor-\\nical trajectories over a long period into the grids, a similar \\n\\U0001d440\\U0001d43a is built, with each  entry being the average number of \\nvehicles  traveling  in  a  particular  grid  and  at  a  particular \\ntime  slot.  So, \\U0001d440\\u2032\\U0001d43a  denotes  the  real-time  high-level  traffic \\nconditions  in  a  city  and \\U0001d440\\U0001d43a  indicates  the  historical  high-\\nlevel traffic patterns. The difference between the same en-\\ntries of the two matrices suggests the deviation of current \\nhigh-level traffic conditions from their historical averages. \\nBy  combining  these  matrices,  i.e.  \\U0001d44b = \\U0001d440\\u2032\\U0001d45f||\\U0001d440\\U0001d45f  and  \\U0001d44c =\\n\\U0001d440\\u2032\\U0001d43a||\\U0001d440\\U0001d43a, a coupled matrix factorization is applied to \\U0001d44b, \\U0001d44c, \\nand \\U0001d44d, with the objective function as Equation 13. \\n\\n \\n\\n              + \\U0001d7062\\n\\n2\\n\\n||\\U0001d44d \\u2212 \\U0001d445\\U0001d439\\U0001d447||2 + \\U0001d7063\\n2\\n\\n||\\U0001d44c \\u2212 \\U0001d447(\\U0001d43a; \\U0001d43a)\\U0001d447||2 +\\n2\\n(||\\U0001d447||2 + ||\\U0001d445||\\n\\n||\\U0001d44b \\u2212 \\U0001d447(\\U0001d445; \\U0001d445)\\U0001d447||2 \\n\\n\\U0001d7061\\n2\\n+||\\U0001d43a||2 + ||\\U0001d439||2)          (13) \\n\\nwhere  \\u2225\\u2219\\u2225  denotes  the  Frobenius  norm.  The  first  three \\nterms  in  the  objective  function  control  the  loss  in  matrix \\nfactorization, and the last term is a regularization of pen-\\nalty to prevent over-fitting.    \\n\\n \\n\\nFig. 11 Estimate traffic conditions based on trajectories \\n\\n5.2.3 Manifold Alignment \\nManifold alignment utilizes the relationships of instances \\nwithin each dataset to strengthen the knowledge of the re-\\nlationships between the datasets, thereby ultimately map-\\nping initially disparate datasets to a joint latent space [64]. \\nManifold  alignment  is  closely  related  to  other  manifold \\nlearning techniques for dimensionality reduction, such as \\nIsomap [60], locally linear embedding [51], and Laplacian \\nEigenmaps [7]. Given a dataset, these algorithms attempt \\nto identify the low dimensional manifold structure of that \\ndataset  and  preserve  that  structure  in  a  low  dimensional \\nembedding of the dataset. Manifold alignment follows the \\nsame  paradigm  but  embeds  multiple  datasets.  There  are \\ntwo key ideas in manifold alignment:  \\n      1)  Manifold  alignment  preserves  the  correspondences \\nacross datasets; it also preserves the individual structures \\nwithin each dataset by mapping similar instances in each \\ndataset to similar locations in the Euclidean space. As illus-\\ntrated in Fig. 12, manifold alignment maps two datasets (\\U0001d44b, \\n\\U0001d44c)  to  a  new  joint  latent  space  (\\U0001d453(\\U0001d44b), \\U0001d454(\\U0001d44c)),  where  locally \\nsimilar  instances  within  each  dataset  and  corresponding \\ninstances across datasets are close or identical in that space. \\nThe two similarities are modeled by a lossy function with \\ntwo parts: one for preserving the local similarity within a \\ndataset, and the other for the correspondences across dif-\\nferent datasets. \\n     Formally, with \\U0001d450 datasets \\U0001d44b1, \\U0001d44b2, \\u2026 , \\U0001d44b\\U0001d450, the local similar-\\nity within each data set is modeled by Equation 14: \\n          \\U0001d436\\U0001d706(\\U0001d439\\U0001d44e) = \\u2211 ||\\U0001d439\\U0001d44e(\\U0001d456, . ) \\u2212 \\U0001d439\\U0001d44e(\\U0001d457, . )||2 \\u2219 \\U0001d44a\\U0001d44e(\\U0001d456, \\U0001d457)\\n,        (14) \\nwhere  \\U0001d44b\\U0001d44e is the a-th dataset, which is a \\U0001d45b\\U0001d44e \\xd7 \\U0001d45d\\U0001d44e data matrix \\nwith \\U0001d45b\\U0001d44e observations  and \\U0001d45d\\U0001d44e features. \\U0001d439\\U0001d44e is  the  embedding \\nof \\U0001d44b\\U0001d44e; \\U0001d44a\\U0001d44e is an \\U0001d45b\\U0001d44e \\xd7 \\U0001d45b\\U0001d44e matrix, where \\U0001d44a\\U0001d44e(\\U0001d456, \\U0001d457) is the similar-\\nity  between  instance \\U0001d44b\\U0001d44e(\\U0001d456, . ) and \\U0001d44b\\U0001d44e(\\U0001d457, . ).  The  sum  is  taken \\nover all pairs of instances in that dataset. \\U0001d436\\U0001d706(\\U0001d439\\U0001d44e) is the cost \\nof  preserving  the  local  similarities  within \\U0001d44b\\U0001d44e.  If  two  data \\ninstances, \\U0001d44b\\U0001d44e(\\U0001d456, . ) and \\U0001d44b\\U0001d44e(\\U0001d457, . ),  from \\U0001d44b\\U0001d44e are  similar,  which \\n\\n\\U0001d456,\\U0001d457\\n\\ng1g2g3g16g4g5g6g7g8g9g10g11g12g13g14g15r1d1r2r3r4r6r5A) Coupled matrix factorizationB) Extract features for a road segmentg1g2g16titi+1tjMGg1g2g16r1r2rntiti+1tjMrr1r2rnr1r2rnf1f2fkXYZM GfrfpM rfgC) Divide a city into grids\\x0cYU ZHENG:  METHODOLOGIES FOR CROSS-DOMAIN DATA FUSION: AN OVERVIEW 11 \\n\\nhappens  when \\U0001d44a\\U0001d44e(\\U0001d456, \\U0001d457) is  larger,  their  locations  in  the  la-\\ntent  space,  \\U0001d439\\U0001d44e(\\U0001d456, . )  and  \\U0001d439\\U0001d44e(\\U0001d456, . )  should  be  closer,  i.e. \\n||\\U0001d439\\U0001d44e(\\U0001d456, . ) \\u2212 \\U0001d439\\U0001d44e(\\U0001d457, . )||2  is  small.  \\U0001d437\\U0001d44e  is  an  \\U0001d45b\\U0001d44e \\xd7 \\U0001d45b\\U0001d44e  diagonal \\nmatrix with \\U0001d437\\U0001d44e(\\U0001d456, \\U0001d456) = \\u2211 \\U0001d44a\\U0001d44e(\\U0001d456, \\U0001d457)\\n. \\U0001d43f\\U0001d44e = \\U0001d437\\U0001d44e \\u2212 \\U0001d44a\\U0001d44e is the La-\\nplacian associated with \\U0001d44b\\U0001d44e. \\n\\n\\U0001d457\\n\\nFig. 12. Manifold alignment of two data sets [64] \\n\\n \\n\\nTo  preserve  the  correspondence  information  about  in-\\nstances between two datasets, \\U0001d44b\\U0001d44e and \\U0001d44b\\U0001d44f, the cost of each \\npair of correspondence is \\U0001d436\\U0001d458(\\U0001d439\\U0001d44e, \\U0001d439\\U0001d44f ):  \\n\\nlow-dimensional embedding of multiple datasets that pre-\\nserves any known correspondences across them [64]. \\n     Example 12: Zheng et al. [87] infer the fine-grained noise \\nsituation by using 311 complaint data together with social \\nmedia, road network data, and POIs. As shown in Fig. 13, \\nthey model the noise situation of NYC with a three dimen-\\nsion tensor, where the three dimensions stand for regions, \\nnoise  categories,  and  time  slots,  respectively.  An  entry \\n\\U0001d49c(\\U0001d456, \\U0001d457, \\U0001d458) stores the total number of 311 complaints of cate-\\ngory \\U0001d450\\U0001d457 in  region \\U0001d45f\\U0001d456 and  time  slot \\U0001d461\\U0001d458 over  the  given  period \\nof time. This is a very sparse tensor, as there may not be \\npeople reporting noise situation anytime and anywhere. If \\nthe tensor can be filled completely, we are able to know the \\nnoise situation throughout the city.  \\n\\n\\U0001d456,\\U0001d457\\n\\n \\U0001d436\\U0001d458(\\U0001d439\\U0001d44e, \\U0001d439\\U0001d44f ) = \\u2211 ||\\U0001d439\\U0001d44e(\\U0001d456, . ) \\u2212 \\U0001d439\\U0001d44f(\\U0001d457, . )||2 \\u2219 \\U0001d44a\\U0001d44e,\\U0001d44f(\\U0001d456, \\U0001d457)\\n\\n,     (15) \\nwhere \\U0001d44a\\U0001d44e,\\U0001d44f(\\U0001d456, \\U0001d457) is the similarity, or the strength of corre-\\nspondence, of two instances, \\U0001d44b\\U0001d44e(\\U0001d456, . ) and \\U0001d44b\\U0001d44f(\\U0001d457, . ). If the two \\ndata points are in a stronger correspondence, which hap-\\npens when \\U0001d44a\\U0001d44e,\\U0001d44f(\\U0001d456, \\U0001d457)  is larger, their locations in the latent \\nspace, \\U0001d439\\U0001d44e(\\U0001d456, . ) and \\U0001d439\\U0001d44f(\\U0001d457, . ), should be closer together. Typi-\\ncally, \\U0001d44a\\U0001d44e,\\U0001d44f(\\U0001d456, \\U0001d457) =1 if \\U0001d44b\\U0001d44e(\\U0001d456, . ) and \\U0001d44b\\U0001d44f(\\U0001d457, . ) are in correspond-\\nence. So, the complete lossy function is: \\n\\U0001d4361(\\U0001d4391, \\U0001d4392 , \\u2026 , \\U0001d439\\U0001d458) = \\U0001d462 \\u2219 \\u2211 \\U0001d436\\U0001d706(\\U0001d439\\U0001d44e)\\n+ \\U0001d463 \\u2219 \\u2211\\nTypically, \\U0001d462 = \\U0001d463 =1. \\n\\n\\U0001d436\\U0001d458(\\U0001d439\\U0001d44e, \\U0001d439\\U0001d44f)\\n\\n;  (16) \\n\\n\\U0001d44e\\u2260\\U0001d44f\\n\\n\\U0001d44e\\n\\n2) At the algorithmic level, manifold alignment assumes \\nthe disparate datasets to be aligned have the same under-\\nlying  manifold  structure.  The  second  loss  function  is \\nsimply the loss function for Laplacian eigenmaps using the \\njoint adjacency matrix: \\n\\n         \\U0001d4362(\\U0001d405 ) = \\u2211 ||\\U0001d405(\\U0001d456, . ) \\u2212 \\U0001d405(\\U0001d457, . )||2 \\u2219 \\U0001d416\\U0001d44e,\\U0001d44f(\\U0001d456, \\U0001d457)\\n\\n\\U0001d456,\\U0001d457\\n\\n;          (17) \\n\\nwhere the sum is taken over all pairs of instances from all \\ndatasets; \\U0001d405 is the unified representation of all the datasets \\nand \\U0001d416 is  the (\\u2211 \\U0001d45b\\U0001d44e\\U0001d44e\\n) joint  adjacency  matrix  of  all \\nthe datasets.  \\n\\n\\xd7 \\u2211 \\U0001d45b\\U0001d44e\\U0001d44e\\n\\n                   \\U0001d416 = (\\n\\n\\u22ee\\n\\n\\u22f1\\n\\n\\u22ee\\n\\n).                   (18) \\n\\n\\U0001d463\\U0001d44a1 \\U0001d462\\U0001d44a1,2 \\u22ef \\U0001d462\\U0001d44a1,\\U0001d450\\n\\n\\U0001d462\\U0001d44a\\U0001d450,1 \\U0001d462\\U0001d44a\\U0001d450,2 \\u22ef \\U0001d463\\U0001d44a\\U0001d450\\n\\n\\U0001d458\\n\\nEquation 19 denotes that if two data instances, \\U0001d44b\\U0001d44e(\\U0001d456, . ) and \\n\\U0001d44b\\U0001d44e(\\U0001d457, . ), are similar, regardless of whether they are in the \\nsame  dataset  (\\U0001d44e = \\U0001d44f)  or  from  different  datasets  (\\U0001d44e \\u2260 \\U0001d44f), \\nwhich  happens  when \\U0001d416(\\U0001d456, \\U0001d457) is  larger  in  either  case,  their \\nlocations  in  the  latent  space, \\U0001d405(\\U0001d456, . ) and \\U0001d405(\\U0001d456, . ),  should  be \\ncloser  together.  Making  use  of  the  fact  that ||M(\\U0001d456, . )||2 =\\n\\u2211 \\U0001d440(\\U0001d456, \\U0001d458)2\\n and that the Laplacian is a quadratic difference \\noperator, \\n         \\U0001d4362(\\U0001d405 ) = \\u2211 \\u2211 ||\\U0001d405(\\U0001d456, \\U0001d458) \\u2212 \\U0001d405(\\U0001d457, \\U0001d458)||2 \\u2219 \\U0001d416\\U0001d44e,\\U0001d44f(\\U0001d456, \\U0001d457)\\n                     =\\u2211 \\u2211 ||\\U0001d405(\\U0001d456, \\U0001d458) \\u2212 \\U0001d405(\\U0001d457, \\U0001d458)||2 \\u2219 \\U0001d416\\U0001d44e,\\U0001d44f(\\U0001d456, \\U0001d457)\\n \\n\\U0001d456,\\U0001d457\\n                     =\\u2211 \\U0001d461\\U0001d45f(\\n\\U0001d405(. , \\U0001d458)\\u2032\\U0001d40b\\U0001d405(. , \\U0001d458))= \\U0001d461\\U0001d45f(\\U0001d405\\u2032\\U0001d40b\\U0001d405)               (19) \\nWhere  \\U0001d461\\U0001d45f(\\u2219)  denotes  the  matrix  trace;  \\U0001d40b = \\U0001d403 \\u2212 \\U0001d416  is  the \\njoint Laplacian matrix of all the datasets. \\U0001d403 is an (\\u2211 \\U0001d45b\\U0001d44e\\U0001d44e\\n\\xd7\\n\\u2211 \\U0001d45b\\U0001d44e\\U0001d44e\\n. Standard \\nmanifold learning algorithms are then invoked on \\U0001d40b to ob-\\ntain  a  joint  latent  representation  of  the  original  datasets. \\nManifold alignment can therefore be viewed as a form of \\nconstrained  joint  dimensionality  reduction  that  finds  a \\n\\n) diagonal matrix with \\U0001d403\\U0001d44e(\\U0001d456, \\U0001d456) = \\u2211 \\U0001d416(\\U0001d456, \\U0001d457)\\n\\n  \\n\\n\\U0001d456,\\U0001d457\\n\\n\\U0001d458\\n\\n\\U0001d458\\n\\n\\U0001d458\\n\\n\\U0001d457\\n\\n \\n\\nFig. 13. Context-aware tensor decomposition with manifold \\n\\n     To deal with the data sparsity problem, they extract \\nthree categories of features, geographical features, hu-\\nman  mobility  features  and  noise  category  correlation \\nfeatures  (denoted  by  matrices  \\U0001d44b ,  \\U0001d44c ,  and  \\U0001d44d ),  from \\nPOI/road  network  data,  user  check-ins,  and  311  data, \\nrespectively. For example, a row of matrix \\U0001d44b denotes a \\nregion, and each column stands for a road network fea-\\nture, such as the number of intersections and the total \\nlength of road segments  in the  region. Matrix \\U0001d44b incor-\\nporates the similarity between two regions in terms of \\ntheir geographic features. Intuitively, regions with sim-\\nilar geographic features could have a similar noise situ-\\nation. \\U0001d44d \\u2208 \\u211d\\U0001d440\\xd7\\U0001d440 is the correlation matrix between differ-\\nent categories of noise. \\U0001d44d(\\U0001d456, \\U0001d457) denotes how often a cate-\\ngory of noise \\U0001d450\\U0001d456 co-occurs with another category \\U0001d450\\U0001d457. \\n     These features are used as contexts in a context-aware \\ntensor decomposition approach to supplement the missing \\nentries  of  the  tensor.  More  specifically, \\U0001d49c is  decomposed \\ninto the multiplication of a few (low-rank) matrices and a \\ncore tensor (or just a few vectors), based on \\U0001d49c\\u2019s non-zero \\nentries. Matrix \\U0001d44b can be factorized into the multiplication \\nof  two  matrices,  \\U0001d44b = \\U0001d445 \\xd7 \\U0001d448 ,  where  \\U0001d445 \\u2208 \\u211d\\U0001d441\\xd7\\U0001d451\\U0001d445  and  \\U0001d448 \\u2208\\n\\u211d\\U0001d451\\U0001d445\\xd7\\U0001d443 are low rank latent factors for regions and geograph-\\nical features, respectively. Likewise, matrix \\U0001d44c can be factor-\\nized  into  the  multiplication  of  two  matrices, \\U0001d44c = \\U0001d447 \\xd7 \\U0001d445\\U0001d447 , \\nwhere \\U0001d447 \\u2208 \\u211d\\U0001d43f\\xd7\\U0001d451\\U0001d447 is a low rank latent factor matrix for time \\nslots. \\U0001d451\\U0001d447 and \\U0001d451\\U0001d445 are usually very small. The objective func-\\ntion is defined as Equation 20: \\n\\n\\u2112(\\U0001d446, \\U0001d445, \\U0001d436, \\U0001d447, \\U0001d448) =\\n\\n\\u2016\\U0001d49c \\u2212 \\U0001d446 \\xd7\\U0001d445 \\U0001d445 \\xd7\\U0001d436 \\U0001d436 \\xd7\\U0001d447 \\U0001d447\\u20162 + \\n\\n\\u2016\\U0001d44b \\u2212 \\U0001d445\\U0001d448\\u20162 +  \\n\\n\\U0001d7061\\n(\\u2016\\U0001d446\\u20162 +\\n2\\n\\u2016\\U0001d445\\u20162 + \\u2016\\U0001d436\\u20162 + \\u2016\\U0001d447\\u20162 + \\u2016\\U0001d448\\u20162) ;                                             (20) \\n\\n\\u2016\\U0001d44c \\u2212 \\U0001d447\\U0001d445\\U0001d447\\u20162 +\\n\\ntr(\\U0001d436\\U0001d447\\U0001d43f\\U0001d44d\\U0001d436) +\\n\\n\\U0001d7063\\n2\\n\\n\\U0001d7064\\n2\\n\\n1\\n2\\n\\U0001d7062\\n2\\n\\nwhere \\u2016\\U0001d49c \\u2212 \\U0001d446 \\xd7\\U0001d445 \\U0001d445 \\xd7\\U0001d436 \\U0001d436 \\xd7\\U0001d447 \\U0001d447\\u20162 is  to  control  the  error  of \\n\\n \\n\\nCategoriesRegionsCategoriesCategoriesRegionsFeaturesAX = R\\xd7UZTime slotsRegionsYY = T\\xd7RTX\\x0c12 \\n\\nIEEE TRANSACTIONS ON BIG DATA, MANUSCRIPT ID \\n\\ndecomposing \\U0001d49c; \\u2016\\U0001d44b \\u2212 \\U0001d445\\U0001d448\\u20162 is  to  control  the  error  of  fac-\\ntorization of \\U0001d44b; \\u2016\\U0001d44c \\u2212 \\U0001d447\\U0001d445\\U0001d447\\u20162 is to control the error of factor-\\nization of \\U0001d44c; \\u2016\\U0001d446\\u20162 + \\u2016\\U0001d445\\u20162 + \\u2016\\U0001d436\\u20162 + \\u2016\\U0001d447\\u20162 + \\u2016\\U0001d448\\u20162 is a regular-\\nization penalty to avoid over-fitting;  \\U0001d7061, \\U0001d7062, \\U0001d7063, and \\U0001d7064 are \\nparameters controlling the contribution of each part dur-\\ning the collaborative decomposition. Here, matrix \\U0001d44b and \\U0001d44c \\nshare  the  same  dimension  of  region  with  tensor \\U0001d49c.  Ten-\\nsor  \\U0001d49c  has  a  common  dimension  of  time  with  \\U0001d44c  and  a \\nshared dimension of category with \\U0001d44d. Thus, they share la-\\ntent  spaces  for  region,  time  and  category.  This  idea  has \\nbeen  introduced  in  the  coupled-matrix  factorization.  \\nt r(\\U0001d436\\U0001d447\\U0001d43f\\U0001d44d\\U0001d436) is  derived  from  Equation  19  of  the  manifold \\nalignment:  \\n\\u2211 \\u2016\\U0001d436(\\U0001d456, . ) \\u2212 \\U0001d436(\\U0001d457, . )\\u20162\\U0001d44d\\U0001d456\\U0001d457\\n= \\u2211 \\u2211 \\u2016\\U0001d436(\\U0001d456, \\U0001d458) \\u2212 \\U0001d436(\\U0001d457, \\U0001d458)\\u20162\\U0001d44d\\U0001d456\\U0001d457\\n  \\n= \\U0001d461\\U0001d45f(\\U0001d436\\U0001d447(\\U0001d437 \\u2212 \\U0001d44d)\\U0001d436) = \\U0001d461\\U0001d45f(\\U0001d436\\U0001d447\\U0001d43f\\U0001d44d\\U0001d436),                                 (21) \\nwhere  \\U0001d436 \\u2208 \\u211d\\U0001d440\\xd7\\U0001d451\\U0001d436  is  the  latent  space  of  category;  \\U0001d437\\U0001d456\\U0001d456 =\\n\\u2211 \\U0001d44d\\U0001d456\\U0001d457\\n is a diagonal matrix, and \\U0001d43f\\U0001d44d = \\U0001d437 \\u2212 \\U0001d44d is the Laplacian \\nmatrix of the category correlation graph. \\U0001d461\\U0001d45f(\\U0001d436\\U0001d447\\U0001d43f\\U0001d44d\\U0001d436), which \\nguarantees two (e.g. the \\U0001d456th and \\U0001d457th) noise categories with \\na  higher  similarity  (i.e., \\U0001d44d\\U0001d456\\U0001d457 is  bigger)  should  also  have  a \\ncloser distance in the new latent space \\U0001d436. In this case, only \\none data set, i.e. 311 data, is involved in the manifold align-\\nment.  So, \\U0001d403 = \\U0001d437 .  As  there  is  no  closed-form  solution  for \\nfinding the global optimal result of the objective function \\n(shown  in  Equation  20),  a  numeric  method,  gradient  de-\\nscent, is employed to find a local optimization. \\n\\n\\U0001d456,\\U0001d457\\n\\n\\U0001d458\\n\\n\\U0001d456,\\U0001d457\\n\\n\\U0001d456\\n\\nency (or causality) between two different  variables is de-\\nnoted  by  the  edge  connecting  them.  The  structure  of  a \\ngraphical model can be learned from data automatically or \\npre-defined by human knowledge. Graphical models usu-\\nally contain hidden variables to be inferred. The learning \\nprocess of graphical models is to estimate the probabilistic \\ndependency  between  different  variables  given  the  ob-\\nserved  data.  Expectation  and  Maximization  (EM)  algo-\\nrithms are commonly used methods. The inference process \\nis to predict the status of hidden variables, given the values \\nof  observed  variables  and  learned  parameters.  The  infer-\\nence algorithms include deterministic algorithms, such as \\nvariational methods, and stochastic algorithms like Gibbs \\nSampling. More details about graphical models can be re-\\nferred to [9][36]. \\n      Example  13:  Shang  et  al.  [53]  propose  inferring  traffic \\nvolume  on  a  road  based  on  POIs,  road  networks,  travel \\nspeed and weather. Fig. 14 presents the graphical structure \\nof the traffic volume inference (TVI) model, where a gray \\nnode denotes a hidden variable and white nodes are obser-\\nvations.  One  TVI  model  is  trained  for  each  level  of  road \\nsegments.  \\n\\n5.3. Probabilistic Dependency-Based Fusion \\nA probabilistic graphical model is a probabilistic model for \\nwhich a graph expresses the conditional dependence struc-\\nture between random variables. Generally, it uses a graph-\\nbased  representation  as  the  foundation  for  encoding  a \\ncomplete distribution over a multi-dimensional space. The \\ngraph can be regarded as a compact or factorized represen-\\ntation  of  a  set  of  independences  that  hold  in  the  specific \\ndistribution. Two branches of graphical representations of \\ndistributions  are  commonly  used,  namely,  Bayesian  Net-\\nworks and Markov Networks (also called Markov Random \\nField [34]). Both families encompass the properties of fac-\\ntorization and independences, but they differ in the set of \\nindependences they can encode and the factorization of the \\ndistribution that they induce [9]. For instance, a Bayesian \\nNetwork is a directed acyclic graph that factorizes the joint \\nprobability  of \\U0001d45b variables \\U0001d44b1, \\U0001d44b2,\\u2026, \\U0001d44b\\U0001d45b as \\U0001d443[\\U0001d44b1, \\U0001d44b2, \\u2026 , \\U0001d44b\\U0001d45b] =\\n\\u220f \\U0001d443[\\U0001d44b\\U0001d456|\\U0001d443\\U0001d434(\\U0001d44b\\U0001d456)]\\n. Markov Network is a set of random var-\\niables  having  a  Markov  property  described  by  an  undi-\\nrected  graph,  which  may  be  cyclic.  Thus,  a  Markov  net-\\nwork  can  represent  certain  dependencies  that  a  Bayesian \\nnetwork cannot (such as cyclic dependencies). On the other \\nhand,  it  cannot  represent  certain  dependencies  that  a \\nBayesian network can (such as induced dependencies). \\n     This  category  of  approaches  bridges  the  gap  between \\ndifferent datasets by the probabilistic dependency, which \\nemphasize more about the interaction rather than the sim-\\nilarity between two objects. This is different from the simi-\\nlarity-based methods introduced in Section 5. For example, \\nvariables  (i.e.  features  extracted  from  different  datasets) \\nare  represented  by  nodes,  and  the  probabilistic  depend-\\n\\n\\U0001d45b\\n\\U0001d456=1\\n\\nFig. 14. The graphical structure of TVI model  \\n\\n \\n\\n    Specifically, the traffic volume on each road lane \\U0001d441\\U0001d44e (i.e., \\nthe number of vehicles per minute per lane) of a road seg-\\nment is influenced by four major factors, consisting of the \\nweather conditions \\U0001d464, time of day \\U0001d461, the type of road \\U0001d703, and \\nthe volume of observed sample vehicles \\U0001d441\\U0001d461. Furthermore, \\na road\\u2019s \\U0001d703 is co-determined by its road network features \\U0001d453\\U0001d45f \\n(such as \\U0001d45f. \\U0001d459\\U0001d452\\U0001d45b), global position feature \\U0001d453\\U0001d454, and surrounding \\nPOIs \\U0001d6fc which is influenced by \\U0001d453\\U0001d45d and the total number of \\nPOIs \\U0001d441\\U0001d45d. \\U0001d463\\u0305 and \\U0001d451\\U0001d463 are the average travel speed and speed \\nvariance,  respectively,  inferred  by  TSE  model. \\U0001d463\\u0305 is  deter-\\nmined by \\U0001d703, \\U0001d441\\U0001d44e, and \\U0001d464. \\U0001d451\\U0001d463 is co-determined by \\U0001d441\\U0001d461, \\U0001d441\\U0001d44e, and \\n\\U0001d463\\u0305. Due to the hidden nodes, the conditional probability of \\n\\U0001d441\\U0001d44e cannot be drawn simply by counting the occurrence of \\neach  condition.  An  EM  algorithm  was  proposed  to  learn \\nthe parameters in an unsupervised manner. \\n     Example 14: Yuan et al. [74][76] infer the functional re-\\ngions in a city using road network data, points of interests, \\nand human mobility learned from a large number of taxi \\ntrips. As depicted in Fig. 15, a LDA (Latent Dirichlet Allo-\\ncation)-variant-based  inference  model  was  proposed,  re-\\ngarding a region as a document, a function as a topic, cat-\\negories  of  POIs  (e.g.,  restaurants  and  shopping  malls)  as \\nmetadata  (like  authors,  affiliations,  and  key  words),  and \\nhuman mobility patterns as words. The mobility pattern is \\ndefined as the commuting patterns between regions. That \\nis when people leave a region and where they are heading \\nto, and when people arrive at a region and where did there \\ncome from. Each commuting pattern stands for one word \\n\\n \\n\\nfpfgtNt\\u0275 NavdvfrwNp\\u03b1 \\x0cYU ZHENG:  METHODOLOGIES FOR CROSS-DOMAIN DATA FUSION: AN OVERVIEW 13 \\n\\ndescribing a region, while the frequency of the pattern de-\\nnotes the number of occurrence of a word in a document.  \\n\\n\\u211d|\\U0001d47e\\U0001d48a| is the parameter of the Dirichlet prior on the per-topic \\nword distributions of \\U0001d47e\\U0001d48a. A word \\U0001d464 in \\U0001d47e\\U0001d456 is one of the cat-\\negories \\U0001d460\\U0001d456\\u2019s instances pertain to, e.g. \\U0001d47e1 = {\\U0001d4501, \\U0001d4502, \\u2026 , \\U0001d450\\U0001d45a}.  \\n\\n \\n\\nFig. 15. Learning functional regions based on multiple da-\\n\\ntasets: A graphical model approach \\n\\n      By  feeding  POIs  (denoted  as \\U0001d465\\U0001d45f)  and  human  mobility \\npatterns  (denoted  as  \\U0001d45a\\U0001d45f,\\U0001d45b )  into  different  parts  of  this \\nmodel,  a  region  is  represented  by  a  distribution  of  func-\\ntions, each of which is further denoted by a distribution of \\nmobility patterns. \\U0001d441 stands for the number of words (i.e., \\nmobility  patterns  in  a  region); \\U0001d445  denotes  the  number  of \\ndocuments  (regions);  \\U0001d43e  is  the  number  of  topics,  which \\nshould be predefined. Before running the model, a city was \\npartitioned into disjointed regions using major roads like \\nhigh ways and ring roads. So, this example also uses stage-\\nbased data fusion techniques. This model can be estimated \\nusing  EM  and  inferred  using  Gibbs  sampling.  Different \\nfrom  the  basic  LDA  model  [5],  the  Dirichlet  prior  \\n is  now  specified  for  individual  regions  based  on  the  ob-\\nserved POI features of each region.    \\n\\n Example 15: Zheng et al. [90] combine multiple datasets \\ngenerated in a region to better estimate the distribution of \\na sparse dataset in the region and the latent function of the \\nregion, based on the following two insights. First, different \\ndatasets in a region can mutually reinforce each other. Sec-\\nond, a dataset can reference across different regions. Fig. \\n16 depicts the graphical presentation of the model, entitled \\nMSLT (Multi-Source Latent Topic Model). \\n\\n    \\n\\nFig.16. The graphic presentation of the MSLT model   \\n\\n\\U0001d487 is a vector storing the features extracted from the road \\nnetwork and POIs located in a region. \\U0001d73c \\u2208 \\u211d\\U0001d458\\xd7|\\U0001d487| is a matrix \\nwith each row \\U0001d73c\\U0001d461 corresponding to a latent topic; \\U0001d458 denotes \\nthe  number  of  topics  and  |\\U0001d487| means  the  number  of  ele-\\nments in \\U0001d487. The value of each entry in \\U0001d73c follows a Gaussian \\ndistribution with a mean \\U0001d707 and a standard deviation \\U0001d70e. \\U0001d736 \\u2208\\n\\u211d\\U0001d458 is a parameter of the Dirichlet prior on the per-region \\ntopic distributions. \\U0001d73d \\u2208 \\u211d\\U0001d458 is the topic distribution for  re-\\ngion \\U0001d451. \\U0001d4e6 = {\\U0001d47e1, \\U0001d47e2, \\u2026 , \\U0001d47e|\\U0001d47a|} is a collection of word sets, \\nwhere \\U0001d47e\\U0001d456 is a word set corresponding to dataset \\U0001d460\\U0001d456 and |\\U0001d47a| \\ndenotes the number of datasets involved in the MSLT. \\U0001d737 \\u2208\\n\\n \\n\\nAs illustrated in Fig. 16 B), different datasets share  the \\nsame distribution of topics controlled by \\U0001d73d\\U0001d451, but having its \\nown topic-word distributions \\U0001d74b\\U0001d456, 1 \\u2264 \\U0001d456 \\u2264 |\\U0001d47a|, indicated by \\narrows  with  different  colors. \\U0001d74b\\U0001d456\\U0001d467 is  a  vector  denoting  the \\nword distribution of topic \\U0001d467 in word set \\U0001d47e\\U0001d456.  This is differ-\\nent from LDA and its variant DMR [44], which have a sin-\\ngle word set and topic-word distribution. The topic distri-\\nbution \\U0001d73d\\U0001d451 of a region and the topic-word distribution \\U0001d74b\\U0001d456 of \\na dataset \\U0001d460\\U0001d456 are used to calculate the underlying distribu-\\ntion  of  each  category  in  \\U0001d460\\U0001d456 ,  if  \\U0001d460\\U0001d456  is  very  sparse,  e.g. \\n\\U0001d45d\\U0001d45f\\U0001d45c\\U0001d45d(\\U0001d464\\U0001d456) = \\u2211 \\U0001d703\\U0001d451\\U0001d461\\U0001d711\\U0001d461\\U0001d464\\U0001d456\\n\\n. \\n\\n\\U0001d461\\n\\n5.4. Transfer Learning-Based Data Fusion \\nA major assumption in many machine learning and data \\nmining algorithms is that the training and future data must \\nbe in the same feature space and have the same distribu-\\ntion.  However,  in  many  real-world  applications,  this  as-\\nsumption may not hold. For example, we sometimes have \\na classification task in one domain of interest, but we only \\nhave sufficient training data in another domain of interest, \\nwhere the latter data may be in a different feature space or \\nfollow  a  different  data  distribution.  Different  from  semi-\\nsupervised learning, which assumes that the distributions \\nof  the  labeled  and  unlabeled  data  are  the  same,  transfer \\nlearning, in contrast, allows the domains, tasks, and distri-\\nbutions used in training and testing to be different.  \\n       In the real world, we observe many examples of trans-\\nfer learning. For instance, learning to recognize tables may \\nhelp recognize chairs. Learning riding a bike may help rid-\\ning a moto-cycle. Such examples are also widely witnessed \\nin  the  digital  world.  For  instance,  by  analyzing  a  user\\u2019s \\ntransaction records in Amazon, we can diagnose their in-\\nterests, which may be transferred into another application \\nof  travel  recommendation.  The  knowledge  learned  from \\none city\\u2019s traffic data may be transferred to another city.   \\n\\n5.4.1. Transfer between the Same Type of Datasets  \\nPan and Yang et al. [50] present a good survey that classi-\\nfies transfer learning into three categories, based on differ-\\nent tasks and situations between the source and target do-\\nmains, as shown in Table 2. Fig. 17 presents another taxon-\\nomy  of  transfer  learning  according  to  whether  label  data \\nare available in source and target domains.  \\n     Transductive  learning  is  proposed  to  handle  the  cases \\nwhere  the  task  is  the  same but  the  source  and  target  do-\\nmain are different. Furthermore, there are two sub-catego-\\nries of the difference between source and target domains.  \\nIn the first category, the feature spaces between domains \\nare the same, but the marginal probability distributions are \\ndifferent. Most of the existing works on transfer learning \\nfall into this category. For example, in a traffic prediction \\ntask, we can transfer the traffic data from a city to another \\none where training data are limited. In the second category, \\nthe feature spaces between domains are different. For ex-\\nample, one domain has Chinese web pages; the other have \\nEnglish web pages. But, the task is the same, i.e. clustering \\nweb  pages  according  to  the  similarity  of  their  semantic \\n\\n\\u03c32\\u03bbk K\\u03c6k K\\u03b2 RN\\u03b1k \\u03b8k zr,n mr,n xr \\u03c3 \\u03b7 \\u03bc\\u03b1 \\u03b8 \\u03c61 fzw1 zw2 zw|s| \\u03c62 \\u03c6|s| \\u03b2 z1 z2 zk z1 z2 zk z1 z2 zk c1 c2 cm cm+1 cn cw \\u03bb1\\u03c61cm+2 cn+1 cn+2 \\u03c62\\u03c63\\u03bb2\\u03bb3s1 \\u03b8 c1 cm cm+1 cn cn+1 cw \\u03b81\\u03b8k\\u03b82\\u03c611\\u03c6kwz1 z2 zk A) Graphic representation of MSLTB) Topic-words distribution across different data sourcess2 s3 W1 W2 W3 \\x0c14 \\n\\nIEEE TRANSACTIONS ON BIG DATA, MANUSCRIPT ID \\n\\nmeanings. Yang et.al [72] initiate the setting called \\u201cheter-\\nogeneous transfer learning\\u201d to handle this category of sit-\\nuation. There are two directions in this stream of work: 1) \\ntranslation from the source to the target  [18] or 2) projec-\\ntion both domains into a common latent space [72][93]. Alt-\\nhough the source and the target domains are from different \\nfeature spaces in heterogeneous transfer learning, each do-\\nmain itself is homogeneous with a single data source. \\n\\nTable 2. Taxonomy of Transfer Learning (TL) [50] \\n\\nLearning settings \\n\\nTraditional ML \\n\\nInductive \\nlearning / \\nunsuper-\\nvised TL \\n\\nTL \\n\\nSource and tar-\\nget domains \\n\\nthe same \\n\\nthe same \\n\\nSource and \\ntarget tasks \\n\\nthe same \\n\\ndifferent but \\n\\nrelated \\n\\ndifferent but re-\\n\\ndifferent but \\n\\nlated \\n\\nTransductive \\n\\ndifferent but re-\\n\\nlearning \\n\\nlated \\n\\nrelated \\n\\nthe same \\n\\n     Different from Transductive Learning, inductive learn-\\ning handles learning cases where the tasks are different in \\nsource  and  target  domains.  It  focuses  on  storing  know-\\nledge gained while solving one problem and applying it to \\na different but related problem. Multi-task learning (MTL) \\n[3] is a representative approach to inductive transfer learn-\\ning.  MTL  learns  a  problem  together  with  other  related \\nproblems at the same time, using a shared representation. \\nThis  often  leads  to  a  better  model  for  the  main  task,  be-\\ncause it allows the learner to use the commonality among \\nthe  tasks  [14].  MTL  works  well  if  these  tasks  have  some \\ncommonality and are slightly under sampled. Fig. 17 pre-\\nsents two examples of MTL. \\n\\n \\n\\nFig. 17. A different setting of transfer learning \\n\\n      \\n      Example 16: Fig. 18 A) illustrates the transferring learn-\\ning between two classification tasks. One task is to infer an \\nindividual\\u2019s interests in different travel packages in terms \\nof her location history in the physical world (e.g. check-ins \\nfrom a social networking service). The other task is to esti-\\nmate a user\\u2019s interests in different book styles based on the \\nbooks the user has browsed on the Internet. If we happen \\nto have the two datasets from a same user, we can associate \\nthe two tasks in a MTL framework, which learns a shared \\nrepresentation of a user\\u2019s general interests. As the books a \\nuser  has  browsed  may  imply  her  general  interests  and \\ncharacters, which can be transferred into the  travel pack-\\nage  recommendation.  Likewise,  the  knowledge  from  a \\nuser\\u2019s physical location can also help estimate a user\\u2019s in-\\nterests in different book styles. MTL is particularly helpful \\nwhen  the  dataset  we  have  is  sparse;  e.g.  we  only  have  a \\nsmall amount of check-in data of a user.  \\n       Example  17:  Fig.  18  B)  presents  another  example  of \\n\\n \\n\\nMTL,  which  co-predicts  the  air  quality  and  traffic  condi-\\ntions at near future simultaneously. The general insight is \\nthat different traffic conditions will generate different vol-\\numes of air pollutants, therefore impacting the air quality \\ndifferently. Likewise, people tend to go hiking or picnic in \\na day with good air quality, while preferring to minimize \\ntravel in a day with hazardous air quality. As a result, the \\ntraffic  conditions  are  also  affected  by  air  quality.  The \\nshared feature representation of the two datasets can be re-\\ngarded as the latent space of a location in a time slot.   \\n\\n \\nFig. 18. Examples of multi-task transfer learning            \\n\\n5.4.2 Transfer Learning among Multiple Datasets \\nIn the big data era, many machine learning tasks have to \\nharness a diversity of data in a domain so as to achieve a \\nbetter performance. This calls for new techniques that can \\ntransfer the knowledge of multiple datasets from a source \\nto a target domain. For example, a major city like Beijing \\nmay have sufficient datasets (such as traffic, meteorologi-\\ncal,  and  human  mobility  etc.)  to  infer  its  fine-grained  air \\nquality. But, when applying the model to another city, we \\nmay not have some kind of datasets (e.g. traffic) at all or \\nnot enough observations in some datasets (e.g. human mo-\\nbility). Can we transfer the knowledge learned from mul-\\ntiple datasets of Beijing to another city?  \\n      Fig. 19 presents the four situations of transfer learning \\nwhen  dealing  with  multiple  datasets,  where  different \\nshapes denote different datasets (a.k.a. views). As depicted \\nin Fig. 19 A), a target domain has all kinds of datasets (that \\nthe source domain has), each of which has sufficient obser-\\nvations (as the source domain). That is, the target domain \\nhas the same (and sufficient) feature spaces as the source \\ndomain.  This  kind  of  situation  can  be  handled  by  multi-\\nview transfer learning [19][71][77]. For example, Zhang et \\nal. [77] propose a multi-view transfer learning with a large \\nmargin  approach  (MVTL-LM),  which  leverages  both  la-\\nbeled data from the source domain and features from dif-\\nferent views. DISMUTE [19] performs feature selection for \\nmulti-view  cross-domain  learning.  Multi-view  discrimi-\\nnant  transfer  (MDT)  [71]  learns  discriminant  weight  vec-\\ntors for each view to minimize the domain discrepancy and \\nthe view disagreement simultaneously. \\n       As shown in Fig. 19 B), some datasets do not exist in \\nthe target domain, while other datasets are as sufficient as \\nthe source domain. To deal with such kind of dataset (a.k.a. \\nview  structural)  missing  problems,  a  line  of  research  on \\nmulti-view multi-task learning [26][32] has been proposed. \\nHowever,  these  algorithms  cannot  handle  the  situations \\nshown in Fig. 19. C), where a target domain has all kinds \\nof datasets but some datasets may have very few observa-\\ntions  (or  say  very  sparse),  and  the  situation  presented  in \\n\\nTransfer LearningSame TaskTransductive Transfer LearningInductive Transfer LearningLabel data in target domainUnsupervised Transfer LearningLabel in source domainMulti-task LearningSelf-taught LearningYYYNNNPhysical location historyBooks browsed onlineJoint Task LearnerBook categories  {war, romantic, sci-fic}Travel Packages {A, B, C}Traffic DataAir quality dataJoint Task Learner{Good, moderation, Unhealthy}{fast, normal, congestion}A) Book-travel interests co-estimation B) Air quality-traffic co-prediction\\x0cYU ZHENG:  METHODOLOGIES FOR CROSS-DOMAIN DATA FUSION: AN OVERVIEW 15 \\n\\nFig. 19 D), where some datasets do not exist (view missing) \\nand some datasets are very sparse (observation missing). \\nThe problem is still open for research.  \\n\\nFig. 19. Transferring knowledge from multiple datasets  \\n\\n \\n\\n6. DISCUSSION \\n\\nIt is hard to judge which data fusion method is the best, as \\ndifferent methods behave differently in different applica-\\ntions. Table 3 presents a comparison among these data fu-\\nsion  methods  (list  in  the  first  column),  where  the  second \\ncolumn (Meta) indicates if a method can incorporate other \\napproaches  as  a  meta  method.  For  instance,  a  semantic \\nmeaning-based data fusion methods can be employed in a \\nstage-based fusion method. To select a proper data fusion \\nmethod, we need to consider the following factors:  \\n\\nTable 3. Comparison of different data fusion methods \\n\\nMethods \\n\\nStage-based \\n\\nF \\n\\nDirect \\nDNN \\n\\n  Multiview \\n\\nc\\ni\\nt\\nn\\na\\nm\\ne\\nS\\n\\nProbabil. \\nSimilarity \\nTransfer \\n\\nM\\neta \\n\\nY \\nN \\nN \\nY \\nN \\nN \\nY \\n\\nLabels \\n\\nGoals \\n\\nVol \\n\\nPos \\n\\nNA  NA \\nFlex \\nL \\nFlex \\nL \\nFix \\nS \\nS \\nFix \\nFlex \\nS \\nS \\nFix \\n\\nNA \\n\\nF,P,C,O \\nF,P,A,O \\n\\nF,P,O \\n\\nF,P,C,O,A \\n\\nF,A,O \\nF,P,A \\n\\nTrai\\nn \\n\\nScal\\ne \\n\\nS \\n\\nU/S \\nS, SS \\nS/U \\n\\nNA  NA \\nY \\nY \\nY \\nN \\nY \\nY \\n\\nU \\n\\nS/U \\n\\n      1) The volume, properties and insight of datasets that \\nwe have in an application. First, as shown in the third col-\\numn (Vol) of Table 3, the feature-based data fusion meth-\\nods need a large (L) amount of labeled instances as training \\ndata,  while  the  semantic  meaning-based  methods  can  be \\napplied to a dataset with a small (S) number of labeled in-\\nstances. Second, when studying a type of objects, e.g. geo-\\ngraphical regions, we need to consider whether there are \\nsome object instances that can constantly generate labeled \\ndata (titled \\u201cFixed\\u201d or \\u201cFlexible\\u201d in the 4th column \\u201cPosi-\\ntion\\u201d). For example, we can have fixed monitoring stations \\nconstantly  generating  air  quality  data  in  some  regions  in \\nExample 7. On the contrary, we cannot ensure that there \\nare 311 complaints (mentioned in Example 12) constantly \\nreported by people in a region. Sometimes region A and B \\nhave 311 complaints, while at other time intervals Region \\nC, D and E have. In some extreme cases, there are no re-\\ngions with 311 data. That is, regions with 311 data occur \\nflexibly,  which  cannot  formulate  stable  view-class  label \\npairs for a region. As a consequence, it is not appropriate \\nto use a multi-view-based fusion method to handle the 311 \\nexample. On the other hand, the former problem cannot be \\n\\n \\n\\nsolved by a similarity-based fusion method either. As the \\nlocation of a station is fixed, regions with and without la-\\nbels are both fixed. We cannot calculate the similarity be-\\ntween a region with labeled data and another always with-\\nout  data.  Third,  some  datasets  do  not  change  over  time \\nwhile others are temporally dynamic. Directly combining \\nfeatures extracted from static data and those from dynamic \\ndata  would  cause  static  features  ignored  by  a  machine \\nlearning  model.  For  example,  road  networks  and  POIs \\naround a building do not change over time, no matter what \\nlevel of air pollution is over the building. Thus, the feature-\\nbased fusion methods do not work well for this example.  \\n     2)  The  goal,  learning  approach,  and  requirement  of  a \\nmachine learning and data mining task. First, goals of fus-\\ning multiple datasets includes Filling Missing Values (of a \\nsparse dataset) [53][85][87] [88], Predict Future [89], Cau-\\nsality Inference, Object Profiling[73][76][74], and Anomaly \\nDetection  [15][90][49],  etc.  As  presented  in  the  fifth  col-\\numn, probabilistic dependency-based data fusion methods \\ncan  achieve  all  these  goals  (F,  P,  C,  O,  A).  Particularly, \\nBayesian Networks and the straightforward feature-based \\nfusion methods (e.g. when using a linear regression model \\n[24]) are usually good at dealing with causality inference \\nproblems  (C).  The  directed  edges  of  a  Bayesian  Network \\nreveal the causality between different factors (i.e. nodes), \\nand the weight of a feature in a linear regression model de-\\nnotes the importance of a factor to a problem. As raw fea-\\ntures have been converted into a middle-level feature rep-\\nresentation by DNN, the semantic meaning of each feature \\nis not clear any more. Second, the learning methods consist \\nof supervised (S), unsupervised (U) and semi-supervised \\n(SS) learning, as denoted in the sixth column. For example, \\nsupervised and semi-supervised learning approaches can \\nbe  applied  to  multi-view-based  data  fusion  methods. \\nThird, there are some requirements for a data mining task, \\nsuch as efficiency and scalability (shown in the most right \\ncolumn). Generally speaking, it is not easy for probabilistic \\ndependency-based approaches to scale up (N). A graphical \\nmodel with a complex structure, e.g. many (hidden) nodes \\nand  layers,  may  become  intractable.  With  respect  to  the \\nsimilarity-based  data  fusion  methods,  when  a  matrix  be-\\ncomes very large, NMF, which can be operated in parallel, \\ncan be employed to expedite decomposition (Y). \\n     Generally, given the same amount of training data, the \\nstraightforward feature-based methods are not as good as \\nsemantic meaning-based approaches, as there are depend-\\nencies and correlations between features. Adding a spar-\\nsity  regularization  can  alleviate  the  problem  to  some  ex-\\ntents,  but  cannot  solve  it  fundamentally.  In  some  cases \\nwith  a  large  amount  of  labeled  data,  particularly  for  im-\\nages  and  speech  data,  feature-based  fusion  using  DNNs \\ncan perform well. However, the performance of the model \\nheavily relies on tuning parameters. Given a huge model \\nwith many parameters to learn, this is usually a time-con-\\nsuming process that needs the involvement of human ex-\\nperiences. Additionally, there are a few overlaps between \\nthe multi-view-based approach and transfer learning. For \\ninstance,  there  are  multi-view  multi-task  learning  ap-\\nproaches [29].  \\n\\nA) Complete Datasets and instances B) Some datasets missingC) Datasets complete but instance sparseSource DomainTarget DomainSource DomainTarget DomainD) Datasets and instances missing\\x0c16 \\n\\nIEEE TRANSACTIONS ON BIG DATA, MANUSCRIPT ID \\n\\n7. PUBLIC CROSS-DOMAIN BIG DATA \\n\\n7.1 Urban Big Data \\nAdvances  in  sensing  technology  and  large-scale  compu-\\nting infrastructure have been generating a diversity of data \\nin cities. Quite a few cities, like New York City and Chi-\\ncago,  have  opened  their  datasets  to  the  public.  Here  are \\nsome links to the open datasets: \\n\\uf0b7  NYC Open Data: https://data.cityofnewyork.us/. \\n\\uf0b7  Chicago Open Data: https://data.cityofchicago.org/  \\n\\uf0b7  Urban  Computing  in  Microsoft  Research:  http://re-\\n\\nsearch.microsoft.com/en-us/projects/urbancompu-\\nting/default.aspx [84]. \\n\\n\\uf0b7  Urban  Noise:  311  complaint  data  with  social  media, \\nPOIs and road networks in New York City. http://re-\\nsearch.microsoft.com/pubs/217236/Context%20ma-\\ntrices%20and%20tensor%20data.zip [87].  \\n\\n\\uf0b7  Urban  Air:  air  quality  data  with  meteorological  data \\nand weather forecasts in 5 Chinese cities [31][89][85].  \\nhttp://research.microsoft.com/apps/pubs/?id=246398.  \\n\\uf0b7  Traffic speed+POIs+Road network: Features extracted \\nfrom  three  datasets  in  Beijing  have  been  accommo-\\ndated  in  three  matrices,  used  in  Example  11  [53].  \\nhttps://onedrive.live.com/?cid=cf159105855090c5&i\\nd=CF159105855090C5%212774&ithint=file,.zip&au-\\nthkey=!AFBIXgrChcesYC4.  By  adding  a  user  dimen-\\nsion into the data, a tensor is built to describe the travel \\ntime of a particular user on a particular road at a spe-\\ncific  time  slot.  The  data  was  used  in  [65]  and  can  be \\ndownload \\nfollowing  URL:  http://re-\\nsearch.microsoft.com/apps/pubs/?id=217493   \\n\\nfrom \\n\\nthe \\n\\n7.2 Images/Videos and Texts \\n\\n\\uf0b7 \\n\\n\\uf0b7 \\n\\nImage + Text: (2,866 image-text pairs from Wikipedia): \\nhttp://www.svcl.ucsd.edu/projects/crossmodal/ \\nImage + Text (1 million images with captions): \\nhttp://vision.cs.stonybrook.edu/~vicente/sbucap-\\ntions/  \\n\\n\\uf0b7  Video + Text (about 2,000 video snippets; about 40 \\n\\nsentences per snippet) http://research.mi-\\ncrosoft.com/en-us/downloads/38cf15fd-b8df-477e-\\na4e4-a4680caa75af/  \\n\\n\\uf0b7  Microsoft COCO dataset: More than 300,000 images, \\n\\neach of which has 5 captions. http://mscoco.org/. \\n\\n\\uf0b7  TACoS multilevel dataset: 44,762 video-sentence \\n\\npairs. https://www.mpi-inf.mpg.de/depart-\\nments/computer-vision-and-multimodal-compu-\\nting/research/human-activity-recognition/mpii-\\ncooking-activities-dataset/. \\nFlickr30K dataset: 31,783 images, 5 captions per im-\\nage. https://illinois.edu/fb/sec/229675.   \\n\\n\\uf0b7 \\n\\n\\uf0b7  MMDB dataset: 160 sessions of 5-minute interaction \\nfrom 121 children, including all multi-modal signals: \\nimages by camera, audio by microphones, activ-\\nity/accelerometry sensors. \\nhttp://cbi.gatech.edu/mmdb/overview.php  \\n\\n8. CONCLUSION \\n\\nThe proliferation of big data calls for advanced data fusion \\n\\n \\n\\nmethods that can discover knowledge from multiple  dis-\\nparate  datasets  with  underlying  connections.  This  paper \\nsummarizes  existing  data  fusion  methods,  classifying \\nthem into three major categories and giving real examples \\nin each sub-category of methods. This paper explores the \\nrelationship  and  differences  between  different  methods, \\nhelping  people  find proper  data  fusion  methods  to  solve \\nbig  data  problems.  Some  public  multi-modality  datasets \\nhave been shared to facilitate further research into data fu-\\nsion problems. \\n\\nACKNOWLEDGEMENT \\n\\nThanks Ying Wei who helped survey a part of the DNN-\\nbased  and  the  transfer  learning-based  data  fusion  meth-\\nods. Thanks the co-authors (such as, Jing Yuan, Vincent W. \\nZheng,  Tong  Liu,  Yanjie  Fu,  Yanchi  Liu,  Yilun  Wang, \\nJingbo Shang, Wenzhu Tong, and Wei Liu) of my publica-\\ntions that have been referenced in the survey. \\n\\nREFERENCES \\n\\n[1]  S.Abney, \\u201cBootstrapping,\\u201d Proc. of the 40th Annual Meeting of the As-\\n\\n[2] \\n\\n[3] \\n\\nsociation for Computational Linguistics (ACL), pages 360\\u2013367, 2002. \\nJ. Bao, Y. Zheng, and M. F. Mokbel, \\u201cLocation-based and Preference-\\nAware Recommendation Using Sparse Geo-Social Networking Data,\\u201d \\nProc. ACM SIGSPATIAL Conf. Advances in Geographic Information Sys-\\ntems (GIS\\u201912), pp. 199-208, 2012. \\nJ. Baxter, \"A model of inductive bias learning,\" Journal of Artificial In-\\ntelligence Research, 12, pp.149--198, 2000 \\n\\n[4]  C. Bishop. Pattern Recognition and Machine Learning, Springer 2006. \\n[5]  D. Blei, A. Ng, and M. Jordan. Latent dirichlet allocation. The Journal of \\n\\nMachine Learning Research, 3:993\\u20131022, 2003. \\n\\n[6]  M.F. Balcan, A. Blum, and Y. Ke, \\u201cCo-training and expansion: Towards \\nbridging  theory and  practice,\\u201d Computer  Science Department,  page \\n154, 2004. \\n\\n[7]  M. Belkin and P. Niyogi. Laplacian eigenmaps for dimensionality re-\\n\\nduction and data representation. Neural Computation, 15, 2003. \\n\\n[8]  Y. Bengio, A. Courville, and P. Vincent, \\u201cRepresentation learning: A re-\\nview and new perspectives,\\u201d IEEE Transactions on Pattern Analysis and \\nMachine Intelligence, vol. 35, No. 8, pp. 1798-1828, 2013. \\n\\n[9]  Christopher M. Bishop, \"Chapter 8. Graphical Models,\" Pattern Recog-\\n\\nnition and Machine Learning. Springer. pp. 359\\u2013422, 2006. \\n\\n[10]  J. Bleiholder and F. Naumann. Data fusion. ACM Computing Surveys, \\n\\nvol. 41, No. 1, pp. 1\\u201341, 2008. \\n\\n[11]  A. Blum and T. Mitchell, \\u201cCombining labeled and unlabeled data with \\nco-training,\\u201d Proc. of the eleventh annual conference on Computational learn-\\ning theory, pp. 92\\u2013100, 1998. \\n\\n[12]  U. Brefeld and T. Scheffer, \\u201cCo-em support vector learning,\\u201d Proc. of the \\n\\ntwenty-first international conference on Machine learning, pp. 16, 2004. \\n\\n[13]  L. Breiman, \"Random Forests,\" Machine Learning 45 (1): 5\\u201332. 2001. \\n[14]  R. Caruana, \"Multitask learning: A knowledge-based source of induc-\\n\\ntive bias,\" Machine Learning, 28, pp.41--75, 1997. \\n\\n[15]  S. Chawla, Y. Zheng, and J. Hu, \\u201cInferring the root cause in road traffic \\nanomalies,\\u201d Proc. of International Conference on Data Mining (ICDM\\u201912), \\npp. 141-150, 2012. \\n\\n[16]  N. Chen, J. Zhu, and E.P. Xing, \\u201cPredictive subspace learning for multi-\\nview data: A large margin approach,\\u201d Advances in Neural Information \\nProcessing Systems, 24, 2010. \\n\\n[17]  G. E. Dahl, D. Yu, L. Deng, and A. Acero, \\u201cContext-Dependent Pre-\\nTrained Deep Neural Networks for Large Vocabulary Speech Recogni-\\ntion,\\u201d IEEE Trans. Audio, Speech, and Language Processing, vol. 20, no. \\n1, pp. 33-42, Jan. 2012. \\n\\n\\x0cYU ZHENG:  METHODOLOGIES FOR CROSS-DOMAIN DATA FUSION: AN OVERVIEW 17 \\n\\n[18]  W. Dai, Y. Chen, G.-R. Xue, Q. Yang, and Y. Yu. Translated learning: \\nTransfer learning across dierent feature spaces. In NIPS, pages 353\\u2013360, \\n2008. \\n\\n[41]  D. D. Lee and H. S. Seung, \"Algorithms for non-negative matrix factor-\\nization,\" Proc. of Advances in neural information processing systems, pp. 556-\\n562, 2011. \\n\\n[19]  Z. Fang and Z. M. Zhang, \\u201cDiscriminative feature selection for multi-\\nview cross-domain learning,\\u201d Proc. of International Conference on Infor-\\nmation and Knowledge Management (CIKM 2013), pp. 1321\\u20131330, 2013. \\n\\n[20]  Y. Fu, Y. Ge, Y. Zheng, Z. Yao, Y. Liu, H. Xiong, N. Jing Yuan. Sparse \\nReal Estate Ranking with Online User Reviews and Offline Moving Be-\\nhaviors. Proc. of IEEE International Conference on Data Mining (ICDM\\u201914), \\npp. 120-129, 2014. \\n\\n[21]  D. Goldberg, N. David, M. O. Brain, T. Douglas, \\u201cUsing collaborative \\nfiltering to weave an information tapestry,\\u201d Communications of the ACM, \\nvol. 35, no. 12, pp. 61\\u201370, 1992. \\n\\n[22]  G. H. Golub and C. Reinsch, \"Singular value decomposition and least \\nsquares solutions,\" Numerische Mathematik, vol. 14, no. 5, pp.403-420. \\n1970. \\n\\n[23]  M. Gonen and E. Alpaydn, \\u201cMultiple kernel learning algorithms,\\u201d Jour-\\n\\nnal of Machine Learning Research, 12 pp.2211\\u20132268, 2011. \\n\\n[42]  D. Lemire and A. Maclachlan, \\u201cSlope One: Predictors for Online Rat-\\ning-Based Collaborative Filtering,\\u201d Proc. of SIAM Data Mining. 2005. \\n[43]  W. Liu, Y. Zheng, S. Chawla, J. Yuan and X. Xie, \\u201cDiscovering Spatio-\\nTemporal  Causal  Interactions  in  Traffic  Data  Streams,\\u201d  Proc.  ACM \\nSIGKDD  Conf.  Knowledge  Discovery  and  Data  Mining  (KDD\\'11),  pp. \\n1010-1018, 2011. \\n\\n[44]  D. Mimno and A. McCallum. Topic models conditioned on arbitrary \\nfeatures with dirichlet-multinomial regression. Uncertainty in Artificial \\nIntelligence, pages 411\\u2013418, 2008. \\n\\n[45]  A. Nakamura and N. Abe, \\u201cCollaborative Filtering Using Weighted \\nMajority Prediction Algorithms,\\u201d Proc. of the 15th International Conference \\non Machine Learning, pp. 395-403, 1998. \\n\\n[46]  J. Ngiam, A. Khosla, M. Kim, J. Nam, H. Lee, and A. Y. Ng,  \\u201cMulti-\\nmodal deep learning, \\u201d Proc. the 28th International Conference on Machine \\nLearning,  pp. 689-696, 2011. \\n\\n[24]  C. W. Granger, \\u201cInvestigating causal relations by econometric models \\nand cross-spectral methods,\\u201d Econometrica: Journal of the Econometric So-\\nciety, pp. 424-438, 1969. \\n\\n[47]  K. Nigam and R. Ghani, \\u201cAnalyzing the effectiveness and applicability \\nof  co-training,\\u201d  Proc.  of  the  ninth  international  conference  on  Infor-\\nmation and knowledge management, pp. 86\\u201393, 2000. \\n\\n[25]  D. Hardoon, S. Szedmak and J.  Shawe-Taylor, \\u201cCanonical correlation \\nanalysis: An overview with application to learning methods,\\u201d Neural \\ncomputation, 16, 12, 2639-2664. 2004. \\n\\n[26]  J.  He  and  R.  Lawrence,  \\u201cA  graph-based  framework  for  multi-task \\nmulti-view learning,\\u201d Proc. of International Conference on Machine Learn-\\ning, pp. 25\\u201332, 2011. \\n\\n[27]  T.K. Ho, \"Random Decision Forest,\" Proc. of the 3rd International Con-\\n\\nference on Document Analysis and Recognition, pp. 278\\u2013282, 1995. \\n\\n[48]  W. S. Noble, \"Support vector machine applications in computational bi-\\nology,\" In Kernel Methods in Computational Biology, Bernhard Sch\\xa8ol-\\nkopf, Koji Tsuda and Jean-Philippe Vert, editors,  chapter 3. The MIT \\nPress, 2004. \\n\\n[49]  B. Pan, Y. Zheng, D. Wilkie and C. Shahabi, \\u201cCrowd Sensing of Traffic \\nAnomalies based on Human Mobility and Social Media,\\u201d Proc. ACM \\nSIGSPATIAL Conf. Advances in Geographic Information Systems (GIS\\u201913), \\npp. 334-343, 2013. \\n\\n[28]  T.K. Ho, \"The random subspace method for constructing decision for-\\nests,\" IEEE Transactions on Pattern Analysis and Machine Intelligence, 20(8), \\n832-844, 1998. \\n\\n[50]  S. J. Pan and Q. Yang. A survey on transfer learning. IEEE Transaction \\non Knowledge Discovery and Data Engineering, 22, 10, pp. 1345 \\u2013 1359, \\n2010. \\n\\n[29]  Z. Hong, X. Mei, D. Prokhorov, D. Tao, Tracking via robust multi-task \\nmulti-view joint sparse representation. Proc. IEEE International Confer-\\nence on Computer Vision (ICCV\\u201913), pp. 649-656. 2013. \\n\\n[30]  P. O. Hoyer, \"Non-negative matrix factorization with sparseness con-\\nstraints,\" The Journal of Machine Learning Research, 5, pp. 1457-1469, 2004. \\n[31]  H. P. Hsieh, S. D. Lin, Y. Zheng. Inferring Air Quality for Station Loca-\\ntion Recommendation Based on Big Data. Proc.the 21th SIGKDD confer-\\nence on Knowledge Discovery and Data Mining, pp.437-446, 2015. \\n\\n[32]  X. Jin, F. Zhuang, H. Xiong, C. Du, P. Luo, and Q. He. Multi-task multi-\\nview learning for heterogeneous tasks. In CIKM, pages 441\\u2013450, 2014. \\n[33]  M. Kan, S. Shan, H. Zhang, S. Lao and X. Chen, \\u201cMulti-view discrimi-\\nnant analysis,\\u201d Proc. of the 12th European Conference on Computer Vision, \\npp. 808-821, 2012. \\n\\n[34]  R. Kindermann and J. L. Snell, \\u201cMarkov random fields and their appli-\\ncations,\\u201d Vol. 1. Providence, RI: American Mathematical Society, 1980. \\n[35]  V. Klema and A. J. Laub, \"The singular value decomposition: Its com-\\nputation  and  some  applications,\"  Automatic  Control,  IEEE  Transac-\\ntions on, vol. 25, no. 2, pp.164-176, 1980. \\n\\n[36]  D. Koller, N. Friedman. Probabilistic Graphical Models. Massachusetts: \\n\\nMIT Press. p. 1208, 2009. \\n\\n[37]  A. Krizhevsky, I. Sutskever, and G. Hinton, \\u201cImageNet Classification \\nwith Deep Convolutional Neural Networks,\\u201d Proc. Neural Information \\nand Processing Systems, 2012. \\n\\n[38]  P. L. Lai and C. Fyfe, \\u201cKernel and nonlinear canonical correlation anal-\\nysis,\\u201d International Journal of Neural Systems, vol. 10, no. 5, 365-377, 2000. \\n[39]  N. D. Lawrence, \\u201cGaussian process latent variable models for visuali-\\nsation of high dimensional data,\\u201d Advances in neural information pro-\\ncessing systems, 16, pp.329\\u2013336, 2004. \\n\\n[40]  Y. LeCun and M. Ranzato, \\u201cDeep learning tutorial,\\u201d In Tutorials in In-\\n\\nternational Conference on Machine Learning, 2013. \\n\\n[51]  S. Roweis and L. Saul. Nonlinear dimensionality reduction by locally \\n\\nlinear embedding. Science, 290, 2000. \\n\\n[52]  K.  Ryan,  R.  Salakhutdinov,  and  R.  Zemel,  \"Multimodal  neural  lan-\\nguage models,\" Proc. of the 31st International Conference on Machine \\nLearning, pp. 595-603, 2014. \\n\\n[53]  J. Shang, Y. Zheng, W. Tong, E. Chang, and Y. Yu, \\u201cInferring Gas Con-\\nsumption and Pollution Emission of Vehicles throughout a City,\\u201d Proc. \\nACM SIGKDD Conf. Knowledge Discovery and Data Mining (KDD\\'14),  \\npp. 1027-1036, 2014. \\n\\n[54]  A. P. Singh and G. J. Gordon, \"Relational learning via collective matrix \\nfactorization,\" In Proc. of the 14th ACM SIGKDD international confer-\\nence on Knowledge discovery and data mining, pp. 650-658, 2008. \\n\\n[55]  R.  Socher,  A.  Karpathy,  Q.  V.  Le,    C.  D.  Manning,  and  A.  Y.  Ng,  \\n\"Grounded compositional semantics for finding and describing images \\nwith  sentences,\"  Transactions  of  the  Association  for  Computational \\nLinguistics, 2, 207-218, 2014. \\n\\n[56]  N.  Srivastava,  R.  Salakhutdinov,  \\u201cMultimodal  Learning  with  Deep \\nBoltzmann Machines,\\u201d Proc. Neural Information and Processing Systems, \\n2012. \\n\\n[57]  Y. Sun and J. Han, \"Mining heterogeneous information networks: prin-\\nciples  and  methodologies,\"  Synthesis  Lectures  on  Data  Mining  and \\nKnowledge Discovery, vol. 3, no. 2, pp.1-159. 2012. \\n\\n[58]  Y. Sun, J. Han, P. Zhao, Z. Yin, H. Cheng, and T. Wu,  \\u201cRankclus: inte-\\ngrating  clustering  with  ranking  for  heterogeneous  information  net-\\nwork analysis,\\u201d Proc the 12th International Conference on Extending Data-\\nbase Technology: Advances in Database Technology, pp. 565-576, 2009. \\n\\n[59]  Y. Sun, Y. Yu, and J. Han, \"Ranking-based clustering of heterogeneous \\ninformation networks with star network schema,\" Proc the 15th ACM \\nSIGKDD International Conference on Knowledge Discovery and Data Min-\\ning, pp.797-806, 2009. \\n\\n \\n\\n\\x0c18 \\n\\nIEEE TRANSACTIONS ON BIG DATA, MANUSCRIPT ID \\n\\n[60]  J. Tenenbaum, Vin de Silva, and J. Langford. A global geometric frame-\\n\\nwork for non-linear dimensionality reduction. Science, 290, 2000. \\n\\n[61]  M. E. Tipping, Sparse Bayesian learning and the relevance vector ma-\\n\\n[81]  V. W. Zheng, Y. Zheng, X. Xie, Q. Yang, \\u201cTowards Mobile Intelligence: \\nLearning from GPS History Data for Collaborative Recommendation,\\u201d \\nArtificial Intelligence Journal, pp.17-37, 2012. \\n\\nchine, The journal of machine learning research 1 (2001): 211-244. \\n\\n[82]  Y. Zheng. Trajectory Data Mining: An Overview. ACM Transactions \\n\\n[62]  D. G. Tzikas, L. Wei, A. Likas, Y. Yang, and N. P. Galatsanos, \\u201cA tutorial \\non relevance vector machines for regression and classification with ap-\\nplications,\\u201d  University  of  Ioannina,  Ioanni,  GREECE,  Illinois  Institute  of \\nTechnology, Chicago, USA, 2006. \\n\\non Intelligent Systems and Technology, vol. 6, issue 3, pp. 1-29, 2015. \\n\\n[83]  Y. Zheng, X. Chen, Q. Jin, Y. Chen, X. Qu, X. Liu, E. Chang, W.-Y. Ma, \\nY.  Rui,  W.  Sun,  \"A  Cloud-Based  Knowledge  Discovery  System  for \\nMonitoring Fine-Grained Air Quality,\" MSR-TR-2014-40, 2013. \\n\\n[63]  M. Varma and B.R. Babu, \\u201cMore generality in efficient multiple kernel \\nlearning,\\u201d Proc. of the 26th  Annual  International Conference on  Machine \\nLearning, pp. 1065\\u20131072, 2009. \\n\\n[84]  Y. Zheng, L. Capra, O. Wolfson, H. Yang.  \\u201cUrban Computing: Con-\\ncepts, Methodologies, and Applications,\\u201d  ACM Trans. Intelligent Sys-\\ntems and Technology, vol. 5, no. 3, pp. 38-55, 2014. \\n\\n[64]  C. Wang, P. Krafft and S. Mahadevan, \\u201cManifold alignment,\\u201d In Man-\\nifold Learning: Theory and Applications, Eds. Yunqian Ma and Yun \\nFu, CRC press, 2011. \\n\\n[85]  Y. Zheng, F. Liu, H.P. Hsieh, \\u201cU-Air: When Urban Air Quality Infer-\\nence Meets Big Data,\\u201d Proc. of the 19th SIGKDD Conference on Knowledge \\nDiscovery and Data Mining. pp. 1436-1444, 2013. \\n\\n[65]  Y. Wang, Y. Zheng, and Y. Xue, \\u201cTravel Time Estimation of a Path us-\\ning Sparse Trajectories,\\u201d Proc. ACM SIGKDD Conf. Knowledge Discovery \\nand Data Mining (KDD\\'14), pp. 25-34, 2014. \\n\\n[66]  Z. Wang, D. Zhang, X. Zhou, D. Yang, Z.Yu, and Z. Yu, Discovering \\nand  profiling overlapping communities in location-based  social net-\\nworks. Systems, IEEE Transactions on Man, and Cybernetics: Systems, vol. \\n44, no. 4, pp. 499-509, 2014. \\n\\n[67]  X. Xiao, Y. Zheng, Q. Luo, and X. Xie, \\u201cFinding Similar Users Using \\nCategory-Based Location History,\\u201d Proc. ACM SIGSPATIAL Conf. Ad-\\nvances in Geographic Information Systems (GIS\\u201910), pp. 442-445, 2010. \\n\\n[68]  X. Xiao, Y. Zheng, Q. Luo, and X. Xie, \\u201cInferring Social Ties between \\nUsers with Human Location History,\\u201d J. of Ambient Intelligence and Hu-\\nmanized Computing, vol. 5, no. 1, pp. 3-19, 2014. \\n\\n[69]  C. Xu, T. Dacheng, and X. Chao, \"A survey on multi-view learning,\" \\n\\narXiv preprint arXiv: 1304.5634 (2013). \\n\\n[70]  D. Yang, D. Zhang, Z. Yu, and Z. Yu, Fine-grained preference-aware \\nlocation  search  leveraging  crowdsourced  digital  footprints  from \\nLBSNs. Proc. ACM international joint conference on Pervasive and ubiqui-\\ntous computing (UbiComp\\u201913), pp. 479-488, 2013. \\n\\n[71]  P. Yang and W. Gao, \\u201cMulti-view discriminant transfer learning,\\u201d Proc. \\nof International  Joint Conference on Artificial Intelligence,  pp. 1848\\u2013\\n1854, 2013. \\n\\n[72]  Q. Yang, Y. Chen, G.-R. Xue, W. Dai, and Y. Yu, \\u201cHeterogeneous trans-\\nfer learning for image clustering via the social web,\\u201d Proc. of ACL, pp. \\n1\\u20139, 2009. \\n\\n[73]  N. J. Yuan, F. Zhang, D. Lian, K. Zheng, S. Yu, and X. Xie, \\u201cWe know \\nhow you live: exploring the spectrum of urban lifestyles,\\u201d  Proc. the first \\nACM Conference on Online Social Networks, pp. 3-14, 2013. \\n\\n[74]  J.Yuan, Y. Zheng, X. Xie, \\u201cDiscovering regions of different functions in \\na  city  using  human  mobility  and  POIs,\\u201d  Proc.  ACM  SIGKDD  Conf. \\nKnowledge Discovery and Data Mining (KDD\\'12), pp. 186-194, 2012. \\n\\n[75]  N. J. Yuan, Y. Zheng, and X. Xie, \\u201cSegmentation of Urban Areas Using \\n\\nRoad Networks,\\u201d Technical Report MSR-TR-2012-65, 2012. \\n\\n[76]  N. J. Yuan, Y. Zheng, X. Xie, Y. Wang, K. Zheng and H. Xiong, \\u201cDiscov-\\nering Urban Functional Zones Using Latent Activity Trajectories,\\u201d IEEE \\nTransactions on Knowledge and Data Engineering, vol. 27, no. 3, pp. 1041-\\n4347, 2015 \\n\\n[77]  D. Zhang, J. He, Y. Liu, L. Si, and R. Lawrence. Multi-view transfer \\nlearning with a large margin approach. Proc. of the 17th SIGKDD confer-\\nence on Knowledge Discovery and Data Mining, pp. 1208\\u20131216, 2011. \\n\\n[78]  F. Zhang, N. J. Yuan, D. Wilkie, Y. Zheng, X. Xie, \\u201cSensing the Pulse of \\nUrban Refueling Behavior: A Perspective from Taxi Mobility\\u201d, ACM \\nTrans. Intelligent Systems and Technology, submitted for publication. \\n[79]  V. W. Zheng, B. Cao, Y. Zheng, X. Xie, Q. Yang, \\u201cCollaborative Filtering \\nMeets  Mobile  Recommendation:  A  User-centered  Approach,\\u201d  Proc. \\nAAAI Conf. Artificial Intelligence (AAAI\\u201910), pp. 236-241, 2010. \\n\\n[80]  V. W. Zheng, Y. Zheng, X. Xie, Q. Yang, \\u201cCollaborative Location and \\nActivity Recommendations with GPS History Data,\\u201d Proc. International \\nConf. World Wide Web (WWW\\u201910), pp. 1029-1038, 2010. \\n\\n \\n\\n[86]  Y. Zheng, Y. Liu, J. Yuan, X. Xie, \\u201cUrban Computing with Taxicabs,\\u201d \\n\\nProc. ACM Conf. Ubiquitous Computing (UbiComp\\u201911), pp.89-98, 2011. \\n\\n[87]  Y. Zheng, T. Liu, Y. Wang, Y. Zhu, Y. Liu, E. Chang, \\u201cDiagnosing New \\nYork City\\u2019s Noises with Ubiquitous Data,\\u201d Proc. ACM Conf. Ubiquitous \\nComputing (UbiComp\\u201914), pp. 715-725, 2014. \\n\\n[88]  Y. Zheng, X. Xie. Learning travel recommendations from user-gener-\\nated GPS traces. ACM Transaction on Intelligent Systems and Technology, \\nvol. 2, no. 1, 2-19. 2011. \\n\\n[89]  Y. Zheng, X. Yi, M. Li, R. Li, Z. Shan, E. Chang, T. Li, Forecasting Fine-\\nGrained  Air  Quality  Based  on  Big  Data.  Proc.  ACM  SIGKDD  Conf. \\nKnowledge Discovery and Data Mining (KDD\\'15), 2015. \\n\\n[90]  Y. Zheng, H. Zhang, Y. Yu, Detecting Collective Anomalies from Mul-\\ntiple Spatio-Temporal Datasets across Different Domains. Submitted to \\nACM SIGSPATIAL 2015. \\n\\n[91]  Y. Zheng, L. Zhang, Z. Ma, X. Xie, W.-Y. Ma, \\u201cRecommending friends \\nand locations based on individual location history,\\u201d ACM Trans. the \\nWeb, vol 5, no. 1, pp.5-44, 2011. \\n\\n[92]  Z. H. Zhou and M. Li, \\u201cSemi-supervised regression with co-training,\\u201d \\nPro. Of the International Joint Conference on Artificial Intelligence (IJCAI), \\n2005. \\n\\n[93]  Y. Zhu, Y. Chen, Z. Lu, S. J. Pan, G.-R. Xue, Y. Yu, and Q. Yang. Heter-\\n\\nogeneous transfer learning for image classification. In AAAI, 2011. \\n\\n \\n\\nYu Zheng is a lead researcher from Microsoft Re-\\nsearch  and  a  Chair  Professor  at  Shanghai  Jiao \\nTong University. His research interests include big \\ndata  analytics,  spatio-temporal  data  mining,  and \\nubiquitous  computing.  He  leads  the  research  on \\nurban computing in Microsoft Research, passion-\\nate  about  using  big  data  to  tackle  urban  chal-\\nlenges.  He  frequently  publishes  referred  papers \\nas  a  leading  author  at  prestigious  conferences \\nand  journals,  such  as  KDD,  VLDB,  UbiComp,  and  IEEE  TKDE.  He \\nreceived  five  best  paper  awards  from  ICDE\\'13  and  ACM  SIGSPA-\\nTIAL\\u201910,  etc.    Zheng  currently  serves  as  an  Editor-in-Chief  of  ACM \\nTransactions on Intelligent Systems and Technology and as a mem-\\nber of Editorial Advisory Board of IEEE Spectrum. He is also an Edi-\\ntorial Board Member of GeoInformatica and IEEE Transactions on Big \\nData. He has served as chair on over 10 well-known international con-\\nferences,  e.g.  the  program  co-chair  of  ICDE  2014  (Industrial  Track) \\nand UIC 2014. He has been invited to give over 10 keynote speeches \\nat international conferences and forums (e.g. IE 2014 and APEC 2014 \\nSmart  City  Forum).  He  is  an  IEEE  senior  member  and  ACM  senior \\nmember. In 2013, he was named one of the Top Innovators under 35 \\nby  MIT  Technology  Review  (TR35)  and featured  by  Time  Magazine \\nfor his research on urban computing.  In 2014, he was named one of \\nthe top  40 Business  Elites  under 40  in  China  by  Fortune  Magazine, \\nbecause of the business impact of urban computing he has been ad-\\nvocating. Zheng is also an Adjunct Professor at Hong Kong Polytech-\\nnic University, a visiting Chair Professor at XiDian University and an \\nAffiliate Professor at Southwest Jiaotong University. \\n\\n\\x0c', u'http://www.diva-portal.org\\n\\nThis is version of a paper published in Pattern Recognition Letters.\\n\\nCitation for the original published paper (version of record):\\n\\nL\\xe4ngkvist, M., Karlsson, L., Loutfi, A. (2014)\\nA review of unsupervised feature learning and deep learning for time-series modeling.\\nPattern Recognition Letters, 42(1): 11-24\\nhttp://dx.doi.org/10.1016/j.patrec.2014.01.008\\n\\nAccess to the published version may require subscription.\\n\\nN.B. When citing this work, cite the original published paper.\\n\\nPermanent link to this version:\\nhttp://urn.kb.se/resolve?urn=urn:nbn:se:oru:diva-34597\\n\\n\\x0cA Review of Unsupervised Feature Learning and Deep\\n\\nLearning for Time-Series Modeling\\n\\nMartin L\\xe4ngkvista,\\u2217, Lars Karlssona, Amy Lout\\x1ca\\n\\naApplied Autonomous Sensor Systems, School of Science and Technology, \\xd6rebro\\n\\nUniversity, SE-701 82, \\xd6rebro, Sweden\\n\\nAbstract\\n\\nThis paper gives a review of the recent developments in deep learning and un-\\n\\nsupervised feature learning for time-series problems. While these techniques\\n\\nhave shown promise for modeling static data, such as computer vision, ap-\\n\\nplying them to time-series data is gaining increasing attention. This paper\\n\\noverviews the particular challenges present in time-series data and provides a\\n\\nreview of the works that have either applied time-series data to unsupervised\\n\\nfeature learning algorithms or alternatively have contributed to modi\\x1ccations\\n\\nof feature learning algorithms to take into account the challenges present in\\n\\ntime-series data.\\n\\nKeywords:\\n\\ntime-series, unsupervised feature learning, deep learning\\n\\n1. Introduction and Background\\n\\nTime is a natural element that is always present when the human brain\\n\\nis learning tasks like language, vision and motion. Most real-world data\\n\\nhas a temporal component, whether it is measurements of natural processes\\n\\n\\u2217Corresponding author\\nEmail addresses: martin.langkvist@oru.se (Martin L\\xe4ngkvist),\\n\\nlars.karlsson@oru.se (Lars Karlsson), amy.loutfi@oru.se (Amy Lout\\x1c)\\n\\nPreprint submitted to Pattern Recognition Letters\\n\\nMarch 24, 2014\\n\\n\\x0c(weather, sound waves) or man-made (stock market, robotics). Analysis of\\n\\ntime-series data has been the subject of active research for decades (Keogh\\n\\nand Kasetty, 2002; Dietterich, 2002) and is considered by Yang and Wu\\n\\n(2006) as one of the top 10 challenging problems in data mining due to\\n\\nits unique properties. Traditional approaches for modeling sequential data\\n\\ninclude the estimation of parameters from an assumed time-series model,\\n\\nsuch as autoregressive models (L\\xfctkepohl, 2005) and Linear Dynamical Sys-\\n\\ntems (LDS) (Luenberger, 1979), and the popular Hidden Markov Model\\n\\n(HMM) (Rabiner and Juang, 1986). The estimated parameters can then\\n\\nbe used as features in a classi\\x1cer to perform classi\\x1ccation. However, more\\n\\ncomplex, high-dimensional, and noisy real-world time-series data cannot be\\n\\ndescribed with analytical equations with parameters to solve since the dy-\\n\\nnamics are either too complex or unknown (Taylor, 2009) and traditional\\n\\nshallow methods, which contain only a small number of non-linear opera-\\n\\ntions, do not have the capacity to accurately model such complex data.\\n\\nIn order to better model complex real-world data, one approach is to\\n\\ndevelop robust features that capture the relevant information. However, de-\\n\\nveloping domain-speci\\x1cc features for each task is expensive, time-consuming,\\n\\nand requires expertise of the data. The alternative is to use unsupervised\\n\\nfeature learning (Bengio and LeCun, 2007; Bengio et al., 2012; Erhan et al.,\\n\\n2010) in order to learn a layer of feature representations from unlabeled data.\\n\\nThis has the advantage that the unlabeled data, which is plentiful and easy\\n\\nto obtain, is utilized and that the features are learned from the data instead\\n\\nof being hand-crafted. Another bene\\x1ct is that these layers of feature repre-\\n\\nsentations can be stacked to create deep networks, which are more capable\\n\\n2\\n\\n\\x0cof modeling complex structures in the data. Deep networks have been used\\n\\nto achieve state-of-the-art results on a number of benchmark data sets and\\n\\nfor solving di\\x1ecult AI tasks. However, much focus in the feature learning\\n\\ncommunity has been on developing models for static data and not so much\\n\\non time-series data.\\n\\nIn this paper we review the variety of feature learning algorithms that\\n\\nhas been developed to explicitly capture temporal relationships as well as the\\n\\nvarious time-series problems that they have been used on. The properties of\\n\\ntime-series data will be discussed in Section 2 followed by an introduction to\\n\\nunsupervised feature learning and deep learning in Section 3. An overview\\n\\nof some common time-series problems and previous work using deep learning\\n\\nis given in Section 4. Finally, conclusions are given in Section 5.\\n\\n2. Properties of time-series data\\n\\nTime-series data consists of sampled data points taken from a continuous,\\n\\nreal-valued process over time. There are a number of characteristics of time-\\n\\nseries data that make it di\\x1berent from other types of data.\\n\\nFirstly, the sampled time-series data often contain much noise and have\\n\\nhigh dimensionality. To deal with this, signal processing techniques such\\n\\nas dimensionality reduction techniques, wavelet analysis or \\x1cltering can be\\n\\napplied to remove some of the noise and reduce the dimensionality. The use\\n\\nof feature extraction has a number of advantages (Nanopoulos et al., 2001).\\n\\nHowever, valuable information could be lost and the choice of features and\\n\\nsignal processing techniques may require expertise of the data.\\n\\nThe second characteristics of time-series data is that it is not certain\\n\\n3\\n\\n\\x0cthat there are enough information available to understand the process. For\\n\\nexample, in electronic nose data, where an array of sensors with various\\n\\nselectivity for a number of gases are combined to identify a particular smell,\\n\\nthere is no guarantee that the selection of sensors actually are able to identify\\n\\nthe target odour. In \\x1cnancial data when observing a single stock, which only\\n\\nmeasures a small aspect of a complex system, there is most likely not enough\\n\\ninformation in order to predict the future (Fama, 1965).\\n\\nFurther, time-series have an explicit dependency on the time variable.\\n\\nGiven an input x(t) at time t, the model predicts y(t), but an identical input\\n\\nat a later time could be associated with a di\\x1berent prediction. To solve this\\n\\nproblem, the model either has to include more data input from the past or\\n\\nmust have a memory of past inputs. For long-term dependencies the \\x1crst ap-\\n\\nproach could make the input size too large for the model to handle. Another\\n\\nchallenge is that the length of the time-dependencies could be unknown.\\n\\nMany time-series are also non-stationary, meaning that the characteristics\\n\\nof the data, such as mean, variance, and frequency, changes over time. For\\n\\nsome time-series data, the change in frequency is so relevant to the task that it\\n\\nis more bene\\x1ccial to work in the frequency-domain than in the time-domain.\\n\\nFinally, there is a di\\x1berence between time-series data and other types of\\n\\ndata when it comes to invariance. In other domains, for example computer\\n\\nvision, it is important to have features that are invariant to translations,\\n\\nrotations, and scale. Most features used for time-series need to be invariant\\n\\nto translations in time.\\n\\nIn conclusion, time-series data is high-dimensional and complex with\\n\\nunique properties that make them challenging to analyze and model. There\\n\\n4\\n\\n\\x0cis a large interest in representing the time-series data in order to reduce the\\n\\ndimensionality and extract relevant information. The key for any success-\\n\\nful application lies in choosing the right representation. Various time-series\\n\\nproblems contain di\\x1berent degrees of the properties discussed in this section\\n\\nand prior knowledge or assumptions about these properties is often infused\\n\\nin the chosen model or feature representation. There is an increasing in-\\n\\nterest in learning the representation from unlabeled data instead of using\\n\\nhand-designed features. Unsupervised feature learning have shown to be\\n\\nsuccessful at learning layers of feature representations for static data sets\\n\\nand can be combined with deep networks to create more powerful learning\\n\\nmodels. However, the feature learning for time-series data have to be mod-\\n\\ni\\x1ced in order to adjust for the characteristics of time-series data in order to\\n\\ncapture the temporal information as well.\\n\\n3. Unsupervised feature learning and deep learning\\n\\nThis section presents both models that are used for unsupervised feature\\n\\nlearning and models and techniques that are used for modeling temporal\\n\\nrelations. The advantage of learning features from unlabeled data is that the\\n\\nplentiful unlabeled data can be utilized and that potentially better features\\n\\nthan hand-crafted features can be learned. Both these advantages reduce the\\n\\nneed for expertise of the data.\\n\\n3.1. Restricted Boltzmann Machine\\n\\nThe Restricted Boltzmann Machines (RBM) (Hinton et al., 2006; Hinton\\n\\nand Salakhutdinov, 2006; Lee et al., 2008) is a generative probabilistic model\\n\\nbetween input units (visible), x, and latent units (hidden), h, see Figure 1.\\n\\n5\\n\\n\\x0cFigure 1: A 2-layer RBM for static data. The visible units x are fully connected to the\\n\\x1crst hidden layer h1.\\n\\nThe visible and hidden units are connected with a weight matrix, W and\\n\\nhave bias vectors c and b, respectively. There are no connections among\\n\\nthe visible and hidden units. The RBM can be used to model static data.\\n\\nThe energy function and the joint distribution for a given visible and hidden\\n\\nvector is de\\x1cned as:\\n\\nE(x, h) = hT Wx + bT h + cT v\\n\\nP (x, h) =\\n\\n1\\nZ\\n\\nexpE(x,h)\\n\\n(1)\\n\\n(2)\\n\\n6\\n\\n\\u210e1(\\U0001d461) \\U0001d465(\\U0001d461) \\u210e2(\\U0001d461) \\x0cwhere Z is the partition function that ensures that the distribution is nor-\\n\\nmalized. For binary visible and hidden units, the probability that hidden\\n\\nunit hj is activated given visible vector x and the probability that visible\\n\\nunit xi is activated given hidden vector h are given by:\\n\\nP (hj|x) = \\u03c3(bj +\\n\\nP (xi|h) = \\u03c3(ci +\\n\\n(cid:88)\\n(cid:88)\\n\\ni\\n\\nj\\n\\nWijxi)\\n\\nWijhj)\\n\\n(3)\\n\\n(4)\\n\\nwhere \\u03c3(\\xb7) is the activation function. The logistic function, \\u03c3(x) = 1\\n1+e\\u2212x ,\\nis a common choice for the activation function. The parameters W, b, and\\n\\nv, are trained to minimize the reconstruction error using contrastive diver-\\n\\ngence (Hinton, 2002). The learning rule for the RBM is:\\n\\n\\u2202 log P (x)\\n\\n\\u2248 (cid:104)xihj(cid:105)data \\u2212 (cid:104)xihj(cid:105)recon\\n\\n(5)\\nwhere (cid:104)\\xb7(cid:105) is the average value over all training samples. Several RBMs can\\nbe stacked to produce a deep belief network (DBN). In a deep network, the\\n\\n\\u2202Wij\\n\\nactivation of the hidden units in the \\x1crst layer is the input to the second\\n\\nlayer.\\n\\n3.2. Conditional RBM\\n\\nAn extension of RBM that models multivariate time-series data is the\\n\\nconditional RBM (cRBM), see Figure 2. A similar model is the Temporal\\n\\nRBM (Sutskever and Hinton, 2006). The cRBM consists of auto-regressive\\n\\nweights that model short-term temporal structures, and connections between\\n\\n7\\n\\n\\x0cFigure 2: A 2-layer conditional RBM for time-series data. The model order for the \\x1crst\\nand second layer is 3 and 2, respectively.\\n\\npast visible units to the current hidden units. The bias vectors in a cRBM\\n\\ndepend on previous visible units and are de\\x1cned as:\\n\\nn(cid:88)\\nn(cid:88)\\n\\ni=1\\n\\ni=1\\n\\nb\\u2217\\nj = bj +\\n\\nc\\u2217\\ni = cj +\\n\\nBix(t \\u2212 i)\\n\\nAix(t \\u2212 i)\\n\\n(6)\\n\\n(7)\\n\\nwhere Ai is the auto-regressive connections between visible units at time t\\u2212 i\\nand current visible units, Bi is the weight matrix connecting visible layer at\\n\\n8\\n\\n\\U0001d465(\\U0001d461\\u22123) \\U0001d465(\\U0001d461\\u22122) \\U0001d465(\\U0001d461\\u22121) \\u210e1(\\U0001d461\\u22121) \\u210e1(\\U0001d461) \\U0001d465(\\U0001d461) \\u210e2(\\U0001d461) \\x0ctime t \\u2212 i to the current hidden units. The model order is de\\x1cned by the\\nconstant n. The probabilities for going up or down a layer are:\\n\\n(cid:32)\\n(cid:32)\\n\\nbj +\\n\\nci +\\n\\n(cid:88)\\n(cid:88)\\n\\ni\\n\\nj\\n\\n(cid:88)\\n(cid:88)\\n\\nk\\n\\n(cid:88)\\n(cid:88)\\n\\ni\\n\\nk\\n\\ni\\n\\nWijxi +\\n\\nWijhj +\\n\\nP (hj|x) = \\u03c3\\n\\nP (xi|h) = \\u03c3\\n\\n(cid:33)\\n(cid:33)\\n\\nBijkxi(t \\u2212 k)\\n\\nAijkxi(t \\u2212 k)\\n\\n(8)\\n\\n(9)\\n\\nThe parameters \\u03b8 = {W, b, c, A, B}, are trained using contrastive divergence.\\nJust like a RBM, the cRBM can also be used as a module to create deep\\n\\nnetworks.\\n\\n3.3. Gated RBM\\n\\nThe Gated Restricted Boltzmann Machine (GRBM) (Memisevic and Hin-\\n\\nton, 2007) is another extension of the RBM that models the transition be-\\n\\ntween two input vectors. The GRBM models a weight tensor, Wijk, between\\n\\nthe input, x, the output, y, and latent variables, z. The energy function is\\n\\nde\\x1cned as:\\n\\nE(y, z; x) = \\u2212(cid:88)\\n\\nWijkxiyjzk \\u2212(cid:88)\\n\\nbkzk \\u2212(cid:88)\\n\\nijk\\n\\nk\\n\\nj\\n\\ncjyj\\n\\n(10)\\n\\nwhere b and c are the bias vectors for x and y, respectively. The conditional\\n\\nprobability of the transformation and the output image given the input image\\n\\nis:\\n\\np(y, z|x) =\\n\\n1\\n\\nZ(x)\\n\\nexp(\\u2212E(y, z; x))\\n\\n(11)\\n\\n9\\n\\n\\x0cwhere Z(x) is the partition function. Luckily, this quantity does not need to\\n\\nbe computed to perform inference or learning. The probability that hidden\\n\\nunit zi is activated given x and y is given by:\\n\\n(cid:88)\\n\\nP (zk = 1|x, y) = \\u03c3(\\n\\nWijkxiyj + bk)\\n\\n(12)\\n\\nij\\n\\nLearning the parameters is performed with an approximation method of the\\n\\ngradient called contrastive divergence (Hinton, 2002). Each latent variable\\n\\nzk learns a simple transformation that together are combined the represent\\n\\nthe full transformation. By \\x1cxating a learned transformation z and given\\n\\nan input image x, the output image y is the selected transformation applied\\n\\nto the input image. Similarly, for a \\x1cxed input image x, a given image y\\n\\ncreates a RBM that learns the transformation z by reconstructing y. These\\n\\nproperties could not be achieved with a regular RBM with input units sim-\\n\\nply being the concatenated images x and y since the latent variables would\\n\\nonly learn the spatial information for that particular image pair and not the\\n\\ngeneral transformation. The large number of parameters due to the weight\\n\\ntensor makes it impractical for large image sizes. A factored form of the\\n\\nthree-way tensor has been proposed to reduce the number of parameters to\\n\\nlearn (Memisevic and Hinton, 2010).\\n\\n3.4. Auto-encoder\\n\\nA model that does not have a partition function is the auto-encoder (Ran-\\n\\nzato et al., 2006; Bengio et al., 2007; Bengio, 2007), see Figure 3. The auto-\\n\\nencoder was \\x1crst introduced as a dimensionality reduction algorithm.\\n\\nIn\\n\\nfact, a basic linear auto-encoder learns essentially the same representation as\\n\\n10\\n\\n\\x0cFigure 3: A 1-layer auto-encoder for static time-series input. The input is the concate-\\nnation of current and past frames of visible data x. The reconstruction of x is denoted\\n\\u02c6x.\\n\\na Principal Component Analysis (PCA). The layers of visible units, x, hid-\\n\\nden units, h, and the reconstruction of the visible units, \\u02c6x, are connected via\\nweight matrices W1 and W2 and the hidden layer and reconstruction layer\\nhave bias vectors b1 and b2, respectively.\\nIt is common in auto-encoders\\nto have tied weights, that is, W2 = (W1)T . This works as a regularizer\\n\\nas it constrains the allowed parameter space and reduces the number of pa-\\n\\nrameters to learn (Bengio et al., 2012). The feed-forward activations are\\n\\ncalculated as:\\n\\n11\\n\\n\\U0001d465(\\U0001d461\\u22122) \\U0001d465(\\U0001d461\\u22121) \\U0001d465(\\U0001d461) \\u210e(\\U0001d461) \\U0001d465 (\\U0001d461\\u22122) \\U0001d465 (\\U0001d461\\u22121) \\U0001d465 (\\U0001d461) \\x0c(cid:88)\\n(cid:88)\\n\\ni\\n\\nhj = \\u03c3(\\n\\n\\u02c6xi = \\u03c3(\\n\\nW 1\\n\\njixi + b1\\nj )\\n\\nW 2\\n\\nijhj + b2\\ni )\\n\\n(13)\\n\\n(14)\\n\\nj\\n\\nwhere \\u03c3(\\xb7) is the activation function. As with the RBM, a common choice\\nis the logistic activation function. The cost function to be minimized is\\n\\nexpressed as:\\n\\nJ(\\u03b8) =\\n\\n1\\n2N\\n\\nN(cid:88)\\n\\n(cid:88)\\n\\nn\\n\\ni\\n\\ni \\u2212 \\u02c6xi\\n(x(n)\\n\\n(n))2 +\\n\\n\\u03bb\\n2\\n\\n(cid:88)\\n\\n(cid:88)\\n\\n(cid:88)\\n\\nl\\n\\ni\\n\\nj\\n\\n(W l\\n\\nij)2 + \\u03b2\\n\\n(cid:88)\\n\\n(cid:88)\\n\\nl\\n\\nj\\n\\nKL(\\u03c1||pl\\nj)\\n\\n(15)\\n\\nwhere pl\\n\\nj is the mean activation for unit j in layer l, \\u03c1 is the desired mean\\nactivation, and N is the number of training examples. KL is the Kullback-\\nLeibler (KL) divergence which is de\\x1cned as KL(\\u03c1||pl\\n+ (1 \\u2212\\n\\u03c1) log 1\\u2212\\u03c1\\n. The \\x1crst term is the square root error term that will minimize\\n1\\u2212pl\\nthe reconstruction error. The second term is the L2 weight decay term that\\n\\nj) = \\u03c1 log \\u03c1\\npl\\nj\\n\\nj\\n\\nwill keep the weight matrices close to zero. Finally, the third term is the\\n\\nsparsity penalty term and encourages each unit to only be partially activated\\n\\nas speci\\x1ced by the hyperparameter \\u03c1. The inclusion of these regularization\\n\\nterms prevents the trivial learning of a 1-to-1 mapping of the input to the\\n\\nhidden units. A di\\x1berence between auto-encoders and RBMs is that RBMs do\\n\\nnot require such regularization because the use of stochastic binary hidden\\n\\nunits acts as a very strong regularizer (Hinton, 2012). However, it is not\\n\\nuncommon to introduce an extra sparsity constraint for RBMs (Lee et al.,\\n\\n2008).\\n\\n12\\n\\n\\x0c3.5. Recurrent Neural Network\\n\\nFigure 4: A Recurrent Neural Network (RNN). The input x is transformed to the output\\nrepresentation y via the hidden units h. The hidden units have connections from the input\\nvalues of the current time frame and the hidden units from the previous time frame.\\n\\nA model that have been used for modeling sequential data is the Recur-\\n\\nrent Neural Network (RNN) (H\\xfcsken and Stagge, 2003). Generally, an RNN\\n\\nis obtained from the feedforward network by connecting the neurons\\' output\\n\\nto their inputs, see Figure 4. The short-term time-dependency is modeled by\\n\\nthe hidden-to-hidden connections without using any time delay-taps. They\\n\\nare usually trained iteratively via a procedure known as backpropagation-\\n\\nthrough-time (BPTT). RNNs can be seen as very deep networks with shared\\n\\nparameters at each layer when unfolded in time. This results in the prob-\\n\\n13\\n\\n\\U0001d465(\\U0001d461\\u22122) \\U0001d465(\\U0001d461\\u22121) \\U0001d465(\\U0001d461) \\u210e(\\U0001d461) \\U0001d466(\\U0001d461\\u22122) y(\\U0001d461\\u22121) \\U0001d466(\\U0001d461) \\u210e(\\U0001d461\\u22121) \\u210e(\\U0001d461\\u22122) \\x0clem of vanishing gradients (Pascanu et al., 2012) and has motivated the\\n\\nexploration of second-order methods for deep architectures (Martens and\\n\\nSutskever, 2012) and unsupervised pre-training. An overview of strategies\\n\\nfor training RNNs is provided by Sutskever (2012). A popular extension is\\n\\nthe use of the purpose-built Long-short term memory cell (Hochreiter and\\n\\nSchmidhuber, 1997) that better \\x1cnds long-term dependencies.\\n\\n3.6. Deep learning\\n\\nThe models presented in this section use a non-linear activation function\\n\\non the hidden units. This non-linearity enables a more expressive model that\\n\\ncan learn more abstract representations when multiple modules are stacked\\n\\non top of each other to form a deep network (if linear features would be\\n\\nstacked the result would still be a linear operation). The goal of a deep net-\\n\\nwork is to build features at the lower layers that will disentangle the factors\\n\\nof variations in the input data and then combine these representations at\\n\\nthe higher layers. It has been proposed that a deep network will generalize\\n\\nbetter because it has a more compact representation (Le Roux and Bengio,\\n\\n2008). However, the di\\x1eculty with training multiple layers of hidden units\\n\\nlies in the problem of vanishing gradients when the error signal is backpropa-\\n\\ngated (Bengio et al., 1994). This can be solved by doing unsupervised greedy\\n\\nlayer-wise pre-training of each layer. This acts as an unusual form of regular-\\n\\nization (Erhan et al., 2010) that avoids poor local minima and gives a better\\n\\ninitialization than a random initialization (Bengio et al., 2012). However,\\n\\nthe importance of parameter initialization is not as crucial as other factors\\n\\nsuch as input connections and architecture (Saxe et al., 2011).\\n\\n14\\n\\n\\x0c3.7. Convolution and pooling\\n\\nFigure 5: A 2-layer convolutional neural network.\\n\\nA technique that is particularly interesting for high-dimensional data,\\n\\nsuch as images and time-series data, is convolution. In a convolutional set-\\n\\nting, the hidden units are not fully connected to the input but instead di-\\n\\nvided into locally connected segments, see Figure 5. Convolution has been\\n\\napplied to both RBMs and auto-encoders to create convolutional RBMs (con-\\n\\nvRBM) (Lee et al., 2009b,a) and convolutional auto-encoders (convAE) (Masci\\n\\net al., 2011). A Time-Delay Neural Network (TDNN) is a specialization of\\n\\nArti\\x1ccial Neural Networks (ANN) that exploits the time structure of the\\n\\ninput by performing convolutions on overlapping windows.\\n\\n15\\n\\n\\u210e1(\\U0001d461) \\U0001d465(\\U0001d461) \\u210e2(\\U0001d461) \\x0cA common operator used together with convolution is pooling, which\\n\\ncombines nearby values in input or feature space through a max, average or\\n\\nhistogram operator. The purpose of pooling is to achieve invariance to small\\n\\nlocal distortions and reduce the dimensionality of the feature space. The work\\n\\nby Lee et al. (2009a) introduces probabilistic max-pooling in the context\\n\\nof convolutional RBMs. The Space-Time DBN (ST-DBN) (Bo Chen and\\n\\nde Freitas, 2010) uses convolutional RBMs together with a spatial pooling\\n\\nlayer and a temporal pooling layer to build invariant features from spatio-\\n\\ntemporal data.\\n\\n3.8. Temporal coherence\\n\\nThere are a number of other ways besides the architectural structure\\n\\nthat can be used to capture temporal coherence in data. One way is to in-\\n\\ntroduce a smoothness penalty on the hidden variables in the regularization.\\n\\nThis is done by minimizing the changes in the hidden unit activations from\\none frame to the next by min|h(t) \\u2212 h(t \\u2212 1)|. The motivation behind this\\nis that for sequential data the hidden unit activations should not change\\n\\nmuch if the time-dependent data is fed to the model in a chronological or-\\n\\nder. Other strategies include penalizing the squared di\\x1berence, slow feature\\n\\nanalysis (Wiskott and Sejnowski, 2002), or as a function of other factors, for\\n\\nexample the change in the input data in order to adapt to both slow and\\n\\nrapid changing input data.\\n\\nTemporal coherence is related to invariant feature representations since\\n\\nboth methods want to achieve small changes in the feature representation for\\n\\nsmall changes in the input data. It is suggested in Hinton et al. (2011) that\\n\\nthe pose parameters and a\\x1ene transformations should be modeled instead\\n\\n16\\n\\n\\x0cof using invariant feature representations. In that case, temporal coherence\\n\\nshould be over a group of numbers, such as the position and pose of the\\n\\nobject rather than a single scalar. This could for example be achieved using\\n\\na structured sparsity penalty (Kavukcuoglu et al., 2009).\\n\\n3.9. Hidden Markov Model\\n\\nThe Hidden Markov Model (HMM) (Rabiner and Juang, 1986) is a pop-\\n\\nular model for modeling sequential data and is de\\x1cned by two probability\\ndistributions. The \\x1crst one is the transition distribution P (yt|yt\\u22121), which\\nde\\x1cnes the probability of going from one hidden state y to the next hidden\\nstate. The second one is the observation distribution P (xt|yt), which de\\x1cnes\\nthe relation between observed x values and hidden y states. One assumption\\n\\nis that these distributions are stationary. However, the main problem with\\n\\nHMMs are that they require a discrete state space, often have unrealistic\\n\\nindependence assumptions, and have a limited representational capacity of\\n\\ntheir hidden states (Mohamed and Hinton, 2010). HMMs require 2N hidden\\n\\nstates in order to model N bits of information about the past history.\\n\\n3.10. Summary\\n\\nTable 1 gives a summary of the brie\\x1dy presented models in this section.\\n\\nThe \\x1crst column indicates whether the model is capable of capturing tem-\\n\\nporal relations. A model that captures temporal relations does so by having\\n\\na memory of past inputs. The memory of a model, indicated in the second\\n\\ncolumn, means how many steps back in time an input have on the current\\n\\nframe. Without the temporal order, any permutation of the feature sequence\\n\\n17\\n\\n\\x0cwould yield the same distribution (Humphrey et al., 2013). The implemen-\\n\\ntation of a memory is performed di\\x1berently between the models. In a cRBM,\\n\\ndelay taps are used to create a short-term dependency on past visible units.\\n\\nThe long-term dependency comes from modeling subsequent layers. This\\n\\nmeans that the length of the memory for a cRBM is increased for each added\\n\\nlayer. The model order for a cRBM in one layer is typically below 5 for input\\n\\nsizes around 50. A decrease in the input size would allow a higher model\\n\\norder. In an RNN, hidden units in the current time frame are a\\x1bected by the\\n\\nstate of the hidden units in the previous time frame. This can create a ripple\\n\\ne\\x1bect with a duration of potentially in\\x1cnite time frames. On the other hand,\\n\\nthis ripple e\\x1bect can be prevented by using a forget gate (Gers et al., 2000).\\n\\nThe use of Long-short term memory (Hochreiter and Schmidhuber, 1997) or\\n\\nhessian-free optimizer (Martens and Sutskever, 2012) can produce recurrent\\n\\nnetworks that has a memory of over 100 time steps. The Gated RBM and\\n\\nthe convolutional GRBM models transitions between pairs of input vectors\\n\\nso the memory for these models is 2. The Space-Time DBN (Bo Chen and\\n\\nde Freitas, 2010) models 6 sequences of outputs from the spatial pooling\\n\\nlayer, which is a longer memory than GRBM, but using a lower input size.\\n\\nThe last column in Table 1 indicates if the model is generative (as op-\\n\\nposed to discriminative). A generative model can generate observable data\\n\\ngiven a hidden representation and this ability is mostly used for generating\\n\\nsynthetic data of future time steps. Even though the auto-encoder is not\\n\\ngenerative, a probabilistic interpretation can be made using auto-encoder\\n\\nscoring (Kamyshanska and Memisevic, 2013; Bengio et al., 2013).\\n\\nFor selecting a model for a particular problem, a number of questions\\n\\n18\\n\\n\\x0cTable 1: A summary of commonly used models for feature learning.\\n\\nMethod\\n\\nTemporal\\nrelation\\n\\nRBM\\n-\\nAE\\n-\\n(cid:88)\\nRNN\\n(cid:88)\\ncRBM\\n(cid:88)\\nTDNN\\nANN\\n-\\n(cid:88)\\nGRBM\\nConvGRBM (cid:88)\\nConvRBM -\\n-\\nConvAE\\n(cid:88)\\nST-DBN\\n\\nMemory Typical\\n\\nGenerative\\n\\ninput\\nsize\\n10-1000\\n10-1000\\n50-1000\\n50\\n5-50\\n10-1000\\n<64x64\\n>64x64\\n>64x64\\n>64x64\\n10x10\\n\\n-\\n-\\n1-100\\n2-5\\n2-5\\n-\\n2\\n2\\n-\\n-\\n2-6\\n\\n(cid:88)\\n-\\n(cid:88)\\n(cid:88)\\n-\\n-\\n(cid:88)\\n(cid:88)\\n(cid:88)\\n-\\n(cid:88)\\n\\nshould be taken into consideration: (1) Use a generative or discriminative\\n\\nmodel? (2) What are the properties of the data? and (3) How large is the\\n\\ninput size? A generative model is preferred if the trained model should be\\n\\nused for synthesizing new data or prediction tasks where partial input data\\n\\n(data at t + 1) need to be reconstructed. If the task is to do classi\\x1ccation,\\n\\na discriminative model is su\\x1ecient. A discriminative model will attempt to\\n\\nmodel the training data even if that data is noisy while a generative model\\n\\nwill simply assign a low probability for outliers. This makes a generative\\n\\nmodel more robust for noisy inputs and a better outlier detector. There is\\n\\nalso the factor of training time. Generative models use Gibbs sampling to\\n\\napproximate the derivatives for each parameter update while a discrimina-\\n\\ntive model calculates the exact gradients in one iteration. However, if the\\n\\nsimulation time is an issue, it is a good idea to look for hardware solutions\\n\\n19\\n\\n\\x0cor the choice of optimization method before considering which method is the\\n\\nfastest. When the combination of input size, model parameters, and number\\n\\nof training examples in one training batch is large, the training time could\\n\\nbe decreased by performing the parameter updates on a GPU instead of the\\n\\nCPU. For large-scale problems, i.e., the number of training examples is large,\\n\\nit is recommended to use stochastic gradient descent instead of L-BFGS or\\n\\nconjugate gradient descent as optimization method (Bottou, 2010). Further-\\n\\nmore, if the data has a temporal structure it is not recommended to treat\\n\\nthe input data as a feature vector since this will discard the temporal in-\\n\\nformation.\\n\\nInstead, a model that inherently models temporal relations or\\n\\nincorporates temporal coherence (by regularization or temporal pooling) in\\n\\na static model is a better approach. For high-dimensional problems, like\\n\\nimages which have a pictorial structure, it may be appropriate to use convo-\\n\\nlution. The use of pooling further decreases the number of dimensions and\\n\\nintroduces invariance for small translations of the input data.\\n\\n4. Classical time-series problems\\n\\nIn this section we will highlight some common time-series problems and\\n\\nthe models that have been used to address them in the literature. We will fo-\\n\\ncus on complex problems that require the use of models with hidden variables\\n\\nfor feature representation and where the representations are fully or partially\\n\\nlearned from unlabeled data. A summary of the classical time-series problems\\n\\nthat will be presented in this section is given in Table 2.\\n\\n20\\n\\n\\x0cFigure 6: Four images from the KTH action recognition data set of a person running at\\nframe 100, 105, 110, and 115. The KTH data set also contains videos of walking, jogging,\\nboxing, hand waving, and handclapping.\\n\\n4.1. Videos\\n\\nVideo data are series of images over time (spatio-temporal data) and can\\n\\ntherefore be viewed as high-dimensional time-series data. Figure 6 shows\\n\\na sequence of images from the KTH activity recognition data set1. The\\n\\ntraditional approach to modeling video streams is to treat each individual\\n\\nstatic image and detecting interesting points using common feature detectors\\n\\nsuch as SIFT (Lowe, 1999) or HOG (Dalal and Triggs, 2005). These features\\n\\nare domain-speci\\x1cc for static images and are not easily extended to other\\n\\ndomains such as video (Le et al., 2011).\\n\\nThe approach taken by Stavens and Thrun (2010) learns its own domain-\\n\\noptimized features instead of using pre-de\\x1cned features, but still from static\\n\\nimages. A better approach to modeling videos is to learn image transitions\\n\\ninstead of working with static images. A Gated Restricted Boltzmann Ma-\\n\\nchine (GRBM) (Memisevic and Hinton, 2007) has been used for this purpose\\n\\nwhere the input, x, of the GRBM is the full image in one time frame and\\n\\nthe output y is the full image in the subsequent time frame. However, since\\n\\nthe network is fully connected to the image the method does not scale well\\n\\n1http://www.nada.kth.se/cvap/actions/\\n\\n21\\n\\n\\x0cto larger images and local transformations at multiple locations must be\\n\\nre-learned.\\n\\nA convolutional version of the GRBM using probabilistic max-pooling is\\n\\npresented by Taylor et al. (2010). The use of convolution reduces the number\\n\\nof parameters to learn, allows for larger input sizes, and better handles the\\n\\nlocal a\\x1ene transformations that can appear anywhere in the image. The\\n\\nmodel was validated on synthetic data and a number of benchmark data\\n\\nsets, including the KTH activity recognition data set.\\n\\nThe work by Le et al. (2011) presents an unsupervised spatio-temporal\\n\\nfeature learning method using an extension of Independent Subspace Analysis\\n\\n(ISA) (Hyv\\xe8arinen et al., 2009). The extensions include hierarchical (stacked)\\n\\nconvolutional ISA modules together with pooling. A disadvantage of ISA is\\n\\nthat it does not scale well to large input sizes. The inclusion of convolution\\n\\nand stacking solves this problem by learning on smaller patches of input\\n\\ndata. The method is validated on a number of benchmark sets, including\\n\\nKTH. One advantage of the method is that the use of ISA reduces the need\\n\\nfor tweaking many of the hyperparameters seen in RBM-based methods, such\\n\\nas learning rate, weight decay, convergence parameters, etc.\\n\\nModeling temporal relations in video have also been done using temporal\\n\\npooling. The work by Bo Chen and de Freitas (2010) uses convolutional\\n\\nRBMs as building blocks for spatial pooling and then performs temporal\\n\\npooling on the spatial pooling units. The method is called Space-Time Deep\\n\\nBelief Network (ST-DBN). The ST-DBN allows for invariance and statistical\\n\\ndependencies in both space and time. The method achieved superior perfor-\\n\\nmance on applications such as action recognition and video denoising when\\n\\n22\\n\\n\\x0ccompared to a standard convolutional DBN.\\n\\nThe use of temporal coherence for modeling videos is done by Zou et al.\\n\\n(2011), where an auto-encoder with a L1-cost on the temporal di\\x1berence on\\n\\nthe pooling units is used to learn features that improve object recognition\\n\\non still images. The work by Hyv\\xe4rinen et al. (2003) also uses temporal\\n\\ninformation as a criterion for learning representations.\\n\\nThe use of deep learning, feature learning, and convolution with pooling\\n\\nhas propelled the advances in video processing. Modeling streams of video is\\n\\na natural continuation for deep learning algorithms since they have already\\n\\nbeen shown to be successful at building useful features from static images. By\\n\\nfocusing on learning temporal features in videos, the performance on static\\n\\nimages can be improved, which motivates the need for continuing developing\\n\\ndeep learning algorithms that capture temporal relations. The early attempts\\n\\nat extending deep learning algorithms to video data was done by modeling the\\n\\ntransition between two frames. The use of temporal pooling extends the time-\\n\\ndependencies a model can learn beyond a single frame transition. However,\\n\\nthe time-dependency that has been modeled is still just a few frames. A\\n\\npossible future direction for video processing is to look at models that can\\n\\nlearn longer time-dependencies.\\n\\n4.2. Stock market prediction\\n\\nStock market data are highly complex and di\\x1ecult to predict, even for\\n\\nhuman experts, due to a number of external factors, e.g., politics, global\\n\\neconomy, and trader expectation. The trends in stock market data tend to\\n\\nbe nonlinear, uncertain, and non-stationary. Figure 7 shows the Dow Jones\\n\\nIndustrial Average (DJOI) over a decade. According to the E\\x1ecient Market\\n\\n23\\n\\n\\x0cFigure 7: Dow Jones Industrial Average (DJOI) over a period of 10 years.\\n\\nHypothesis (EMH) (Fama, 1965), stock market prices follow a random walk\\n\\npattern, meaning that a stock has the same probability to go up as it has\\n\\nto go down, resulting in that predictions can not have more than 50% accu-\\n\\nracy (Tsai and Hsiao, 2010). The EMH state that stock prices are largely\\n\\ndriven by \"news\" rather than present and past prices. However, it has also\\n\\nbeen argued that stock market prices do not follow a random walk and that\\n\\nthey can be predicted (Malkiel, 2003). The landscape for acquiring both\\n\\nnews and stock information looks very di\\x1berent today than it did decades\\n\\nago. As an example, it has been shown that predicted stock prices can be\\n\\nimproved if further information is extracted from online social media, such\\n\\nas Twitter feeds (Bollen et al., 2011) and online chat activity (Gruhl et al.,\\n\\n2005).\\n\\nOne model that has emerged and shown to be suitable for stock market\\n\\n24\\n\\n20002001200220032004200620072008200920100.70.80.911.11.21.31.4x 104YearIndex\\x0cprediction is the arti\\x1ccial neural network (ANN) (Atsalakis and Valavanis,\\n\\n2009). This is due to its ability to handle non-linear complex systems. A\\n\\nsurvey of ANNs applied to stock market prediction is given in Li and Ma\\n\\n(2010). However, most approaches of ANN applied to stock prediction have\\n\\ngiven unsatisfactory results (Agrawal et al., 2013). Neural networks with\\n\\nfeedback have also been tried, such as recurrent versions of TDNN (Kim,\\n\\n1998), wavelet transformed features with an RNN (Hsieh et al., 2011), and\\n\\necho state networks (Lin et al., 2009). Many of these methods are applied di-\\n\\nrectly on the raw data, while other papers focus more on the feature selection\\n\\nstep (Tsai and Hsiao, 2010).\\n\\nIn summary, it can be concluded that there is still room to improve ex-\\n\\nisting techniques for making safe and accurate stock prediction systems. If\\n\\nadditional information from sources that a\\x1bect the stock market can be mea-\\n\\nsured and obtained, such as general public opinions from social media (Bollen\\n\\net al., 2011), trading volume (Zhu et al., 2008), market speci\\x1cc domain knowl-\\n\\nedge, and political and economical factors, it can be combined together with\\n\\nthe stock price data to achieve higher stock price predictions (Agrawal et al.,\\n\\n2013). The limited success of applying small, one layer neural networks for\\n\\nstock market prediction and the realization that there is a need to add more\\n\\ninformation to make better predictions indicate that a future direction for\\n\\nstock market prediction is to apply the combined data to more powerful\\n\\nmodels that are able to handle such complex, high-dimensional data. Deep\\n\\nlearning methods for multivariate time-series \\x1ct this description and provide\\n\\nnew interesting approach for the \\x1cnancial \\x1celd and a new challenging appli-\\n\\ncation for the deep learning community, which to the authors knowledge has\\n\\n25\\n\\n\\x0cnot yet been tried.\\n\\n4.3. Speech recognition\\n\\nFigure 8: Raw acoustic signal of the utterance of the sentence \"The quick brown fox jumps\\nover the lazy dog\".\\n\\nSpeech recognition is one area where deep learning has made signi\\x1ccant\\n\\nprogress (Hinton et al., 2012). The problem of speech recognition can be\\n\\ndivided into a variety of sub-problems, such as speaker identi\\x1ccation (Lee\\n\\net al., 2009a), gender identi\\x1ccation (Lee et al., 2009b; Parris and Carey,\\n\\n1996), speech-to-text (Furui et al., 2004) and acoustic modeling. The raw\\n\\ninput data is single channel and highly time and frequency dependent, see\\n\\nFigure 8. A common approach is to use pre-set features that are designed\\n\\nfor speech processing such as Mel-frequency cepstral coe\\x1ecients (MFCC).\\n\\nFor decades, Hidden Markov Models (HMMs) (Rabiner and Juang, 1986)\\n\\nhave been the state-of-the-art technique for speech recognition. A common\\n\\n26\\n\\n12345678\\u22120.03\\u22120.02\\u22120.0100.010.020.03Time [s]\\x0cmethod for discretization of the input data for speech that is required by the\\n\\nHMM is to use Gaussian mixture models (GMM). More recently however,\\n\\nthe Restricted Boltzmann Machines (RBM) have shown to be an adequate\\n\\nalternative for replacing the GMM in the discretization step. A classi\\x1cca-\\n\\ntion error of 20.7% on the TIMIT speech recognition data set2 was achieved\\n\\nby (Mohamed et al., 2012) by training a RBM on MFCC features. A sim-\\n\\nilar setup has been used for large vocabulary speech recognition by Dahl\\n\\net al. (2012). A convolutional deep belief networks was applied by Lee et al.\\n\\n(2009b) to audio data and evaluated on various audio classi\\x1ccation tasks.\\n\\nA number of variations on the RBM have also been tried on speech data.\\n\\nThe mean-covariance RBM (mcRBM) (Ranzato and Hinton, 2010; Ranzato\\n\\net al., 2010) achieved a classi\\x1ccation error of 20.5% on the TIMIT data set\\n\\nby Dahl et al. (2010). A conditional RBM (cRBM) was modi\\x1ced by Mo-\\n\\nhamed and Hinton (2010) by including connections from future instead of\\n\\nonly having connections from the past, which presumably gave better clas-\\n\\nsi\\x1ccation because the near future is more relevant than the more distant\\n\\npast.\\n\\nEarlier, a Time-Delay Neural Network (TDNN) has been used for speech\\n\\nrecognition (Waibel et al., 1989) and a review of TDNN architectures for\\n\\nspeech recognition is given by Sugiyama et al. (1991). However, it has been\\n\\nsuggested that convolution over the frequency instead of the time is better\\n\\nsince the HMM on top models the temporal information.\\n\\nThe recent work by Graves et al. (2013) uses a deep Long Short-term\\n\\nMemory Recurrent Neural Network (RNN) (Hochreiter and Schmidhuber,\\n\\n2http://www.ldc.upenn.edu/Catalog/\\n\\n27\\n\\n\\x0c1997) to achieve a classi\\x1ccation error of 17.7% on the TIMIT data set, which\\n\\nis the best result to date. One di\\x1berence between the approaches of RBM-\\n\\nHMM and RNN is that the RNN can be used as an \\'end-to-end\\' model be-\\n\\ncause it replaces a combination of di\\x1berent techniques that are currently used\\n\\nin sequence modeling, such as the HMM. However, both these approaches\\n\\nstill rely on pre-de\\x1cned features as input.\\n\\nWhile using features such as MFCCs that collapse high dimensional speech\\n\\nsound waves into low dimensional encodings have been successful in speech\\n\\nrecognition systems, such low dimensional encodings may lose some relevant\\n\\ninformation. On the other hand, there are approaches that build their own\\n\\nfeatures instead of using pre-de\\x1cned features. The work by Jaitly and Hin-\\n\\nton (2011) used raw speech as input to a RBM and achieved a classi\\x1ccation\\n\\nerror of 21.8% on the TIMIT data set. Another approach that uses raw\\n\\ndata is learning the auditory codes using spiking population code (Smith\\n\\nand Lewicki, 2005). In this model, each spike encodes the precise time posi-\\n\\ntion and magnitude of a localized, time varying kernel function. The learned\\n\\nrepresentations (basis vectors) show a striking resemblance to the cochlear\\n\\n\\x1clters in the auditory cortex.\\n\\nSimilarly sparse coding for audio classi\\x1ccation is used by Grosse et al.\\n\\n(2007). The authors used features as input and a shift-invariant sparse coding\\n\\nmodel that reconstructs a time-series input using all the basis functions in\\n\\nall possible shifts. The model was evaluated on speaker identi\\x1ccation and\\n\\nmusic genre classi\\x1ccation.\\n\\nA multimodal framework was explored by Ngiam et al. (2011) where\\n\\nvideo data of spoken digits and letters where combined with the audio data\\n\\n28\\n\\n\\x0cto improve the classi\\x1ccation.\\n\\nIn conclusion, there have been a lot of recent improvements to the pre-\\n\\nvious dominance of the features-GMM-HMM structure that has been used\\n\\nin speech recognition. First, there is a trend towards replacing GMM with\\n\\na feature learning model such as deep belief networks or sparse coding. Sec-\\n\\nond, there is a trend towards replacing HMM with other alternatives. One of\\n\\nthem is the conditional random \\x1celd (CRF) (La\\x1berty et al., 2001) that have\\n\\nbeen shown to outperform HMM, see for example the work by van Kasteren\\n\\net al. (2008) and Bengio and Frasconi (1996). However, to date, the best\\n\\nreported result is replacing both parts of GMM-HMM with RNN (Graves\\n\\net al., 2013). A next possible step for speech processing would be to replace\\n\\nthe pre-made features with algorithms that build even better features from\\n\\nraw data.\\n\\n4.4. Music recognition\\n\\nMusic recognition is similar to speech recognition with the exception that\\n\\nthe data can be multivariate and either presented as raw acoustic signals\\n\\nor by discrete chords. In music recognition, a number of sub-problems are\\n\\nconsidered, such as music annotation (genre, chord, instrument, mood classi-\\n\\n\\x1ccation), music retrieval (text-based content search, content-based similarity\\n\\nretrieval, organization), and tempo identi\\x1ccation. For music recognition,\\n\\na commonly used set of features are MFCCs, chroma, constant-Q spectro-\\n\\ngrams (CQT) (Schoerkhuber and Klapuri, 2010), local contrast normalization\\n\\n(LCN) (LeCun et al., 2010), or Compressive Sampling (CS) (Chang et al.,\\n\\n2010). However, there is an increasing interest in learning the features from\\n\\nthe data instead of using highly engineered features based on acoustic knowl-\\n\\n29\\n\\n\\x0cedge. A widely used data set for music genre recognition is GTZAN3. Even\\n\\nthough it is possible to solve many tasks on text-based meta-data, such as\\n\\nuser data (playlists, song history, social structure), there is still a need for\\n\\ncontent-based analysis. The reasons for this is that manual labeling is inef-\\n\\n\\x1ccient due to the large amount of music content and some tasks require the\\n\\nwell-trained ear of an expert, e.g., chord recognition.\\n\\nThe work by Humphrey et al. (2013) gives a review and future directions\\n\\nfor music recognition. In this work, three de\\x1cciencies are identi\\x1ced: hand-\\n\\ncrafted features are sub-optimal and unsustainable to develop for each task,\\n\\nshallow architectures are fundamentally limited, and short-time analysis can-\\n\\nnot encode a musically meaningful structure. To handle these de\\x1cciencies it\\n\\nis proposed to learn features automatically, apply deep architectures, and\\n\\nmodel longer time-dependencies than the current use of data in milliseconds.\\n\\nThe work by Nam et al. (2012) addresses the \\x1crst de\\x1cciency by presenting\\n\\na processing pipeline for automatically learning features for music recogni-\\n\\ntion. The model follows the structure of a high-dimensional single layer net-\\n\\nwork with max-pooling separately after learning the features (Coates et al.,\\n\\n2010). The input data is taken from multiple audio frames and fed into three\\n\\ndi\\x1berent feature learning algorithms, namely K-means clustering, sparse cod-\\n\\ning, and RBM. The learned features gave better performance compared to\\n\\nMFCC, regardless of the feature learning algorithm.\\n\\nSparse coding have been used by Grosse et al. (2007) for learning features\\n\\nfor music genre recognition. The work by Hena\\x1b et al. (2011) used Predictive\\n\\nSparse Decomposition (PSD), which is similar to sparse coding, and achieved\\n\\n3http://marsyas.info/download/data_sets\\n\\n30\\n\\n\\x0can accuracy of 83.4% on the GTZAN data. In this work, the features are au-\\n\\ntomatically learned from CTQ spectograms in an unsupervised manner. The\\n\\nlearned features capture information about which chords are being played in\\n\\na particular frame and produce comparable results to hand-crafted features\\n\\nfor the task of genre recognition. A limitation, however, is that it ignores\\n\\ntemporal dependencies between frames.\\n\\nConvolutional DBNs were used by Lee et al. (2009b) to learn features from\\n\\nspeech and music spectrograms and from engineered features by Dieleman\\n\\net al. (2011). The work by (Hamel and Eck, 2010) also uses convolutional\\n\\nDBN to achieve an accuracy of 84.3% on the GTZAN dataset.\\n\\nSelf-taught learning have also been used for music genre classi\\x1ccation.\\n\\nThe self-taught learning framework attempts to use unlabeled data that does\\n\\nnot share the labels of the classi\\x1ccation task to improve classi\\x1ccation perfor-\\n\\nmance (Raina et al., 2007; Jialin Pan and Yang, 2010). Self-taught learning\\n\\nand sparse coding are used by Markov and Matsui (2012) where unlabeled\\n\\ndata from other music genres other than in the classi\\x1ccation task was used\\n\\nto train the model.\\n\\nIn conclusion, there are many works that use unsupervised feature learn-\\n\\ning methods for music recognition. The motivation for using deep networks\\n\\nis that music itself is structured hierarchically by a combination of chords,\\n\\nmelodies and rhythms that creates motives, phrases, sections and \\x1cnally en-\\n\\ntire pieces (Humphrey et al., 2013). Just like in speech recognition, the input\\n\\ndata is often in some form of spectrograms. Many works leave the natural\\n\\nstep of learning features from raw data as future work (Nam, 2012). Still, as\\n\\nproposed by (Humphrey et al., 2013), even though convolutional networks\\n\\n31\\n\\n\\x0chave given good results on time-frequency representations of audio, there is\\n\\nroom for discovering new and better models.\\n\\n4.5. Motion capture data\\n\\nFigure 9: A sequence of human motion from the CMU motion capture data set.\\n\\nModeling human motion has several applications such as tracking, activ-\\n\\nity recognition, style and content separation, person identi\\x1ccation, computer\\n\\nanimation, and synthesis of new motion data. Motion capture data is col-\\n\\nlected from recordings of movements from several points on the body of a\\n\\nhuman actor. These points can be captured by cameras that either track\\n\\nthe position of strategically placed markers (usually at joint centers) or uses\\n\\nvision-based algorithms for tracking points of interest (Gleicher, 2000). The\\n\\npoints are represented as 3D Cartesian coordinates over time and are used to\\n\\nform a skeletal structure with constant limb lengths by translating the points\\n\\nto relative joint angles. The joint angles can be expressed in Euler angles,\\n\\n32\\n\\n\\u221220020Position\\u221210\\u2212505Rotation\\u221250510Lower back\\u22126\\u22124\\u2212202Upper back\\u22126\\u22124\\u2212202Thorax\\u221215\\u221210\\u221250Lower neck\\u221215\\u221210\\u2212505Upper neck\\u2212202Head\\u2212505x 10\\u221214Clavicle\\u221280\\u221260\\u221240\\u221220020Humerus708090100110Radius\\u2212100102030Wrist\\u221240\\u22122002040Hand6.577.58Finger\\u22121001020Thumb\\u2212505x 10\\u221214Clavicle 2\\u221250050100Humerus 2\\u221230\\u221220\\u221210010 Hand 202040Thumb 2\\u221240\\u221220020Femur406080100Libia\\u221220\\u221210010Foot\\u221230\\u221220\\u2212100Toes\\u221240\\u2212200Femur 2\\u221220020Foot 2\\x0c4D quaternions, or exponential map parameterization (Grassia, 1998) and\\n\\ncan have 1-3 degrees of freedom (DOF) each. The full data set consists of\\n\\nthe orientation and translation of the root and all relative joint angles for\\n\\neach time frame as well as the constant skeleton model. The data is noisy,\\n\\nhigh-dimensional, and multivariate with complex nonlinear relationships. It\\n\\nhas a lower frequency compared to speech and music data and some of the\\n\\nsignals may be task-redundant.\\n\\nSome of the traditional approaches include the work by Brand and Hertz-\\n\\nmann (2000), which models both the style and content of human motion using\\n\\nHidden Markov Models (HMMs). The di\\x1berent styles were learned from un-\\n\\nlabeled data and the trained model was used to synthesize motion data. A\\n\\nlinear dynamical systems was used by Chiappa et al. (2009) to model three\\n\\ndi\\x1berent motions of a human performing the task of holding a cup that has\\n\\na ball attached to it with a string and then try to catch the ball into the cup\\n\\n(game of Balero). A Bayesian mixture of linear Gaussian state-space models\\n\\n(LGSSM) was trained with data from a human learner and used to generate\\n\\nnew motions that was clustered and simulated on a robotic manipulator.\\n\\nBoth HMMs and linear dynamical systems are limited by their ability\\n\\nto model complex full-body motions. The work by Wang et al. (2007) uses\\n\\nGaussian Processes to model three styles of locomotive motion (walk, run,\\n\\nstride) from the CMU motion capture data set4, see Figure 9. The CMU\\n\\ndata set have also been used to generate motion capture from just a few\\n\\ninitialization frames with a Temporal RBM (TRBM) (Sutskever and Hin-\\n\\nton, 2006) and a conditional RBM (cRBM) Taylor et al. (2007). Better\\n\\n4http://mocap.cs.cmu.edu/\\n\\n33\\n\\n\\x0cmodeling and smoother transition between di\\x1berent styles of motions was\\n\\nachieved by adding a second hidden layer to the cRBM, using the Recurrent\\n\\nTRBM (Sutskever et al., 2008), and using the factored conditional RBM\\n\\n(fcRBM) (Taylor and Hinton, 2009). The work by L\\xe4ngkvist and Lout\\x1c\\n\\n(2012) restructures an auto-encoder to resemble a cRBM but is used to per-\\n\\nform classi\\x1ccation on the CMU motion capture data instead of generating\\n\\nnew sequences. The drawbacks with general-purpose models such as Gaus-\\n\\nsian Processes and cRBM are that prior information about motion is not\\n\\nutilized and they have a costly approximation sampling procedure.\\n\\nAn unsupervised hierarchical model that is speci\\x1ccally designed for mod-\\n\\neling locomotion styles was developed by Pan and Torresani (2009) and builds\\n\\non the Hierarchical Bayesian Continuous Pro\\x1cle Model (HB-CPM). A Dy-\\n\\nnamic Factor Graph (DFG), which is an extension of factor graphs, was\\n\\nintroduced by Mirowski and LeCun (2009) and used on motion capture data\\n\\nto \\x1cll in missing data. The advantage of DFG is that it has a constant parti-\\n\\ntion function which avoids the costly approximation sampling procedure that\\n\\nis used in a cRBM.\\n\\nIn summary, analyzing and synthesizing motion capture data is a chal-\\n\\nlenging task and it encourages researchers to further improve learning algo-\\n\\nrithms for dealing with complex, multivariate time-series data. A motiva-\\n\\ntion for using deep learning algorithms for motion capture data is that it\\n\\nhas been suggested that human motion is composed of elementary building\\n\\nblocks (motion templates) and any complex motion is constructed from a\\n\\nlibrary of these previously learned motion templates (Flash and Hochner,\\n\\n2005). Deep networks can, in an unsupervised manner, learn these motion\\n\\n34\\n\\n\\x0ctemplates from raw data and use them to form complex human motions.\\n\\nMotion capture data also provides an interesting platform for feature learn-\\n\\ning from raw data since there is no commonly used feature set for motion\\n\\ncapture data. Therefore, the success of applying deep learning algorithms to\\n\\nmotion data can inspire learning features from raw data in other time-series\\n\\nproblems as well.\\n\\n4.6. Electronic nose data\\n\\nFigure 10: Normalized data from an array of electronic nose sensors.\\n\\nMachine olfaction (Osuna et al., 2003; Gardner and Bartlett, 1999) is a\\n\\n\\x1celd that seeks to quantify and analyze odours using an electronic nose (e-\\n\\nnose). An e-nose is composed of an array of selective gas sensors together\\n\\nwith pattern recognition techniques. Figure 10 shows the readings from an e-\\n\\nnose sensor array. The number of sensors in the array typically ranges from\\n\\n4-30 sensors and are therefore, just like motion capture data, multivariate\\n\\n35\\n\\n1020304050607000.10.20.30.40.50.60.70.8Sample [0.5 Hz]Sensor value\\x0cand may contain redundant signals. The data is also unintuitive and there is\\n\\na lack of expert knowledge that can guide the design of features. E-noses are\\n\\nmostly used in practice for industrial applications such as measuring food,\\n\\nbeverage (Gardner et al., 2000b), and air quality (Zampolli et al., 2004), gas\\n\\nidenti\\x1ccation, and gas source localization Bennetts et al. (2011), but also has\\n\\nmedical applications such as bacteria identi\\x1ccation (Dutta et al., 2002) and\\n\\ndiagnosis (Gardner et al., 2000a).\\n\\nThe traditional approach of analyzing e-nose data involves extracting\\n\\ninformation in the static and dynamic phases of the signals (Gutierrez-Osuna,\\n\\n2002) for the use of static pattern analysis techniques (PCA, discriminant\\n\\nfunction analysis, cluster analysis and neural networks). Some commonly\\n\\nused features are the static sensor response, transient derivatives (Trincavelli\\n\\net al., 2010), area under the curve (Carmona et al., 2006), model parameter\\n\\nidenti\\x1ccation (Vembu et al., 2012), and dynamic analysis (Hines et al., 1999).\\n\\nA popular approach for modeling e-nose data is the Time-Delay Neural\\n\\nNetworks (TDNN) (Waibel et al., 1989).\\n\\nIt has been used for identifying\\n\\nthe smell of spices (Zhang et al., 2003), ternary mixtures (Vito et al., 2007),\\n\\noptimum fermentation time for black tea (Bhattacharya et al., 2008), and\\n\\nvintages of wine (Yamazaki et al., 2001). An RNN have been used for odour\\n\\nlocalization with a mobile robot (Duckett et al., 2001).\\n\\nThe work by Vembu et al. (2012) compares the gas discrimination and\\n\\nlocalization between three approaches: SVM on raw data, SVM on features\\n\\nextracted from auto-regressive and linear dynamical systems, and \\x1cnally a\\n\\nSVMs with kernels specialized for structured data (G\\xe4rtner, 2003). The SVM\\n\\nwith built-in time-aware kernels performed better than techniques that used\\n\\n36\\n\\n\\x0cfeature extraction, even though the features captured temporal information.\\n\\nMore recently, an auto-encoder, RBM, and cRBM have been used for\\n\\nbacteria identi\\x1ccation (L\\xe4ngkvist and Lout\\x1c, 2011) and fast classi\\x1ccation of\\n\\nmeat spoilage markers (L\\xe4ngkvist et al., 2013).\\n\\nE-nose data introduces the challenge of improving models that can deal\\n\\nwith redundant signals. It is not feasible to produce tailor-made sensors for\\n\\neach possible individual gas and combinations of gases of interest. Therefore\\n\\nthe common approach is to use an array of sensors with di\\x1berent properties\\n\\nand leave the discrimination to the pattern analysis software. It is also not\\n\\ndesirable to construct new feature sets for each e-nose application so a data-\\n\\ndriven feature learning method is useful. The early works on e-nose data\\n\\ncreate feature vectors of simple features for each signal such as the static\\n\\nresponse or the slope of dynamic response and then feed it to a classi\\x1cer.\\n\\nRecently, the use of dynamic models such as neural networks with tapped\\n\\ndelays and SVMs with kernels for structured data have shown to improve the\\n\\nperformance over static approaches. The next step is to continue this trend\\n\\nof using dynamical models that constructs robust features that can deal with\\n\\nnoisy inputs in order to quantify and classify odors in more challenging open\\n\\nenvironments with many di\\x1berent simultaneous gas sources.\\n\\n4.7. Physiological data\\n\\nWith physiological data we consider recordings such as electroencephalog-\\n\\nraphy (EEG), magnetoencephalography (MEG), electrocardiography (ECG),\\n\\nand wearable sensors for health monitoring. Figure 11 shows an example of\\n\\nhow physiological data look like. The data can exist both as singular or\\n\\nmultiple channels. The use of a feature learning algorithm is particularly\\n\\n37\\n\\n\\x0cFigure 11: Data from EEG (top two signals), EOG (third and fourth signal), and EMG\\n(bottom signal), recorded with a polysomnograph during sleep.\\n\\nbene\\x1ccial in medical applications because acquiring a labeled medical data\\n\\nset is expensive since the data sets are often very large and require the la-\\n\\nbeling of an expert in the \\x1celd.\\n\\nThe work by Mirowski et al. (2008) compares convolutional networks\\n\\nwith logistic regression and SVMs for epileptic seizure prediction from in-\\n\\ntracranial EEG signals. The features that are used are hand-engineered bi-\\n\\nvariate features between channels that encode relationship between pairs of\\n\\nEEG channels. The result was that convolutional networks achieved only 1\\n\\nfalse-alarm prediction from 21 patients while the SVM had 10 false-alarms.\\n\\nTDNN and ICA has also been used for EEG-based prediction of epileptic\\n\\nseizures (Mirowski et al., 2007). The application of self-organizing maps\\n\\n(SOM) to analyze EMG data is presented by Tucker (1999).\\n\\nA RBM-based method that builds features from raw data for sleep stage\\n\\n38\\n\\n051015202530EMGEOG2EOG1EEG2EEG1Time [s]\\x0cclassi\\x1ccation from 4-channel polysomnography data has been proposed by L\\xe4ngkvist\\n\\net al. (2012). A similar setup was used by Wulsin et al. (2011) for model-\\n\\ning single channel EEG waveforms used for anomaly detection. A DBN\\n\\nis used by (Wang and Shang, 2013) to automatically extract features from\\n\\nraw unlabelled physiological data and achieves better classi\\x1ccation than a\\n\\nfeature-based approach. These recent works show that DBNs can be applied\\n\\nto raw physiological data to e\\x1bectively learn relevant features.\\n\\nA source separation method tailor-made to EEG and MEG signals is pro-\\n\\nposed by Hyv\\xe4rinen et al. (2010). The data is preprocessed by short-time\\n\\nFourier transforms and then fed to an ICA. The work shows that tempo-\\n\\nral correlations are adequately taken into account. Independent Component\\n\\nAnalysis (ICA) has provided to be a new tool to analyze time series and is\\n\\na unifying framework that combines sparseness, temporal coherence, topog-\\n\\nraphy and complex cell pooling in a single model (Hyv\\xe4rinen et al., 2003).\\n\\nA method for how to order the independent components for time-series is\\n\\nexplored by Cheung and Xu (2001).\\n\\nSelf-taught learning has been used with time-series data from wearable\\n\\nhand-motion sensors (Amft, 2011).\\n\\nThe \\x1celd of physiological data is large and many di\\x1berent methods have\\n\\nbeen used. The characteristics of physiological data could be particularly\\n\\ninteresting for the deep learning community because it can be used to explore\\n\\nthe feasibility of learning features from raw data, which hopefully can inspire\\n\\nsimilar approaches in other time-series domains.\\n\\n39\\n\\n\\x0cTable 2: A summary of commonly used time-series problems.\\n\\nProblem\\n\\nStock prediction\\nVideo\\n\\nMulti-\\nvariate\\n-\\n(cid:88)\\n\\nSpeech Recognition -\\n(cid:88)\\n\\nMusic recognition\\n\\n(cid:88)\\nMotion capture\\n(cid:88)\\nE-nose\\nPhysiological data (cid:88)\\n\\nRaw\\ndata\\n(cid:88)\\n(cid:88)\\n\\n((cid:88))\\n\\n-\\n(cid:88)\\n(cid:88)\\n((cid:88))\\n\\nFrequency\\nrich\\n-\\n-\\n(cid:88)\\n\\nCommon\\nfeatures\\n-\\nSIFT,\\nHOG\\nMFCC\\n\\n(cid:88)\\n\\n-\\n-\\n(cid:88)\\n\\nChroma,\\nMFCC\\n-\\nMany\\nMany,\\nspec-\\ntogram\\n\\nBenchmark\\nCommon\\nset\\nmethod\\nANN\\nDJIA\\nConvRBM KTH\\n\\nTIMIT\\n\\nRBM,\\nRNN\\nConvRBM GTZAN\\n\\ncRBM\\nTDNN\\nRBM,\\nAE\\n\\nCMU\\n-\\nPhysioNET\\n\\n4.8. Summary\\n\\nTable 2 gives a summary of the time-series problems that have been pre-\\n\\nsented in this section. The \\x1crst column indicates if the data is multivariate\\n\\n(or only contains one signal, univariate). Stock prediction is often viewed as\\n\\na single channel problem, which explains the di\\x1eculties to produce accurate\\n\\nprediction systems, since stocks depend on a myriad of other factors, and\\n\\narguably not at all on past values of the stock itself. For speech recognition,\\n\\nthe use of multimodal sources can improve performance (Ngiam et al., 2011).\\n\\nThe second column shows which problems have attempted to create fea-\\n\\ntures purely from raw data. Only a few works have attempted this with\\n\\nspeech recognition (Jaitly and Hinton, 2011; Smith and Lewicki, 2005) and\\n\\nphysiological data (Wulsin et al., 2011; L\\xe4ngkvist et al., 2012; Wang and\\n\\nShang, 2013). To the authors knowledge, learning features from raw data\\n\\nhas not been attempted in music recognition. The process of constructing\\n\\nfeatures from raw data has been well demonstrated for vision-tasks but is\\n\\n40\\n\\n\\x0ccautiously used for time-series problems. Models such as TDNN, cRBM and\\n\\nconvolutional RBMs are well suited for being applied to raw data (or slightly\\n\\npre-processed data).\\n\\nThe third column indicates which time-series problems have valuable in-\\n\\nformation in the frequency-domain. For frequency-rich problems, it is un-\\n\\ncommon to attempt to learn features from raw data. A reason for this is\\n\\nthat current feature learning algorithms are yet not well-suited for learning\\n\\nfeatures in the frequency-domain.\\n\\nThe fourth column displays some common features that have been used\\n\\nin the literature. SIFT and HOG have been applied to videos even though\\n\\nthose features are developed for static images. Chroma and MFCC have\\n\\nbeen applied to music recognition, even though they are develop for speech\\n\\nrecognition. The e-nose community have tried a plethora of features. E-\\n\\nnose data is a relatively new \\x1celd where a hand-crafted feature set have\\n\\nnot been developed since this kind of data is complex and unintuitive. For\\n\\nphysiological data, the used features are often a combination of application-\\n\\nspeci\\x1cc features from previous works or hand-crafted features.\\n\\nThe \\x1cfth column reports the most commonly used method(s), or cur-\\n\\nrent state-of-the-art, for each time-series problem. For stock prediction,\\n\\nthe progress has stopped at classical neural networks. The current state-\\n\\nof-the-art augments additional information beside the stock data. For high-\\n\\ndimensional temporal data such as video and music recognition, the convolu-\\n\\ntional version of RBM have been successful. In recent years, the RBM have\\n\\nbeen used for speech recognition but the current state-of-the-art is achieved\\n\\nwith an RNN. The cRBM introduced motion capture data to the deep learn-\\n\\n41\\n\\n\\x0cing community and it is an interesting problem to explore with other meth-\\n\\nods. Single layer neural networks with temporal capabilities have been used\\n\\nto model e-nose data and the use of deep networks is an interesting future\\n\\ndirection for modeling e-nose data.\\n\\nAnd \\x1cnally, the last column indicates a typical benchmark set for each\\n\\nproblem. There is currently no well-known publicly available benchmark\\n\\ndata set for e-nose data. For deep learning to enter the \\x1celd of e-nose data it\\n\\nrequires a large, well-organized data set that would bene\\x1ct both communities.\\n\\nA data base of physiological data is available from PhysioNET (Goldberger\\n\\net al., 2000 (June 13).\\n\\n5. Conclusion\\n\\nUnsupervised feature learning and deep learning techniques have been\\n\\nsuccessfully applied to a variety of domains. While much focus in deep\\n\\nlearning and unsupervised feature learning have been in the computer vi-\\n\\nsion domain, this paper has reviewed some of the successful applications of\\n\\ndeep learning methods to the time-series domain. Some of these approaches\\n\\nhave treated the input as static data but the most successful ones are those\\n\\nthat have modi\\x1ced the deep learning models to better handle time-series\\n\\ndata.\\n\\nThe problem with processing time-series data as static input is that\\n\\nthe importance of time is not captured. Modeling time-series faces many\\n\\nof the same challenges as modeling static data, such as coping with high-\\n\\ndimensional observations and nonlinear relationships between variables, how-\\n\\never, by simply ignoring time and applying models of static data to time series\\n\\n42\\n\\n\\x0cone disregards much of the rich structure present in the data. When taking\\n\\nthis approach, the context of the current input frame is lost and the only\\n\\ntime-dependencies that are captured is within the input size.\\n\\nIn order to\\n\\ncapture long-term dependencies, the input size has to be increased, which\\n\\ncan be impractical for multivariate signals or if the data has very long-term\\n\\ndependencies. The solution is to use a model that incorporates temporal\\n\\ncoherence, performs temporal pooling, or models sequences of hidden unit\\n\\nactivations.\\n\\nThe choice of model and how the data should be presented to the model\\n\\nis highly dependent on the type of data. Within a chosen model there are\\n\\nadditional design choices in terms of connectivity, architecture, and hyperpa-\\n\\nrameters. For these reasons, even though many unsupervised feature learning\\n\\nmodels o\\x1ber to relieve the user of having to come up with useful features for\\n\\nthe current domain, there are still many challenges for applying them to time-\\n\\nseries data. It is also worth noting that many works that construct useful\\n\\nfeatures from the input data actually still use input data from pre-processed\\n\\nfeatures.\\n\\nDeep learning methods o\\x1ber better representation and classi\\x1ccation on a\\n\\nmultitude of time-series problems compared to shallow approaches when con-\\n\\n\\x1cgured and trained properly. There is still room for improving the learning\\n\\nalgorithms speci\\x1ccally for time-series data, e.g., performing signal selection\\n\\nthat deals with redundant signals in multivariate input data. Another possi-\\n\\nble future direction is to develop models that change their internal architec-\\n\\nture during learning or use model averaging in order to capture both short\\n\\nand long-term time dependencies. Further research in this area is needed to\\n\\n43\\n\\n\\x0cdevelop algorithms for time-series modeling that learn even better features\\n\\nand are easier and faster to train. Therefore, there is a need to focus less on\\n\\nthe pre-processing pipeline for a speci\\x1cc time-series problem and focus more\\n\\non learning better feature representations for a general-purpose algorithm for\\n\\nstructured data, regardless of the application.\\n\\nReferences\\n\\nAgrawal, J.G., Chourasia, V.S., Mittra, A.K., 2013. State-of-the-art in stock\\n\\nprediction techniques. International Journal of Advanced Research in Elec-\\n\\ntrical, Electronics and Instrumentation Engineering 2, 1360\\x151366.\\n\\nAmft, O., 2011. Self-taught learning for activity spotting in on-body mo-\\n\\ntion sensor data, in: ISWC 2011: Proceedings of the IEEE International\\n\\nSymposium on Wearable Computing, IEEE. pp. 83\\x1586.\\n\\nAtsalakis, G.S., Valavanis, K.P., 2009. Surveying stock market forecasting\\n\\ntechniques \\xa8c part ii: Soft computing methods. Expert Systems with Ap-\\n\\nplications 36, 5932 \\x15 5941.\\n\\nBengio, Y., 2007. Learning deep architectures for AI. Technical Report 1312.\\n\\nDept. IRO, Universite de Montreal.\\n\\nBengio, Y., Courville, A., Vincent, P., 2012. Unsupervised Feature Learning\\n\\nand Deep Learning: A Review and New Perspectives. Technical Report\\n\\narXiv:1206.5538. U. Montreal. URL: http://arxiv.org/abs/1206.5538.\\n\\nBengio, Y., Frasconi, P., 1996. Input-output HMM\\'s for sequence processing.\\n\\nIEEE Transactions on Neural Networks 7(5), 1231\\x151249.\\n\\n44\\n\\n\\x0cBengio, Y., Lamblin, P., Popovici, D., Larochelle, H., 2007. Greedy layer-\\n\\nwise training of deep networks. Advances in neural information processing\\n\\nsystems 19, 153.\\n\\nBengio, Y., LeCun, Y., 2007. Scaling learning algorithms towards AI, in:\\n\\nBottou, L., Chapelle, O., DeCoste, D., Weston, J. (Eds.), Large-Scale\\n\\nKernel Machines, MIT Press.\\n\\nBengio, Y., Simard, P., Frasconi, P., 1994. Learning longterm dependencies\\n\\nwith gradient descent is di\\x1ecult. IEEE Transactions on Neural Networks\\n\\n5(2), 157\\x15166.\\n\\nBengio, Y., Yao, L., Alain, G., Vincent, P., 2013. Generalized denoising\\n\\nauto-encoders as generative models. CoRR abs/1305.6663.\\n\\nBennetts, V.H., Lilienthal, A.J., Neumann, P.P., Trincavelli, M., 2011. Mo-\\n\\nbile robots for localizing gas emission sources on land\\x1cll sites:\\n\\nis bio-\\n\\ninspiration the way to go? Frontiers in neuroengineering 4.\\n\\nBhattacharya, N., Tudu, B., Jana, A., Ghosh, D., Bandhopadhyaya, R.,\\n\\nBhuyan, M., 2008. Preemptive identi\\x1ccation of optimum fermentation time\\n\\nfor black tea using electronic nose. Sensors and Actuators B: Chemical 131,\\n\\n110\\x15116.\\n\\nBo Chen, Jo-Anne Ting, B.M., de Freitas, N., 2010. Deep learning of in-\\n\\nvariant spatio-temporal features from video, in: NIPS 2010 Deep Learning\\n\\nand Unsupervised Feature Learning Workshop.\\n\\nBollen, J., Mao, H., Zeng, X., 2011. Twitter mood predicts the stock market.\\n\\nJournal of Computational Science 2, 1 \\x15 8.\\n\\n45\\n\\n\\x0cBottou, L., 2010. Large-scale machine learning with stochastic gradient de-\\n\\nscent, in: Lechevallier, Y., Saporta, G. (Eds.), Proceedings of the 19th In-\\n\\nternational Conference on Computational Statistics (COMPSTAT\\'2010),\\n\\nSpringer, Paris, France. pp. 177\\x15187. URL: http://leon.bottou.org/\\n\\npapers/bottou-2010.\\n\\nBrand, M., Hertzmann, A., 2000. Style machines, in: Proceedings of the 27th\\n\\nannual conference on Computer graphics and interactive techniques, ACM\\n\\nPress/Addison-Wesley Publishing Co., New York, NY, USA. pp. 183\\x15192.\\n\\nCarmona, M., Martinez, J., Zalacain, A., Rodriguez-Mendez, M.L., de Saja,\\n\\nJ.A., Alonso, G.L., 2006. Analysis of sa\\x1bron volatile fraction by td\\x15gc\\x15ms\\n\\nand e-nose. European Food Research and Technology 223, 96\\x15101.\\n\\nChang, K., Jang, J., Iliopoulos, C., 2010. Music genre classi\\x1ccation via com-\\n\\npressive sampling, in: Proceedings of the 11th International Conference on\\n\\nMusic Information Retrieval (ISMIR), pp. 387\\x15392.\\n\\nCheung, Y., Xu, L., 2001. Independent component ordering in ica time series\\n\\nanalysis. Neurocomputing 41, 145\\x15152.\\n\\nChiappa, S., Kober, J., Peters, J., 2009. Using bayesian dynamical systems\\n\\nfor motion template libraries. In Adv. in Neural Inform. Proc. Systems 21,\\n\\n297\\x15304.\\n\\nCoates, A., Lee, H., Ng, A.Y., 2010. An Analysis of Single-Layer Networks\\n\\nin Unsupervised Feature Learning. Engineering , 1\\x159.\\n\\nDahl, G., Yu, D., Deng, L., Acero, A., 2012. Context-dependent pre-\\n\\ntrained deep neural networks for large-vocabulary speech recognition. Au-\\n\\n46\\n\\n\\x0cdio, Speech, and Language Processing, IEEE Transactions on 20, 30\\x1542.\\n\\ndoi:10.1109/TASL.2011.2134090.\\n\\nDahl, G.E., Ranzato, M., Mohamed, A., Hinton, G., 2010. Phone recogni-\\n\\ntion with the mean-covariance restricted boltzmann machine. Advances in\\n\\nNeural Information Processing Systems 23, 469\\x15477.\\n\\nDalal, N., Triggs, B., 2005. Histograms of oriented gradients for human\\n\\ndetection, in: In CVPR.\\n\\nDieleman, S., Brakel, P., Schrauwen, B., 2011. Audio-based music classi\\x1c-\\n\\ncation with a pretrained convolutional network, in: In The International\\n\\nSociety for Music Information Retrieval (ISMIR).\\n\\nDietterich, T.G., 2002. Machine learning for sequential data: A review,\\n\\nin: Structural, Syntactic, and Statistical Pattern Recognition, Springer-\\n\\nVerlag. pp. 15\\x1530.\\n\\nDuckett, T., Axelsson, M., Sa\\x1eotti, A., 2001. Learning to locate an odour\\n\\nsource with a mobile robot, in: Robotics and Automation, 2001. Proceed-\\n\\nings 2001 ICRA. IEEE International Conference on, pp. 4017\\x154022 vol.4.\\n\\ndoi:10.1109/ROBOT.2001.933245.\\n\\nDutta, R., Hines, E., Gardner, J., Boilot, P., 2002. Bacteria classi\\x1ccation\\n\\nusing cyranose 320 electronic nose. Biomedical Engineering Online 1, 4.\\n\\nErhan, D., Bengio, Y., Courville, A., Manzagol, P., Vincent, P., Bengio, S.,\\n\\n2010. Why does unsupervised pre-training help deep learning? Journal of\\n\\nMachine Learning Research 11, 625\\x15660.\\n\\n47\\n\\n\\x0cFama, E.F., 1965. The behavior of stock-market prices. The Journal of\\n\\nBusiness 1, 34\\x15105.\\n\\nFlash, T., Hochner, B., 2005. Motor primitives in vertebrates and inverte-\\n\\nbrates. Current Opinion in Neurobiology 15(6), 660\\x15666.\\n\\nFurui, S., Kikuchi, T., Shinnaka, Y., Hori, C., 2004. Speech-to-text and\\n\\nspeech-to-speech summarization of spontaneous speech. Speech and Audio\\n\\nProcessing, IEEE Transactions on 12, 401\\x15408.\\n\\nGardner, J., Bartlett, P., 1999. Electronic Noses, Principles and Applications.\\n\\nOxford University Press, New York, NY, USA.\\n\\nGardner, J.W., Shin, H.W., Hines, E.L., 2000a. An electronic nose system\\n\\nto diagnose illness. Sensors and Actuators B: Chemical 70, 19\\x1524.\\n\\nGardner, J.W., Shin, H.W., Hines, E.L., Dow, C.S., 2000b. An electronic nose\\n\\nsystem for monitoring the quality of potable water. Sensors and Actuators\\n\\nB: Chemical 69, 336\\x15341.\\n\\nG\\xe4rtner, T., 2003. A survey of kernels for structured data. SIGKDD Explor.\\n\\nNewsl. 5, 49\\x1558.\\n\\nGers, F.A., Schmidhuber, J., Cummins, F., 2000. Learning to Forget: Con-\\n\\ntinual Prediction with LSTM. Neural Computation 12, 2451\\x152471.\\n\\nGleicher, M., 2000. Animation from observation: Motion capture and motion\\n\\nediting. SIGGRAPH Computer Graphics 33, 51\\x1554.\\n\\n48\\n\\n\\x0cGoldberger, A.L., Amaral, L.A.N., Glass, L., Hausdor\\x1b, J.M., Ivanov,\\n\\nP.C., Mark, R.G., Mietus, J.E., Moody, G.B., Peng, C.K., Stan-\\n\\nley, H.E., 2000 (June 13).\\n\\nPhysioBank, PhysioToolkit, and Phys-\\n\\nioNet: Components of a new research resource for complex physio-\\n\\nlogic signals. Circulation 101, e215\\x15e220. Circulation Electronic Pages:\\n\\nhttp://circ.ahajournals.org/cgi/content/full/101/23/e215.\\n\\nGrassia, F.S., 1998. Practical parameterization of rotations using the expo-\\n\\nnential map. J. Graph. Tools 3, 29\\x1548.\\n\\nGraves, A., Mohamed, A., Hinton, G., 2013. Speech recognition with deep re-\\n\\ncurrent neural networks, in: The 38th International Conference on Acous-\\n\\ntics, Speech, and Signal Processing (ICASSP).\\n\\nGrosse, R., Raina, R., Kwong, H., Ng, A.Y., 2007. Shift-invariant sparse\\n\\ncoding for audio classi\\x1ccation, in: Conference on Uncertainty in Arti\\x1ccial\\n\\nIntelligence (UAI).\\n\\nGruhl, D., Guha, R., Kumar, R., Novak, J., Tomkins, A., 2005. The predic-\\n\\ntive power of online chatter, in: Proceedings of the eleventh ACM SIGKDD\\n\\ninternational conference on Knowledge discovery in data mining, pp. 78\\x15\\n\\n87.\\n\\nGutierrez-Osuna, R., 2002. Pattern analysis for machine olfaction: A review.\\n\\nIEEE Sensors Journal 2(3), 189\\x15202.\\n\\nHamel, P., Eck, D., 2010. Learning features from music audio with deep belief\\n\\nnetworks, in: 11th International Society for Music Information Retrieval\\n\\nConference (ISMIR).\\n\\n49\\n\\n\\x0cHena\\x1b, M., Jarrett, K., Kavukcuoglu, K., LeCun, Y., 2011. Unsupervised\\n\\nlearning of sparse features for scalable audio classi\\x1ccation, in: Proceedings\\n\\nof International Symposium on Music Information Retrieval (ISMIR\\'11).\\n\\nHines, E., Llobet, E., Gardner, J., 1999. Electronic noses: a review of signal\\n\\nprocessing techniques. Circuits, Devices and Systems, IEE Proceedings -\\n\\n146, 297\\x15310.\\n\\nHinton, G., Deng, L., Yu, D., Dahl, G.E., Mohamed, A.r., Jaitly, N., Senior,\\n\\nA., Vanhoucke, V., Nguyen, P., Sainath, T.N., et al., 2012. Deep neural\\n\\nnetworks for acoustic modeling in speech recognition: The shared views of\\n\\nfour research groups. Signal Processing Magazine, IEEE 29, 82\\x1597.\\n\\nHinton, G., Salakhutdinov, R., 2006. Reducing the dimensionality of data\\n\\nwith neural networks. Science 313(5786), 504\\x15507.\\n\\nHinton, G.E., 2002. Training products of experts by minimizing contrastive\\n\\ndivergence. Neural Computation 14, 1771 \\x15 1800.\\n\\nHinton, G.E., 2012. A practical guide to training restricted boltzmann ma-\\n\\nchines, in: Montavon, G., Orr, G.B., M\\xfcller, K.R. (Eds.), Neural Networks:\\n\\nTricks of the Trade. Springer Berlin Heidelberg. volume 7700 of Lecture\\n\\nNotes in Computer Science, pp. 599\\x15619. URL: http://dx.doi.org/10.\\n\\n1007/978-3-642-35289-8_32, doi:10.1007/978-3-642-35289-8_32.\\n\\nHinton, G.E., Krizhevsky, A., Wang, S.D., 2011. Transforming auto-\\n\\nencoders, in: Proceedings of the 21th international conference on Arti\\x1ccial\\n\\nneural networks - Volume Part I, pp. 44\\x1551.\\n\\n50\\n\\n\\x0cHinton, G.E., S., O., Y., T., 2006. A fast learning algorithm for deep belief\\n\\nnets. Neural Computation 18 , 1527\\x151554.\\n\\nHochreiter, S., Schmidhuber, J., 1997. Long short-term memory. Neural\\n\\nComputation 9, 1735\\x151780.\\n\\nHsieh, T.J., Hsiao, H.F., Yeh, W.C., 2011. Forecasting stock markets using\\n\\nwavelet transforms and recurrent neural networks: An integrated system\\n\\nbased on arti\\x1ccial bee colony algorithm. Applied Soft Computing 11, 2510\\n\\n\\x15 2525.\\n\\nHumphrey, E.J., Bello, J.P., LeCun, Y., 2013. Feature learning and deep\\n\\narchitectures: new directions for music informatics. Journal of Intelligent\\n\\nInformation Systems 41, 461\\x15481.\\n\\nH\\xfcsken, M., Stagge, P., 2003. Recurrent Neural Networks for Time Series\\n\\nClassi\\x1ccation. Neurocomputing 50, 223\\x15235.\\n\\nHyv\\xe4rinen, A., Hurri, J., V\\xe4yrynen, J., 2003. Bubbles: a unifying framework\\n\\nfor low-level statistical properties of natural image sequences. J. Opt. Soc.\\n\\nAm. A 20, 1237\\x151252.\\n\\nHyv\\xe4rinen, A., Ramkumar, P., Parkkonen, L., Hari, R., 2010.\\n\\nIndepen-\\n\\ndent component analysis of short-time Fourier transforms for spontaneous\\n\\nEEG/MEG analysis. NeuroImage 49(1), 257\\x15271.\\n\\nHyv\\xe8arinen, A., Hurri, J., Hoyer, P.O., 2009. Natural Image Statistics. vol-\\n\\nume 39. Springer.\\n\\n51\\n\\n\\x0cJaitly, N., Hinton, G., 2011. Learning a better representation of speech\\n\\nsoundwaves using restricted boltzmann machines, in: Acoustics, Speech\\n\\nand Signal Processing (ICASSP), 2011 IEEE International Conference on,\\n\\nIEEE. pp. 5884\\x155887.\\n\\nJialin Pan, S., Yang, Q., 2010. A survey on transfer learning. IEEE Trans-\\n\\nactions On Knowledge and Data Engineering 22.\\n\\nKamyshanska, H., Memisevic, R., 2013. On autoencoder scoring, in: Pro-\\n\\nceedings of the 30th International Conference on Machine Learning (ICML-\\n\\n13), JMLR Workshop and Conference Proceedings. pp. 720\\x15728.\\n\\nvan Kasteren, T., Noulas, A., Kr\\xf6se, B., 2008. Conditional random \\x1celds\\n\\nversus hidden markov models for activity recognition in temporal sensor\\n\\ndata, in: In Proceedings of the 14th Annual Conference of the Advanced\\n\\nSchool for Computing and Imaging (ASCI\\'08), The Netherlands.\\n\\nKavukcuoglu, K., Ranzato, M., Fergus, R., Le-Cun, Y., 2009. Learning\\n\\ninvariant features through topographic \\x1clter maps, in: Computer Vision\\n\\nand Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, IEEE.\\n\\npp. 1605\\x151612.\\n\\nKeogh, E., Kasetty, S., 2002. On the need for time series data mining bench-\\n\\nmarks: A survey and empirical demonstration, in: In proceedings of the\\n\\n8th ACM SIGKDD International Conference on Knowledge Discovery and\\n\\nData Mining, pp. 102\\x15111.\\n\\nKim, S.S., 1998. Time-delay recurrent neural network for temporal correla-\\n\\ntions and prediction. Neurocomputing 20, 253 \\x15 263.\\n\\n52\\n\\n\\x0cLa\\x1berty, J.D., McCallum, A., Pereira, F.C.N., 2001. Conditional random\\n\\n\\x1celds: Probabilistic models for segmenting and labeling sequence data,\\n\\nin: Proceedings of the Eighteenth International Conference on Machine\\n\\nLearning, Morgan Kaufmann Publishers Inc., San Francisco, CA, USA.\\n\\npp. 282\\x15289.\\n\\nL\\xe4ngkvist, M., Coradeschi, S., Lout\\x1c, A., Rayappan, J.B.B., 2013. Fast\\n\\nClassi\\x1ccation of Meat Spoilage Markers Using Nanostructured ZnO Thin\\n\\nFilms and Unsupervised Feature Learning. Sensors 13(2), 1578\\x151592.\\n\\nDoi:10.3390/s130201578.\\n\\nL\\xe4ngkvist, M., Karlsson, L., Lout\\x1c, A., 2012. Sleep stage classi\\x1ccation using\\n\\nunsupervised feature learning. Advances in Arti\\x1ccial Neural Systems 2012.\\n\\nDoi:10.1155/2012/107046.\\n\\nL\\xe4ngkvist, M., Lout\\x1c, A., 2011. Unsupervised feature learning for electronic\\n\\nnose data applied to bacteria identi\\x1ccation in blood, in: NIPS workshop\\n\\non Deep Learning and Unsupervised Feature Learning.\\n\\nL\\xe4ngkvist, M., Lout\\x1c, A., 2012. Not all signals are created equal: Dynamic\\n\\nobjective auto-encoder for multivariate data, in: NIPS workshop on Deep\\n\\nLearning and Unsupervised Feature Learning.\\n\\nLe, Q.V., Zou, W.Y., Yeung, S.Y., Ng, A.Y., 2011. Learning hierarchical\\n\\ninvariant spatio-temporal features for action recognition with independent\\n\\nsubspace analysis, in: Computer Vision and Pattern Recognition (CVPR).\\n\\nLe Roux, N., Bengio, Y., 2008. Representational power of restricted Boltz-\\n\\n53\\n\\n\\x0cmann machines and deep belief networks. Neural Computation 20, 1631\\x15\\n\\n1649.\\n\\nLeCun, Y., Kavukvuoglu, K., Farabet, C., 2010. Convolutional networks and\\n\\napplications in vision, in: Proc. International Symposium on Circuits and\\n\\nSystems (ISCAS\\xa1\\xaf10), IEEE.\\n\\nLee, H., Ekanadham, C., Ng, A.Y., 2008. Sparse deep belief net model for\\n\\nvisual area V2, in: Advances in Neural Information Processing Systems\\n\\n20, pp. 873\\x15880.\\n\\nLee, H., Grosse, R., Ranganath, R., Ng, A.Y., 2009a. Convolutional deep\\n\\nbelief networks for scalable unsupervised learning of hierarchical represen-\\n\\ntations, in: Twenty-Sixth International Conference on Machine Learning.\\n\\nLee, H., Largman, Y., Pham, P., Ng, A.Y., 2009b. Unsupervised feature\\n\\nlearning for audio classi\\x1ccation using convolutional deep belief networks,\\n\\nin: Advances in Neural Information Processing Systems 22, pp. 1096\\x151104.\\n\\nLi, Y., Ma, W., 2010. Applications of arti\\x1ccial neural networks in \\x1cnancial\\n\\neconomics: A survey, in: Proceedings of the 2010 International Symposium\\n\\non Computational Intelligence and Design - Volume 01, IEEE Computer\\n\\nSociety. pp. 211\\x15214.\\n\\nLin, X., Yang, Z., Song, Y., 2009. Short-term stock price prediction based on\\n\\necho state networks. Expert Systems with Applications 36, 7313 \\x15 7317.\\n\\nLowe, D., 1999. Object recognition from local scale-invariant features, in: In\\n\\nICCV.\\n\\n54\\n\\n\\x0cLuenberger, D., 1979. Introduction to Dynamic Systems: Theory, Models,\\n\\nand Applications. Wiley.\\n\\nL\\xfctkepohl, H., 2005. New Introduction to Multiple Time Series Analysis.\\n\\nSpringer-Verlag.\\n\\nMalkiel, B., 2003. The e\\x1ecient market hypothesis and its critics. The Journal\\n\\nof Economic Perspectives 17. Http://dx.doi.org/10.2307/3216840.\\n\\nMarkov, K., Matsui, T., 2012. Music genre classi\\x1ccation using self-taught\\n\\nlearning via sparse coding, in: Acoustics, Speech and Signal Processing\\n\\n(ICASSP), 2012 IEEE International Conference on, pp. 1929\\x151932.\\n\\nMartens, J., Sutskever, I., 2012. Training deep and recurrent neural networks\\n\\nwith hessian-free optimization, in: Neural Networks: Tricks of the Trade.\\n\\nSpringer Berlin Heidelberg. volume 7700 of Lecture Notes in Computer\\n\\nScience.\\n\\nMasci, J., Meier, U., Cire\\xb3an, D., Schmidhuber, J., 2011. Stacked convolu-\\n\\ntional auto-encoders for hierarchical feature extraction, in: Proceedings of\\n\\nthe 21th international conference on Arti\\x1ccial neural networks - Volume\\n\\nPart I, pp. 52\\x1559.\\n\\nMemisevic, R., Hinton, G., 2007. Unsupervised learning of image transforma-\\n\\ntions, in: IEEE Conference on Computer Vision and Pattern Recognition\\n\\n(CVPR), pp. 1\\x158.\\n\\nMemisevic, R., Hinton, G.E., 2010. Learning to represent spatial transforma-\\n\\ntions with factored higher-order boltzmann machines. Neural Computation\\n\\n22, 1473\\x151492.\\n\\n55\\n\\n\\x0cMirowski, P., LeCun, Y., 2009. Dynamic factor graphs for time series model-\\n\\ning. Machine Learning and Knowledge Discovery in Databases , 128\\x15143.\\n\\nMirowski, P., Madhavan, D., LeCun, Y., 2007. Time-delay neural networks\\n\\nand independent component analysis for eeg-based prediction of epileptic\\n\\nseizures propagation, in: Association for the Advancement of Arti\\x1ccial\\n\\nIntelligence Conference.\\n\\nMirowski, P.W., LeCun, Y., Madhavan, D., Kuzniecky, R., 2008. Comparing\\n\\nSVM and convolutional networks for epileptic seizure prediction from in-\\n\\ntracranial EEG, in: Machine Learning for Signal Processing, 2008. MLSP\\n\\n2008. IEEE Workshop on, IEEE. pp. 244\\x15249.\\n\\nMohamed, A., Dahl, G.E., Hinton, G., 2012. Acoustic modeling using deep\\n\\nbelief networks. IEEE Transactions on Audio, Speech, and Language Pro-\\n\\ncessing archive 20(1), 14\\x1522.\\n\\nMohamed, A., Hinton, G., 2010. Phone recognition using restricted boltz-\\n\\nmann machines, in: Acoustics Speech and Signal Processing (ICASSP),\\n\\n2010 IEEE International Conference on, pp. 4354\\x154357. doi:10.1109/\\n\\nICASSP.2010.5495651.\\n\\nNam, J., 2012. Learning Feature Representations for Music Classi\\x1ccation.\\n\\nPh.D. thesis. Stanford University.\\n\\nNam, J., Herrera, J., Slaney, M., Smith, J.O., 2012. Learning Sparse Feature\\n\\nRepresentations for Music Annotation and Retrieval, in: In The Interna-\\n\\ntional Society for Music Information Retrieval (ISMIR), pp. 565\\x15570.\\n\\n56\\n\\n\\x0cNanopoulos, A., Alcock, R., Manolopoulos, Y., 2001. Feature-based classi-\\n\\n\\x1ccation of time-series data. International Journal of Computer Research\\n\\n10, 49\\x1561.\\n\\nNgiam, J., Khosla, A., Kim, M., Nam, J., Lee, H., Ng, A.Y., 2011. Multi-\\n\\nmodal deep learning, in: In Proceedings of the Twenty-Eigth International\\n\\nConference on Machine Learning.\\n\\nOsuna, G.R., Nagle, T.H., Kermani, B., Schi\\x1bman, S.S., 2003. HandBook of\\n\\nMachine Olfaction, electronic nose technology. Wiley-Vch Verlag GmbH &\\n\\nCo. KGaA. chapter Signal Conditioning and Preprocessing. pp. 105\\x15132.\\n\\nPan, W., Torresani, L., 2009. Unsupervised hierarchical modeling of locomo-\\n\\ntion styles, in: Proceedings of the 26th Annual International Conference\\n\\non Machine Learning, pp. 785\\x15792.\\n\\nParris, E., Carey, M., 1996. Language independent gender identi\\x1ccation, in:\\n\\nAcoustics, Speech, and Signal Processing, 1996. ICASSP-96. Conference\\n\\nProceedings., 1996 IEEE International Conference on, pp. 685\\x15688 vol. 2.\\n\\nPascanu, R., Mikolov, T., Bengio, Y., 2012. Understanding the exploding gra-\\n\\ndient problem. Computing Research Repository (CoRR) abs/1211.5063.\\n\\nRabiner, L., Juang, B., 1986. An introduction to hidden markov models.\\n\\nIEEE ASSP Magazine 3(1), 4\\x1516.\\n\\nRaina, R., Battle, A., Lee, H., Packer, B., Ng, A.Y., 2007. Self-taught\\n\\nlearning: Transfer learning from unlabeled data, in: Proceedings of the\\n\\nTwenty-fourth International Conference on Machine Learning.\\n\\n57\\n\\n\\x0cRanzato, M., Hinton, G., 2010. \\xa1\\xb0modeling pixel means and covariances\\n\\nusing factorized third-order boltzmann machines, in: in Proc. of Computer\\n\\nVision and Pattern Recognition Conference (CVPR 2010).\\n\\nRanzato, M., Krizhevsky, A., Hinton, G., 2010. Factored 3-way restricted\\n\\nboltzmann machines for modeling natural images, in:\\n\\nin Proceedings of\\n\\nthe International Conference on Arti\\x1ccial Intelligence and Statistics.\\n\\nRanzato, M., Poultney, C., Chopra, S., LeCun, Y., 2006. E\\x1ecient learning of\\n\\nsparse representations with an energy-based model, in: et al., J.P. (Ed.),\\n\\nAdvances in Neural Information Processing Systems (NIPS 2006), MIT\\n\\nPress.\\n\\nSaxe, A., Koh, P., Chen, Z., Bhand, M., Suresh, B., Ng, A.Y., 2011. On\\n\\nrandom weights and unsupervised feature learning, in: In Proceedings of\\n\\nthe Twenty-Eighth International Conference on Machine Learning.\\n\\nSchoerkhuber, C., Klapuri, A., 2010. Constant-q transform toolbox for music\\n\\nprocessing, in: 7th Sound and Music Computing Conference.\\n\\nSmith, E., Lewicki, M.S., 2005. Learning e\\x1ecient auditory codes using spikes\\n\\npredicts cochlear \\x1clters, in: In Advances in Neural Information Processing\\n\\nSystems, MIT Press.\\n\\nStavens, D., Thrun, S., 2010. Unsupervised learning of invariant features\\n\\nusing video, in: Computer Vision and Pattern Recognition (CVPR), 2010\\n\\nIEEE Conference on, pp. 1649\\x151656.\\n\\nSugiyama, M., Sawai, H., Waibel, A., 1991. Review of tdnn (time delay neural\\n\\n58\\n\\n\\x0cnetwork) architectures for speech recognition, in: Circuits and Systems,\\n\\n1991., IEEE International Sympoisum on, pp. 582\\x15585 vol.1.\\n\\nSutskever, I., 2012. Training Recurrent Neural Networks. Ph.D. thesis. Uni-\\n\\nversity of Toronto.\\n\\nSutskever, I., Hinton, G., 2006. Learning multilevel distributed represen-\\n\\ntations for high-dimensional sequences. Technical Report. University of\\n\\nToronto.\\n\\nSutskever, I., Hinton, G.E., Taylor, G.W., 2008. The recurrent temporal\\n\\nrestricted boltzmann machine, in: Advances in Neural Information Pro-\\n\\ncessing Systems, pp. 1601\\x151608.\\n\\nTaylor, G., Fergus, R., LeCun, Y., Bregler, C., 2010. Convolutional learning\\n\\nof spatio-temporal features, in: Proc. European Conference on Computer\\n\\nVision (ECCV\\'10).\\n\\nTaylor, G., Hinton, G., 2009. Factored conditional restricted boltzmann\\n\\nmachines for modeling motion style, in: Proc. of the 26th International\\n\\nConference on Machine Learning (ICML).\\n\\nTaylor, G.W., 2009.\\n\\nComposable, distributed-state models for high-\\n\\ndimensional time series. Ph.D. thesis. Departmet of Computer Science\\n\\nUniversity of Toronto.\\n\\nTaylor, G.W., Hinton, G.E., Roweis, S.T., 2007. Modeling human motion\\n\\nusing binary latent variables. Advances in neural information processing\\n\\nsystems 19, 1345.\\n\\n59\\n\\n\\x0cTrincavelli, M., Coradeschi, S., Lout\\x1c, A., S\\xf6derquist, B., Thunberg, P.,\\n\\n2010. Direct identi\\x1ccation of bacteria in blood culture samples using an\\n\\nelectronic nose. IEEE Trans Biomedical Engineering 57, 2884\\x152890.\\n\\nTsai, C.F., Hsiao, Y.C., 2010. Combining multiple feature selection meth-\\n\\nods for stock prediction: Union, intersection, and multi-intersection ap-\\n\\nproaches. Decision Support Systems 50, 258 \\x15 269.\\n\\nTucker, C., 1999. Self-organizing maps for time series analysis of electromyo-\\n\\ngraphic data, in: Neural Networks, 1999. IJCNN \\'99. International Joint\\n\\nConference on, pp. 3577\\x153580.\\n\\nVembu, S., Vergara, A., Muezzinoglu, M.K., Huerta, R., 2012. On time\\n\\nseries features and kernels for machine olfaction. Sensors and Actuators\\n\\nB: Chemical 174, 535\\x15546.\\n\\nVito, S.D., Castaldo, A., Lo\\x1bredo, F., Massera, E., Polichetti, T., Nasti, I.,\\n\\nVacca, P., Quercia, L., Francia, G.D., 2007. Gas concentration estimation\\n\\nin ternary mixtures with room temperature operating sensor array using\\n\\ntapped delay architectures. Sensors and Actuators B: Chemical 124, 309\\n\\n\\x15 316.\\n\\nWaibel, A., Hanazawa, T., Hinton, G., Shikano, K., Lang, K., 1989. Phoneme\\n\\nrecognition using time-delay neural networks.\\n\\nIEEE Trans. Acoust.,\\n\\nSpeech, Signal Processing 37, 328\\x15339.\\n\\nWang, D., Shang, Y., 2013. Modeling physiological data with deep belief\\n\\nnetworks. International Journal of Information and Education Technology\\n\\n3.\\n\\n60\\n\\n\\x0cWang, J.M., Fleet, D.J., Hertzmann, A., 2007. Multi-factor gaussian pro-\\n\\ncess models for style-content separation, in: International Conference of\\n\\nMachine Learning (ICML), pp. 975\\x15\\xa8C982.\\n\\nWiskott, L., Sejnowski, T.J., 2002. Slow feature analysis: Unsupervised\\n\\nlearning of invariances. Neural computation 14, 715\\x15770.\\n\\nWulsin, D., Gupta, J., Mani, R., Blanco, J., Litt, B., 2011. Modeling\\n\\nelectroencephalography waveforms with semi-supervised deep belief nets:\\n\\nfaster classi\\x1ccation and anomaly measurement. Journal of Neural Engi-\\n\\nneering 8, 1741 \\x15 2552.\\n\\nYamazaki, A., Ludermir, T., De Souto, M.C.P., 2001. Classi\\x1ccation of vin-\\n\\ntages of wine by arti\\x1ccial nose using time delay neural networks. Electron-\\n\\nics Letters 37, 1466\\x151467.\\n\\nYang, Q., Wu, X., 2006. 10 challenging problems in data mining research.\\n\\nInternational Journal of Information Technology & Decision Making 05,\\n\\n597\\x15604.\\n\\nZampolli, S., Elmi, I., Ahmed, F., Passini, M., Cardinali, G., Nicoletti, S.,\\n\\nDori, L., 2004. An electronic nose based on solid state sensor arrays for\\n\\nlow-cost indoor air quality monitoring applications. Sensors and Actuators\\n\\nB: Chemical 101, 39\\x1546.\\n\\nZhang, H., Balaban, M.O., Principe, J.C., 2003. Improving pattern recogni-\\n\\ntion of electronic nose data with time-delay neural networks. Sensors and\\n\\nActuators B: Chemical 96, 385\\x15389.\\n\\n61\\n\\n\\x0cZhu, X., Wang, H., Xu, L., Li, H., 2008. Predicting stock index increments\\n\\nby neural networks: The role of trading volume under di\\x1berent horizons.\\n\\nExpert Systems with Applications 34, 3043 \\x15 3054.\\n\\nZou, W.Y., Ng, A.Y., Yu, K., 2011. Unsupervised learning of visual in-\\n\\nvariance with temporal coherence, in: In NIPS 2011 Workshop on Deep\\n\\nLearning and Unsupervised Feature Learning.\\n\\n62\\n\\n\\x0c', u'Deep Domain Confusion: Maximizing for Domain Invariance\\n\\nEric Tzeng, Judy Hoffman, Ning Zhang\\n\\nUC Berkeley, EECS & ICSI\\n\\n{etzeng,jhoffman,nzhang}@eecs.berkeley.edu\\n\\nKate Saenko\\n\\nUMass Lowell, CS\\nsaenko@cs.uml.edu\\n\\nTrevor Darrell\\n\\nUC Berkeley, EECS & ICSI\\n\\ntrevor@eecs.berkeley.edu\\n\\n4\\n1\\n0\\n2\\n \\nc\\ne\\nD\\n0\\n1\\n\\n \\n\\n \\n \\n]\\n\\nV\\nC\\n.\\ns\\nc\\n[\\n \\n \\n\\n1\\nv\\n4\\n7\\n4\\n3\\n\\n.\\n\\n2\\n1\\n4\\n1\\n:\\nv\\ni\\nX\\nr\\na\\n\\nAbstract\\n\\nRecent reports suggest that a generic supervised deep\\nCNN model trained on a large-scale dataset reduces, but\\ndoes not remove, dataset bias on a standard benchmark.\\nFine-tuning deep models in a new domain can require a\\nsigni\\ufb01cant amount of data, which for many applications is\\nsimply not available. We propose a new CNN architecture\\nwhich introduces an adaptation layer and an additional do-\\nmain confusion loss, to learn a representation that is both\\nsemantically meaningful and domain invariant. We addi-\\ntionally show that a domain confusion metric can be used\\nfor model selection to determine the dimension of an adap-\\ntation layer and the best position for the layer in the CNN\\narchitecture. Our proposed adaptation method offers em-\\npirical performance which exceeds previously published re-\\nsults on a standard benchmark visual domain adaptation\\ntask.\\n\\n1. Introduction\\n\\nDataset bias is a well known problem with traditional\\nsupervised approaches to image recognition [32]. A num-\\nber of recent theoretical and empirical results have shown\\nthat supervised methods\\u2019 test error increases in proportion\\nto the difference between the test and training input distri-\\nbution [3, 5, 29, 32]. In the last few years several methods\\nfor visual domain adaptation have been suggested to over-\\ncome this issue [10, 33, 2, 29, 25, 22, 17, 16, 19, 20], but\\nwere limited to shallow models. The traditional approach\\nto adapting deep models has been \\ufb01ne-tuning; see [15] for\\na recent example.\\n\\nDirectly \\ufb01ne-tuning a deep network\\u2019s parameters on a\\nsmall amount of labeled target data turns out to be prob-\\nlematic. Fortunately, pre-trained deep models do perform\\nwell in novel domains. Recently, [11, 21] showed that using\\nthe deep mid-level features learned on ImageNet, instead\\nof the more conventional bag-of-words features, effectively\\nremoved the bias in some of the domain adaptation settings\\nin the Of\\ufb01ce dataset [29]. These algorithms transferred the\\nrepresentation from a large scale domain, ImageNet, as well\\n\\nFigure 1: Our architecture optimizes a deep CNN for both\\nclassi\\ufb01cation loss as well as domain invariance. The model\\ncan be trained for supervised adaptation, when there is a\\nsmall amount of target labels available, or unsupervised\\nadaptation, when no target labels are available. We intro-\\nduce domain invariance through domain confusion guided\\nselection of the depth and width of the adaptation layer, as\\nwell as an additional domain loss term during \\ufb01ne-tuning\\nthat directly minimizes the distance between source and tar-\\nget representations.\\n\\nas using all of the data in that domain as source data for ap-\\npropriate categories. However, these methods have no way\\nto select a representation from the deep architecture and in-\\nstead report results across multiple layer selection choices.\\nDataset bias was classically illustrated in computer vi-\\nsion by way of the \\u201cname the dataset\\u201d game of Torralba and\\nEfros [32]. Indeed, this turns out to be formally connected\\nto measures of domain discrepancy [23, 6]. Optimizing for\\ndomain invariance, therefore, can be considered equivalent\\nto the task of learning to predict the class labels while si-\\nmultaneously \\ufb01nding a representation that makes the do-\\n\\n1\\n\\n.\".\".\"conv1!conv5!fc6!fc7!fc8!.\".\".\"conv1!conv5!fc6!fc7!fc8!Unlabeled\"Images\"Labeled\"Images\"fc_adapt!fc_adapt!domain!loss!classi\\ufb01cation!loss!\\x0cmains appear as similar as possible. This principle forms\\nthe essence of our proposed approach. We learn deep rep-\\nresentations by optimizing over a loss which includes both\\nclassi\\ufb01cation error on the labeled data as well as a domain\\nconfusion loss which seeks to make the domains indistin-\\nguishable.\\n\\nWe propose a new CNN architecture, outlined in Fig-\\nure 1, which uses an adaptation layer along with a do-\\nmain confusion loss based on maximum mean discrepancy\\n(MMD) [6] to automatically learn a representation jointly\\ntrained to optimize for classi\\ufb01cation and domain invariance.\\nWe show that our domain confusion metric can be used both\\nto select the dimension of the adaptation layers, choose an\\neffective placement for a new adaptation layer within a pre-\\ntrained CNN architecture, and \\ufb01ne-tune the representation.\\nOur architecture can be used to solve both supervised\\nadaptation, when a small amount of target labeled data is\\navailable, and unsupervised adaptation, when no labeled\\ntarget training data is available. We provide a comprehen-\\nsive evaluation on the popular Of\\ufb01ce benchmark for classi-\\n\\ufb01cation across visually distinct domains [29]. We demon-\\nstrate that by jointly optimizing for domain confusion and\\nclassi\\ufb01cation, we are able to signi\\ufb01cantly outperform the\\ncurrent state-of-the-art visual domain adaptation results. In\\nfact, for the case of minor pose, resolution, and lighting\\nchanges, our algorithm is able to achieve 96% accuracy\\non the target domain, demonstrating that we have in fact\\nlearned a representation that is invariant to these biases.\\n\\n2. Related work\\n\\nThe concept of visual dataset bias was popularized\\nin [32]. There have been many approaches proposed in\\nrecent years to solve the visual domain adaptation prob-\\nlem. All recognize that there is a shift in the distribu-\\ntion of the source and target data representations. In fact,\\nthe size of a domain shift is often measured by the dis-\\ntance between the source and target subspace representa-\\ntions [6, 13, 23, 26, 28]. A large number of methods have\\nsought to overcome this difference by learning a feature\\nspace transformation to align the source and target represen-\\ntations [29, 25, 13, 16]. For the supervised adaptation sce-\\nnario, when a limited amount of labeled data is available in\\nthe target domain, some approaches have been proposed to\\nlearn a target classi\\ufb01er regularized against the source clas-\\nsi\\ufb01er [33, 2, 1]. Others have sought to both learn a feature\\ntransformation and regularize a target classi\\ufb01er simultane-\\nously [20, 12].\\n\\nRecently,\\n\\nsupervised convolutional neural network\\n(CNN) based feature representations have been shown to\\nbe extremely effective for a variety of visual recognition\\ntasks [24, 11, 15, 30]. In particular, using deep representa-\\ntions dramatically reduce the effect of resolution and light-\\ning on domain shifts [11, 21].\\n\\nFigure 2: For biased datasets (left), classi\\ufb01ers learned in a\\nsource domain do not necessarily transfer well to target do-\\nmains. By optimizing an objective that simultaneously min-\\nimizes classi\\ufb01cation error and maximizes domain confusion\\n(right), we can learn representations that are discriminative\\nand domain invariant.\\n\\nParallel CNN architectures such as Siamese networks\\nhave been shown to be effective for learning invariant repre-\\nsentations [7, 9]. However, training these networks requires\\nlabels for each training instance, so it is unclear how to ex-\\ntend these methods to unsupervised settings.\\n\\nMultimodal deep learning architectures have also been\\nexplored to learn representations that are invariant to dif-\\nferent input modalities [27]. However, this method oper-\\nated primarily in a generative context and therefore did not\\nleverage the full representational power of supervised CNN\\nrepresentations.\\n\\nTraining a joint source and target CNN architecture was\\nproposed by [8], but was limited to two layers and so was\\nsigni\\ufb01cantly outperformed by the methods which used a\\ndeeper architecture [24], pre-trained on a large auxiliary\\ndata source (ex: ImageNet [4]).\\n\\n[14] proposed pre-training with a denoising auto en-\\ncoder, then training a two-layer network simultaneously\\nwith the MMD domain confusion loss. This effectively\\nlearns a domain invariant representation, but again, because\\nthe learned network is relatively shallow, it lacks the strong\\nsemantic representation that is learned by directly optimiz-\\ning a classi\\ufb01cation objective with a supervised deep CNN.\\n\\n3. Training CNN-based domain invariant rep-\\n\\nresentations\\nWe introduce a new convolutional neural network (CNN)\\narchitecture which we use to learn a visual representation\\nthat is both domain invariant and which offers strong se-\\nmantic separation.\\nIt has been shown that a pre-trained\\nCNN can be adapted for a new task through \\ufb01ne-tuning [15,\\n\\nO\"X\"X\"X\"X\"X\"O\"O\"O\"O\"Source!Target!O\"X\"X\"X\"X\"X\"O\"O\"O\"O\"X\"X\"X\"X\"X\"O\"O\"O\"O\"O\"Maximize domain!confusion!Source!Target!Minimize classi\\ufb01cation!error!\\x0c30, 18]. However, in the domain adaptation scenario there\\nis little, or no, labeled training data in the target domain so\\nwe can not directly \\ufb01ne-tune for the categories of interest,\\nC in the target domain, T . Instead, we will use data from a\\nrelated, but distinct source domain, S, where more labeled\\ndata is available from the corresponding categories, C.\\n\\nDirectly training a classi\\ufb01er using only the source data\\noften leads to over\\ufb01tting to the source distribution, causing\\nreduced performance at test time when recognizing in the\\ntarget domain. Our intuition is that if we can learn a rep-\\nresentation that minimizes the distance between the source\\nand target distributions, then we can train a classi\\ufb01er on the\\nsource labeled data and directly apply it to the target domain\\nwith minimal loss in accuracy.\\n\\nTo minimize this distance, we consider the standard\\ndistribution distance metric, Maximum Mean Discrepancy\\n(MMD) [6]. This distance is computed with respect to a\\nparticular representation, \\u03c6(\\xb7).\\nIn our case, we de\\ufb01ne a\\nrepresentation, \\u03c6(\\xb7), which operates on source data points,\\nxs \\u2208 XS, and target data points, xt \\u2208 XT . Then an empiri-\\ncal approximation to this distance is computed as followed:\\n\\nMMD(XS, XT ) =\\n\\n(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) 1\\n\\n|XS|\\n\\n(cid:88)\\n\\nxs\\u2208XS\\n\\n(cid:88)\\n\\nxt\\u2208XT\\n\\n\\u03c6(xs) \\u2212 1\\n|XT|\\n\\n(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) (1)\\n\\n\\u03c6(xt)\\n\\nAs Figure 2 shows, not only do we want to minimize the\\ndistance between domains (or maximize the domain confu-\\nsion), but we want a representation which is conducive to\\ntraining strong classi\\ufb01ers. Such a representation would en-\\nable us to learn strong classi\\ufb01ers that readily transfer across\\ndomains. One approach to meeting both these criteria is to\\nminimize the loss:\\n\\nL = LC(XL, y) + \\u03bbMMD2(XS, XT )\\n\\n(2)\\nwhere LC(XL, y) denotes classi\\ufb01cation loss on the avail-\\nable labeled data, XL, and the ground truth labels, y, and\\nMMD(XS, XT ) denotes the distance between the source\\ndata, XS, and the target data, XT . The hyperparameter \\u03bb\\ndetermines how strongly we would like to confuse the do-\\nmains.\\n\\nOne approach to minimizing this loss is to take a \\ufb01xed\\nCNN, which is already a strong classi\\ufb01cation representa-\\ntion, and use MMD to decide which layer to use activations\\nfrom to minimize the domain distribution distance. We can\\nthen use this representation to train another classi\\ufb01er for the\\nclasses we are interested in recognizing. This can be viewed\\nas coordinate descent on Eqn. 2: we take a network that was\\ntrained to minimize LC, select the representation that mini-\\nmizes MMD, then use that representation to again minimize\\nLC.\\n\\nHowever, this approach is limited in that it cannot di-\\nrectly adapt the representation\\u2014instead, it is constrained to\\n\\nselecting from a set of \\ufb01xed representations. Thus, we pro-\\npose creating a network to directly optimize the classi\\ufb01ca-\\ntion and domain confusion objectives, outlined in Figure 1.\\nWe begin with the Krizhevsky architecture [24], which\\nhas \\ufb01ve convolutional and pooling layers and three fully\\nconnected layers with dimensions {4096, 4096, |C|}. We\\nadditionally add a lower dimensional, \\u201cbottleneck,\\u201d adapta-\\ntion layer. Our intuition is that a lower dimensional layer\\ncan be used to regularize the training of the source classi-\\n\\ufb01er and prevent over\\ufb01tting to the particular nuances of the\\nsource distribution. We place the domain distance loss on\\ntop of the \\u201cbottleneck\\u201d layer to directly regularize the rep-\\nresentation to be invariant to the source and target domains.\\nThere are two model selection choices that must be made\\nto add our adaptation layer and the domain distance loss.\\nWe must choose where in the network to place the adapta-\\ntion layer and we must choose the dimension of the layer.\\nWe use the MMD metric to make both of these decisions.\\nFirst, as previously discussed, for our initial \\ufb01xed repre-\\nsentation we \\ufb01nd the layer which minimizes the empirical\\nMMD distance between all available source and target data,\\nin our experiments this corresponded to placing the layer\\nafter the fully connected layer, f c7.\\n\\nNext, we must determine the dimension for our adapta-\\ntion layer. We solve this problem with a grid search, where\\nwe \\ufb01ne-tune multiple networks using various dimensions\\nand compute the MMD in the new lower dimension repre-\\nsentation, \\ufb01nally choosing the dimension which minimizes\\nthe source and target distance.\\n\\nBoth the selection of which layer\\u2019s representation to use\\n(\\u201cdepth\\u201d) and how large the adaptation layer should be\\n(\\u201cwidth\\u201d) are guided by MMD, and thus can be seen as de-\\nscent steps on our overall objective.\\n\\nOur architecture (see Figure 1) consists of a source and\\ntarget CNN, with shared weights. Only the labeled exam-\\nples are used to compute the classi\\ufb01cation loss, while all\\ndata is used from both domains to compute the domain con-\\nfusion loss. The network is jointly trained on all available\\nsource and target data.\\n\\nThe objective outlined in Eqn. 2 is easily represented by\\nthis convolutional neural network where MMD is computed\\nover minibatches of source and target data. We simply use\\na fork at the top of the network, after the adaptation layer.\\nOne branch uses the labeled data and trains a classi\\ufb01er, and\\nthe other branch uses all the data and computes MMD be-\\ntween source and target.\\n\\nAfter \\ufb01ne-tuning this architecture, owing to the two\\nterms in the joint loss, the adaptation layer learns a represen-\\ntation that can effectively discriminate between the classes\\nin question due to the classi\\ufb01cation loss term, while still re-\\nmaining invariant to domain shift due the MMD term. We\\nexpect that such a representation will thus enable increased\\nadaptation performance.\\n\\n\\x0cFigure 3: Maximum mean discrepancy and test accuracy\\nfor different choices of representation layer. We observe\\nthat MMD between source and target and accuracy on the\\ntarget domain test set seem inversely related, indicating that\\nMMD can be used to help select a layer for adaptation.\\n\\nFigure 4: Maximum mean discrepancy and test accuracy\\nfor different values of adaptation layer dimensionality. We\\nobserve that MMD between source and target and accuracy\\non the target domain test set seem inversely related, indicat-\\ning that MMD can be used to help select a dimensionality\\nto use.\\n\\n4. Evaluation\\n\\nWe evaluate our adaptation algorithm on a standard do-\\nmain adaptation dataset with small-scale source domains.\\nWe show that our algorithm is effectively able to adapt a\\ndeep CNN representation to a target domain with limited or\\nno target labeled data.\\n\\nThe Of\\ufb01ce [29] dataset is a collection of images from\\nthree distinct domains: Amazon, DSLR, and Webcam. The\\n31 categories in the dataset consist of objects commonly en-\\ncountered in of\\ufb01ce settings, such as keyboards, \\ufb01le cabinets,\\nand laptops. The largest domain has 2817 labeled images.\\nWe evaluate our method across 5 random train/test\\nsplits for each of the 3 transfer tasks commonly used\\nfor evaluation (Amazon\\u2192Webcam, DSLR\\u2192Webcam, and\\nWebcam\\u2192DSLR) and report averages and standard errors\\nfor each setting. We compare in both supervised and un-\\nsupervised scenarios against the numbers reported by six\\nrecently published methods.\\n\\nWe follow the standard training protocol for this dataset\\nof using 20 source examples per category for the Amazon\\nsource domain and 8 images per category for Webcam or\\nDSLR as the source domains [29, 16]. For the supervised\\nadaptation setting we assume 3 labeled target examples per\\ncategory.\\n\\n4.1. Evaluating adaptation layer placement\\n\\nWe begin with an evaluation of our representation selec-\\ntion strategy. Using a pre-trained convolutional neural net-\\nwork, we extract features from source and target data using\\nthe representations at each fully connected layer. We can\\n\\nthen compute the MMD between source and target at each\\nlayer. Since a lower MMD indicates that the representation\\nis more domain invariant, we expect the representation with\\nthe lowest MMD to achieve the highest performance after\\nadaptation.\\nTo test this hypothesis, for one of the Amazon\\u2192Webcam\\nsplits we apply a simple domain adaptation baseline intro-\\nduced by Daum\\xb4e III [10] to compute test accuracy for the\\ntarget domain. Figure 3 shows a comparison of MMD and\\nadaptation performance across different choices of bridge\\nlayers. We see that MMD correctly ranks the representa-\\ntions, singling out f c7 as the best performing layer and f c6\\nas the worst. Therefore, we add our adaptation layer after\\nf c7 for the remaining experiments.\\n4.2. Choosing the adaptation layer dimension\\n\\nBefore we can learn a new representation via our pro-\\nposed \\ufb01ne-tuning method, we must determine how wide this\\nrepresentation should be. Again, we use MMD as the decid-\\ning metric.\\n\\nIn order to determine what dimensionality our learned\\nadaptation layer should have, we train a variety of networks\\nwith different widths on the Amazon\\u2192Webcam task, as this\\nis the most challenging of the three. In particular, we try dif-\\nferent widths varying from 64 to 4096, stepping by a power\\nof two each time. Once the networks are trained, we then\\ncompute MMD between source and target for each of the\\nlearned representations. Our method then selects the dimen-\\nsionality that minimizes the MMD between the source and\\ntarget data.\\n\\nFC6FC7FC8808182838485Test accuracyRepresentation layer  2030405060708090Maximum mean discrepancyTest accuracyMaximum mean discrepancy6412825651210242048409684.58585.58686.58787.5Test accuracyAdaptation layer width  567891011Maximum mean discrepancyTest accuracyMaximum mean discrepancy\\x0cA \\u2192 W D \\u2192 W W \\u2192 D Average\\n53.0\\n59.9\\n68.5\\n73.3\\n\\nGFK(PLS,PCA) [16] 46.4 \\xb1 0.5 61.3 \\xb1 0.4 66.3 \\xb1 0.4\\nSA [13]\\n52.8 \\xb1 3.7 76.6 \\xb1 1.7 76.2 \\xb1 2.5\\nDA-NBNN [31]\\nDLID [8]\\n80.7 \\xb1 2.3 94.8 \\xb1 1.2\\nDeCAF6 S+T [11]\\n53.6 \\xb1 0.2 71.2 \\xb1 0.0 83.5 \\xb1 0.0\\nDaNN [14]\\n84.1 \\xb1 0.6 95.4 \\xb1 0.4 96.3 \\xb1 0.3\\nOurs\\n\\n69.4\\n91.9\\n\\n45.0\\n\\n64.8\\n\\n78.2\\n\\n69.9\\n\\n51.9\\n\\n89.9\\n\\n\\u2013\\n\\n\\u2013\\n\\nTable 1: Multi-class accuracy evaluation on the standard supervised adaptation setting with the Of\\ufb01ce dataset. We evaluate on\\nall 31 categories using the standard experimental protocol from [29]. Here, we compare against six state-of-the-art domain\\nadaptation methods.\\n\\nA \\u2192 W D \\u2192 W W \\u2192 D Average\\n36.4\\n40.8\\n52.6\\n60.0\\n\\nGFK(PLS,PCA) [16] 15.0 \\xb1 0.4 44.6 \\xb1 0.3 49.7 \\xb1 0.5\\nSA [13]\\n23.3 \\xb1 2.7 67.2 \\xb1 1.9 67.4 \\xb1 3.0\\nDA-NBNN [31]\\nDLID [8]\\n52.2 \\xb1 1.7 91.5 \\xb1 1.5\\nDeCAF6 S [11]\\n35.0 \\xb1 0.2 70.5 \\xb1 0.0 74.3 \\xb1 0.0\\nDaNN [14]\\n59.4 \\xb1 0.8 92.5 \\xb1 0.3 91.7 \\xb1 0.8\\nOurs\\n\\n59.9\\n81.2\\n\\n68.9\\n\\n26.1\\n\\n84.9\\n\\n50.1\\n\\n15.3\\n\\n56.9\\n\\n\\u2013\\n\\n\\u2013\\n\\nTable 2: Multi-class accuracy evaluation on the standard unsupervised adaptation setting with the Of\\ufb01ce dataset. We evaluate\\non all 31 categories using the standard experimental protocol from [16]. Here, we compare against six state-of-the-art domain\\nadaptation methods.\\n\\nTo verify that MMD makes the right choice, again we\\ncompare MMD with performance on a test set. Figure 4\\nshows that we select 256 dimensions for the adaptation\\nlayer, and although this setting is not the one that maxi-\\nmizes test performance, it appears to be a reasonable choice.\\nIn particular, using MMD avoids choosing either extreme,\\nnear which performance suffers. It is worth noting that the\\nplot has quite a few irregularities\\u2014perhaps \\ufb01ner sampling\\nwould allow for a more accurate choice.\\n\\n4.3. Fine-tuning with domain confusion regulariza-\\n\\ntion\\n\\nOnce we have settled on our choice of adaptation layer\\ndimensionality, we can begin \\ufb01ne-tuning using the joint loss\\ndescribed in Section 3. However, we need to set the regular-\\nization hyperparameter \\u03bb. Setting \\u03bb too low will cause the\\nMMD regularizer have no effect on the learned representa-\\ntion, but setting \\u03bb too high will regularize too heavily and\\nlearn a degenerate representation in which all points are too\\nclose together. We set the regularization hyperparameter to\\n\\u03bb = 0.25, which makes the objective primarily weighted to-\\nwards classi\\ufb01cation, but with enough regularization to avoid\\nover\\ufb01tting.\\n\\nWe use the same \\ufb01ne-tuning architecture for both unsu-\\n\\npervised and supervised. However, in the supervised set-\\nting, the classi\\ufb01er is trained on data from both domains,\\nwhereas in the unsupervised setting, due to the lack of la-\\nbeled training data, the classi\\ufb01er sees only source data. In\\nboth settings, the MMD regularizer sees all of the data,\\nsince it does not require labels.\\n\\nFinally, because the adaptation layer and classi\\ufb01er are\\nbeing trained from scratch, we set their learning rates to be\\n10 times higher than the lower layers of the network that\\nwere copied from the pre-trained model. Fine-tuning then\\nproceeds via standard backpropagation optimization.\\n\\nThe supervised adaptation setting results are shown in\\nTable 1 and the unsupervised adaptation results are shown\\nin Table 2. We notice that our algorithm dramatically out-\\nperforms all of the competing methods. The distinct im-\\nprovement of our method demonstrates that the adaptation\\nlayer learned via MMD regularized \\ufb01ne-tuning is able to\\nsuccesfully transfer to a new target domain.\\n\\nIn order to determine how MMD regularization affects\\nlearning, we also compare the learning curves with and\\nwithout regularization on the Amazon\\u2192Webcam transfer\\ntask in Figure 5. We see that, although the unregularized\\nversion is initially faster to train, it quickly begins over-\\n\\ufb01tting, and test accuracy suffers. In contrast, using MMD\\n\\n\\x0cFigure 5: A plot of the test accuracy on an unsupervised Amazon\\u2192Webcam split during the \\ufb01rst 700 iterations of \\ufb01ne-tuning\\nfor both regularized and unregularized methods. Although initially the unregularized training achieves better performance,\\nit over\\ufb01ts to the source data. In contrast, using regularization prevents over\\ufb01tting, so although initial learning is slower we\\nultimately see better \\ufb01nal performance.\\n\\nregularization prevents the network from over\\ufb01tting to the\\nsource data, and although training takes longer, the regular-\\nization results in a higher \\ufb01nal test accuracy.\\n\\nTo further demonstrate the domain invariance of our\\nlearned representation, we plot in Figure 6 a t-SNE em-\\nbedding of Amazon and Webcam images using our learned\\nrepresentation and compare it to an embedding created with\\nf c7 in the pretrained model. Examining the embeddings,\\nwe see that our learned representation exhibits tighter class\\nclustering while mixing the domains within each cluster.\\nWhile there is weak clustering in the f c7 embedding, we\\n\\ufb01nd that most tight clusters consist of data points from one\\ndomain or the other, but rarely both.\\n\\n4.4. Historical Progress on the Of\\ufb01ce Dataset\\n\\nIn Figure 7 we report historical progress on the standard\\nOf\\ufb01ce dataset since it\\u2019s introduction. We indicate methods\\nwhich use traditional features (ex: SURF BoW) with a blue\\ncircle and methods which use deep representations with a\\nred square. We show two adaptation scenarios. The \\ufb01rst\\nscenario is a supervised adaptation task for visually distant\\ndomains (Amazon\\u2192Webcam). For this task our algorithm\\noutperforms DeCAF by 3.4% multiclass accuracy. Finally,\\nwe show the hardest task of unsupervised adaptation for that\\nsame shift. Here we show that our method provides the most\\nsigni\\ufb01cant improvement of 5.5% multiclass accuracy.\\n\\n5. Conclusion\\n\\nIn this paper, we presented an objective function for\\nlearning domain invariant representations for classi\\ufb01cation.\\nThis objective makes use of an additional domain confu-\\nsion term to ensure that domains are indistinguishable in\\nthe learned representation. We then presented a variety of\\nways to optimize this objective, ranging from simple repre-\\nsentation selection from a \\ufb01xed pool to a full convolutional\\narchitecture that directly minimizes the objective via back-\\npropagation.\\n\\nOur full method, which uses MMD both to select the\\ndepth and width of the architecture while using it as a regu-\\nlarizer during \\ufb01ne-tuning, achieves state-of-the-art perfor-\\nmance on the standard visual domain adaptation bench-\\nmark, beating previous methods by a considerable margin.\\nThese experiments show that incorporating a domain\\nconfusion term into the discriminative representation learn-\\ning process is an effective way to ensure that the learned\\nrepresentation is both useful for classi\\ufb01cation and invariant\\nto domain shifts.\\n\\nAcknowledgments\\n\\nThis work was supported in part by DARPA\\u2019s MSEE and\\nSMISC programs, NSF awards IIS-1427425, IIS-1212798,\\nand IIS-1116411, Toyota, and the Berkeley Vision and\\nLearning Center.\\n\\n 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0 100 200 300 400 500 600 700Test accuracyTraining iterationsunregularizedregularized\\x0cFigure 6: t-SNE embeddings of Amazon (blue) and Webcam (green) images using our supervised 256-dimensional repre-\\nsentation learned with MMD regularization (top left) and the original f c7 representation from the pre-trained model (bottom\\nright). Observe that the clusters formed by our representation separate classes while mixing domains much more effectively\\nthan the original representation that was not trained for domain invariance. For example, in f c7-space the Amazon monitors\\nand Webcam monitors are separated into distinct clusters, whereas with our learned representation all monitors irrespective\\nof domain are mixed into the same cluster.\\n\\n\\x0c(a) A\\u2192W supervised adaptation\\n\\n(b) A\\u2192W unsupervised adaptation\\n\\nFigure 7: Rapid progress over the last few years on a standard visual domain adaptation dataset, Of\\ufb01ce [29]. We show methods\\non Amazon\\u2192Webcam that use traditional hand designed visual representations with blue circles and methods that use deep\\nrepresentations are depicted with red squares. For the supervised task, our method achieves 84% multiclass accuracy, an\\nincrease of 3%. For the unsupervised task, our method achieves 60% multiclass accuracy, an increase of 6%.\\n\\nReferences\\n\\n[1] L. T. Alessandro Bergamo. Exploiting weakly-labeled web\\nimages to improve object classi\\ufb01cation: a domain adaptation\\napproach. In Neural Information Processing Systems (NIPS),\\nDec. 2010. 2\\n\\n[2] Y. Aytar and A. Zisserman. Tabula rasa: Model transfer for\\n\\nobject category detection. In Proc. ICCV, 2011. 1, 2\\n\\n[3] S. Ben-David, J. Blitzer, K. Crammer, F. Pereira, et al. Anal-\\nysis of representations for domain adaptation. Proc. NIPS,\\n2007. 1\\n\\n[4] A. Berg, J. Deng, and L. Fei-Fei.\\n\\nImageNet Large Scale\\n\\nVisual Recognition Challenge 2012. 2012. 2\\n\\n[5] J. Blitzer, K. Crammer, A. Kulesza, F. Pereira, and J. Wort-\\nman. Learning bounds for domain adaptation. In Proc. NIPS,\\n2007. 1\\n\\n[6] K. M. Borgwardt, A. Gretton, M. J. Rasch, H.-P. Kriegel,\\nB. Sch\\xa8olkopf, and A. J. Smola. Integrating structured biolog-\\nical data by kernel maximum mean discrepancy. In Bioinfor-\\nmatics, 2006. 1, 2, 3\\n\\n[7] J. Bromley, J. W. Bentz, L. Bottou, I. Guyon, Y. LeCun,\\nC. Moore, E. S\\xa8ackinger, and R. Shah. Signature veri\\ufb01ca-\\nInterna-\\ntion using a siamese time delay neural network.\\ntional Journal of Pattern Recognition and Arti\\ufb01cial Intelli-\\ngence, 7(04):669\\u2013688, 1993. 2\\n\\n[8] S. Chopra, S. Balakrishnan, and R. Gopalan. DLID: Deep\\nlearning for domain adaptation by interpolating between do-\\nmains. In ICML Workshop on Challenges in Representation\\nLearning, 2013. 2, 5\\n\\n[9] S. Chopra, R. Hadsell, and Y. LeCun. Learning a similar-\\nity metric discriminatively, with application to face veri\\ufb01ca-\\nIn Computer Vision and Pattern Recognition, 2005.\\ntion.\\n\\nCVPR 2005. IEEE Computer Society Conference on, vol-\\nume 1, pages 539\\u2013546. IEEE, 2005. 2\\n\\n[10] H. Daum\\xb4e III. Frustratingly easy domain adaptation. In ACL,\\n\\n2007. 1, 4\\n\\n[11] J. Donahue, Y. Jia, O. Vinyals, J. Hoffman, N. Zhang,\\nE. Tzeng, and T. Darrell. DeCAF: A Deep Convolutional\\nActivation Feature for Generic Visual Recognition. In Proc.\\nICML, 2014. 1, 2, 5\\n\\n[12] L. Duan, D. Xu, and I. W. Tsang. Learning with aug-\\nIn\\n\\nmented features for heterogeneous domain adaptation.\\nProc. ICML, 2012. 2\\n\\n[13] B. Fernando, A. Habrard, M. Sebban, and T. Tuytelaars. Un-\\nsupervised visual domain adaptation using subspace align-\\nment. In Proc. ICCV, 2013. 2, 5\\n\\n[14] M. Ghifary, W. B. Kleijn, and M. Zhang.\\n\\nDomain\\nadaptive neural networks for object recognition. CoRR,\\nabs/1409.6041, 2014. 2, 5\\n\\n[15] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich fea-\\nture hierarchies for accurate object detection and semantic\\nsegmentation. arXiv e-prints, 2013. 1, 2\\n\\n[16] B. Gong, Y. Shi, F. Sha, and K. Grauman. Geodesic \\ufb02ow\\nkernel for unsupervised domain adaptation. In Proc. CVPR,\\n2012. 1, 2, 4, 5\\n\\n[17] R. Gopalan, R. Li, and R. Chellappa. Domain adaptation\\nfor object recognition: An unsupervised approach. In Proc.\\nICCV, 2011. 1\\n\\n[18] J. Hoffman, S. Guadarrama, E. Tzeng, R. Hu, J. Donahue,\\nR. Girshick, T. Darrell, and K. Saenko. LSDA: Large scale\\nIn Neural Information Pro-\\ndetection through adaptation.\\ncessing Systems (NIPS), 2014. 2\\n\\n[19] J. Hoffman, B. Kulis, T. Darrell, and K. Saenko. Discovering\\nlatent domains for multisource domain adaptation. In Proc.\\nECCV, 2012. 1\\n\\n201120122013201420150102030405060708090Supervised Amazon \\u2212> Webcam: Accuracy over TimeMulticlass AccuracySVMs+tGFKDLIDDeCAFOurs201120122013201420150102030405060Unsupervised Amazon \\u2212> Webcam: Accuracy over TimeMulticlass AccuracySGFGFKDLIDDeCAFOurs\\x0c[20] J. Hoffman, E. Rodner, J. Donahue, K. Saenko, and T. Dar-\\nrell. Ef\\ufb01cient learning of domain-invariant image represen-\\ntations. In Proc. ICLR, 2013. 1, 2\\n\\n[21] J. Hoffman, E. Tzeng, J. Donahue, , Y. Jia, K. Saenko, and\\nT. Darrell. One-shot learning of supervised deep convolu-\\ntional models. In arXiv 1312.6204; presented at ICLR Work-\\nshop, 2014. 1, 2\\n\\n[22] A. Khosla, T. Zhou, T. Malisiewicz, A. Efros, and A. Tor-\\nralba. Undoing the damage of dataset bias. In Proc. ECCV,\\n2012. 1\\n\\n[23] D. Kifer, S. Ben-David, and J. Gehrke. Detecting change in\\n\\ndata streams. In Proc. VLDB, 2004. 1, 2\\n\\n[24] A. Krizhevsky, I. Sutskever, and G. E. Hinton.\\n\\nImageNet\\nIn\\n\\nclassi\\ufb01cation with deep convolutional neural networks.\\nProc. NIPS, 2012. 2, 3\\n\\n[25] B. Kulis, K. Saenko, and T. Darrell. What you saw is not\\nwhat you get: Domain adaptation using asymmetric kernel\\ntransforms. In Proc. CVPR, 2011. 1, 2\\n\\n[26] Y. Mansour, M. Mohri, and A. Rostamizadeh. Domain adap-\\ntation: Learning bounds and algorithms. In COLT, 2009. 2\\n[27] J. Ngiam, A. Khosla, M. Kim, J. Nam, H. Lee, and A. Y.\\nNg. Multimodal deep learning. In Proceedings of the 28th\\nInternational Conference on Machine Learning (ICML-11),\\npages 689\\u2013696, 2011. 2\\n\\n[28] S. J. Pan, I. W. Tsang, J. T. Kwok, and Q. Yang. Domain\\nadaptation via transfer component analysis. In IJCA, 2009.\\n2\\n\\n[29] K. Saenko, B. Kulis, M. Fritz, and T. Darrell. Adapting vi-\\nsual category models to new domains. In Proc. ECCV, 2010.\\n1, 2, 4, 5, 8\\n\\n[30] P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus,\\nand Y. LeCun. Overfeat: Integrated recognition, localiza-\\ntion and detection using convolutional networks. CoRR,\\nabs/1312.6229, 2013. 2\\n\\n[31] T. Tommasi and B. Caputo. Frustratingly easy NBNN do-\\n\\nmain adaptation. In Proc. ICCV, 2013. 5\\n\\n[32] A. Torralba and A. Efros. Unbiased look at dataset bias. In\\n\\nProc. CVPR, 2011. 1, 2\\n\\n[33] J. Yang, R. Yan, and A. Hauptmann. Adapting SVM classi-\\n\\ufb01ers to data with shifted distributions. In ICDM Workshops,\\n2007. 1, 2\\n\\n\\x0c', u'Multilingual Models for Compositional Distributed Semantics\\n\\nKarl Moritz Hermann and Phil Blunsom\\n\\nDepartment of Computer Science\\n\\n{karl.moritz.hermann,phil.blunsom}@cs.ox.ac.uk\\n\\nUniversity of Oxford\\nOxford, OX1 3QD, UK\\n\\n4\\n1\\n0\\n2\\n\\n \\nr\\np\\nA\\n7\\n1\\n\\n \\n\\n \\n \\n]\\nL\\nC\\n.\\ns\\nc\\n[\\n \\n \\n\\n1\\nv\\n1\\n4\\n6\\n4\\n\\n.\\n\\n4\\n0\\n4\\n1\\n:\\nv\\ni\\nX\\nr\\na\\n\\nAbstract\\n\\nWe present a novel technique for learn-\\ning semantic representations, which ex-\\ntends the distributional hypothesis to mul-\\ntilingual data and joint-space embeddings.\\nOur models leverage parallel data and\\nlearn to strongly align the embeddings of\\nsemantically equivalent sentences, while\\nmaintaining suf\\ufb01cient distance between\\nthose of dissimilar sentences. The mod-\\nels do not rely on word alignments or\\nany syntactic information and are success-\\nfully applied to a number of diverse lan-\\nguages. We extend our approach to learn\\nsemantic representations at the document\\nlevel, too. We evaluate these models on\\ntwo cross-lingual document classi\\ufb01cation\\ntasks, outperforming the prior state of the\\nart. Through qualitative analysis and the\\nstudy of pivoting effects we demonstrate\\nthat our representations are semantically\\nplausible and can capture semantic rela-\\ntionships across languages without paral-\\nlel data.\\n\\nIntroduction\\n\\n1\\nDistributed representations of words provide the\\nbasis for many state-of-the-art approaches to var-\\nious problems in natural language processing to-\\nday. Such word embeddings are naturally richer\\nrepresentations than those of symbolic or discrete\\nmodels, and have been shown to be able to capture\\nboth syntactic and semantic information. Success-\\nful applications of such models include language\\nmodelling (Bengio et al., 2003), paraphrase detec-\\ntion (Erk and Pad\\xb4o, 2008), and dialogue analysis\\n(Kalchbrenner and Blunsom, 2013).\\n\\nWithin a monolingual context, the distributional\\nhypothesis (Firth, 1957) forms the basis of most\\napproaches for learning word representations. In\\n\\nFigure 1: Model with parallel input sentences a and b. The\\nmodel minimises the distance between the sentence level en-\\ncoding of the bitext. Any composition functions (CVM) can\\nbe used to generate the compositional sentence level repre-\\nsentations.\\n\\nthis work, we extend this hypothesis to multilin-\\ngual data and joint-space embeddings. We present\\na novel unsupervised technique for learning se-\\nmantic representations that leverages parallel cor-\\npora and employs semantic transfer through com-\\npositional representations. Unlike most methods\\nfor learning word representations, which are re-\\nstricted to a single language, our approach learns\\nto represent meaning across languages in a shared\\nmultilingual semantic space.\\n\\nWe present experiments on two corpora. First,\\nwe show that for cross-lingual document clas-\\nsi\\ufb01cation on the Reuters RCV1/RCV2 corpora\\n(Lewis et al., 2004), we outperform the prior state\\nof the art (Klementiev et al., 2012). Second,\\nwe also present classi\\ufb01cation results on a mas-\\nsively multilingual corpus which we derive from\\nthe TED corpus (Cettolo et al., 2012). The re-\\nsults on this task, in comparison with a number of\\nstrong baselines, further demonstrate the relevance\\nof our approach and the success of our method\\nin learning multilingual semantic representations\\nover a wide range of languages.\\n\\n\\x0c2 Overview\\n\\nDistributed representation learning describes the\\ntask of learning continuous representations for dis-\\ncrete objects. Here, we focus on learning seman-\\ntic representations and investigate how the use of\\nmultilingual data can improve learning such rep-\\nresentations at the word and higher level. We\\npresent a model\\nlearns to represent each\\nword in a lexicon by a continuous vector in Rd.\\nSuch distributed representations allow a model to\\nshare meaning between similar words, and have\\nbeen used to capture semantic, syntactic and mor-\\nphological content (Collobert and Weston, 2008;\\nTurian et al., 2010, inter alia).\\n\\nthat\\n\\nWe describe a multilingual objective function\\nthat uses a noise-contrastive update between se-\\nmantic representations of different languages to\\nlearn these word embeddings. As part of this, we\\nuse a compositional vector model (CVM, hence-\\nforth) to compute semantic representations of sen-\\ntences and documents. A CVM learns seman-\\ntic representations of larger syntactic units given\\nthe semantic representations of their constituents\\n(Clark and Pulman, 2007; Mitchell and Lapata,\\n2008; Baroni and Zamparelli, 2010; Grefenstette\\nand Sadrzadeh, 2011; Socher et al., 2012; Her-\\nmann and Blunsom, 2013, inter alia).\\n\\nA key difference between our approach and\\nthose listed above is that we only require sentence-\\naligned parallel data in our otherwise unsuper-\\nvised learning function. This removes a number of\\nconstraints that normally come with CVM mod-\\nels, such as the need for syntactic parse trees, word\\nalignment or annotated data as a training signal.\\nAt the same time, by using multiple CVMs to\\ntransfer information between languages, we en-\\nable our models to capture a broader semantic con-\\ntext than would otherwise be possible.\\n\\nThe idea of extracting semantics from multilin-\\ngual data stems from prior work in the \\ufb01eld of\\nsemantic grounding. Language acquisition in hu-\\nmans is widely seen as grounded in sensory-motor\\nexperience (Bloom, 2001; Roy, 2003). Based\\non this idea, there have been some attempts at\\nusing multi-modal data for learning better vec-\\ntor representations of words (e.g. Srivastava and\\nSalakhutdinov (2012)). Such methods, however,\\nare not easily scalable across languages or to large\\namounts of data for which no secondary or tertiary\\nrepresentation might exist.\\n\\nParallel data in multiple languages provides an\\n\\nalternative to such secondary representations, as\\nparallel texts share their semantics, and thus one\\nlanguage can be used to ground the other. Some\\nwork has exploited this idea for transferring lin-\\nguistic knowledge into low-resource languages or\\nto learn distributed representations at the word\\nlevel (Klementiev et al., 2012; Zou et al., 2013;\\nLauly et al., 2013, inter alia). So far almost all\\nof this work has been focused on learning multi-\\nlingual representations at the word level. As dis-\\ntributed representations of larger expressions have\\nbeen shown to be highly useful for a number of\\ntasks, it seems to be a natural next step to attempt\\nto induce these, too, cross-lingually.\\n\\n3 Approach\\n\\nMost prior work on learning compositional se-\\nmantic representations employs parse trees on\\ntheir training data to structure their composition\\nfunctions (Socher et al., 2012; Hermann and Blun-\\nsom, 2013, inter alia). Further, these approaches\\ntypically depend on speci\\ufb01c semantic signals such\\nas sentiment- or topic-labels for their objective\\nfunctions. While these methods have been shown\\nto work in some cases, the need for parse trees and\\nannotated data limits such approaches to resource-\\nfortunate languages. Our novel method for learn-\\ning compositional vectors removes these require-\\nments, and as such can more easily be applied to\\nlow-resource languages.\\n\\nSpeci\\ufb01cally, we attempt to learn semantics from\\nmultilingual data. The idea is that, given enough\\nparallel data, a shared representation of two paral-\\nlel sentences would be forced to capture the com-\\nmon elements between these two sentences. What\\nparallel sentences share, of course, are their se-\\nmantics. Naturally, different languages express\\nmeaning in different ways. We utilise this di-\\nversity to abstract further from mono-lingual sur-\\nface realisations to deeper semantic representa-\\ntions. We exploit this semantic similarity across\\nlanguages by de\\ufb01ning a bilingual (and trivially\\nmultilingual) energy as follows.\\nand\\ng : Y \\u2192 Rd, which map sentences from lan-\\nguages x and y onto distributed semantic\\nrepresentations in Rd. Given a parallel corpus C,\\nwe then de\\ufb01ne the energy of the model given two\\nsentences (a, b) \\u2208 C as:\\n\\nf : X \\u2192 Rd\\n\\nfunctions\\n\\nAssume\\n\\ntwo\\n\\nEbi(a, b) = (cid:107)f (a) \\u2212 g(b)(cid:107)2\\n\\n(1)\\n\\n\\x0cWe want to minimize Ebi for all semantically\\nequivalent sentences in the corpus.\\nIn order to\\nprevent the model from degenerating, we fur-\\nther introduce a noise-constrastive large-margin\\nupdate which ensures that the representations of\\nnon-aligned sentences observe a certain margin\\nfrom each other. For every pair of parallel sen-\\ntences (a, b) we sample a number of additional\\nsentence pairs (\\xb7, n) \\u2208 C, where n\\u2014with high\\nprobability\\u2014is not semantically equivalent to a.\\nWe use these noise samples as follows:\\n\\nEhl(a, b, n) = [m + Ebi(a, b) \\u2212 Ebi(a, n)]+\\n\\nwhere [x]+ = max(x, 0) denotes the standard\\nhinge loss and m is the margin. This results in\\nthe following objective function:\\n\\nJ(\\u03b8) =\\n\\nEhl(a, b, ni) +\\n\\n(cid:107)\\u03b8(cid:107)2\\n\\n\\u03bb\\n2\\n\\n(cid:32) k(cid:88)\\n\\n(cid:88)\\n\\n(a,b)\\u2208C\\n\\ni=1\\n\\n(cid:33)\\n\\n(2)\\n\\nwhere \\u03b8 is the set of all model variables.\\n\\n3.1 Two Composition Models\\nThe objective function in Equation 2 could be cou-\\npled with any two given vector composition func-\\ntions f, g from the literature. As we aim to apply\\nour approach to a wide range of languages, we fo-\\ncus on composition functions that do not require\\nany syntactic information. We evaluate the follow-\\ning two composition functions.\\n\\nThe \\ufb01rst model, ADD, represents a sentence by\\nthe sum of its word vectors. This is a distributed\\nbag-of-words approach as sentence ordering is not\\ntaken into account by the model.\\n\\nSecond, the BI model is designed to capture bi-\\ngram information, using a non-linearity over bi-\\ngram pairs in its composition function:\\n\\nf (x) =\\n\\ntanh (xi\\u22121 + xi)\\n\\n(3)\\n\\ni=1\\n\\nThe use of a non-linearity enables the model to\\nlearn interesting interactions between words in a\\ndocument, which the bag-of-words approach of\\nADD is not capable of learning. We use the hy-\\nperbolic tangent as activation function.\\n\\n3.2 Document-level Semantics\\nFor a number of tasks, such as topic modelling,\\nrepresentations of objects beyond the sentence\\nlevel are required. While most approaches to com-\\npositional distributed semantics end at the word\\n\\nn(cid:88)\\n\\nFigure 2: Description of a parallel document-level composi-\\ntional vector model (DOC). The model recursively computes\\nsemantic representations for each sentence of a document and\\nthen for the document itself, treating the sentence vectors as\\ninputs for a second CVM.\\n\\nlevel, our model extends to document-level learn-\\ning quite naturally, by recursively applying the\\ncomposition and objective function (Equation 2)\\nto compose sentences into documents. This is\\nachieved by \\ufb01rst computing semantic representa-\\ntions for each sentence in a document. Next, these\\nrepresentations are used as inputs in a higher-level\\nCVM, computing a semantic representation of a\\ndocument (Figure 2).\\n\\nThis recursive approach integrates document-\\nlevel representations into the learning process.\\nWe can thus use corpora of parallel documents\\u2014\\nregardless of whether they are sentence aligned or\\nnot\\u2014to propagate a semantic signal back to the\\nindividual words. If sentence alignment is avail-\\nable, of course, the document-signal can simply\\nbe combined with the sentence-signal, as we did\\nwith the experiments described in \\xa75.3.\\n\\nThis concept of learning compositional repre-\\nsentations for documents contrasts with prior work\\n(Socher et al., 2011; Klementiev et al., 2012, inter\\nalia) who rely on summing or averaging sentence-\\nvectors if representations beyond the sentence-\\nlevel are required for a particular task.\\n\\nWe evaluate the models presented in this paper\\nboth with and without the document-level signal.\\nWe refer to the individual models used as ADD and\\nBI if used without, and as DOC/ADD and DOC/BI\\nis used with the additional document composition\\nfunction and error signal.\\n\\n4 Corpora\\n\\nWe use two corpora for learning semantic rep-\\nresentations and performing the experiments de-\\nscribed in this paper.\\n\\n\\x0cThe Europarl corpus v71 (Koehn, 2005) was\\nused during initial development and testing of\\nour approach, as well as to learn the representa-\\ntions used for the Cross-Lingual Document Clas-\\nsi\\ufb01cation task described in \\xa75.2. We considered\\nthe English-German and English-French language\\npairs from this corpus. From each pair the \\ufb01nal\\n100,000 sentences were reserved for development.\\nSecond, we developed a massively multilin-\\ngual corpus based on the TED corpus2 for IWSLT\\n2013 (Cettolo et al., 2012). This corpus contains\\nEnglish transcriptions and multilingual, sentence-\\naligned translations of talks from the TED confer-\\nence. While the corpus is aimed at machine trans-\\nlation tasks, we use the keywords associated with\\neach talk to build a subsidiary corpus for multilin-\\ngual document classi\\ufb01cation as follows.3\\n\\nThe development sections provided with the\\nIWSLT 2013 corpus were again reserved for de-\\nvelopment. We removed approximately 10 per-\\ncent of the training data in each language to cre-\\nate a test corpus (all talks with id \\u2265 1,400). The\\nnew training corpus consists of a total of 12,078\\nparallel documents distributed across 12 language\\npairs4.\\nIn total, this amounts to 1,678,219 non-\\nEnglish sentences (the number of unique English\\nsentences is smaller as many documents are trans-\\nlated into multiple languages and thus appear re-\\npeatedly in the corpus). Each document (talk) con-\\ntains one or several keywords. We used the 15\\nmost frequent keywords for the topic classi\\ufb01cation\\nexperiments described in section \\xa75.3.\\n\\nBoth corpora were pre-processed using the set\\nof tools provided by cdec5 for tokenizing and low-\\nercasing the data. Further, all empty sentences and\\ntheir translations were removed from the corpus.\\n\\n5 Experiments\\n\\nWe report results on two experiments. First, we\\nreplicate the cross-lingual document classi\\ufb01cation\\ntask of Klementiev et al. (2012),\\nlearning dis-\\ntributed representations on the Europarl corpus\\nand evaluating on documents from the Reuters\\nRCV1/RCV2 corpora. Subsequently, we design a\\n\\n1http://www.statmt.org/europarl/\\n2https://wit3.fbk.eu/\\n3http://www.clg.ox.ac.uk/tedcldc/\\n4English to Arabic, German, French, Spanish, Italian,\\nDutch, Polish, Brazilian Portuguese, Romanian, Russian and\\nTurkish. Chinese, Farsi and Slowenian were removed due to\\nthe small size of those datasets.\\n\\n5http://cdec-decoder.org/\\n\\nmulti-label classi\\ufb01cation task using the TED cor-\\npus, both for training and evaluating. The use of\\na wider range of languages in the second experi-\\nments allows us to better evaluate our models\\u2019 ca-\\npabilities in learning a shared multilingual seman-\\ntic representation. We also investigate the learned\\nembeddings from a qualitative perspective in \\xa75.4.\\n5.1 Learning\\nAll model weights were randomly initialised us-\\ning a Gaussian distribution (\\xb5=0, \\u03c32=0.1). We\\nused the available development data to set our\\nmodel parameters. For each positive sample we\\nused a number of noise samples (k \\u2208 {1, 10, 50}),\\nrandomly drawn from the corpus at each training\\nepoch. All our embeddings have dimensionality\\nd=128, with the margin set to m=d.6 Further, we\\nuse L2 regularization with \\u03bb=1 and step-size in\\n{0.01, 0.05}. We use 100 iterations for the RCV\\ntask, 500 for the TED single and 5 for the joint\\ncorpora. We use the adaptive gradient method,\\nAdaGrad (Duchi et al., 2011), for updating the\\nweights of our models, in a mini-batch setting (b \\u2208\\n{10, 50}). All settings, our model implementation\\nand scripts to replicate our experiments are avail-\\nable at http://www.karlmoritz.com/.\\n\\n5.2 RCV1/RCV2 Document Classi\\ufb01cation\\nWe evaluate our models on the cross-lingual doc-\\nument classi\\ufb01cation (CLDC, henceforth) task \\ufb01rst\\ndescribed in Klementiev et al. (2012). This task in-\\nvolves learning language independent embeddings\\nwhich are then used for document classi\\ufb01cation\\nacross the English-German language pair. For this,\\nCLDC employs a particular kind of supervision,\\nnamely using supervised training data in one lan-\\nguage and evaluating without further supervision\\nin another. Thus, CLDC can be used to establish\\nwhether our learned representations are semanti-\\ncally useful across multiple languages.\\n\\nWe follow the experimental setup described in\\nKlementiev et al. (2012), with the exception that\\nwe learn our embeddings using solely the Europarl\\ndata and use the Reuters corpora only during for\\nclassi\\ufb01er training and testing. Each document in\\nthe classi\\ufb01cation task is represented by the aver-\\nage of the d-dimensional representations of all its\\nsentences. We train the multiclass classi\\ufb01er using\\nan averaged perceptron (Collins, 2002) with the\\nsame settings as in Klementiev et al. (2012).\\n\\n6On the RCV task we also report results for d=40 which\\n\\nmatches the dimensionality of Klementiev et al. (2012).\\n\\n\\x0cModel\\nMajority Class\\nGlossed\\nMT\\nI-Matrix\\n\\nen \\u2192 de\\n46.8\\n65.1\\n68.1\\n77.6\\n\\nde \\u2192 en\\n46.8\\n68.6\\n67.4\\n71.1\\n\\ndim = 40\\nADD\\nADD+\\nBI\\nBI+\\n\\ndim = 128\\nADD\\nADD+\\nBI\\nBI+\\n\\n83.7\\n86.2\\n83.4\\n86.9\\n\\n86.4\\n87.7\\n86.1\\n88.1\\n\\n71.4\\n76.9\\n69.2\\n74.3\\n\\n74.7\\n77.5\\n79.0\\n79.2\\n\\nTable 1: Classi\\ufb01cation accuracy for training on English and\\nGerman with 1000 labeled examples on the RCV corpus.\\nCross-lingual compositional representations (ADD, BI and\\ntheir multilingual extensions), I-Matrix (Klementiev et al.,\\n2012) translated (MT) and glossed (Glossed) word baselines,\\nand the majority class baseline. The baseline results are from\\nKlementiev et al. (2012).\\n\\nWe present results from four models. The ADD\\nmodel is trained on 500k sentence pairs of the\\nEnglish-German parallel section of the Europarl\\ncorpus. The ADD+ model uses an additional 500k\\nparallel sentences from the English-French cor-\\npus, resulting in one million English sentences,\\neach paired up with either a German or a French\\nsentence, with BI and BI+ trained accordingly.\\nThe motivation behind ADD+ and BI+ is to inves-\\ntigate whether we can learn better embeddings by\\nintroducing additional data from other languages.\\nA similar idea exists in machine translation where\\nEnglish is frequently used to pivot between other\\nlanguages (Cohn and Lapata, 2007).\\n\\nThe actual CLDC experiments are performed\\nby training on English and testing on German doc-\\numents and vice versa. Following prior work, we\\nuse varying sizes between 100 and 10,000 docu-\\nments when training the multiclass classi\\ufb01er. The\\nresults of this task across training sizes are in Fig-\\nure 3. Table 1 shows the results for training on\\n1,000 documents compared with the results pub-\\nlished in Klementiev et al. (2012). Our models\\noutperform the prior state of the art, with the BI\\nmodels performing slightly better than the ADD\\nmodels. As the relative results indicate, the addi-\\ntion of a second language improves model perfor-\\n\\nmance. It it interesting to note that results improve\\nin both directions of the task, even though no addi-\\ntional German data was used for the \\u2018+\\u2018 models.\\n\\n5.3 TED Corpus Experiments\\nHere we describe our experiments on the TED cor-\\npus, which enables us to scale up to multilingual\\nlearning. Consisting of a large number of rela-\\ntively short and parallel documents, this corpus al-\\nlows us to evaluate the performance of the DOC\\nmodel described in \\xa73.2.\\n\\nWe use the training data of the corpus to learn\\ndistributed representations across 12 languages.\\nTraining is performed in two settings. In the sin-\\ngle mode, vectors are learnt from a single lan-\\nguage pair (en-X), while in the joint mode vector-\\nlearning is performed on all parallel sub-corpora\\nsimultaneously. This setting causes words from\\nall languages to be embedded in a single semantic\\nspace.\\nFirst, we evaluate the effect of the document-\\nlevel error signal (DOC, described in \\xa73.2), as well\\nas whether our multilingual learning method can\\nextend to a larger variety of languages. We train\\nDOC models, using both ADD and BI as CVM\\n(DOC/ADD, DOC/BI), both in the single and joint\\nmode. For comparison, we also train ADD and\\nDOC models without the document-level error sig-\\nnal. The resulting document-level representations\\nare used to train classi\\ufb01ers (system and settings as\\nin \\xa75.2) for each language, which are then evalu-\\nated in the paired language. In the English case\\nwe train twelve individual classi\\ufb01ers, each using\\nthe training data of a single language pair only.\\nAs described in \\xa74, we use 15 keywords for the\\nclassi\\ufb01cation task. Due to space limitations, we\\nreport cumulative results in the form of F1-scores\\nthroughout this paper.\\nMT System We develop a machine translation\\nbaseline as follows. We train a machine translation\\ntool on the parallel training data, using the devel-\\nopment data of each language pair to optimize the\\ntranslation system. We use the cdec decoder (Dyer\\net al., 2010) with default settings for this purpose.\\nWith this system we translate the test data, and\\nthen use a Na\\xa8\\u0131ve Bayes classi\\ufb01er7 for the actual\\nexperiments. To exemplify, this means the de\\u2192ar\\nresult is produced by training a translation system\\nfrom Arabic to German. The Arabic test set is\\ntranslated into German. A classi\\ufb01er is then trained\\n\\n7We use the implementation in Mallet (McCallum, 2002)\\n\\n\\x0cSetting\\n\\nen \\u2192 L2\\nMT System\\nADD single\\nBI single\\nDOC/ADD single\\nDOC/BI single\\nDOC/ADD joint\\nDOC/BI joint\\nL2 \\u2192 en\\nMT System\\nADD single\\nBI single\\nDOC/ADD single\\nDOC/BI single\\nDOC/ADD joint\\nDOC/BI joint\\n\\nArabic German Spanish French Italian Dutch Polish Pt-Br Roman. Russian Turkish\\n\\nLanguages\\n\\n0.429\\n0.328\\n0.375\\n0.410\\n0.389\\n0.392\\n0.372\\n\\n0.448\\n0.380\\n0.354\\n0.452\\n0.406\\n0.396\\n0.343\\n\\n0.465\\n0.343\\n0.360\\n0.424\\n0.428\\n0.405\\n0.369\\n\\n0.469\\n0.337\\n0.411\\n0.476\\n0.442\\n0.388\\n0.375\\n\\n0.518\\n0.401\\n0.379\\n0.383\\n0.416\\n0.443\\n0.451\\n\\n0.486\\n0.446\\n0.344\\n0.422\\n0.365\\n0.399\\n0.369\\n\\n0.526 0.514 0.505 0.445 0.470\\n0.275 0.282 0.317 0.141 0.227\\n0.431 0.465 0.421 0.435 0.329\\n0.476 0.485 0.264 0.402 0.354\\n0.445 0.473 0.219 0.403 0.400\\n0.447 0.475 0.453 0.394 0.409\\n0.429 0.404 0.433 0.417 0.399\\n\\n0.358 0.481 0.463 0.460 0.374\\n0.293 0.357 0.295 0.327 0.235\\n0.426 0.439 0.428 0.443 0.357\\n0.464 0.461 0.251 0.400 0.338\\n0.479 0.460 0.235 0.393 0.380\\n0.415 0.461 0.478 0.352 0.399\\n0.419 0.398 0.438 0.353 0.391\\n\\n0.493\\n0.282\\n0.426\\n0.418\\n0.467\\n0.446\\n0.453\\n\\n0.486\\n0.293\\n0.426\\n0.407\\n0.426\\n0.412\\n0.430\\n\\n0.432\\n0.338\\n0.423\\n0.448\\n0.421\\n0.476\\n0.439\\n\\n0.404\\n0.355\\n0.442\\n0.471\\n0.467\\n0.343\\n0.375\\n\\n0.409\\n0.241\\n0.481\\n0.452\\n0.457\\n0.417\\n0.418\\n\\n0.441\\n0.375\\n0.403\\n0.435\\n0.477\\n0.343\\n0.388\\n\\nTable 2: F1-scores for the TED document classi\\ufb01cation task for individual languages. Results are re-\\nported for both directions (training on English, evaluating on L2 and vice versa). Bold indicates best\\nresult, underline best result amongst the vector-based systems.\\n\\nTraining\\nLanguage\\n\\nArabic\\nGerman\\nSpanish\\nFrench\\nItalian\\nDutch\\nPolish\\nPortuguese\\nRomanian\\nRussian\\nTurkish\\n\\nTest Language\\n\\n0.378\\n\\nArabic German Spanish French Italian Dutch Polish Pt-Br Rom\\u2019n Russian Turkish\\n0.397\\n0.443\\n0.382\\n0.398\\n0.352\\n0.395\\n0.408\\n0.431\\n0.402\\n0.447\\n\\n0.436 0.432 0.444 0.438 0.389 0.425\\n0.474 0.460 0.464 0.440 0.375 0.417\\n0.420 0.439 0.435 0.415 0.390\\n0.474 0.429 0.403 0.418\\n0.393 0.339 0.347\\n0.405 0.386\\n0.401\\n\\n0.487\\n0.461 0.466\\n0.463 0.464 0.460\\n0.449 0.444 0.430 0.441\\n0.476 0.447 0.486 0.458 0.403\\n0.473 0.476 0.460 0.434 0.416 0.433\\n0.492 0.427 0.438 0.452 0.430 0.419\\n0.479 0.433 0.427 0.423 0.439 0.367\\n\\n0.368\\n0.353\\n0.383\\n0.398\\n0.377\\n0.359\\n0.391\\n0.416\\n0.372\\n0.376\\n\\n0.446\\n0.458\\n0.427\\n0.415\\n0.382\\n0.407\\n0.398\\n0.431\\n0.444\\n\\n0.355\\n0.366\\n0.405\\n0.354\\n0.386\\n0.392\\n0.320\\n0.352\\n0.352\\n\\n0.420\\n0.447\\n0.424\\n0.458\\n0.376\\n0.415\\n0.434\\n0.457\\n\\n0.441\\n0.434\\n\\n0.411\\n\\nTable 3: F1-scores for TED corpus document classi\\ufb01cation results when training and testing on two\\nlanguages that do not share any parallel data. We train a DOC/ADD model on all en-L2 language pairs\\ntogether, and then use the resulting embeddings to train document classi\\ufb01ers in each language. These\\nclassi\\ufb01ers are subsequently used to classify data from all other languages.\\n\\nSetting\\n\\nRaw Data NB\\nSenna\\nPolyglot\\nsingle Setting\\nDOC/ADD\\nDOC/BI\\njoint Setting\\nDOC/ADD\\nDOC/BI\\n\\nEnglish Arabic German Spanish French Italian Dutch Polish Pt-Br Roman. Russian Turkish\\n0.513\\n\\n0.532 0.524 0.522 0.415 0.465\\n\\n0.526\\n\\n0.471\\n\\n0.509\\n\\n0.465\\n\\nLanguages\\n\\n0.481 0.469\\n0.400\\n0.382 0.416\\n\\n0.270\\n\\n0.418\\n\\n0.361 0.332 0.228 0.323 0.194\\n\\n0.300\\n\\n0.402\\n\\n0.295\\n\\n0.462 0.422\\n0.474 0.432\\n\\n0.475 0.371\\n0.378 0.329\\n\\n0.429\\n0.362\\n\\n0.386\\n0.358\\n\\n0.394\\n0.336\\n\\n0.472\\n0.472\\n\\n0.481 0.458 0.252 0.385 0.363\\n0.444 0.469 0.197 0.414 0.395\\n\\n0.451 0.398 0.439 0.304 0.394\\n0.454 0.399 0.409 0.340 0.431\\n\\n0.431\\n0.445\\n\\n0.453\\n0.379\\n\\n0.471\\n0.436\\n\\n0.402\\n0.395\\n\\n0.435\\n0.428\\n\\n0.441\\n0.435\\n\\nTable 4: F1-scores on the TED corpus document classi\\ufb01cation task when training and evaluating on the\\nsame language. Baseline embeddings are Senna (Collobert et al., 2011) and Polyglot (Al-Rfou\\u2019 et al.,\\n2013).\\n\\n\\x0c)\\n\\n%\\n\\n(\\n\\ny\\nc\\na\\nr\\nu\\nc\\nc\\nA\\nn\\no\\ni\\nt\\na\\nc\\n\\ufb01\\n\\ni\\ns\\ns\\na\\nl\\nC\\n\\n80\\n\\n70\\n\\n60\\n\\n90\\n\\n80\\n\\n70\\n\\n60\\n\\n50\\n\\n100\\n\\n200\\n\\n500\\n\\n1000\\n\\n5000\\n\\n10k\\n\\n100\\n\\n200\\n\\n500\\n\\n1000\\n\\n5000\\n\\n10k\\n\\nTraining Documents (de)\\nBI+\\n\\nADD+\\n\\nTraining Documents (en)\\n\\nI-Matrix\\n\\nMT\\n\\nGlossed\\n\\nFigure 3: Classi\\ufb01cation accuracy for a number of models (see Table 1 for model descriptions). The left chart shows results for\\nthese models when trained on German data and evaluated on English data, the right chart vice versa.\\n\\non the German training data and evaluated on the\\ntranslated Arabic. While we developed this system\\nas a baseline, it must be noted that the classi\\ufb01er of\\nthis system has access to signi\\ufb01cantly more infor-\\nmation (all words in the document) as opposed to\\nour models (one embedding per document), and\\nwe do not expect to necessarily beat this system.\\n\\nThe results of this experiment are in Table 2.\\nWhen comparing the results between the ADD\\nmodel and the models trained using the document-\\nlevel error signal, the bene\\ufb01t of this additional sig-\\nnal becomes clear. The joint training mode leads\\nto a relative improvement when training on En-\\nglish data and evaluating in a second language.\\nThis suggests that the joint mode improves the\\nquality of the English embeddings more than it\\naffects the L2-embeddings. More surprising, per-\\nhaps, is the relative performance between the ADD\\nand BI composition functions, especially when\\ncompared to the results in \\xa75.2, where the BI mod-\\nels relatively consistently performed better. We\\nsuspect that the better performance of the additive\\ncomposition function on this task is related to the\\nsmaller amount of training data available which\\ncould cause sparsity issues for the bigram model.\\nAs expected, the MT system slightly outper-\\nforms our models on most language pairs. How-\\never, the overall performance of the models is\\ncomparable to that of the MT system. Consider-\\ning the relative amount of information available\\nduring the classi\\ufb01er training phase, this indicates\\nthat our learned representations are semantically\\nuseful, capturing almost the same amount of infor-\\nmation as available to the Na\\xa8\\u0131ve Bayes classi\\ufb01er.\\nWe next investigate linguistic transfer across\\n\\nlanguages. We re-use the embeddings learned\\nwith the DOC/ADD joint model from the previ-\\nous experiment for this purpose, and train clas-\\nsi\\ufb01ers on all non-English languages using those\\nembeddings. Subsequently, we evaluate their per-\\nformance in classifying documents in the remain-\\ning languages. Results for this task are in Table 3.\\nWhile the results across language-pairs might not\\nbe very insightful, the overall good performance\\ncompared with the results in Table 2 implies that\\nwe learnt semantically meaningful vectors and in\\nfact a joint embedding space across thirteen lan-\\nguages.\\n\\nIn a third evaluation (Table 4), we apply the em-\\nbeddings learnt with out models to a monolingual\\nclassi\\ufb01cation task, enabling us to compare with\\nprior work on distributed representation learning.\\nIn this experiment a classi\\ufb01er is trained in one lan-\\nguage and then evaluated in the same. We again\\nuse a Na\\xa8\\u0131ve Bayes classi\\ufb01er on the raw data to es-\\ntablish a reasonable upper bound.\\n\\nWe compare our embeddings with the SENNA\\nembeddings, which achieve state of the art per-\\nformance on a number of tasks (Collobert et al.,\\n2011). Additionally, we use the Polyglot embed-\\ndings of Al-Rfou\\u2019 et al. (2013), who published\\nword embeddings across 100 languages, including\\nall languages considered in this paper. We repre-\\nsent each document by the mean of its word vec-\\ntors and then apply the same classi\\ufb01er training and\\ntesting regime as with our models. Even though\\nboth of these sets of embeddings were trained on\\nmuch larger datasets than ours, our models outper-\\nform these baselines on all languages\\u2014even out-\\nperforming the Na\\xa8\\u0131ve Bayes system on on several\\n\\n\\x0cdent and gender-speci\\ufb01c expressions Mr President\\nand Madam President as well as gender-speci\\ufb01c\\nequivalents in French and German. The projec-\\ntion demonstrates a number of interesting results:\\nFirst, the model correctly clusters the words into\\nthree groups, corresponding to the three English\\nforms and their associated translations. Second, a\\nseparation between genders can be observed, with\\nmale forms on the bottom half of the chart and fe-\\nmale forms on the top, with the neutral the presi-\\ndent in the vertical middle. Finally, if we assume\\na horizontal line going through the president, this\\nline could be interpreted as a \\u201cgender divide\\u201d, with\\nmale and female versions of one expression mir-\\nroring each other on that line. In the case of the\\npresident and its translations, this effect becomes\\neven clearer, with the neutral English expression\\nbeing projected close to the mid-point between\\neach other language\\u2019s gender-speci\\ufb01c versions.\\n\\nThese results further support our hypothesis that\\nthe bilingual contrastive error function can learn\\nsemantically plausible embeddings and further-\\nmore, that it can abstract away from mono-lingual\\nsurface realisations into a shared semantic space\\nacross languages.\\n\\n6 Related Work\\n\\nDistributed Representations Distributed repre-\\nsentations can be learned through a number of ap-\\nproaches. In their simplest form, distributional in-\\nformation from large corpora can be used to learn\\nembeddings, where the words appearing within a\\ncertain window of the target word are used to com-\\npute that word\\u2019s embedding. This is related to\\ntopic-modelling techniques such as LSA (Dumais\\net al., 1988), LSI, and LDA (Blei et al., 2003), but\\nthese methods use a document-level context, and\\ntend to capture the topics a word is used in rather\\nthan its more immediate syntactic context.\\n\\nNeural language models are another popular ap-\\nproach for inducing distributed word representa-\\ntions (Bengio et al., 2003). They have received a\\nlot of attention in recent years (Collobert and We-\\nston, 2008; Mnih and Hinton, 2009; Mikolov et\\nal., 2010, inter alia) and have achieved state of the\\nart performance in language modelling. Collobert\\net al. (2011) further popularised using neural net-\\nwork architectures for learning word embeddings\\nfrom large amounts of largely unlabelled data by\\nshowing the embeddings can then be used to im-\\nprove standard supervised tasks.\\n\\nFigure 4: t-SNE projections for a number of English, French\\nand German words as represented by the BI+ model. Even\\nthough the model did not use any parallel French-German\\ndata during training, it learns semantic similarity between\\nthese two languages using English as a pivot, and semanti-\\ncally clusters words across all languages.\\n\\nFigure 5: t-SNE projections for a number of short phrases in\\nthree languages as represented by the BI+ model. The pro-\\njection demonstrates linguistic transfer through a pivot by. It\\nseparates phrases by gender (red for female, blue for male,\\nand green for neutral) and aligns matching phrases across lan-\\nguages.\\n\\nlanguages. While this may partly be attributed to\\nthe fact that our vectors were learned on in-domain\\ndata, this is still a very positive outcome.\\n\\n5.4 Linguistic Analysis\\nWhile the classi\\ufb01cation experiments focused on\\nestablishing the semantic content of the sentence\\nlevel representations, we also want to brie\\ufb02y in-\\nvestigate the induced word embeddings. We use\\nthe BI+ model trained on the Europarl corpus for\\nthis purpose. Figure 4 shows the t-SNE projec-\\ntions for a number of English, French and German\\nwords. Even though the model did not use any par-\\nallel French-German data during training, it still\\nmanaged to learn semantic word-word similarity\\nacross these two languages.\\n\\nGoing one step further, Figure 5 shows t-SNE\\nprojections for a number of short phrases in these\\nthree languages. We use the English the presi-\\n\\n\\x0cUnsupervised word representations can easily\\nbe plugged into a variety of NLP related tasks.\\nTasks, where the use of distributed representations\\nhas resulted in improvements include topic mod-\\nelling (Blei et al., 2003) or named entity recogni-\\ntion (Turian et al., 2010; Collobert et al., 2011).\\nCompositional Vector Models For a number of\\nimportant problems, semantic representations of\\nindividual words do not suf\\ufb01ce, but instead a se-\\nmantic representation of a larger structure\\u2014e.g. a\\nphrase or a sentence\\u2014is required. Self-evidently,\\nsparsity prevents the learning of such representa-\\ntions using the same collocational methods as ap-\\nplied to the word level. Most literature instead fo-\\ncuses on learning composition functions that rep-\\nresent the semantics of a larger structure as a func-\\ntion of the representations of its parts.\\n\\nVery simple composition functions have been\\nshown to suf\\ufb01ce for tasks such as judging bi-\\ngram semantic similarity (Mitchell and Lapata,\\n2008). More complex composition functions us-\\ning matrix-vector composition, convolutional neu-\\nral networks or tensor composition have proved\\nuseful in tasks such as sentiment analysis (Socher\\net al., 2011; Hermann and Blunsom, 2013), rela-\\ntional similarity (Turney, 2012) or dialogue analy-\\nsis (Kalchbrenner and Blunsom, 2013).\\nMultilingual Representation Learning Most\\nresearch on distributed representation induction\\nhas focused on single languages. English, with its\\nlarge number of annotated resources, has enjoyed\\nmost attention. However, there exists a corpus of\\nprior work on learning multilingual embeddings\\nor on using parallel data to transfer linguistic in-\\nformation across languages. One has to differen-\\ntiate between approaches such as Al-Rfou\\u2019 et al.\\n(2013), that learn embeddings across a large va-\\nriety of languages and models such as ours, that\\nlearn joint embeddings, that is a projection into a\\nshared semantic space across multiple languages.\\nRelated to our work, Yih et al. (2011) proposed\\nS2Nets to learn joint embeddings of tf-idf vectors\\nfor comparable documents. Their architecture op-\\ntimises the cosine similarity of documents, using\\nrelative semantic similarity scores during learn-\\ning. More recently, Lauly et al. (2013) proposed a\\nbag-of-words autoencoder model, where the bag-\\nof-words representation in one language is used to\\ntrain the embeddings in another. By placing their\\nvocabulary in a binary branching tree, the prob-\\nabilistic setup of this model is similar to that of\\n\\nMnih and Hinton (2009). Similarly, Sarath Chan-\\ndar et al. (2013) train a cross-lingual encoder,\\nwhere an autoencoder is used to recreate words in\\ntwo languages in parallel. This is effectively the\\nlinguistic extension of Ngiam et al. (2011), who\\nused a similar method for audio and video data.\\nHermann and Blunsom (2014) propose a large-\\nmargin learner for multilingual word representa-\\ntions, similar to the basic additive model proposed\\nhere, which, like the approaches above, relies on a\\nbag-of-words model for sentence representations.\\nKlementiev et al. (2012), our baseline in \\xa75.2,\\nuse a form of multi-agent\\nlearning on word-\\naligned parallel data to transfer embeddings from\\none language to another. Earlier work, Haghighi\\net al. (2008), proposed a method for inducing\\nbilingual lexica using monolingual feature repre-\\nsentations and a small initial lexicon to bootstrap\\nwith. This approach has recently been extended\\nby Mikolov et al. (2013a), Mikolov et al. (2013b),\\nwho developed a method for learning transforma-\\ntion matrices to convert semantic vectors of one\\nlanguage into those of another.\\nIs was demon-\\nstrated that this approach can be applied to im-\\nprove tasks related to machine translation. Their\\nCBOW model is also worth noting for its sim-\\nilarities to the ADD composition function used\\nhere. Using a slightly different approach, Zou et\\nal. (2013), also learned bilingual embeddings for\\nmachine translation.\\n\\n7 Conclusion\\n\\nTo summarize, we have presented a novel method\\nfor learning multilingual word embeddings using\\nparallel data in conjunction with a multilingual ob-\\njective function for compositional vector models.\\nThis approach extends the distributional hypoth-\\nesis to multilingual joint-space representations.\\nCoupled with very simple composition functions,\\nvectors learned with this method outperform the\\nstate of the art on the task of cross-lingual docu-\\nment classi\\ufb01cation. Further experiments and anal-\\nysis support our hypothesis that bilingual signals\\nare a useful tool for learning distributed represen-\\ntations by enabling models to abstract away from\\nmono-lingual surface realisations into a deeper se-\\nmantic space.\\n\\nAcknowledgements\\n\\nThis work was supported by a Xerox Foundation\\nAward and EPSRC grant number EP/K036580/1.\\n\\n\\x0cReferences\\nR. Al-Rfou\\u2019, B. Perozzi, and S. Skiena. 2013. Poly-\\nglot: Distributed word representations for multilin-\\ngual nlp. In Proceedings of CoNLL.\\n\\nM. Baroni and R. Zamparelli.\\n\\nNouns\\nare vectors, adjectives are matrices: Representing\\nadjective-noun constructions in semantic space. In\\nProceedings of EMNLP.\\n\\n2010.\\n\\nY. Bengio, R. Ducharme, P. Vincent, and C. Janvin.\\n2003. A neural probabilistic language model. Jour-\\nnal of Machine Learning Research, 3:1137\\u20131155,\\nMarch.\\n\\nD. M. Blei, A. Y. Ng, and M. I. Jordan. 2003. Latent\\ndirichlet allocation. Journal of Machine Learning\\nResearch, 3:993\\u20131022.\\n\\nP. Bloom. 2001. Precis of how children learn the\\nmeanings of words. Behavioral and Brain Sciences,\\n24:1095\\u20131103.\\n\\nM. Cettolo, C. Girardi, and M. Federico. 2012. Wit3:\\nWeb inventory of transcribed and translated talks. In\\nProceedings of EAMT.\\n\\nS. Clark and S. Pulman. 2007. Combining symbolic\\nand distributional models of meaning. In Proceed-\\nings of AAAI Spring Symposium on Quantum Inter-\\naction. AAAI Press.\\n\\nT. Cohn and M. Lapata. 2007. Machine translation by\\ntriangulation: Making effective use of multi-parallel\\ncorpora. In Proceedings of ACL.\\n\\nM. Collins. 2002. Discriminative training methods\\nfor hidden markov models: Theory and experiments\\nwith perceptron algorithms. In Proceedings of ACL-\\nEMNLP.\\n\\nR. Collobert and J. Weston. 2008. A uni\\ufb01ed architec-\\nture for natural language processing: Deep neural\\nnetworks with multitask learning. In Proceedings of\\nICML.\\n\\nR. Collobert,\\n\\nJ. Weston, L. Bottou, M. Karlen,\\nK. Kavukcuoglu, and P. Kuksa. 2011. Natural lan-\\nguage processing (almost) from scratch. Journal of\\nMachine Learning Research, 12:2493\\u20132537.\\n\\nJ. Duchi, E. Hazan, and Y. Singer. 2011. Adaptive sub-\\ngradient methods for online learning and stochas-\\ntic optimization. Journal of Machine Learning Re-\\nsearch, 12:2121\\u20132159, July.\\n\\nS. T. Dumais, G. W. Furnas, T. K. Landauer, S. Deer-\\nwester, and R. Harshman. 1988. Using latent se-\\nmantic analysis to improve access to textual infor-\\nmation. In Proceedings of the SIGCHI Conference\\non Human Factors in Computing Systems.\\n\\nC. Dyer, A. Lopez,\\n\\nJ. Ganitkevitch,\\n\\nJ. Weese,\\nF. Ture, P. Blunsom, H. Setiawan, V. Eidelman, and\\nP. Resnik. 2010. cdec: A Decoder, Alignment, and\\nLearning framework for \\ufb01nite-state and context-free\\ntranslation models. In Proceedings of ACL.\\n\\nK. Erk and S. Pad\\xb4o. 2008. A structured vector space\\nmodel for word meaning in context. Proceedings of\\nEMNLP.\\n\\nJ. R. Firth. 1957. A synopsis of linguistic theory 1930-\\n\\n55. 1952-59:1\\u201332.\\n\\nE. Grefenstette and M. Sadrzadeh.\\n\\n2011. Experi-\\nmental support for a categorical compositional dis-\\nIn Proceedings of\\ntributional model of meaning.\\nEMNLP.\\n\\nA. Haghighi, P. Liang, T. Berg-Kirkpatrick, and\\nD. Klein. 2008. Learning bilingual lexicons from\\nmonolingual corpora. In Proceedings of ACL-HLT.\\n\\nK. M. Hermann and P. Blunsom. 2013. The Role of\\nSyntax in Vector Space Models of Compositional\\nSemantics. In Proceedings of ACL.\\n\\nK. M. Hermann and P. Blunsom. 2014. Multilingual\\nDistributed Representations without Word Align-\\nment. In Proceedings of ICLR.\\n\\nN. Kalchbrenner and P. Blunsom. 2013. Recurrent\\nconvolutional neural networks for discourse compo-\\nsitionality. Proceedings of the ACL Workshop on\\nContinuous Vector Space Models and their Compo-\\nsitionality.\\n\\nA. Klementiev, I. Titov, and B. Bhattarai. 2012.\\n\\nIn-\\nducing crosslingual distributed representations of\\nwords. In Proceedings of COLING.\\n\\nP. Koehn. 2005. Europarl: A Parallel Corpus for Sta-\\ntistical Machine Translation. In Proceedings of the\\nMachine Translation Summit.\\n\\nS. Lauly, A. Boulanger, and H. Larochelle.\\n\\n2013.\\nLearning multilingual word representations using a\\nbag-of-words autoencoder. In Deep Learning Work-\\nshop at NIPS.\\n\\nD. D. Lewis, Y. Yang, T. G. Rose, and F. Li. 2004.\\nRcv1: A new benchmark collection for text catego-\\nrization research. Journal of Machine Learning Re-\\nsearch, 5:361\\u2013397, December.\\n\\nA. K. McCallum. 2002. Mallet: A machine learning\\n\\nfor language toolkit. http://mallet.cs.umass.edu.\\n\\nT. Mikolov, M. Kara\\ufb01\\xb4at, L. Burget, J. \\u02c7Cernock\\xb4y, and\\n2010. Recurrent neural network\\nIn Proceedings of INTER-\\n\\nS. Khudanpur.\\nbased language model.\\nSPEECH.\\n\\nT. Mikolov, K. Chen, G. Corrado, and J. Dean. 2013a.\\nEf\\ufb01cient Estimation of Word Representations in\\nVector Space. CoRR.\\n\\nT. Mikolov, Q. V. Le, and I. Sutskever. 2013b. Ex-\\nploiting Similarities among Languages for Machine\\nTranslation. CoRR.\\n\\nJ. Mitchell and M. Lapata. 2008. Vector-based models\\nof semantic composition. In In Proceedings of ACL.\\n\\n\\x0cA. Mnih and G. Hinton. 2009. A scalable hierarchi-\\nIn Proceedings of\\n\\ncal distributed language model.\\nNIPS.\\n\\nJ. Ngiam, A. Khosla, M. Kim, J. Nam, H. Lee, and\\nIn\\n\\nA. Y. Ng. 2011. Multimodal deep learning.\\nICML.\\n\\nD. Roy. 2003. Grounded spoken language acquisition:\\nIEEE Transactions\\n\\nExperiments in word learning.\\non Multimedia, 5(2):197\\u2013209, June.\\n\\nA. P. Sarath Chandar, M. K. Mitesh, B. Ravindran,\\nV. Raykar, and A. Saha. 2013. Multilingual deep\\nlearning. In Deep Learning Workshop at NIPS.\\n\\nR. Socher, J. Pennington, E. H. Huang, A. Y. Ng, and\\nC. D. Manning. 2011. Semi-supervised recursive\\nautoencoders for predicting sentiment distributions.\\nIn Proceedings of EMNLP.\\n\\nR. Socher, B. Huval, C. D. Manning, and A. Y. Ng.\\n2012. Semantic compositionality through recursive\\nIn Proceedings of EMNLP-\\nmatrix-vector spaces.\\nCoNLL, pages 1201\\u20131211.\\n\\nN. Srivastava and R. Salakhutdinov. 2012. Multimodal\\nIn Pro-\\n\\nlearning with deep boltzmann machines.\\nceedings of NIPS.\\n\\nJ. Turian, L. Ratinov, and Y. Bengio. 2010. Word rep-\\nresentations: a simple and general method for semi-\\nsupervised learning. In Proceedings of ACL.\\n\\nP. D. Turney. 2012. Domain and function: A dual-\\nspace model of semantic relations and compositions.\\nJournal of Arti\\ufb01cial Intelligence Research, 44:533\\u2013\\n585.\\n\\nW.-T. Yih, K. Toutanova, J. C. Platt, and C. Meek.\\n2011. Learning discriminative projections for text\\nsimilarity measures. In Proceedings of CoNLL.\\n\\nW. Y. Zou, R. Socher, D. Cer, and C. D. Manning.\\n2013. Bilingual word embeddings for phrase-based\\nmachine translation. In Proceedings of EMNLP.\\n\\n\\x0c', u'Deep Learning:\\nMethods and Applications\\n\\nLi Deng\\nMicrosoft Research\\nOne Microsoft Way\\nRedmond, WA 98052; USA\\ndeng@microsoft.com\\nDong Yu\\nMicrosoft Research\\nOne Microsoft Way\\nRedmond, WA 98052; USA\\nDong.Yu@microsoft.com\\n\\nBoston \\u2014 Delft\\n\\nFull text available at: http://dx.doi.org/10.1561/2000000039\\x0cFoundations and Trends R(cid:1) in Signal Processing\\n\\nPublished, sold and distributed by:\\nnow Publishers Inc.\\nPO Box 1024\\nHanover, MA 02339\\nUnited States\\nTel. +1-781-985-4510\\nwww.nowpublishers.com\\nsales@nowpublishers.com\\nOutside North America:\\nnow Publishers Inc.\\nPO Box 179\\n2600 AD Delft\\nThe Netherlands\\nTel. +31-6-51115274\\nThe preferred citation for this publication is\\nL. Deng and D. Yu. Deep Learning: Methods and Applications. Foundations and\\nTrends R(cid:1) in Signal Processing, vol. 7, nos. 3\\u20134, pp. 197\\u2013387, 2013.\\nThis Foundations and Trends R(cid:1) issue was typeset in LATEX using a class \\ufb01le designed\\nby Neal Parikh. Printed on acid-free paper.\\nISBN: 978-1-60198-815-7\\nc(cid:1) 2014 L. Deng and D. Yu\\n\\nAll rights reserved. No part of this publication may be reproduced, stored in a retrieval\\nsystem, or transmitted in any form or by any means, mechanical, photocopying, recording\\nor otherwise, without prior written permission of the publishers.\\nPhotocopying. In the USA: This journal is registered at the Copyright Clearance Cen-\\nter, Inc., 222 Rosewood Drive, Danvers, MA 01923. Authorization to photocopy items for\\ninternal or personal use, or the internal or personal use of speci\\ufb01c clients, is granted by\\nnow Publishers Inc for users registered with the Copyright Clearance Center (CCC). The\\n\\u2018services\\u2019 for users can be found on the internet at: www.copyright.com\\nFor those organizations that have been granted a photocopy license, a separate system\\nof payment has been arranged. Authorization does not extend to other kinds of copy-\\ning, such as that for general distribution, for advertising or promotional purposes, for\\ncreating new collective works, or for resale. In the rest of the world: Permission to pho-\\ntocopy must be obtained from the copyright owner. Please apply to now Publishers Inc.,\\nPO Box 1024, Hanover, MA 02339, USA; Tel. +1 781 871 0245; www.nowpublishers.com;\\nsales@nowpublishers.com\\nnow Publishers Inc. has an exclusive license to publish this material worldwide. Permission\\nto use this content must be obtained from the copyright license holder. Please apply to\\nnow Publishers, PO Box 179, 2600 AD Delft, The Netherlands, www.nowpublishers.com;\\ne-mail: sales@nowpublishers.com\\n\\nFull text available at: http://dx.doi.org/10.1561/2000000039\\x0cFoundations and Trends R(cid:1) in Signal Processing\\n\\nVolume 7, Issues 3\\u20134, 2013\\n\\nEditorial Board\\n\\nEditor-in-Chief\\n\\nYonina Eldar\\nTechnion - Israel Institute of Technology\\nIsrael\\nEditors\\n\\nRobert M. Gray\\nFounding Editor-in-Chief\\nStanford University\\nPao-Chi Chang\\nNCU, Taiwan\\nPamela Cosman\\nUC San Diego\\nMichelle E\\ufb00ros\\nCaltech\\nYariv Ephraim\\nGMU\\nAlfonso Farina\\nSelex ES\\nSadaoki Furui\\nTokyo Tech\\nGeorgios Giannakis\\nUniversity of Minnesota\\nVivek Goyal\\nBoston University\\nSinan Gunturk\\nCourant Institute\\nChristine Guillemot\\nINRIA\\nRobert W. Heath, Jr.\\nUT Austin\\n\\nSheila Hemami\\nCornell University\\nLina Karam\\nArizona State U\\nNick Kingsbury\\nUniversity of Cambridge\\nAlex Kot\\nNTU, Singapore\\nJelena Kovacevic\\nCMU\\nGeert Leus\\nTU Delft\\nJia Li\\nPenn State\\nHenrique Malvar\\nMicrosoft Research\\nB.S. Manjunath\\nUC Santa Barbara\\nUrbashi Mitra\\nUSC\\nBj\\xf6rn Ottersten\\nKTH Stockholm\\nThrasos Pappas\\nNorthwestern University\\n\\nVincent Poor\\nPrinceton University\\nAnna Scaglione\\nUC Davis\\nMihaela van der Shaar\\nUCLA\\nNicholas D. Sidiropoulos\\nTU Crete\\nMichael Unser\\nEPFL\\nP. P. Vaidyanathan\\nCaltech\\nAmi Wiesel\\nHebrew U\\nMin Wu\\nUniversity of Maryland\\nJosiane Zerubia\\nINRIA\\n\\nFull text available at: http://dx.doi.org/10.1561/2000000039\\x0cEditorial Scope\\n\\nTopics\\nFoundations and Trends R(cid:1) in Signal Processing publishes survey and\\ntutorial articles in the following topics:\\n\\nprocessing\\n\\nprocessing\\n\\n\\u2022 Adaptive signal processing\\n\\u2022 Audio signal processing\\n\\u2022 Biological and biomedical signal\\n\\u2022 Complexity in signal processing\\n\\u2022 Digital signal processing\\n\\u2022 Distributed and network signal\\n\\u2022 Image and video processing\\n\\u2022 Linear and nonlinear \\ufb01ltering\\n\\u2022 Multidimensional signal\\n\\u2022 Multimodal signal processing\\n\\u2022 Multirate signal processing\\n\\u2022 Multiresolution signal processing\\n\\u2022 Nonlinear signal processing\\n\\u2022 Randomized algorithms in signal\\n\\u2022 Sensor and multiple source signal\\n\\nprocessing\\n\\nprocessing\\n\\nprocessing, source separation\\n\\n\\u2022 Signal decompositions, subband\\nand transform methods, sparse\\nrepresentations\\n\\n\\u2022 Signal processing for\\n\\ncommunications\\n\\n\\u2022 Signal processing for security and\\nforensic analysis, biometric signal\\nprocessing\\n\\n\\u2022 Signal quantization, sampling,\\nanalog-to-digital conversion,\\ncoding and compression\\n\\u2022 Signal reconstruction,\\n\\ndigital-to-analog conversion,\\nenhancement, decoding and\\ninverse problems\\n\\n\\u2022 Speech/audio/image/video\\n\\n\\u2022 Speech and spoken language\\n\\ncompression\\n\\nprocessing\\n\\n\\u2022 Statistical/machine learning\\n\\u2022 Statistical signal processing\\n\\nInformation for Librarians\\nFoundations and Trends R(cid:1) in Signal Processing, 2013, Volume 7, 4 issues. ISSN\\npaper version 1932-8346. ISSN online version 1932-8354. Also available as a\\ncombined paper and online subscription.\\n\\nFull text available at: http://dx.doi.org/10.1561/2000000039\\x0cFoundations and Trends R(cid:1) in Signal Processing\\nVol. 7, Nos. 3\\u20134 (2013) 197\\u2013387\\nc(cid:1) 2014 L. Deng and D. Yu\\nDOI: 10.1561/2000000039\\n\\nDeep Learning: Methods and Applications\\n\\nLi Deng\\n\\nMicrosoft Research\\nOne Microsoft Way\\n\\nRedmond, WA 98052; USA\\n\\ndeng@microsoft.com\\n\\nDong Yu\\n\\nMicrosoft Research\\nOne Microsoft Way\\n\\nRedmond, WA 98052; USA\\nDong.Yu@microsoft.com\\n\\nFull text available at: http://dx.doi.org/10.1561/2000000039\\x0cContents\\n\\nEndorsement\\n\\n1 Introduction\\n\\n1.1 De\\ufb01nitions and background . . . . . . . . . . . . . . . . .\\n. . . . . . . . . . . . . .\\n1.2 Organization of this monograph\\n\\n2 Some Historical Context of Deep Learning\\n\\n3 Three Classes of Deep Learning Networks\\n\\n3.1 A three-way categorization . . . . . . . . . . . . . . . . .\\n3.2 Deep networks for unsupervised or generative learning . . .\\n3.3 Deep networks for supervised learning . . . . . . . . . . .\\n3.4 Hybrid deep networks . . . . . . . . . . . . . . . . . . . .\\n\\n4 Deep Autoencoders \\u2014 Unsupervised Learning\\n\\nIntroduction . . . . . . . . . . . . . . . . . . . . . . . . .\\n4.1\\n4.2 Use of deep autoencoders to extract speech features\\n. . .\\n4.3 Stacked denoising autoencoders . . . . . . . . . . . . . . .\\n4.4 Transforming autoencoders . . . . . . . . . . . . . . . . .\\n\\n5 Pre-Trained Deep Neural Networks \\u2014 A Hybrid\\n\\n5.1 Restricted Boltzmann machines . . . . . . . . . . . . . . .\\n\\n1\\n\\n3\\n3\\n7\\n\\n10\\n\\n19\\n19\\n21\\n28\\n31\\n\\n35\\n35\\n36\\n40\\n44\\n\\n46\\n46\\n\\nii\\n\\nFull text available at: http://dx.doi.org/10.1561/2000000039\\x0c5.2 Unsupervised layer-wise pre-training . . . . . . . . . . . .\\n. . . . . . . . . . . . . . .\\n5.3\\n\\nInterfacing DNNs with HMMs\\n\\n6 Deep Stacking Networks and Variants \\u2014\\n\\nSupervised Learning\\n6.1\\nIntroduction . . . . . . . . . . . . . . . . . . . . . . . . .\\n6.2 A basic architecture of the deep stacking network . . . . .\\n6.3 A method for learning the DSN weights\\n. . . . . . . . . .\\n6.4 The tensor deep stacking network . . . . . . . . . . . . . .\\n6.5 The Kernelized deep stacking network . . . . . . . . . . .\\n\\n7 Selected Applications in Speech and Audio Processing\\n\\n7.1 Acoustic modeling for speech recognition . . . . . . . . . .\\n7.2 Speech synthesis . . . . . . . . . . . . . . . . . . . . . . .\\n7.3 Audio and music processing . . . . . . . . . . . . . . . . .\\n\\niii\\n\\n50\\n53\\n\\n55\\n55\\n57\\n59\\n60\\n62\\n\\n67\\n67\\n91\\n93\\n\\n8 Selected Applications in Language\\n\\n97\\nModeling and Natural Language Processing\\n8.1 Language modeling . . . . . . . . . . . . . . . . . . . . .\\n98\\n8.2 Natural language processing . . . . . . . . . . . . . . . . . 104\\n\\n9 Selected Applications in Information Retrieval\\n113\\n9.1 A brief introduction to information retrieval\\n. . . . . . . . 113\\n9.2 SHDA for document indexing and retrieval . . . . . . . . . 115\\n9.3 DSSM for document retrieval . . . . . . . . . . . . . . . . 116\\n9.4 Use of deep stacking networks for information retrieval\\n. . 122\\n\\n10 Selected Applications in Object Recognition\\n\\nand Computer Vision\\n125\\n10.1 Unsupervised or generative feature learning . . . . . . . . 126\\n10.2 Supervised feature learning and classi\\ufb01cation . . . . . . . . 129\\n\\n11 Selected Applications in Multimodal\\n\\nand Multi-task Learning\\n136\\n11.1 Multi-modalities: Text and image . . . . . . . . . . . . . . 137\\n. . . . . . . . . . . . 141\\n11.2 Multi-modalities: Speech and image\\n11.3 Multi-task learning within the speech, NLP or image\\n. . . 144\\n\\nFull text available at: http://dx.doi.org/10.1561/2000000039\\x0civ\\n\\n12 Conclusion\\n\\nReferences\\n\\n148\\n\\n154\\n\\nFull text available at: http://dx.doi.org/10.1561/2000000039\\x0cEndorsement\\n\\n\\u201cIn the past few years, deep learning has rapidly evolved into the\\nde-facto approach for acoustic modeling in automatic speech recogni-\\ntion (ASR), showing tremendous improvement in accuracy, robustness,\\nand cross-language generalizability over conventional approaches. This\\ntimely book is written by the pioneers of deep learning innovations\\nand applications to ASR, who, as early as 2010, \\ufb01rst succeeded in large\\nvocabulary speech recognition using deep learning. This was accom-\\nplished using a special form of the deep neural net, developed by the\\nauthors, perfectly \\ufb01t for fast decoding as required by industrial deploy-\\nment of ASR technology. In addition to recounting this remarkable\\nadvance which ignited the industry-scale adoption of deep learning in\\nASR, this book also provides an overview of a sweeping range of up-\\nto-date deep learning methodologies and its application to a variety of\\nsignal and information processing tasks, including not only ASR but\\nalso computer vision, language modeling, text processing, multimodal\\nlearning, and information retrieval. This is the \\ufb01rst and the most valu-\\nable book for \\u201cdeep and wide learning\\u201d of deep learning, not to be\\nmissed by anyone who wants to know the breath taking impact of deep\\nlearning in many facets of information processing, especially ASR, all\\nof vital importance to our modern technological society.\\u201d\\n\\n\\u2014 Sadaoki Furui, President of Toyota Technological Institute at\\nChicago, and Professor at the Tokyo Institute of Technology\\n\\nFull text available at: http://dx.doi.org/10.1561/2000000039\\x0cAbstract\\n\\nThis monograph provides an overview of general deep learning method-\\nology and its applications to a variety of signal and information pro-\\ncessing tasks. The application areas are chosen with the following three\\ncriteria in mind: (1) expertise or knowledge of the authors; (2) the\\napplication areas that have already been transformed by the successful\\nuse of deep learning technology, such as speech recognition and com-\\nputer vision; and (3) the application areas that have the potential to be\\nimpacted signi\\ufb01cantly by deep learning and that have been experienc-\\ning research growth, including natural language and text processing,\\ninformation retrieval, and multimodal information processing empow-\\nered by multi-task deep learning.\\n\\nL. Deng and D. Yu. Deep Learning: Methods and Applications. Foundations and\\nTrends R(cid:1) in Signal Processing, vol. 7, nos. 3\\u20134, pp. 197\\u2013387, 2013.\\nDOI: 10.1561/2000000039.\\n\\nFull text available at: http://dx.doi.org/10.1561/2000000039\\x0c1\\n\\nIntroduction\\n\\n1.1 De\\ufb01nitions and background\\n\\nSince 2006, deep structured learning, or more commonly called deep\\nlearning or hierarchical learning, has emerged as a new area of machine\\nlearning research [20, 163]. During the past several years, the techniques\\ndeveloped from deep learning research have already been impacting\\na wide range of signal and information processing work within the\\ntraditional and the new, widened scopes including key aspects of\\nmachine learning and arti\\ufb01cial intelligence; see overview articles in\\n[7, 20, 24, 77, 94, 161, 412], and also the media coverage of this progress\\nin [6, 237]. A series of workshops, tutorials, and special issues or con-\\nference special sessions in recent years have been devoted exclusively\\nto deep learning and its applications to various signal and information\\nprocessing areas. These include:\\n\\n\\u2022 2008 NIPS Deep Learning Workshop;\\n\\u2022 2009 NIPS Workshop on Deep Learning for Speech Recognition\\n\\nand Related Applications;\\n\\n\\u2022 2009 ICML Workshop on Learning Feature Hierarchies;\\n\\n3\\n\\nFull text available at: http://dx.doi.org/10.1561/2000000039\\x0c4\\n\\nIntroduction\\n\\n\\u2022 2011 ICML Workshop on Learning Architectures, Representa-\\ntions, and Optimization for Speech and Visual Information Pro-\\ncessing;\\n\\n\\u2022 2012 ICASSP Tutorial on Deep Learning for Signal and Informa-\\n\\ntion Processing;\\n\\n\\u2022 2012 ICML Workshop on Representation Learning;\\n\\u2022 2012 Special Section on Deep Learning for Speech and Language\\nProcessing in IEEE Transactions on Audio, Speech, and Lan-\\nguage Processing (T-ASLP, January);\\n\\n\\u2022 2010, 2011, and 2012 NIPS Workshops on Deep Learning and\\n\\nUnsupervised Feature Learning;\\n\\n\\u2022 2013 NIPS Workshops on Deep Learning and on Output Repre-\\n\\nsentation Learning;\\n\\n\\u2022 2013 Special Issue on Learning Deep Architectures in IEEE\\nTransactions on Pattern Analysis and Machine Intelligence\\n(T-PAMI, September).\\n\\n\\u2022 2013 International Conference on Learning Representations;\\n\\u2022 2013 ICML Workshop on Representation Learning Challenges;\\n\\u2022 2013 ICML Workshop on Deep Learning for Audio, Speech, and\\n\\nLanguage Processing;\\n\\n\\u2022 2013 ICASSP Special Session on New Types of Deep Neural Net-\\nwork Learning for Speech Recognition and Related Applications.\\n\\nThe authors have been actively involved in deep learning research and\\nin organizing or providing several of the above events, tutorials, and\\neditorials. In particular, they gave tutorials and invited lectures on\\nthis topic at various places. Part of this monograph is based on their\\ntutorials and lecture material.\\n\\nBefore embarking on describing details of deep learning, let\\u2019s pro-\\nvide necessary de\\ufb01nitions. Deep learning has various closely related\\nde\\ufb01nitions or high-level descriptions:\\n\\n\\u2022 De\\ufb01nition 1: A class of machine learning techniques that\\nexploit many layers of non-linear information processing for\\n\\nFull text available at: http://dx.doi.org/10.1561/2000000039\\x0c1.1. De\\ufb01nitions and background\\n\\n5\\n\\nsupervised or unsupervised feature extraction and transforma-\\ntion, and for pattern analysis and classi\\ufb01cation.\\n\\u2022 De\\ufb01nition 2: \\u201cA sub-\\ufb01eld within machine learning that is based\\non algorithms for learning multiple levels of representation in\\norder to model complex relationships among data. Higher-level\\nfeatures and concepts are thus de\\ufb01ned in terms of lower-level\\nones, and such a hierarchy of features is called a deep architec-\\nture. Most of these models are based on unsupervised learning of\\nrepresentations.\\u201d (Wikipedia on \\u201cDeep Learning\\u201d around March\\n2012.)\\n\\u2022 De\\ufb01nition 3: \\u201cA sub-\\ufb01eld of machine learning that is based\\non learning several levels of representations, corresponding to a\\nhierarchy of features or factors or concepts, where higher-level\\nconcepts are de\\ufb01ned from lower-level ones, and the same lower-\\nlevel concepts can help to de\\ufb01ne many higher-level concepts. Deep\\nlearning is part of a broader family of machine learning methods\\nbased on learning representations. An observation (e.g., an image)\\ncan be represented in many ways (e.g., a vector of pixels), but\\nsome representations make it easier to learn tasks of interest (e.g.,\\nis this the image of a human face?) from examples, and research\\nin this area attempts to de\\ufb01ne what makes better representations\\nand how to learn them.\\u201d (Wikipedia on \\u201cDeep Learning\\u201d around\\nFebruary 2013.)\\n\\u2022 De\\ufb01nition 4: \\u201cDeep learning is a set of algorithms in machine\\nlearning that attempt to learn in multiple levels, correspond-\\ning to di\\ufb00erent levels of abstraction. It typically uses arti\\ufb01cial\\nneural networks. The levels in these learned statistical models\\ncorrespond to distinct levels of concepts, where higher-level con-\\ncepts are de\\ufb01ned from lower-level ones, and the same lower-\\nlevel concepts can help to de\\ufb01ne many higher-level concepts.\\u201d\\nSee Wikipedia http://en.wikipedia.org/wiki/Deep_learning on\\n\\u201cDeep Learning\\u201d as of this most recent update in October 2013.\\n\\u2022 De\\ufb01nition 5: \\u201cDeep Learning is a new area of Machine Learning\\nresearch, which has been introduced with the objective of moving\\nMachine Learning closer to one of its original goals: Arti\\ufb01cial\\n\\nFull text available at: http://dx.doi.org/10.1561/2000000039\\x0c6\\n\\nIntroduction\\n\\nIntelligence. Deep Learning is about learning multiple levels of\\nrepresentation and abstraction that help to make sense of data\\nsuch as images, sound, and text.\\u201d See https://github.com/lisa-\\nlab/DeepLearningTutorials\\n\\nNote that the deep learning that we discuss in this monograph is\\nabout learning with deep architectures for signal and information pro-\\ncessing. It is not about deep understanding of the signal or infor-\\nmation, although in many cases they may be related. It should also\\nbe distinguished from the overloaded term in educational psychology:\\n\\u201cDeep learning describes an approach to learning that is character-\\nized by active engagement, intrinsic motivation, and a personal search\\nfor meaning.\\u201d http://www.blackwellreference.com/public/tocnode?id=\\ng9781405161251_chunk_g97814051612516_ss1-1\\n\\nCommon among the various high-level descriptions of deep learning\\nabove are two key aspects: (1) models consisting of multiple layers\\nor stages of nonlinear information processing; and (2) methods for\\nsupervised or unsupervised learning of\\nfeature representation at\\nsuccessively higher, more abstract layers. Deep learning is in the\\nintersections among the research areas of neural networks, arti\\ufb01cial\\nintelligence, graphical modeling, optimization, pattern recognition,\\nand signal processing. Three important reasons for the popularity\\nof deep learning today are the drastically increased chip processing\\nabilities (e.g., general-purpose graphical processing units or GPGPUs),\\nthe signi\\ufb01cantly increased size of data used for training, and the recent\\nadvances\\nin machine learning and signal/information processing\\nresearch. These advances have enabled the deep learning methods\\nto e\\ufb00ectively exploit complex, compositional nonlinear functions, to\\nlearn distributed and hierarchical feature representations, and to make\\ne\\ufb00ective use of both labeled and unlabeled data.\\n\\nActive researchers in this area include those at University of\\nToronto, New York University, University of Montreal, Stanford\\nUniversity, Microsoft Research (since 2009), Google (since about\\n2011), IBM Research (since about 2011), Baidu (since 2012), Facebook\\n(since 2013), UC-Berkeley, UC-Irvine,\\nIDSIA, University\\nCollege London, University of Michigan, Massachusetts Institute of\\n\\nIDIAP,\\n\\nFull text available at: http://dx.doi.org/10.1561/2000000039\\x0c1.2. Organization of this monograph\\n\\n7\\n\\nTechnology, University of Washington, and numerous other places; see\\nhttp://deeplearning.net/deep-learning-research-groups-and-labs/\\nfor\\na more detailed list. These researchers have demonstrated empirical\\nsuccesses of deep learning in diverse applications of computer vision,\\nphonetic recognition, voice search, conversational speech recognition,\\nspeech and image feature coding, semantic utterance classi\\ufb01ca-\\ntion, natural language understanding, hand-writing recognition, audio\\nprocessing, information retrieval, robotics, and even in the analysis of\\nmolecules that may lead to discovery of new drugs as reported recently\\nby [237].\\n\\nIn addition to the reference list provided at the end of this mono-\\ngraph, which may be outdated not long after the publication of this\\nmonograph, there are a number of excellent and frequently updated\\nreading lists, tutorials, software, and video lectures online at:\\n\\n\\u2022 http://deeplearning.net/reading-list/\\n\\u2022 http://u\\ufb02dl.stanford.edu/wiki/index.php/\\n\\nUFLDL_Recommended_Readings\\n\\n\\u2022 http://www.cs.toronto.edu/\\u223chinton/\\n\\u2022 http://deeplearning.net/tutorial/\\n\\u2022 http://u\\ufb02dl.stanford.edu/wiki/index.php/UFLDL_Tutorial\\n\\n1.2 Organization of this monograph\\n\\nThe rest of the monograph is organized as follows:\\n\\nIn Section 2, we provide a brief historical account of deep learning,\\nmainly from the perspective of how speech recognition technology has\\nbeen hugely impacted by deep learning, and how the revolution got\\nstarted and has gained and sustained immense momentum.\\n\\nIn Section 3, a three-way categorization scheme for a majority of\\nthe work in deep learning is developed. They include unsupervised,\\nsupervised, and hybrid deep learning networks, where in the latter cat-\\negory unsupervised learning (or pre-training) is exploited to assist the\\nsubsequent stage of supervised learning when the \\ufb01nal tasks pertain to\\nclassi\\ufb01cation. The supervised and hybrid deep networks often have the\\n\\nFull text available at: http://dx.doi.org/10.1561/2000000039\\x0c8\\n\\nIntroduction\\n\\nsame type of architectures or the structures in the deep networks, but\\nthe unsupervised deep networks tend to have di\\ufb00erent architectures\\nfrom the others.\\n\\nSections 4\\u20136 are devoted, respectively, to three popular types of\\ndeep architectures, one from each of the classes in the three-way cat-\\negorization scheme reviewed in Section 3. In Section 4, we discuss\\nin detail deep autoencoders as a prominent example of the unsuper-\\nvised deep learning networks. No class labels are used in the learning,\\nalthough supervised learning methods such as back-propagation are\\ncleverly exploited when the input signal itself, instead of any label\\ninformation of interest to possible classi\\ufb01cation tasks, is treated as the\\n\\u201csupervision\\u201d signal.\\n\\nIn Section 5, as a major example in the hybrid deep network cate-\\ngory, we present in detail the deep neural networks with unsupervised\\nand largely generative pre-training to boost the e\\ufb00ectiveness of super-\\nvised training. This bene\\ufb01t is found critical when the training data\\nare limited and no other appropriate regularization approaches (i.e.,\\ndropout) are exploited. The particular pre-training method based on\\nrestricted Boltzmann machines and the related deep belief networks\\ndescribed in this section has been historically signi\\ufb01cant as it ignited\\nthe intense interest in the early applications of deep learning to speech\\nrecognition and other information processing tasks. In addition to this\\nretrospective review, subsequent development and di\\ufb00erent paths from\\nthe more recent perspective are discussed.\\n\\nIn Section 6, the basic deep stacking networks and their several\\nextensions are discussed in detail, which exemplify the discrimina-\\ntive, supervised deep learning networks in the three-way classi\\ufb01cation\\nscheme. This group of deep networks operate in many ways that are\\ndistinct from the deep neural networks. Most notably, they use target\\nlabels in constructing each of many layers or modules in the overall\\ndeep networks. Assumptions made about part of the networks, such as\\nlinear output units in each of the modules, simplify the learning algo-\\nrithms and enable a much wider variety of network architectures to\\nbe constructed and learned than the networks discussed in Sections 4\\nand 5.\\n\\nFull text available at: http://dx.doi.org/10.1561/2000000039\\x0c1.2. Organization of this monograph\\n\\n9\\n\\nIn Sections 7\\u201311, we select a set of typical and successful applica-\\ntions of deep learning in diverse areas of signal and information process-\\ning. In Section 7, we review the applications of deep learning to speech\\nrecognition, speech synthesis, and audio processing. Subsections sur-\\nrounding the main subject of speech recognition are created based on\\nseveral prominent themes on the topic in the literature.\\n\\nIn Section 8, we present recent results of applying deep learning to\\nlanguage modeling and natural language processing, where we highlight\\nthe key recent development in embedding symbolic entities such as\\nwords into low-dimensional, continuous-valued vectors.\\n\\nSection 9 is devoted to selected applications of deep learning to\\n\\ninformation retrieval including web search.\\n\\nIn Section 10, we cover selected applications of deep learning to\\nimage object recognition in computer vision. The section is divided to\\ntwo main classes of deep learning approaches: (1) unsupervised feature\\nlearning, and (2) supervised learning for end-to-end and joint feature\\nlearning and classi\\ufb01cation.\\n\\nSelected applications to multi-modal processing and multi-task\\nlearning are reviewed in Section 11, divided into three categories\\naccording to the nature of the multi-modal data as inputs to the deep\\nlearning systems. For single-modality data of speech, text, or image,\\na number of recent multi-task learning studies based on deep learning\\nmethods are reviewed in the literature.\\n\\nFinally, conclusions are given in Section 12 to summarize the mono-\\n\\ngraph and to discuss future challenges and directions.\\n\\nThis short monograph contains the material expanded from two\\ntutorials that the authors gave, one at APSIPA in October 2011 and\\nthe other at ICASSP in March 2012. Substantial updates have been\\nmade based on the literature up to January 2014 (including the mate-\\nrials presented at NIPS-2013 and at IEEE-ASRU-2013 both held in\\nDecember of 2013), focusing on practical aspects in the fast develop-\\nment of deep learning research and technology during the interim years.\\n\\nFull text available at: http://dx.doi.org/10.1561/2000000039\\x0cReferences\\n\\n[1] O. Abdel-Hamid, L. Deng, and D. Yu. Exploring convolutional neural\\nnetwork structures and optimization for speech recognition. Proceedings\\nof Interspeech, 2013.\\n\\n[2] O. Abdel-Hamid, L. Deng, D. Yu, and H. Jiang. Deep segmental neural\\n\\nnetworks for speech recognition. In Proceedings of Interspeech. 2013.\\n\\n[3] O. Abdel-Hamid, A. Mohamed, H. Jiang, and G. Penn. Applying convo-\\nlutional neural networks concepts to hybrid NN-HMM model for speech\\nrecognition.\\nIn Proceedings of International Conference on Acoustics\\nSpeech and Signal Processing (ICASSP). 2012.\\n\\n[4] A. Acero, L. Deng, T. Kristjansson, and J. Zhang. HMM adaptation\\nusing vector taylor series for noisy speech recognition. In Proceedings\\nof Interspeech. 2000.\\n\\n[5] G. Alain and Y. Bengio. What regularized autoencoders learn from the\\ndata generating distribution. In Proceedings of International Conference\\non Learning Representations (ICLR). 2013.\\n\\n[6] G. Anthes. Deep learning comes of age. Communications of the Asso-\\n\\nciation for Computing Machinery (ACM), 56(6):13\\u201315, June 2013.\\n\\n[7] I. Arel, C. Rose, and T. Karnowski. Deep machine learning \\u2014 a new\\nfrontier in arti\\ufb01cial intelligence. IEEE Computational Intelligence Mag-\\nazine, 5:13\\u201318, November 2010.\\n\\n154\\n\\nFull text available at: http://dx.doi.org/10.1561/2000000039\\x0cReferences\\n\\n155\\n\\n[8] E. Arisoy, T. Sainath, B. Kingsbury, and B. Ramabhadran. Deep neural\\nnetwork language models. In Proceedings of the Joint Human Language\\nTechnology Conference and the North American Chapter of the Associ-\\nation of Computational Linguistics (HLT-NAACL) Workshop. 2012.\\n\\n[9] O. Aslan, H. Cheng, D. Schuurmans, and X. Zhang. Convex two-layer\\nIn Proceedings of Neural Information Processing Systems\\n\\nmodeling.\\n(NIPS). 2013.\\n\\n[10] J. Ba and B. Frey. Adaptive dropout for training deep neural networks.\\nIn Proceedings of Neural Information Processing Systems (NIPS). 2013.\\n[11] J. Baker, L. Deng, J. Glass, S. Khudanpur, C.-H. Lee, N. Morgan, and\\nD. O\\u2019Shaughnessy. Research developments and directions in speech\\nrecognition and understanding.\\nIEEE Signal Processing Magazine,\\n26(3):75\\u201380, May 2009.\\n\\n[12] J. Baker, L. Deng, J. Glass, S. Khudanpur, C.-H. Lee, N. Morgan, and\\nD. O\\u2019Shaughnessy. Updated MINS report on speech recognition and\\nunderstanding. IEEE Signal Processing Magazine, 26(4), July 2009.\\n\\n[13] P. Baldi and P. Sadowski. Understanding dropout. In Proceedings of\\n\\nNeural Information Processing Systems (NIPS). 2013.\\n\\n[14] E. Battenberg, E. Schmidt, and J. Bello.\\n\\nDeep learning for\\nmusic,\\nInternational Conference on Acoustics\\nSpeech and Signal Processing (ICASSP) (http://www.icassp2014.org/\\nspecial_sections.html#ss8), 2014.\\n\\nsession at\\n\\nspecial\\n\\n[15] E. Batternberg and D. Wessel. Analyzing drum patterns using condi-\\ntional deep belief networks. In Proceedings of International Symposium\\non Music Information Retrieval (ISMIR). 2012.\\n\\n[16] P. Bell, P. Swietojanski, and S. Renals. Multi-level adaptive networks\\nin tandem and hybrid ASR systems.\\nIn Proceedings of International\\nConference on Acoustics Speech and Signal Processing (ICASSP). 2013.\\n[17] Y. Bengio. Arti\\ufb01cial neural networks and their application to sequence\\nrecognition. Ph.D. Thesis, McGill University, Montreal, Canada, 1991.\\n[18] Y. Bengio. New distributed probabilistic language models. Technical\\n\\nReport, University of Montreal, 2002.\\n\\n[19] Y. Bengio. Neural net language models. Scholarpedia, 3, 2008.\\n[20] Y. Bengio. Learning deep architectures for AI.\\n\\nin Foundations and\\n\\nTrends in Machine Learning, 2(1):1\\u2013127, 2009.\\n\\nFull text available at: http://dx.doi.org/10.1561/2000000039\\x0c156\\n\\nReferences\\n\\n[21] Y. Bengio. Deep learning of representations for unsupervised and trans-\\nfer learning. Journal of Machine Learning Research Workshop and Con-\\nference Proceedings, 27:17\\u201337, 2012.\\n\\n[22] Y. Bengio. Deep learning of representations: Looking forward. In Sta-\\n\\ntistical Language and Speech Processing, pages 1\\u201337. Springer, 2013.\\n\\n[23] Y. Bengio, N. Boulanger, and R. Pascanu. Advances in optimizing recur-\\nrent networks. In Proceedings of International Conference on Acoustics\\nSpeech and Signal Processing (ICASSP). 2013.\\n\\n[24] Y. Bengio, A. Courville, and P. Vincent. Representation learning: A\\nreview and new perspectives. IEEE Transactions on Pattern Analysis\\nand Machine Intelligence (PAMI), 38:1798\\u20131828, 2013.\\n\\n[25] Y. Bengio, R. De Mori, G. Flammia, and R. Kompe. Global optimiza-\\ntion of a neural network-hidden markov model hybrid. IEEE Transac-\\ntions on Neural Networks, 3:252\\u2013259, 1992.\\n\\n[26] Y. Bengio, R. Ducharme, P. Vincent, and C. Jauvin. A neural proba-\\nbilistic language model. In Proceedings of Neural Information Processing\\nSystems (NIPS). 2000.\\n\\n[27] Y. Bengio, R. Ducharme, P. Vincent, and C. Jauvin. A neural proba-\\nbilistic language model. Journal of Machine Learning Research, 3:1137\\u2013\\n1155, 2003.\\n\\n[28] Y. Bengio, P. Lamblin, D. Popovici, and H. Larochelle. Greedy layer-\\nwise training of deep networks. In Proceedings of Neural Information\\nProcessing Systems (NIPS). 2006.\\n\\n[29] Y. Bengio, P. Simard, and P. Frasconi. Learning long-term dependen-\\nIEEE Transactions on Neural\\n\\ncies with gradient descent is di\\ufb03cult.\\nNetworks, 5:157\\u2013166, 1994.\\n\\n[30] Y. Bengio, E. Thibodeau-Laufer, and J. Yosinski. Deep generative\\nstochastic networks trainable by backprop.\\narXiv 1306:1091, 2013.\\nalso accepted to appear in Proceedings of International Conference on\\nMachine Learning (ICML), 2014.\\n\\n[31] Y. Bengio, L. Yao, G. Alain, and P. Vincent. Generalized denoising\\nautoencoders as generative models. In Proceedings of Neural Informa-\\ntion Processing Systems (NIPS). 2013.\\n\\n[32] J. Bergstra and Y. Bengio. Random search for hyper-parameter opti-\\n\\nmization. Journal on Machine Learning Research, 3:281\\u2013305, 2012.\\n\\nFull text available at: http://dx.doi.org/10.1561/2000000039\\x0cReferences\\n\\n157\\n\\n[33] A. Biem, S. Katagiri, E. McDermott, and B. Juang. An application\\nof discriminative feature extraction to \\ufb01lter-bank-based speech recog-\\nnition. IEEE Transactions on Speech and Audio Processing, 9:96\\u2013110,\\n2001.\\n\\n[34] J. Bilmes. Dynamic graphical models. IEEE Signal Processing Maga-\\n\\nzine, 33:29\\u201342, 2010.\\n\\n[35] J. Bilmes and C. Bartels. Graphical model architectures for speech\\n\\nrecognition. IEEE Signal Processing Magazine, 22:89\\u2013100, 2005.\\n\\n[36] A. Bordes, X. Glorot, J. Weston, and Y. Bengio. A semantic matching\\nenergy function for learning with multi-relational data \\u2014 application\\nto word-sense disambiguation. Machine Learning, May 2013.\\n\\n[37] A. Bordes, J. Weston, R. Collobert, and Y. Bengio. Learning structured\\nembeddings of knowledge bases. In Proceedings of Association for the\\nAdvancement of Arti\\ufb01cial Intelligence (AAAI). 2011.\\n\\n[38] L. Bottou. From machine learning to machine reasoning: An essay.\\n\\nJournal of Machine Learning Research, 14:3207\\u20133260, 2013.\\n\\n[39] L. Bottou and Y. LeCun. Large scale online learning. In Proceedings of\\n\\nNeural Information Processing Systems (NIPS). 2004.\\n\\n[40] N. Boulanger-Lewandowski, Y. Bengio, and P. Vincent. Modeling\\nTemporal dependencies in high-dimensional sequences: Application to\\npolyphonic music generation and transcription. In Proceedings of Inter-\\nnational Conference on Machine Learning (ICML). 2012.\\n\\n[41] N. Boulanger-Lewandowski, Y. Bengio, and P. Vincent. Audio chord\\nrecognition with recurrent neural networks. In Proceedings of Interna-\\ntional Symposium on Music Information Retrieval (ISMIR). 2013.\\n\\n[42] H. Bourlard and N. Morgan. Connectionist Speech Recognition: A\\n\\nHybrid Approach. Kluwer, Norwell, MA, 1993.\\n\\n[43] J. Bouvrie. Hierarchical learning: Theory with applications in speech\\n\\nand vision. Ph.D. thesis, MIT, 2009.\\n\\n[44] L. Breiman. Stacked regression. Machine Learning, 24:49\\u201364, 1996.\\n[45] J. Bridle, L. Deng, J. Picone, H. Richards, J. Ma, T. Kamm, M. Schus-\\nter, S. Pike, and R. Reagan. An investigation of segmental hidden\\ndynamic models of speech coarticulation for automatic speech recogni-\\ntion. Final Report for 1998 Workshop on Language Engineering, CLSP,\\nJohns Hopkins, 1998.\\n\\n[46] P. Cardinal, P. Dumouchel, and G. Boulianne. Large vocabulary speech\\nIEEE Transactions on Audio,\\n\\nrecognition on parallel architectures.\\nSpeech, and Language Processing, 21(11):2290\\u20132300, November 2013.\\n\\nFull text available at: http://dx.doi.org/10.1561/2000000039\\x0c158\\n\\nReferences\\n\\n[47] R. Caruana. Multitask learning. Machine Learning, 28:41\\u201375, 1997.\\n[48] J. Chen and L. Deng. A primal-dual method for training recurrent\\nneural networks constrained by the echo-state property. In Proceedings\\nof International Conference on Learning Representations. April 2014.\\n[49] X. Chen, A. Eversole, G. Li, D. Yu, and F. Seide. Pipelined back-\\npropagation for context-dependent deep neural networks. In Proceedings\\nof Interspeech. 2012.\\n\\n[50] R. Chengalvarayan and L. Deng. Hmm-based speech recognition using\\nstate-dependent, discriminatively derived transforms on Mel-warped\\nDFT features.\\nIEEE Transactions on Speech and Audio Processing,\\npages 243\\u2013256, 1997.\\n\\n[51] R. Chengalvarayan and L. Deng. Use of generalized dynamic feature\\nparameters for speech recognition. IEEE Transactions on Speech and\\nAudio Processing, pages 232\\u2013242, 1997a.\\n\\n[52] R. Chengalvarayan and L. Deng. Speech trajectory discrimination using\\nthe minimum classi\\ufb01cation error learning. IEEE Transactions on Speech\\nand Audio Processing, 6(6):505\\u2013515, 1998.\\n\\n[53] Y. Cho and L. Saul. Kernel methods for deep learning. In Proceedings of\\nNeural Information Processing Systems (NIPS), pages 342\\u2013350. 2009.\\n[54] D. Ciresan, A. Giusti, L. Gambardella, and J. Schmidhuber. Deep neural\\nnetworks segment neuronal membranes in electron microscopy images.\\nIn Proceedings of Neural Information Processing Systems (NIPS). 2012.\\n[55] D. Ciresan, U. Meier, L. Gambardella, and J. Schmidhuber. Deep, big,\\nsimple neural nets for handwritten digit recognition. Neural Computa-\\ntion, December 2010.\\n\\n[56] D. Ciresan, U. Meier, J. Masci, and J. Schmidhuber. A committee of\\nneural networks for tra\\ufb03c sign classi\\ufb01cation. In Proceedings of Interna-\\ntional Joint Conference on Neural Networks (IJCNN). 2011.\\n\\n[57] D. Ciresan, U. Meier, and J. Schmidhuber. Multi-column deep neural\\nnetworks for image classi\\ufb01cation. In Proceedings of Computer Vision\\nand Pattern Recognition (CVPR). 2012.\\n\\n[58] D. C. Ciresan, U. Meier, and J. Schmidhuber. Transfer learning for Latin\\nand Chinese characters with deep neural networks. In Proceedings of\\nInternational Joint Conference on Neural Networks (IJCNN). 2012.\\n\\n[59] A. Coates, B. Huval, T. Wang, D. Wu, A. Ng, and B. Catanzaro. Deep\\nlearning with COTS HPC. In Proceedings of International Conference\\non Machine Learning (ICML). 2013.\\n\\nFull text available at: http://dx.doi.org/10.1561/2000000039\\x0cReferences\\n\\n159\\n\\n[60] W. Cohen and R. V. de Carvalho. Stacked sequential learning.\\n\\nIn\\nProceedings of International Joint Conference on Arti\\ufb01cial Intelligence\\n(IJCAI), pages 671\\u2013676. 2005.\\n\\n[61] R. Collobert. Deep learning for e\\ufb03cient discriminative parsing.\\n\\nIn\\nProceedings of Arti\\ufb01cial Intelligence and Statistics (AISTATS). 2011.\\n[62] R. Collobert and J. Weston. A uni\\ufb01ed architecture for natural language\\nprocessing: Deep neural networks with multitask learning. In Proceed-\\nings of International Conference on Machine Learning (ICML). 2008.\\n[63] R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu, and\\nP. Kuksa. Natural language processing (almost) from scratch. Journal\\non Machine Learning Research, 12:2493\\u20132537, 2011.\\n\\n[64] G. Dahl, M. Ranzato, A. Mohamed, and G. Hinton. Phone recognition\\nwith the mean-covariance restricted boltzmann machine. In Proceedings\\nof Neural Information Processing Systems (NIPS), volume 23, pages\\n469\\u2013477. 2010.\\n\\n[65] G. Dahl, T. Sainath, and G. Hinton. Improving deep neural networks\\nfor LVCSR using recti\\ufb01ed linear units and dropout.\\nIn Proceedings\\nof International Conference on Acoustics Speech and Signal Processing\\n(ICASSP). 2013.\\n\\n[66] G. Dahl, J. Stokes, L. Deng, and D. Yu. Large-scale malware classi\\ufb01-\\ncation using random projections and neural networks. In Proceedings\\nof International Conference on Acoustics Speech and Signal Processing\\n(ICASSP). 2013.\\n\\n[67] G. Dahl, D. Yu, L. Deng, and A. Acero. Context-dependent DBN-\\nHMMs in large vocabulary continuous speech recognition. In Proceed-\\nings of International Conference on Acoustics Speech and Signal Pro-\\ncessing (ICASSP). 2011.\\n\\n[68] G. Dahl, D. Yu, L. Deng, and A. Acero. Context-dependent, pre-trained\\ndeep neural networks for large vocabulary speech recognition. IEEE\\nTransactions on Audio, Speech, & Language Processing, 20(1):30\\u201342,\\nJanuary 2012.\\n\\n[69] J. Dean, G. Corrado, R. Monga, K. Chen, M. Devin, Q. Le, M. Mao,\\nM. Ranzato, A. Senior, P. Tucker, K. Yang, and A. Ng. Large scale\\ndistributed deep networks. In Proceedings of Neural Information Pro-\\ncessing Systems (NIPS). 2012.\\n\\n[70] K. Demuynck and F. Triefenbach. Porting concepts from DNNs back\\nIn Proceedings of the Automatic Speech Recognition and\\n\\nto GMMs.\\nUnderstanding Workshop (ASRU). 2013.\\n\\nFull text available at: http://dx.doi.org/10.1561/2000000039\\x0c160\\n\\nReferences\\n\\n[71] L. Deng. A generalized hidden Markov model with state-conditioned\\nSignal Processing,\\n\\ntrend functions of time for the speech signal.\\n27(1):65\\u201378, 1992.\\n\\n[72] L. Deng. A stochastic model of speech incorporating hierarchical nonsta-\\ntionarity. IEEE Transactions on Speech and Audio Processing, 1(4):471\\u2013\\n475, 1993.\\n\\n[73] L. Deng. A dynamic, feature-based approach to the interface between\\nphonology and phonetics for speech modeling and recognition. Speech\\nCommunication, 24(4):299\\u2013323, 1998.\\n\\n[74] L. Deng. Computational models for speech production.\\n\\nIn Compu-\\ntational Models of Speech Pattern Processing, pages 199\\u2013213. Springer\\nVerlag, 1999.\\n\\n[75] L. Deng. Switching dynamic system models for speech articulation and\\nacoustics. In Mathematical Foundations of Speech and Language Pro-\\ncessing, pages 115\\u2013134. Springer-Verlag, New York, 2003.\\n\\n[76] L. Deng. Dynamic Speech Models \\u2014 Theory, Algorithm, and Applica-\\n\\ntion. Morgan & Claypool, December 2006.\\n\\n[77] L. Deng. An overview of deep-structured learning for information pro-\\ncessing. In Proceedings of Asian-Paci\\ufb01c Signal & Information Process-\\ning Annual Summit and Conference (APSIPA-ASC). October 2011.\\n\\n[78] L. Deng. The MNIST database of handwritten digit images for machine\\nlearning research. IEEE Signal Processing Magazine, 29(6), November\\n2012.\\n\\n[79] L. Deng. Design and learning of output representations for speech recog-\\nnition. In Neural Information Processing Systems (NIPS) Workshop on\\nLearning Output Representations. December 2013.\\n\\n[80] L. Deng. A tutorial survey of architectures, algorithms, and applications\\nfor deep learning.\\nIn Asian-Paci\\ufb01c Signal & Information Processing\\nAssociation Transactions on Signal and Information Processing. 2013.\\n[81] L. Deng, O. Abdel-Hamid, and D. Yu. A deep convolutional neural\\nnetwork using heterogeneous pooling for trading acoustic invariance\\nwith phonetic confusion.\\nIn Proceedings of International Conference\\non Acoustics Speech and Signal Processing (ICASSP). 2013.\\n\\n[82] L. Deng, A. Acero, L. Jiang, J. Droppo, and X. Huang. High perfor-\\nmance robust speech recognition using stereo training data.\\nIn Pro-\\nceedings of International Conference on Acoustics Speech and Signal\\nProcessing (ICASSP). 2001.\\n\\nFull text available at: http://dx.doi.org/10.1561/2000000039\\x0cReferences\\n\\n161\\n\\n[83] L. Deng and M. Aksmanovic. Speaker-independent phonetic classi\\ufb01-\\ncation using hidden markov models with state-conditioned mixtures of\\ntrend functions. IEEE Transactions on Speech and Audio Processing,\\n5:319\\u2013324, 1997.\\n\\n[84] L. Deng, M. Aksmanovic, D. Sun, and J. Wu. Speech recognition using\\nhidden Markov models with polynomial regression functions as nonsta-\\ntionary states.\\nIEEE Transactions on Speech and Audio Processing,\\n2(4):507\\u2013520, 1994.\\n\\n[85] L. Deng and J. Chen. Sequence classi\\ufb01cation using the high-level fea-\\ntures extracted from deep neural networks. In Proceedings of Interna-\\ntional Conference on Acoustics Speech and Signal Processing (ICASSP).\\n2014.\\n\\n[86] L. Deng and K. Erler. Structural design of a hidden Markov model\\nbased speech recognizer using multi-valued phonetic features: Compar-\\nison with segmental speech units. Journal of the Acoustical Society of\\nAmerica, 92(6):3058\\u20133067, 1992.\\n\\n[87] L. Deng, K. Hassanein, and M. Elmasry. Analysis of correlation struc-\\nture for a neural predictive model with application to speech recognition.\\nNeural Networks, 7(2):331\\u2013339, 1994.\\n\\n[88] L. Deng, X. He, and J. Gao. Deep stacking networks for informa-\\ntion retrieval. In Proceedings of International Conference on Acoustics\\nSpeech and Signal Processing (ICASSP). 2013c.\\n\\n[89] L. Deng, G. Hinton, and B. Kingsbury. New types of deep neural\\nnetwork learning for speech recognition and related applications: An\\noverview.\\nIn Proceedings of International Conference on Acoustics\\nSpeech and Signal Processing (ICASSP). 2013b.\\n\\n[90] L. Deng and X. D. Huang. Challenges in adopting speech recognition.\\nCommunications of the Association for Computing Machinery (ACM),\\n47(1):11\\u201313, January 2004.\\n\\n[91] L. Deng, B. Hutchinson, and D. Yu. Parallel training of deep stacking\\n\\nnetworks. In Proceedings of Interspeech. 2012b.\\n\\n[92] L. Deng, M. Lennig, V. Gupta, F. Seitz, P. Mermelstein, and P. Kenny.\\nPhonemic hidden Markov models with continuous mixture output den-\\nsities for large vocabulary word recognition.\\nIEEE Transactions on\\nSignal Processing, 39(7):1677\\u20131681, 1991.\\n\\n[93] L. Deng, M. Lennig, F. Seitz, and P. Mermelstein. Large vocabulary\\nword recognition using context-dependent allophonic hidden Markov\\nmodels. Computer Speech and Language, 4(4):345\\u2013357, 1990.\\n\\nFull text available at: http://dx.doi.org/10.1561/2000000039\\x0c162\\n\\nReferences\\n\\n[94] L. Deng, J. Li, K. Huang, Yao, D. Yu, F. Seide, M. Seltzer, G. Zweig,\\nX. He, J. Williams, Y. Gong, and A. Acero. Recent advances in deep\\nlearning for speech research at Microsoft.\\nIn Proceedings of Interna-\\ntional Conference on Acoustics Speech and Signal Processing (ICASSP).\\n2013a.\\n\\n[95] L. Deng and X. Li. Machine learning paradigms in speech recogni-\\ntion: An overview. IEEE Transactions on Audio, Speech, & Language,\\n21:1060\\u20131089, May 2013.\\n\\n[96] L. Deng and J. Ma. Spontaneous speech recognition using a statistical\\ncoarticulatory model for the vocal tract resonance dynamics. Journal\\nof the Acoustical Society America, 108:3036\\u20133048, 2000.\\n\\n[97] L. Deng and D. O\\u2019Shaughnessy. Speech Processing \\u2014 A Dynamic and\\n\\nOptimization-Oriented Approach. Marcel Dekker, 2003.\\n\\n[98] L. Deng, G. Ramsay, and D. Sun. Production models as a structural\\nbasis for automatic speech recognition. Speech Communication, 33(2\\u2013\\n3):93\\u2013111, August 1997.\\n\\n[99] L. Deng and H. Sameti. Transitional speech units and their represen-\\ntation by regressive Markov states: Applications to speech recognition.\\nIEEE Transactions on speech and audio processing, 4(4):301\\u2013306, July\\n1996.\\n\\n[100] L. Deng, M. Seltzer, D. Yu, A. Acero, A. Mohamed, and G. Hinton.\\nIn\\n\\nBinary coding of speech spectrograms using a deep autoencoder.\\nProceedings of Interspeech. 2010.\\n\\n[101] L. Deng and D. Sun. A statistical approach to automatic speech\\nrecognition using the atomic speech units constructed from overlap-\\nping articulatory features. Journal of the Acoustical Society of America,\\n85(5):2702\\u20132719, 1994.\\n\\n[102] L. Deng, G. Tur, X. He, and D. Hakkani-Tur. Use of kernel deep convex\\nnetworks and end-to-end learning for spoken language understanding.\\nIn Proceedings of IEEE Workshop on Spoken Language Technologies.\\nDecember 2012.\\n\\n[103] L. Deng, K. Wang, A. Acero, H. W. Hon, J. Droppo, C. Boulis, Y. Wang,\\nD. Jacoby, M. Mahajan, C. Chelba, and X. Huang. Distributed speech\\nprocessing in mipad\\u2019s multimodal user interface. IEEE Transactions on\\nSpeech and Audio Processing, 10(8):605\\u2013619, 2002.\\n\\n[104] L. Deng, J. Wu, J. Droppo, and A. Acero. Dynamic compensation of\\nHMM variances using the feature enhancement uncertainty computed\\nfrom a parametric model of speech distortion. IEEE Transactions on\\nSpeech and Audio Processing, 13(3):412\\u2013421, 2005.\\n\\nFull text available at: http://dx.doi.org/10.1561/2000000039\\x0cReferences\\n\\n163\\n\\n[105] L. Deng and D. Yu. Use of di\\ufb00erential cepstra as acoustic features\\nin hidden trajectory modeling for phonetic recognition. In Proceedings\\nof International Conference on Acoustics Speech and Signal Processing\\n(ICASSP). 2007.\\n\\n[106] L. Deng and D. Yu. Deep convex network: A scalable architecture for\\n\\nspeech pattern classi\\ufb01cation. In Proceedings of Interspeech. 2011.\\n\\n[107] L. Deng, D. Yu, and A. Acero. A bidirectional target \\ufb01ltering model of\\nspeech coarticulation: Two-stage implementation for phonetic recogni-\\ntion. IEEE Transactions on Audio and Speech Processing, 14(1):256\\u2013\\n265, January 2006.\\n\\n[108] L. Deng, D. Yu, and A. Acero. Structured speech modeling.\\n\\nIEEE\\nTransactions on Audio, Speech and Language Processing, 14(5):1492\\u2013\\n1504, September 2006.\\n\\n[109] L. Deng, D. Yu, and G. Hinton. Deep learning for speech recognition and\\nrelated applications. Neural Information Processing Systems (NIPS)\\nWorkshop, 2009.\\n\\n[110] L. Deng, D. Yu, and J. Platt. Scalable stacking and learning for build-\\ning deep architectures. In Proceedings of International Conference on\\nAcoustics Speech and Signal Processing (ICASSP). 2012a.\\n\\n[111] T. Deselaers, S. Hasan, O. Bender, and H. Ney. A deep learning\\napproach to machine transliteration. In Proceedings of 4th Workshop on\\nStatistical Machine Translation, pages 233\\u2013241. Athens, Greece, March\\n2009.\\n\\n[112] A. Diez. Automatic language recognition using deep neural networks.\\n\\nThesis, Universidad Autonoma de Madrid, SPAIN, September 2013.\\n\\n[113] P. Dognin and V. Goel. Combining stochastic average gradient and\\nhessian-free optimization for sequence training of deep neural networks.\\nIn Proceedings of the Automatic Speech Recognition and Understanding\\nWorkshop (ASRU). 2013.\\n\\n[114] D. Erhan, Y. Bengio, A. Courvelle, P. Manzagol, P. Vencent, and S. Ben-\\ngio. Why does unsupervised pre-training help deep learning? Journal\\non Machine Learning Research, pages 201\\u2013208, 2010.\\n\\n[115] R. Fernandez, A. Rendel, B. Ramabhadran, and R. Hoory. F0 contour\\nprediction with a deep belief network-gaussian process hybrid model. In\\nProceedings of International Conference on Acoustics Speech and Signal\\nProcessing (ICASSP), pages 6885\\u20136889. 2013.\\n\\n[116] S. Fine, Y. Singer, and N. Tishby. The hierarchical hidden Markov\\nmodel: Analysis and applications. Machine Learning, 32:41\\u201362, 1998.\\n\\nFull text available at: http://dx.doi.org/10.1561/2000000039\\x0c164\\n\\nReferences\\n\\n[117] A. Frome, G. Corrado, J. Shlens, S. Bengio, J. Dean, M. Ranzato, and\\nT. Mikolov. Devise: A deep visual-semantic embedding model. In Pro-\\nceedings of Neural Information Processing Systems (NIPS). 2013.\\n\\n[118] Q. Fu, X. He, and L. Deng. Phone-discriminating minimum classi\\ufb01ca-\\ntion error (p-mce) training for phonetic recognition. In Proceedings of\\nInterspeech. 2007.\\n\\n[119] M. Gales. Model-based approaches to handling uncertainty. In Robust\\nSpeech Recognition of Uncertain or Missing Data: Theory and Applica-\\ntion, pages 101\\u2013125. Springer, 2011.\\n\\n[120] J. Gao, X. He, and J.-Y. Nie. Clickthrough-based translation models\\nfor web search: From word models to phrase models. In Proceedings of\\nConference on Information and Knowledge Management (CIKM). 2010.\\n[121] J. Gao, X. He, W. Yih, and L. Deng. Learning semantic representations\\nfor the phrase translation model.\\nIn Proceedings of Neural Informa-\\ntion Processing Systems (NIPS) Workshop on Deep Learning. December\\n2013.\\n\\n[122] J. Gao, X. He, W. Yih, and L. Deng. Learning semantic representations\\nfor the phrase translation model. MSR-TR-2013-88, September 2013.\\n[123] J. Gao, X. He, W. Yih, and L. Deng. Learning continuous phrase rep-\\nresentations for translation modeling. In Proceedings of Association for\\nComputational Linguistics (ACL). 2014.\\n\\n[124] J. Gao, K. Toutanova, and W.-T. Yih. Clickthrough-based latent seman-\\ntic models for web search. In Proceedings of Special Interest Group on\\nInformation Retrieval (SIGIR). 2011.\\n\\n[125] R. Gens and P. Domingo. Discriminative learning of sum-product net-\\n\\nworks. Neural Information Processing Systems (NIPS), 2012.\\n\\n[126] D. George. How the brain might work: A hierarchical and temporal\\nmodel for learning and recognition. Ph.D. thesis, Stanford University,\\n2008.\\n\\n[127] M. Gibson and T. Hain. Error approximation and minimum phone error\\nacoustic model estimation. IEEE Transactions on Audio, Speech, and\\nLanguage Processing, 18(6):1269\\u20131279, August 2010.\\n\\n[128] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature\\nhierarchies for accurate object detection and semantic segmentation.\\narXiv:1311.2524v1, 2013.\\n\\n[129] X. Glorot and Y. Bengio. Understanding the di\\ufb03culty of training deep\\nfeed-forward neural networks. In Proceedings of Arti\\ufb01cial Intelligence\\nand Statistics (AISTATS). 2010.\\n\\nFull text available at: http://dx.doi.org/10.1561/2000000039\\x0cReferences\\n\\n165\\n\\n[130] X. Glorot, A. Bordes, and Y. Bengio. Deep sparse recti\\ufb01er neural\\nnetworks. In Proceedings of Arti\\ufb01cial Intelligence and Statistics (AIS-\\nTATS). April 2011.\\n\\n[131] I. Goodfellow, M. Mirza, A. Courville, and Y. Bengio. Multi-prediction\\ndeep boltzmann machines. In Proceedings of Neural Information Pro-\\ncessing Systems (NIPS). 2013.\\n\\n[132] E. Grais, M. Sen, and H. Erdogan. Deep neural networks for single\\n\\nchannel source separation. arXiv:1311.2746v1, 2013.\\n\\n[133] A. Graves. Sequence transduction with recurrent neural networks. Rep-\\nresentation Learning Workshop, International Conference on Machine\\nLearning (ICML), 2012.\\n\\n[134] A. Graves, S. Fernandez, F. Gomez, and J. Schmidhuber. Connection-\\nist temporal classi\\ufb01cation: Labeling unsegmented sequence data with\\nrecurrent neural networks. In Proceedings of International Conference\\non Machine Learning (ICML). 2006.\\n\\n[135] A. Graves, N. Jaitly, and A. Mohamed. Hybrid speech recognition with\\ndeep bidirectional LSTM. In Proceedings of the Automatic Speech Recog-\\nnition and Understanding Workshop (ASRU). 2013.\\n\\n[136] A. Graves, A. Mohamed, and G. Hinton. Speech recognition with deep\\nrecurrent neural networks. In Proceedings of International Conference\\non Acoustics Speech and Signal Processing (ICASSP). 2013.\\n\\n[137] F. Grezl and P. Fousek. Optimizing bottle-neck features for LVCSR. In\\nProceedings of International Conference on Acoustics Speech and Signal\\nProcessing (ICASSP). 2008.\\n\\n[138] C. Gulcehre, K. Cho, R. Pascanu, and Y. Bengio.\\n\\nLearned-\\nnorm pooling for deep feedforward and recurrent neural networks.\\nhttp://arxiv.org/abs/1311.1780, 2014.\\n\\n[139] M. Gutmann and A. Hyvarinen. Noise-contrastive estimation of unnor-\\nmalized statistical models, with applications to natural image statistics.\\nJournal of Machine Learning Research, 13:307\\u2013361, 2012.\\n\\n[140] T. Hain, L. Burget, J. Dines, P. Garner, F. Grezl, A. Hannani, M. Hui-\\njbregts, M. Kara\\ufb01at, M. Lincoln, and V. Wan. Transcribing meetings\\nwith the AMIDA systems. IEEE Transactions on Audio, Speech, and\\nLanguage Processing, 20:486\\u2013498, 2012.\\n\\n[141] P. Hamel and D. Eck. Learning features from music audio with deep\\nbelief networks. In Proceedings of International Symposium on Music\\nInformation Retrieval (ISMIR). 2010.\\n\\nFull text available at: http://dx.doi.org/10.1561/2000000039\\x0c166\\n\\nReferences\\n\\n[142] G. Hawkins, S. Ahmad, and D. Dubinsky. Hierarchical temporal mem-\\nory including HTM cortical learning algorithms. Numenta Technical\\nReport, December 10 2010.\\n\\n[143] J. Hawkins and S. Blakeslee. On Intelligence: How a New Understanding\\nof the Brain will lead to the Creation of Truly Intelligent Machines.\\nTimes Books, New York, 2004.\\n\\n[144] X. He and L. Deng. Speech recognition, machine translation, and speech\\ntranslation \\u2014 a unifying discriminative framework. IEEE Signal Pro-\\ncessing Magazine, 28, November 2011.\\n\\n[145] X. He and L. Deng. Optimization in speech-centric information process-\\ning: Criteria and techniques. In Proceedings of International Conference\\non Acoustics Speech and Signal Processing (ICASSP). 2012.\\n\\n[146] X. He and L. Deng.\\n\\nSpeech-centric information processing: An\\n\\noptimization-oriented approach. In Proceedings of the IEEE. 2013.\\n\\n[147] X. He, L. Deng, and W. Chou. Discriminative learning in sequential pat-\\ntern recognition \\u2014 a unifying review for optimization-oriented speech\\nrecognition. IEEE Signal Processing Magazine, 25:14\\u201336, 2008.\\n\\n[148] G. Heigold, H. Ney, P. Lehnen, T. Gass, and R. Schluter. Equivalence of\\ngenerative and log-liner models. IEEE Transactions on Audio, Speech,\\nand Language Processing, 19(5):1138\\u20131148, February 2011.\\n\\n[149] G. Heigold, H. Ney, and R. Schluter. Investigations on an EM-style opti-\\nmization algorithm for discriminative training of HMMs. IEEE Trans-\\nactions on Audio, Speech, and Language Processing, 21(12):2616\\u20132626,\\nDecember 2013.\\n\\n[150] G. Heigold, V. Vanhoucke, A. Senior, P. Nguyen, M. Ranzato, M. Devin,\\nand J. Dean. Multilingual acoustic models using distributed deep neu-\\nral networks. In Proceedings of International Conference on Acoustics\\nSpeech and Signal Processing (ICASSP). 2013.\\n\\n[151] I. Heintz, E. Fosler-Lussier, and C. Brew. Discriminative input stream\\ncombination for conditional random \\ufb01eld phone recognition.\\nIEEE\\nTransactions on Audio, Speech, and Language Processing, 17(8):1533\\u2013\\n1546, November 2009.\\n\\n[152] M. Henderson, B. Thomson, and S. Young. Deep neural network\\napproach for the dialog state tracking challenge. In Proceedings of Spe-\\ncial Interest Group on Disclosure and Dialogue (SIGDIAL). 2013.\\n\\n[153] M. Hermans and B. Schrauwen. Training and analysing deep recur-\\nrent neural networks. In Proceedings of Neural Information Processing\\nSystems (NIPS). 2013.\\n\\nFull text available at: http://dx.doi.org/10.1561/2000000039\\x0cReferences\\n\\n167\\n\\n[154] H. Hermansky, D. Ellis, and S. Sharma. Tandem connectionist feature\\nextraction for conventional HMM systems. In Proceedings of Interna-\\ntional Conference on Acoustics Speech and Signal Processing (ICASSP).\\n2000.\\n\\n[155] Y. Hifny and S. Renals. Speech recognition using augmented conditional\\nIEEE Transactions on Audio, Speech, and Language\\n\\nrandom \\ufb01elds.\\nProcessing, 17(2):354\\u2013365, February 2009.\\n\\n[156] G. Hinton. Mapping part-whole hierarchies into connectionist networks.\\n\\nArti\\ufb01cial Intelligence, 46:47\\u201375, 1990.\\n\\n[157] G. Hinton. Preface to the special issue on connectionist symbol pro-\\n\\ncessing. Arti\\ufb01cial Intelligence, 46:1\\u20134, 1990.\\n\\n[158] G. Hinton. The ups and downs of Hebb synapses. Canadian Psychology,\\n\\n44:10\\u201313, 2003.\\n\\n[159] G. Hinton. A practical guide to training restricted boltzmann machines.\\n\\nUTML Tech Report 2010-003, Univ. Toronto, August 2010.\\n\\n[160] G. Hinton. A better way to learn features. Communications of the\\nAssociation for Computing Machinery (ACM), 54(10), October 2011.\\n\\n[161] G. Hinton, L. Deng, D. Yu, G. Dahl, A. Mohamed, N. Jaitly, A. Senior,\\nV. Vanhoucke, P. Nguyen, T. Sainath, and B. Kingsbury. Deep neu-\\nral networks for acoustic modeling in speech recognition. IEEE Signal\\nProcessing Magazine, 29(6):82\\u201397, November 2012.\\n\\n[162] G. Hinton, A. Krizhevsky, and S. Wang. Transforming autoencoders. In\\nProceedings of International Conference on Arti\\ufb01cial Neural Networks.\\n2011.\\n\\n[163] G. Hinton, S. Osindero, and Y. Teh. A fast learning algorithm for deep\\n\\nbelief nets. Neural Computation, 18:1527\\u20131554, 2006.\\n\\n[164] G. Hinton and R. Salakhutdinov. Reducing the dimensionality of data\\n\\nwith neural networks. Science, 313(5786):504\\u2013507, July 2006.\\n\\n[165] G. Hinton and R. Salakhutdinov. Discovering binary codes for docu-\\nments by learning deep generative models. Topics in Cognitive Science,\\npages 1\\u201318, 2010.\\n\\n[166] G. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and R. Salakhut-\\ndinov. Improving neural networks by preventing co-adaptation of fea-\\nture detectors. arXiv: 1207.0580v1, 2012.\\n\\n[167] S. Hochreiter.\\n\\nUntersuchungen zu dynamischen neuronalen net-\\nzen. Diploma thesis, Institut fur Informatik, Technische Universitat\\nMunchen, 1991.\\n\\nFull text available at: http://dx.doi.org/10.1561/2000000039\\x0c168\\n\\nReferences\\n\\n[168] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural\\n\\nComputation, 9:1735\\u20131780, 1997.\\n\\n[169] E. Huang, R. Socher, C. Manning, and A. Ng. Improving word represen-\\ntations via global context and multiple word prototypes. In Proceedings\\nof Association for Computational Linguistics (ACL). 2012.\\n\\n[170] J. Huang, J. Li, L. Deng, and D. Yu. Cross-language knowledge transfer\\nusing multilingual deep neural networks with shared hidden layers. In\\nProceedings of International Conference on Acoustics Speech and Signal\\nProcessing (ICASSP). 2013.\\n\\n[171] P. Huang, L. Deng, M. Hasegawa-Johnson, and X. He. Random fea-\\ntures for kernel deep convex network. In Proceedings of International\\nConference on Acoustics Speech and Signal Processing (ICASSP). 2013.\\n[172] P. Huang, X. He, J. Gao, L. Deng, A. Acero, and L. Heck. Learning\\ndeep structured semantic models for web search using clickthrough data.\\nAssociation for Computing Machinery (ACM) International Conference\\nInformation and Knowledge Management (CIKM), 2013.\\n\\n[173] P. Huang, K. Kumar, C. Liu, Y. Gong, and L. Deng. Predicting speech\\nrecognition con\\ufb01dence using deep learning with word identity and score\\nfeatures. In Proceedings of International Conference on Acoustics Speech\\nand Signal Processing (ICASSP). 2013.\\n\\n[174] S. Huang and S. Renals. Hierarchical bayesian language models for\\nconversational speech recognition. IEEE Transactions on Audio, Speech,\\nand Language Processing, 18(8):1941\\u20131954, November 2010.\\n\\n[175] X. Huang, A. Acero, C. Chelba, L. Deng, J. Droppo, D. Duchene,\\nJ. Goodman, and H. Hon. Mipad: A multimodal interaction proto-\\ntype. In Proceedings of International Conference on Acoustics Speech\\nand Signal Processing (ICASSP). 2001.\\n\\n[176] Y. Huang, D. Yu, Y. Gong, and C. Liu. Semi-supervised GMM and DNN\\nacoustic model training with multi-system combination and con\\ufb01dence\\nre-calibration. In Proceedings of Interspeech, pages 2360\\u20132364. 2013.\\n\\n[177] E. Humphrey and J. Bello. Rethinking automatic chord recognition\\nIn Proceedings of International\\n\\nwith convolutional neural networks.\\nConference on Machine Learning and Application (ICMLA). 2012a.\\n\\n[178] E. Humphrey, J. Bello, and Y. LeCun. Moving beyond feature design:\\nDeep architectures and automatic feature learning in music informat-\\nics. In Proceedings of International Symposium on Music Information\\nRetrieval (ISMIR). 2012.\\n\\nFull text available at: http://dx.doi.org/10.1561/2000000039\\x0cReferences\\n\\n169\\n\\n[179] E. Humphrey, J. Bello, and Y. LeCun. Feature learning and deep archi-\\ntectures: New directions for music informatics. Journal of Intelligent\\nInformation Systems, 2013.\\n\\n[180] B. Hutchinson, L. Deng, and D. Yu. A deep architecture with bilinear\\nmodeling of hidden representations: Applications to phonetic recogni-\\ntion. In Proceedings of International Conference on Acoustics Speech\\nand Signal Processing (ICASSP). 2012.\\n\\n[181] B. Hutchinson, L. Deng, and D. Yu. Tensor deep stacking net-\\nworks. IEEE Transactions on Pattern Analysis and Machine Intelli-\\ngence, 35:1944\\u20131957, 2013.\\n\\n[182] D. Imseng, P. Motlicek, P. Garner, and H. Bourlard. Impact of deep\\nMLP architecture on di\\ufb00erent modeling techniques for under-resourced\\nspeech recognition. In Proceedings of the Automatic Speech Recognition\\nand Understanding Workshop (ASRU). 2013.\\n\\n[183] N. Jaitly and G. Hinton. Learning a better representation of speech\\nsound waves using restricted boltzmann machines.\\nIn Proceedings of\\nInternational Conference on Acoustics Speech and Signal Processing\\n(ICASSP). 2011.\\n\\n[184] N. Jaitly, P. Nguyen, and V. Vanhoucke. Application of pre-trained deep\\nneural networks to large vocabulary speech recognition. In Proceedings\\nof Interspeech. 2012.\\n\\n[185] K. Jarrett, K. Kavukcuoglu, and Y. LeCun. What is the best multi-\\nstage architecture for object recognition? In Proceedings of International\\nConference on Computer Vision, pages 2146\\u20132153. 2009.\\n\\n[186] H. Jiang and X. Li. Parameter estimation of statistical models using\\nconvex optimization: An advanced method of discriminative training\\nfor speech and language processing. IEEE Signal Processing Magazine,\\n27(3):115\\u2013127, 2010.\\n\\n[187] B. Juang, S. Levinson, and M. Sondhi. Maximum likelihood estimation\\nfor multivariate mixture observations of Markov chains. IEEE Trans-\\nactions on Information Theory, 32:307\\u2013309, 1986.\\n\\n[188] B.-H. Juang, W. Chou, and C.-H. Lee. Minimum classi\\ufb01cation error\\nIEEE Transactions On Speech\\n\\nrate methods for speech recognition.\\nand Audio Processing, 5:257\\u2013265, 1997.\\n\\n[189] S. Kahou et al. Combining modality speci\\ufb01c deep neural networks for\\nemotion recognition in video. In Proceedings of International Conference\\non Multimodal Interaction (ICMI). 2013.\\n\\nFull text available at: http://dx.doi.org/10.1561/2000000039\\x0c170\\n\\nReferences\\n\\n[190] S. Kang, X. Qian, and H. Meng. Multi-distribution deep belief network\\nfor speech synthesis.\\nIn Proceedings of International Conference on\\nAcoustics Speech and Signal Processing (ICASSP), pages 8012\\u20138016.\\n2013.\\n\\n[191] Y. Kashiwagi, D. Saito, N. Minematsu, and K. Hirose. Discriminative\\npiecewise linear transformation based on deep learning for noise robust\\nautomatic speech recognition. In Proceedings of the Automatic Speech\\nRecognition and Understanding Workshop (ASRU). 2013.\\n\\n[192] K. Kavukcuoglu, P. Sermanet, Y. Boureau, K. Gregor, M. Mathieu,\\nand Y. LeCun. Learning convolutional feature hierarchies for visual\\nrecognition. In Proceedings of Neural Information Processing Systems\\n(NIPS). 2010.\\n\\n[193] H. Ketabdar and H. Bourlard. Enhanced phone posteriors for improving\\nspeech recognition systems. IEEE Transactions on Audio, Speech, and\\nLanguage Processing, 18(6):1094\\u20131106, August 2010.\\n\\n[194] B. Kingsbury. Lattice-based optimization of sequence classi\\ufb01cation cri-\\nteria for neural-network acoustic modeling. In Proceedings of Interna-\\ntional Conference on Acoustics Speech and Signal Processing (ICASSP).\\n2009.\\n\\n[195] B. Kingsbury, T. Sainath, and H. Soltau. Scalable minimum bayes\\nrisk training of deep neural network acoustic models using distributed\\nhessian-free optimization. In Proceedings of Interspeech. 2012.\\n\\n[196] R. Kiros, R. Zemel, and R. Salakhutdinov. Multimodal neural lan-\\nguage models. In Proceedings of Neural Information Processing Systems\\n(NIPS) Deep Learning Workshop. 2013.\\n\\n[197] T. Ko and B. Mak. Eigentriphones for context-dependent acoustic mod-\\neling. IEEE Transactions on Audio, Speech, and Language Processing,\\n21(6):1285\\u20131294, 2013.\\n\\n[198] A. Krizhevsky, I. Sutskever, and G. Hinton. Imagenet classi\\ufb01cation with\\ndeep convolutional neural networks. In Proceedings of Neural Informa-\\ntion Processing Systems (NIPS). 2012.\\n\\n[199] Y. Kubo, T. Hori, and A. Nakamura. Integrating deep neural networks\\ninto structural classi\\ufb01cation approach based on weighted \\ufb01nite-state\\ntransducers. In Proceedings of Interspeech. 2012.\\n\\n[200] R. Kurzweil. How to Create a Mind. Viking Books, December 2012.\\n[201] P. Lal and S. King. Cross-lingual automatic speech recognition using\\ntandem features. IEEE Transactions on Audio, Speech, and Language\\nProcessing, 21(12):2506\\u20132515, December 2013.\\n\\nFull text available at: http://dx.doi.org/10.1561/2000000039\\x0cReferences\\n\\n171\\n\\n[202] K. Lang, A. Waibel, and G. Hinton. A time-delay neural network archi-\\ntecture for isolated word recognition. Neural Networks, 3(1):23\\u201343, 1990.\\n[203] H. Larochelle and Y. Bengio. Classi\\ufb01cation using discriminative\\nIn Proceedings of International Con-\\n\\nrestricted boltzmann machines.\\nference on Machine Learning (ICML). 2008.\\n\\n[204] D. Le and P. Mower. Emotion recognition from spontaneous speech\\nusing hidden markov models with deep belief networks.\\nIn Proceed-\\nings of the Automatic Speech Recognition and Understanding Workshop\\n(ASRU). 2013.\\n\\n[205] H. Le, A. Allauzen, G. Wisniewski, and F. Yvon. Training continuous\\nspace language models: Some practical issues. In Proceedings of Empiri-\\ncal Methods in Natural Language Processing (EMNLP), pages 778\\u2013788.\\n2010.\\n\\n[206] H. Le, I. Oparin, A. Allauzen, J. Gauvain, and F. Yvon. Structured\\noutput layer neural network language model. In Proceedings of Interna-\\ntional Conference on Acoustics Speech and Signal Processing (ICASSP).\\n2011.\\n\\n[207] H. Le, I. Oparin, A. Allauzen, J.-L. Gauvain, and F. Yvon. Struc-\\ntured output layer neural network language models for speech recogni-\\ntion. IEEE Transactions on Audio, Speech, and Language Processing,\\n21(1):197\\u2013206, January 2013.\\n\\n[208] Q. Le, J. Ngiam, A. Coates, A. Lahiri, B. Prochnow, and A. Ng. On\\noptimization methods for deep learning. In Proceedings of International\\nConference on Machine Learning (ICML). 2011.\\n\\n[209] Q. Le, M. Ranzato, R. Monga, M. Devin, G. Corrado, K. Chen, J. Dean,\\nand A. Ng. Building high-level features using large scale unsupervised\\nlearning. In Proceedings of International Conference on Machine Learn-\\ning (ICML). 2012.\\n\\n[210] Y. LeCun. Learning invariant feature hierarchies.\\n\\nIn Proceedings of\\n\\nEuropean Conference on Computer Vision (ECCV). 2012.\\n\\n[211] Y. LeCun and Y. Bengio. Convolutional networks for images, speech,\\nand time series.\\nIn M. Arbib, editor, The Handbook of Brain The-\\nory and Neural Networks, pages 255\\u2013258. MIT Press, Cambridge, Mas-\\nsachusetts, 1995.\\n\\n[212] Y. LeCun, L. Bottou, Y. Bengio, and P. Ha\\ufb00ner. Gradient-based learn-\\ning applied to document recognition. Proceedings of the IEEE, 86:2278\\u2013\\n2324, 1998.\\n\\nFull text available at: http://dx.doi.org/10.1561/2000000039\\x0c172\\n\\nReferences\\n\\n[213] Y. LeCun, S. Chopra, M. Ranzato, and F. Huang. Energy-based models\\nin document recognition and computer vision. In Proceedings of Inter-\\nnational Conference on Document Analysis and Recognition (ICDAR).\\n2007.\\n\\n[214] C.-H. Lee. From knowledge-ignorant to knowledge-rich modeling: A new\\nspeech research paradigm for next-generation automatic speech recog-\\nnition. In Proceedings of International Conference on Spoken Language\\nProcessing (ICSLP), pages 109\\u2013111. 2004.\\n\\n[215] H. Lee, R. Grosse, R. Ranganath, and A. Ng. Convolutional deep belief\\nnetworks for scalable unsupervised learning of hierarchical representa-\\ntions. In Proceedings of International Conference on Machine Learning\\n(ICML). 2009.\\n\\n[216] H. Lee, R. Grosse, R. Ranganath, and A. Ng. Unsupervised learning\\nof hierarchical representations with convolutional deep belief networks.\\nCommunications of the Association for Computing Machinery (ACM),\\n54(10):95\\u2013103, October 2011.\\n\\n[217] H. Lee, Y. Largman, P. Pham, and A. Ng. Unsupervised feature learning\\nIn\\n\\nfor audio classi\\ufb01cation using convolutional deep belief networks.\\nProceedings of Neural Information Processing Systems (NIPS). 2010.\\n\\n[218] P. Lena, K. Nagata, and P. Baldi. Deep spatiotemporal architectures\\nand learning for protein structure prediction. In Proceedings of Neural\\nInformation Processing Systems (NIPS). 2012.\\n\\n[219] S. Levine. Exploring deep and recurrent architectures for optimal con-\\n\\ntrol. arXiv:1311.1761v1.\\n\\n[220] J. Li, L. Deng, Y. Gong, and R. Haeb-Umbach. An overview of\\nnoise-robust automatic speech recognition. IEEE/Association for Com-\\nputing Machinery (ACM) Transactions on Audio, Speech, and Language\\nProcessing, pages 1\\u201333, 2014.\\n\\n[221] J. Li, D. Yu, J. Huang, and Y. Gong.\\n\\nImproving wideband speech\\nrecognition using mixed-bandwidth training data in CD-DNN-HMM.\\nIn Proceedings of IEEE Spoken Language Technology (SLT). 2012.\\n\\n[222] L. Li, Y. Zhao, D. Jiang, and Y. Zhang etc. Hybrid deep neural network\\u2013\\nhidden markov model (DNN-HMM) based speech emotion recognition.\\nIn Proceedings Conference on A\\ufb00ective Computing and Intelligent Inter-\\naction (ACII), pages 312\\u2013317. September 2013.\\n\\n[223] H. Liao. Speaker adaptation of context dependent deep neural net-\\nworks. In Proceedings of International Conference on Acoustics Speech\\nand Signal Processing (ICASSP). 2013.\\n\\nFull text available at: http://dx.doi.org/10.1561/2000000039\\x0cReferences\\n\\n173\\n\\n[224] H. Liao, E. McDermott, and A. Senior. Large scale deep neural network\\nacoustic modeling with semi-supervised training data for youtube video\\ntranscription. In Proceedings of the Automatic Speech Recognition and\\nUnderstanding Workshop (ASRU). 2013.\\n\\n[225] H. Lin, L. Deng, D. Yu, Y. Gong, A. Acero, and C.-H. Lee. A study on\\nmultilingual acoustic modeling for large vocabulary ASR. In Proceedings\\nof International Conference on Acoustics Speech and Signal Processing\\n(ICASSP). 2009.\\n\\n[226] Y. Lin, F. Lv, S. Zhu, M. Yang, T. Cour, K. Yu, L. Cao, and T. Huang.\\nLarge-scale image classi\\ufb01cation: Fast feature extraction and SVM train-\\ning.\\nIn Proceedings of Computer Vision and Pattern Recognition\\n(CVPR). 2011.\\n\\n[227] Z. Ling, L. Deng, and D. Yu. Modeling spectral envelopes using\\nrestricted boltzmann machines and deep belief networks for statisti-\\ncal parametric speech synthesis. IEEE Transactions on Audio Speech\\nLanguage Processing, 21(10):2129\\u20132139, 2013.\\n\\n[228] Z. Ling, L. Deng, and D. Yu. Modeling spectral envelopes using\\nrestricted boltzmann machines for statistical parametric speech synthe-\\nsis. In International Conference on Acoustics Speech and Signal Pro-\\ncessing (ICASSP), pages 7825\\u20137829. 2013.\\n\\n[229] Z. Ling, K. Richmond, and J. Yamagishi. Articulatory control of HMM-\\nbased parametric speech synthesis using feature-space-switched multi-\\nple regression.\\nIEEE Transactions on Audio, Speech, and Language\\nProcessing, 21, January 2013.\\n\\n[230] L. Lu, K. Chin, A. Ghoshal, and S. Renals. Joint uncertainty decoding\\nfor noise robust subspace gaussian mixture models. IEEE Transactions\\non Audio, Speech, and Language Processing, 21(9):1791\\u20131804, 2013.\\n\\n[231] J. Ma and L. Deng. A path-stack algorithm for optimizing dynamic\\nregimes in a statistical hidden dynamical model of speech. Computer,\\nSpeech and Language, 2000.\\n\\n[232] J. Ma and L. Deng. E\\ufb03cient decoding strategies for conversational\\nspeech recognition using a constrained nonlinear state-space model.\\nIEEE Transactions on Speech and Audio Processing, 11(6):590\\u2013602,\\n2003.\\n\\n[233] J. Ma and L. Deng. Target-directed mixture dynamic models for spon-\\ntaneous speech recognition. IEEE Transactions on Speech and Audio\\nProcessing, 12(1):47\\u201358, 2004.\\n\\nFull text available at: http://dx.doi.org/10.1561/2000000039\\x0c174\\n\\nReferences\\n\\n[234] A. Maas, A. Hannun, and A. Ng. Recti\\ufb01er nonlinearities improve neural\\nnetwork acoustic models. International Conference on Machine Learn-\\ning (ICML) Workshop on Deep Learning for Audio, Speech, and Lan-\\nguage Processing, 2013.\\n\\n[235] A. Maas, Q. Le, T. O\\u2019Neil, O. Vinyals, P. Nguyen, and P. Ng. Recurrent\\nneural networks for noise reduction in robust ASR. In Proceedings of\\nInterspeech. 2012.\\n\\n[236] C. Manning, P. Raghavan, and H. Sch\\xfctze. Introduction to Information\\n\\nRetrieval. Cambridge University Press, 2009.\\n\\n[237] J. Marko\\ufb00. Scientists see promise in deep-learning programs. New York\\n\\nTimes, November 24 2012.\\n\\n[238] J. Martens. Deep learning with hessian-free optimization. In Proceedings\\n\\nof International Conference on Machine Learning (ICML). 2010.\\n\\n[239] J. Martens and I. Sutskever. Learning recurrent neural networks with\\nhessian-free optimization. In Proceedings of International Conference\\non Machine Learning (ICML). 2011.\\n\\n[240] D. McAllester. A PAC-bayesian tutorial with a dropout bound. ArX-\\n\\nive1307.2118, July 2013.\\n\\n[241] I. McGraw, I. Badr, and J. R. Glass. Learning lexicons from speech\\nusing a pronunciation mixture model. IEEE Transactions on Audio,\\nSpeech, and Language Processing, 21(2):357,366, February 2013.\\n\\n[242] G. Mesnil, X. He, L. Deng, and Y. Bengio. Investigation of recurrent-\\nneural-network architectures and learning methods for spoken language\\nunderstanding. In Proceedings of Interspeech. 2013.\\n\\n[243] Y. Miao and F. Metze. Improving low-resource CD-DNN-HMM using\\ndropout and multilingual DNN training. In Proceedings of Interspeech.\\n2013.\\n\\n[244] Y. Miao, S. Rawat, and F. Metze. Deep maxout networks for low\\nIn Proceedings of the Automatic Speech\\n\\nresource speech recognition.\\nRecognition and Understanding Workshop (ASRU). 2013.\\n\\n[245] T. Mikolov. Statistical language models based on neural networks.\\n\\nPh.D. thesis, Brno University of Technology, 2012.\\n\\n[246] T. Mikolov, K. Chen, G. Corrado, and J. Dean. E\\ufb03cient estimation of\\nword representations in vector space. In Proceedings of International\\nConference on Learning Representations (ICLR). 2013.\\n\\nFull text available at: http://dx.doi.org/10.1561/2000000039\\x0cReferences\\n\\n175\\n\\n[247] T. Mikolov, A. Deoras, D. Povey, L. Burget, and J. Cernocky. Strategies\\nfor training large scale neural network language models. In Proceedings\\nof the IEEE Automatic Speech Recognition and Understanding Work-\\nshop (ASRU). 2011.\\n\\n[248] T. Mikolov, M. Kara\\ufb01at, L. Burget, J. Cernocky, and S. Khudanpur.\\nRecurrent neural network based language model.\\nIn Proceedings of\\nInternational Conference on Acoustics Speech and Signal Processing\\n(ICASSP), pages 1045\\u20131048. 2010.\\n\\n[249] T. Mikolov, Q. Le, and I. Sutskever. Exploiting similarities among lan-\\n\\nguages for machine translation. arXiv:1309.4168v1, 2013.\\n\\n[250] T. Mikolov, I. Sutskever, K. Chen, G. Corrado, and J. Dean. Distributed\\nIn\\n\\nrepresentations of words and phrases and their compositionality.\\nProceedings of Neural Information Processing Systems (NIPS). 2013.\\n\\n[251] Y. Minami, E. McDermott, A. Nakamura, and S. Katagiri. A recogni-\\ntion method with parametric trajectory synthesized using direct rela-\\ntions between static and dynamic feature vector time series. In Pro-\\nceedings of International Conference on Acoustics Speech and Signal\\nProcessing (ICASSP), pages 957\\u2013960. 2002.\\n\\n[252] A. Mnih and G. Hinton. Three new graphical models for statistical lan-\\nguage modeling. In Proceedings of International Conference on Machine\\nLearning (ICML), pages 641\\u2013648. 2007.\\n\\n[253] A. Mnih and G. Hinton. A scalable hierarchical distributed lan-\\nguage model. In Proceedings of Neural Information Processing Systems\\n(NIPS), pages 1081\\u20131088. 2008.\\n\\n[254] A. Mnih and K. Kavukcuoglu. Learning word embeddings e\\ufb03ciently\\nwith noise-contrastive estimation. In Proceedings of Neural Information\\nProcessing Systems (NIPS). 2013.\\n\\n[255] A. Mnih and W.-T. Teh. A fast and simple algorithm for training\\nneural probabilistic language models. In Proceedings of International\\nConference on Machine Learning (ICML), pages 1751\\u20131758. 2012.\\n\\n[256] V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wier-\\nstra, and M. Riedmiller. Playing arari with deep reinforcement learning.\\nNeural Information Processing Systems (NIPS) Deep Learning Work-\\nshop, 2013. also arXiv:1312.5602v1.\\n\\n[257] A. Mohamed, G. Dahl, and G. Hinton. Deep belief networks for phone\\nrecognition. In Proceedings of Neural Information Processing Systems\\n(NIPS) Workshop Deep Learning for Speech Recognition and Related\\nApplications. 2009.\\n\\nFull text available at: http://dx.doi.org/10.1561/2000000039\\x0c176\\n\\nReferences\\n\\n[258] A. Mohamed, G. Dahl, and G. Hinton. Acoustic modeling using deep\\nbelief networks. IEEE Transactions on Audio, Speech, & Language Pro-\\ncessing, 20(1), January 2012.\\n\\n[259] A. Mohamed, G. Hinton, and G. Penn. Understanding how deep belief\\nnetworks perform acoustic modelling. In Proceedings of International\\nConference on Acoustics Speech and Signal Processing (ICASSP). 2012.\\n[260] A. Mohamed, D. Yu, and L. Deng. Investigation of full-sequence train-\\nIn Proceedings of\\n\\ning of deep belief networks for speech recognition.\\nInterspeech. 2010.\\n\\n[261] N. Morgan. Deep and wide: Multiple layers in automatic speech recog-\\nnition. IEEE Transactions on Audio, Speech, & Language Processing,\\n20(1), January 2012.\\n\\n[262] N. Morgan, Q. Zhu, A. Stolcke, K. Sonmez, S. Sivadas, T. Shinozaki,\\nM. Ostendorf, P. Jain, H. Hermansky, D. Ellis, G. Doddington, B. Chen,\\nO. Cretin, H. Bourlard, and M. Athineos. Pushing the envelope \\u2014 aside\\n[speech recognition].\\nIEEE Signal Processing Magazine, 22(5):81\\u201388,\\nSeptember 2005.\\n\\n[263] F. Morin and Y. Bengio. Hierarchical probabilistic neural network lan-\\nIn Proceedings of Arti\\ufb01cial Intelligence and Statistics\\n\\nguage models.\\n(AISTATS). 2005.\\n\\n[264] K. Murphy. Machine Learning \\u2014 A Probabilistic Perspective. The MIT\\n\\nPress, 2012.\\n\\n[265] V. Nair and G. Hinton. 3-d object recognition with deep belief nets. In\\n\\nProceedings of Neural Information Processing Systems (NIPS). 2009.\\n\\n[266] T. Nakashika, R. Takashima, T. Takiguchi, and Y. Ariki. Voice conver-\\nsion in high-order eigen space using deep belief nets. In Proceedings of\\nInterspeech. 2013.\\n\\n[267] H. Ney. Speech translation: Coupling of recognition and translation. In\\nProceedings of International Conference on Acoustics Speech and Signal\\nProcessing (ICASSP). 1999.\\n\\n[268] J. Ngiam, Z. Chen, P. Koh, and A. Ng. Learning deep energy models. In\\nProceedings of International Conference on Machine Learning (ICML).\\n2011.\\n\\n[269] J. Ngiam, A. Khosla, M. Kim, J. Nam, H. Lee, and A. Ng. Multimodal\\ndeep learning. In Proceedings of International Conference on Machine\\nLearning (ICML). 2011.\\n\\nFull text available at: http://dx.doi.org/10.1561/2000000039\\x0cReferences\\n\\n177\\n\\n[270] M. Norouzi, T. Mikolov, S. Bengio, J. Shlens, A. Frome, G. Corrado,\\nand J. Dean. Zero-shot learning by convex combination of semantic\\nembeddings. arXiv:1312.5650v2, 2013.\\n\\n[271] N. Oliver, A. Garg, and E. Horvitz. Layered representations for learning\\nand inferring o\\ufb03ce activity from multiple sensory channels. Computer\\nVision and Image Understanding, 96:163\\u2013180, 2004.\\n\\n[272] B. Olshausen. Can \\u2018deep learning\\u2019 o\\ufb00er deep insights about visual rep-\\nresentation? Neural Information Processing Systems (NIPS) Workshop\\non Deep Learning and Unsupervised Feature Learning, 2012.\\n\\n[273] M. Ostendorf. Moving beyond the \\u2018beads-on-a-string\\u2019 model of speech.\\nIn Proceedings of the Automatic Speech Recognition and Understanding\\nWorkshop (ASRU). 1999.\\n\\n[274] M. Ostendorf, V. Digalakis, and O. Kimball. From HMMs to segment\\nmodels: A uni\\ufb01ed view of stochastic modeling for speech recognition.\\nIEEE Transactions on Speech and Audio Processing, 4(5), September\\n1996.\\n\\n[275] L. Oudre, C. Fevotte, and Y. Grenier. Probabilistic template-based\\nchord recognition. IEEE Transactions on Audio, Speech, and Language\\nProcessing, 19(8):2249\\u20132259, November 2011.\\n\\n[276] H. Palangi, L. Deng, and R. Ward. Learning input and recurrent weight\\nmatrices in echo state networks. Neural Information Processing Systems\\n(NIPS) Deep Learning Workshop, December 2013.\\n\\n[277] H. Palangi, R. Ward, and L. Deng. Using deep stacking network to\\nimprove structured compressive sensing with multiple measurement vec-\\ntors. In Proceedings of International Conference on Acoustics Speech\\nand Signal Processing (ICASSP). 2013.\\n\\n[278] G. Papandreou, A. Katsamanis, V. Pitsikalis, and P. Maragos. Adap-\\ntive multimodal fusion by uncertainty compensation with application to\\naudiovisual speech recognition. IEEE Transactions on Audio, Speech,\\nand Language Processing, 17:423\\u2013435, 2009.\\n\\n[279] R. Pascanu, C. Gulcehre, K. Cho, and Y. Bengio. How to construct deep\\nrecurrent neural networks. In Proceedings of International Conference\\non Learning Representations (ICLR). 2014.\\n\\n[280] R. Pascanu, T. Mikolov, and Y. Bengio. On the di\\ufb03culty of training\\nrecurrent neural networks. In Proceedings of International Conference\\non Machine Learning (ICML). 2013.\\n\\n[281] J. Peng, L. Bo, and J. Xu. Conditional neural \\ufb01elds. In Proceedings of\\n\\nNeural Information Processing Systems (NIPS). 2009.\\n\\nFull text available at: http://dx.doi.org/10.1561/2000000039\\x0c178\\n\\nReferences\\n\\n[282] P. Picone, S. Pike, R. Regan, T. Kamm, J. bridle, L. Deng, Z. Ma,\\nH. Richards, and M. Schuster.\\nInitial evaluation of hidden dynamic\\nmodels on conversational speech. In Proceedings of International Con-\\nference on Acoustics Speech and Signal Processing (ICASSP). 1999.\\n\\n[283] J. Pinto, S. Garimella, M. Magimai-Doss, H. Hermansky, and\\nH. Bourlard. Analysis of MLP-based hierarchical phone posterior prob-\\nability estimators. IEEE Transactions on Audio, Speech, and Language\\nProcessing, 19(2), February 2011.\\n\\n[284] C. Plahl, T. Sainath, B. Ramabhadran, and D. Nahamoo. Improved\\npre-training of deep belief networks using sparse encoding symmet-\\nric machines. In Proceedings of International Conference on Acoustics\\nSpeech and Signal Processing (ICASSP). 2012.\\n\\n[285] C. Plahl, R. Schl\\xfcter, and H. Ney. Hierarchical bottleneck features for\\n\\nLVCSR. In Proceedings of Interspeech. 2010.\\n\\n[286] T. Plate. Holographic reduced representations. IEEE Transactions on\\n\\nNeural Networks, 6(3):623\\u2013641, May 1995.\\n\\n[287] T. Poggio. How the brain might work: The role of information and\\nlearning in understanding and replicating intelligence. In G. Jacovitt,\\nA. Pettorossi, R. Consolo, and V. Senni, editors, Information: Science\\nand Technology for the New Century, pages 45\\u201361. Lateran University\\nPress, 2007.\\n\\n[288] J. Pollack. Recursive distributed representations. Arti\\ufb01cial Intelligence,\\n\\n46:77\\u2013105, 1990.\\n\\n[289] H. Poon and P. Domingos. Sum-product networks: A new deep archi-\\ntecture. In Proceedings of Uncertainty in Arti\\ufb01cial Intelligence. 2011.\\n[290] D. Povey and P. Woodland. Minimum phone error and I-smoothing\\nfor improved discriminative training.\\nIn Proceedings of International\\nConference on Acoustics Speech and Signal Processing (ICASSP). 2002.\\n[291] R. Prabhavalkar and E. Fosler-Lussier. Backpropagation training for\\nmultilayer conditional random \\ufb01eld based phone recognition. In Pro-\\nceedings of International Conference on Acoustics Speech and Signal\\nProcessing (ICASSP). 2010.\\n\\n[292] A. Prince and P. Smolensky. Optimality: From neural networks to uni-\\n\\nversal grammar. Science, 275:1604\\u20131610, 1997.\\n\\n[293] L. Rabiner. A tutorial on hidden markov models and selected applica-\\ntions in speech recognition. In Proceedings of the IEEE, pages 257\\u2013286.\\n1989.\\n\\nFull text available at: http://dx.doi.org/10.1561/2000000039\\x0cReferences\\n\\n179\\n\\n[294] M. Ranzato, Y. Boureau, and Y. LeCun. Sparse feature learning for\\ndeep belief networks. In Proceedings of Neural Information Processing\\nSystems (NIPS). 2007.\\n\\n[295] M. Ranzato, S. Chopra, Y. LeCun, and F.-J. Huang. Energy-based\\nmodels in document recognition and computer vision.\\nIn Proceed-\\nings of International Conference on Document Analysis and Recognition\\n(ICDAR). 2007.\\n\\n[296] M. Ranzato and G. Hinton. Modeling pixel means and covariances using\\nfactorized third-order boltzmann machines. In Proceedings of Computer\\nVision and Pattern Recognition (CVPR). 2010.\\n\\n[297] M. Ranzato, C. Poultney, S. Chopra, and Y. LeCun. E\\ufb03cient learning\\nof sparse representations with an energy-based model. In Proceedings\\nof Neural Information Processing Systems (NIPS). 2006.\\n\\n[298] M. Ranzato, J. Susskind, V. Mnih, and G. Hinton. On deep generative\\nmodels with applications to recognition. In Proceedings of Computer\\nVision and Pattern Recognition (CVPR). 2011.\\n\\n[299] C. Rathinavalu and L. Deng. Construction of state-dependent dynamic\\nparameters by maximum likelihood: Applications to speech recognition.\\nSignal Processing, 55(2):149\\u2013165, 1997.\\n\\n[300] S. Rennie, K. Fouset, and P. Dognin. Factorial hidden restricted boltz-\\nmann machines for noise robust speech recognition.\\nIn Proceedings\\nof International Conference on Acoustics Speech and Signal Processing\\n(ICASSP). 2012.\\n\\n[301] S. Rennie, H. Hershey, and P. Olsen. Single-channel multi-talker speech\\nrecognition \\u2014 graphical modeling approaches. IEEE Signal Processing\\nMagazine, 33:66\\u201380, 2010.\\n\\n[302] M. Riedmiller and H. Braun. A direct adaptive method for faster back-\\nIn Proceedings of the\\n\\npropagation learning: The RPROP algorithm.\\nIEEE International Conference on Neural Networks. 1993.\\n\\n[303] S. Rifai, P. Vincent, X. Muller, X. Glorot, and Y. Bengio. Contractive\\nautoencoders: Explicit invariance during feature extraction. In Proceed-\\nings of International Conference on Machine Learning (ICML), pages\\n833\\u2013840. 2011.\\n\\n[304] A. Robinson. An application of recurrent nets to phone probability\\nestimation. IEEE Transactions on Neural Networks, 5:298\\u2013305, 1994.\\n[305] T. Sainath, L. Horesh, B. Kingsbury, A. Aravkin, and B. Ramabhad-\\nran. Accelerating hessian-free optimization for deep neural networks by\\nimplicit pre-conditioning and sampling. arXiv: 1309.1508v3, 2013.\\n\\nFull text available at: http://dx.doi.org/10.1561/2000000039\\x0c180\\n\\nReferences\\n\\n[306] T. Sainath, B. Kingsbury, A. Mohamed, G. Dahl, G. Saon, H. Soltau,\\nT. Beran, A. Aravkin, and B. Ramabhadran.\\nImprovements to deep\\nconvolutional neural networks for LVCSR. In Proceedings of the Auto-\\nmatic Speech Recognition and Understanding Workshop (ASRU). 2013.\\n[307] T. Sainath, B. Kingsbury, A. Mohamed, and B. Ramabhadran. Learn-\\ning \\ufb01lter banks within a deep neural network framework. In Proceed-\\nings of The Automatic Speech Recognition and Understanding Workshop\\n(ASRU). 2013.\\n\\n[308] T. Sainath, B. Kingsbury, and B. Ramabhadran. Autoencoder bottle-\\nneck features using deep belief networks. In Proceedings of International\\nConference on Acoustics Speech and Signal Processing (ICASSP). 2012.\\n[309] T. Sainath, B. Kingsbury, B. Ramabhadran, P. Novak, and\\nA. Mohamed. Making deep belief networks e\\ufb00ective for large vocab-\\nulary continuous speech recognition. In Proceedings of the Automatic\\nSpeech Recognition and Understanding Workshop (ASRU). 2011.\\n\\n[310] T. Sainath, B. Kingsbury, V. Sindhwani, E. Arisoy, and B. Ramabhad-\\nran. Low-rank matrix factorization for deep neural network training\\nwith high-dimensional output targets. In Proceedings of International\\nConference on Acoustics Speech and Signal Processing (ICASSP). 2013.\\n[311] T. Sainath, B. Kingsbury, H. Soltau, and B. Ramabhadran. Optimiza-\\ntion techniques to improve training speed of deep neural networks for\\nlarge speech tasks. IEEE Transactions on Audio, Speech, and Language\\nProcessing, 21(11):2267\\u20132276, November 2013.\\n\\n[312] T. Sainath, A. Mohamed, B. Kingsbury, and B. Ramabhadran. Con-\\nvolutional neural networks for LVCSR. In Proceedings of International\\nConference on Acoustics Speech and Signal Processing (ICASSP). 2013.\\n[313] T. Sainath, B. Ramabhadran, M. Picheny, D. Nahamoo, and\\nD. Kanevsky. Exemplar-based sparse representation features: From\\nTIMIT to LVCSR. IEEE Transactions on Speech and Audio Processing,\\nNovember 2011.\\n\\n[314] R. Salakhutdinov and G. Hinton. Semantic hashing. In Proceedings of\\nSpecial Interest Group on Information Retrieval (SIGIR) Workshop on\\nInformation Retrieval and Applications of Graphical Models. 2007.\\n\\n[315] R. Salakhutdinov and G. Hinton. Deep boltzmann machines. In Pro-\\n\\nceedings of Arti\\ufb01cial Intelligence and Statistics (AISTATS). 2009.\\n\\n[316] R. Salakhutdinov and G. Hinton. A better way to pretrain deep boltz-\\nmann machines. In Proceedings of Neural Information Processing Sys-\\ntems (NIPS). 2012.\\n\\nFull text available at: http://dx.doi.org/10.1561/2000000039\\x0cReferences\\n\\n181\\n\\n[317] G. Saon, H. Soltau, D. Nahamoo, and M. Picheny. Speaker adaptation\\nof neural network acoustic models using i-vectors. In Proceedings of the\\nAutomatic Speech Recognition and Understanding Workshop (ASRU).\\n2013.\\n\\n[318] R. Sarikaya, G. Hinton, and B. Ramabhadran. Deep belief nets for nat-\\nural language call-routing. In Proceedings of International Conference\\non Acoustics Speech and Signal Processing (ICASSP), pages 5680\\u20135683.\\n2011.\\n\\n[319] E. Schmidt and Y. Kim. Learning emotion-based acoustic features with\\ndeep belief networks. In Proceedings IEEE of Signal Processing to Audio\\nand Acoustics. 2011.\\n\\n[320] H. Schwenk. Continuous space translation models for phrase-based sta-\\ntistical machine translation. In Proceedings of Computional Linguistics.\\n2012.\\n\\n[321] H. Schwenk, A. Rousseau, and A. Mohammed. Large, pruned or contin-\\nuous space language models on a gpu for statistical machine translation.\\nIn Proceedings of the Joint Human Language Technology Conference\\nand the North American Chapter of the Association of Computational\\nLinguistics (HLT-NAACL) 2012 Workshop on the future of language\\nmodeling for Human Language Technology (HLT), pages 11\\u201319.\\n\\n[322] F. Seide, H. Fu, J. Droppo, G. Li, and D. Yu. On parallelizability of\\nstochastic gradient descent for speech DNNs. In Proceedings of Interna-\\ntional Conference on Acoustics Speech and Signal Processing (ICASSP).\\n2014.\\n\\n[323] F. Seide, G. Li, X. Chen, and D. Yu. Feature engineering in context-\\ndependent deep neural networks for conversational speech transcription.\\nIn Proceedings of the Automatic Speech Recognition and Understanding\\nWorkshop (ASRU), pages 24\\u201329. 2011.\\n\\n[324] F. Seide, G. Li, and D. Yu. Conversational speech transcription using\\ncontext-dependent deep neural networks. In Proceedings of Interspeech,\\npages 437\\u2013440. 2011.\\n\\n[325] M. Seltzer, D. Yu, and E. Wang. An investigation of deep neural net-\\nworks for noise robust speech recognition. In Proceedings of Interna-\\ntional Conference on Acoustics Speech and Signal Processing (ICASSP).\\n2013.\\n\\n[326] M. Shannon, H. Zen, and W. Byrne. Autoregressive models for statisti-\\ncal parametric speech synthesis. IEEE Transactions on Audio, Speech,\\nLanguage Processing, 21(3):587\\u2013597, 2013.\\n\\nFull text available at: http://dx.doi.org/10.1561/2000000039\\x0c182\\n\\nReferences\\n\\n[327] H. Sheikhzadeh and L. Deng. Waveform-based speech recognition using\\nhidden \\ufb01lter models: Parameter selection and sensitivity to power nor-\\nmalization.\\nIEEE Transactions on on Speech and Audio Processing\\n(ICASSP), 2:80\\u201391, 1994.\\n\\n[328] Y. Shen, X. He, J. Gao, L. Deng, and G. Mesnil. Learning semantic\\nrepresentations using convolutional neural networks for web search. In\\nProceedings World Wide Web. 2014.\\n\\n[329] K. Simonyan, A. Vedaldi, and A. Zisserman. Deep \\ufb01sher networks for\\nlarge-scale image classi\\ufb01cation. In Proceedings of Neural Information\\nProcessing Systems (NIPS). 2013.\\n\\n[330] M. Siniscalchi, J. Li, and C. Lee. Hermitian polynomial for speaker\\nadaptation of connectionist speech recognition systems. IEEE Trans-\\nactions on Audio, Speech, and Language Processing, 21(10):2152\\u20132161,\\n2013a.\\n\\n[331] M. Siniscalchi, T. Svendsen, and C.-H. Lee. A bottom-up modular\\nsearch approach to large vocabulary continuous speech recognition.\\nIEEE Transactions on Audio, Speech, Language Processing, 21, 2013.\\n[332] M. Siniscalchi, D. Yu, L. Deng, and C.-H. Lee. Exploiting deep neu-\\nral networks for detection-based speech recognition. Neurocomputing,\\n106:148\\u2013157, 2013.\\n\\n[333] M. Siniscalchi, D. Yu, L. Deng, and C.-H. Lee. Speech recognition using\\nIEEE Signal\\n\\nlong-span temporal patterns in a deep network model.\\nProcessing Letters, 20(3):201\\u2013204, March 2013.\\n\\n[334] G. Sivaram and H. Hermansky.\\n\\nphoneme recognition.\\nguage Processing, 20(1), January 2012.\\n\\nSparse multilayer perceptrons for\\nIEEE Transactions on Audio, Speech, & Lan-\\n\\n[335] P. Smolensky. Tensor product variable binding and the representation\\nof symbolic structures in connectionist systems. Arti\\ufb01cial Intelligence,\\n46:159\\u2013216, 1990.\\n\\n[336] P. Smolensky and G. Legendre. The Harmonic Mind \\u2014 From Neu-\\nral Computation to Optimality-Theoretic Grammar. The MIT Press,\\nCambridge, MA, 2006.\\n\\n[337] J. Snoek, H. Larochelle, and R. Adams. Practical bayesian optimization\\nof machine learning algorithms. In Proceedings of Neural Information\\nProcessing Systems (NIPS). 2012.\\n\\n[338] R. Socher. New directions in deep learning: Structured models, tasks,\\nand datasets. Neural Information Processing Systems (NIPS) Workshop\\non Deep Learning and Unsupervised Feature Learning, 2012.\\n\\nFull text available at: http://dx.doi.org/10.1561/2000000039\\x0cReferences\\n\\n183\\n\\n[339] R. Socher, Y. Bengio, and C. Manning. Deep learning for NLP.\\nTutorial at Association of Computational Logistics (ACL), 2012, and\\nNorth American Chapter of the Association of Computational Linguis-\\ntics (NAACL), 2013. http://www.socher.org/index.php/DeepLearning\\nTutorial.\\n\\n[340] R. Socher, D. Chen, C. Manning, and A. Ng. Reasoning with neural\\ntensor networks for knowledge base completion. In Proceedings of Neural\\nInformation Processing Systems (NIPS). 2013.\\n\\n[341] R. Socher and L. Fei-Fei. Connecting modalities: Semi-supervised\\nsegmentation and annotation of images using unaligned text corpora.\\nIn Proceedings of Computer Vision and Pattern Recognition (CVPR).\\n2010.\\n\\n[342] R. Socher, M. Ganjoo, H. Sridhar, O. Bastani, C. Manning, and A. Ng.\\nZero-shot learning through cross-modal transfer. In Proceedings of Neu-\\nral Information Processing Systems (NIPS). 2013b.\\n\\n[343] R. Socher, Q. Le, C. Manning, and A. Ng. Grounded compositional\\nsemantics for \\ufb01nding and describing images with sentences. Neu-\\nral Information Processing Systems (NIPS) Deep Learning Workshop,\\n2013c.\\n\\n[344] R. Socher, C. Lin, A. Ng, and C. Manning. Parsing natural scenes\\nand natural language with recursive neural networks. In Proceedings of\\nInternational Conference on Machine Learning (ICML). 2011.\\n\\n[345] R. Socher, J. Pennington, E. Huang, A. Ng, and C. Manning. Dynamic\\npooling and unfolding recursive autoencoders for paraphrase detection.\\nIn Proceedings of Neural Information Processing Systems (NIPS). 2011.\\n[346] R. Socher, J. Pennington, E. Huang, A. Ng, and C. Manning. Semi-\\nsupervised recursive autoencoders for predicting sentiment distribu-\\ntions. In Proceedings of Empirical Methods in Natural Language Pro-\\ncessing (EMNLP). 2011.\\n\\n[347] R. Socher, A. Perelygin, J. Wu, J. Chuang, C. Manning, A. Ng, and\\nC. Potts. Recursive deep models for semantic compositionality over a\\nsentiment treebank.\\nIn Proceedings of Empirical Methods in Natural\\nLanguage Processing (EMNLP). 2013.\\n\\n[348] N. Srivastava and R. Salakhutdinov. Multimodal learning with deep\\nboltzmann machines. In Proceedings of Neural Information Processing\\nSystems (NIPS). 2012.\\n\\nFull text available at: http://dx.doi.org/10.1561/2000000039\\x0c184\\n\\nReferences\\n\\n[349] N. Srivastava and R. Salakhutdinov. Discriminative transfer learning\\nwith tree-based priors. In Proceedings of Neural Information Processing\\nSystems (NIPS). 2013.\\n\\n[350] R. Srivastava, J. Masci, S. Kazerounian, F. Gomez, and J. Schmidhuber.\\nCompete to compute. In Proceedings of Neural Information Processing\\nSystems (NIPS). 2013.\\n\\n[351] T. Stafylakis, P. Kenny, M. Senoussaoui, and P. Dumouchel. Prelimi-\\nnary investigation of boltzmann machine classi\\ufb01ers for speaker recogni-\\ntion. In Proceedings of Odyssey, pages 109\\u2013116. 2012.\\n\\n[352] V. Stoyanov, A. Ropson, and J. Eisner. Empirical risk minimization of\\ngraphical model parameters given approximate inference, decoding, and\\nmodel structure. In Proceedings of Arti\\ufb01cial Intelligence and Statistics\\n(AISTATS). 2011.\\n\\n[353] H. Su, G. Li, D. Yu, and F. Seide. Error back propagation for sequence\\ntraining of context-dependent deep networks for conversational speech\\ntranscription. In Proceedings of International Conference on Acoustics\\nSpeech and Signal Processing (ICASSP). 2013.\\n\\n[354] A. Subramanya, L. Deng, Z. Liu, and Z. Zhang. Multi-sensory speech\\nprocessing: Incorporating automatically extracted hidden dynamic\\ninformation. In Proceedings of IEEE International Conference on Mul-\\ntimedia & Expo (ICME). Amsterdam, July 2005.\\n\\n[355] J. Sun and L. Deng. An overlapping-feature based phonological model\\nincorporating linguistic constraints: Applications to speech recognition.\\nJournal on Acoustical Society of America, 111(2):1086\\u20131101, 2002.\\n\\n[356] I. Sutskever. Training recurrent neural networks. Ph.D. Thesis, Univer-\\n\\nsity of Toronto, 2013.\\n\\n[357] I. Sutskever, J. Martens, and G. Hinton. Generating text with recurrent\\nneural networks. In Proceedings of International Conference on Machine\\nLearning (ICML). 2011.\\n\\n[358] Y. Tang and C. Eliasmith. Deep networks for robust visual recogni-\\ntion. In Proceedings of International Conference on Machine Learning\\n(ICML). 2010.\\n\\n[359] Y. Tang and R. Salakhutdinov. Learning Stochastic Feedforward Neural\\n\\nNetworks. NIPS, 2013.\\n\\n[360] A. Tarralba, R. Fergus, and Y. Weiss. Small codes and large image\\ndatabases for recognition. In Proceedings of Computer Vision and Pat-\\ntern Recognition (CVPR). 2008.\\n\\nFull text available at: http://dx.doi.org/10.1561/2000000039\\x0cReferences\\n\\n185\\n\\n[361] G. Taylor, G. E. Hinton, and S. Roweis. Modeling human motion using\\nbinary latent variables. In Proceedings of Neural Information Processing\\nSystems (NIPS). 2007.\\n\\n[362] S. Thomas, M. Seltzer, K. Church, and H. Hermansky. Deep neural\\nnetwork features and semi-supervised training for low resource speech\\nrecognition. In Proceedings of Interspeech. 2013.\\n\\n[363] T. Tieleman. Training restricted boltzmann machines using approx-\\nIn Proceedings of International\\n\\nimations to the likelihood gradient.\\nConference on Machine Learning (ICML). 2008.\\n\\n[364] K. Tokuda, Y. Nankaku, T. Toda, H. Zen, H. Yamagishi, and K. Oura.\\nSpeech synthesis based on hidden markov models. Proceedings of the\\nIEEE, 101(5):1234\\u20131252, 2013.\\n\\n[365] F. Triefenbach, A. Jalalvand, K. Demuynck, and J.-P. Martens. Acoustic\\nIEEE Transactions on Audio,\\n\\nmodeling with hierarchical reservoirs.\\nSpeech, and Language Processing, 21(11):2439\\u20132450, November 2013.\\n\\n[366] G. Tur, L. Deng, D. Hakkani-T\\xfcr, and X. He. Towards deep under-\\nstanding: Deep convex networks for semantic utterance classi\\ufb01cation. In\\nProceedings of International Conference on Acoustics Speech and Signal\\nProcessing (ICASSP). 2012.\\n\\n[367] J. Turian, L. Ratinov, and Y. Bengio. Word representations: A simple\\nIn Proceedings of\\n\\nand general method for semi-supervised learning.\\nAssociation for Computational Linguistics (ACL). 2010.\\n\\n[368] Z. T\\xfcske, M. Sundermeyer, R. Schl\\xfcter, and H. Ney. Context-dependent\\nMLPs for LVCSR: TANDEM, hybrid or both? In Proceedings of Inter-\\nspeech. 2012.\\n\\n[369] B. Uria, S. Renals, and K. Richmond. A deep neural network for\\nacoustic-articulatory speech inversion. Neural Information Processing\\nSystems (NIPS) Workshop on Deep Learning and Unsupervised Feature\\nLearning, 2011.\\n\\n[370] R. van Dalen and M. Gales. Extended VTS for noise-robust speech\\nrecognition. IEEE Transactions on Audio, Speech, and Language Pro-\\ncessing, 19(4):733\\u2013743, 2011.\\n\\n[371] A. van den Oord, S. Dieleman, and B. Schrauwen. Deep content-based\\nmusic recommendation. In Proceedings of Neural Information Process-\\ning Systems (NIPS). 2013.\\n\\n[372] V. Vasilakakis, S. Cumani, and P. Laface. Speaker recognition by means\\nIn Proceedings of Biometric Technologies in\\n\\nof deep belief networks.\\nForensic Science. 2013.\\n\\nFull text available at: http://dx.doi.org/10.1561/2000000039\\x0c186\\n\\nReferences\\n\\n[373] K. Vesely, A. Ghoshal, L. Burget, and D. Povey. Sequence-discriminative\\ntraining of deep neural networks. In Proceedings of Interspeech. 2013.\\n[374] K. Vesely, M. Hannemann, and L. Burget. Semi-supervised training of\\ndeep neural networks. In Proceedings of the Automatic Speech Recogni-\\ntion and Understanding Workshop (ASRU). 2013.\\n\\n[375] P. Vincent. A connection between score matching and denoising autoen-\\n\\ncoder. Neural Computation, 23(7):1661\\u20131674, 2011.\\n\\n[376] P. Vincent, H. Larochelle, I. Lajoie, Y. Bengio, and P. Manzagol.\\nStacked denoising autoencoders: Learning useful representations in a\\ndeep network with a local denoising criterion. Journal of Machine\\nLearning Research, 11:3371\\u20133408, 2010.\\n\\n[377] O. Vinyals, Y. Jia, L. Deng, and T. Darrell. Learning with recursive\\nperceptual representations. In Proceedings of Neural Information Pro-\\ncessing Systems (NIPS). 2012.\\n\\n[378] O. Vinyals and D. Povey. Krylov subspace descent for deep learning. In\\nProceedings of Arti\\ufb01cial Intelligence and Statistics (AISTATS). 2012.\\n[379] O. Vinyals and S. Ravuri. Comparing multilayer perceptron to deep\\nbelief network tandem features for robust ASR.\\nIn Proceedings of\\nInternational Conference on Acoustics Speech and Signal Processing\\n(ICASSP). 2011.\\n\\n[380] O. Vinyals, S. Ravuri, and D. Povey. Revisiting recurrent neural net-\\nworks for robust ASR. In Proceedings of International Conference on\\nAcoustics Speech and Signal Processing (ICASSP). 2012.\\n\\n[381] S. Wager, S. Wang, and P. Liang. Dropout training as adaptive reg-\\nularization. In Proceedings of Neural Information Processing Systems\\n(NIPS). 2013.\\n\\n[382] A. Waibel, T. Hanazawa, G. Hinton, K. Shikano, and K. Lang. Phoneme\\nrecognition using time-delay neural networks. IEEE Transactions on\\nAcoustical Speech, and Signal Processing, 37:328\\u2013339, 1989.\\n\\n[383] G. Wang and K. Sim. Context-dependent modelling of deep neural\\nnetwork using logistic regression. In Proceedings of the Automatic Speech\\nRecognition and Understanding Workshop (ASRU). 2013.\\n\\n[384] G. Wang and K. Sim. Regression-based context-dependent modeling\\nof deep neural networks for speech recognition. IEEE/Association for\\nComputing Machinery (ACM) Transactions on Audio, Speech, and Lan-\\nguage Processing, 2014.\\n\\nFull text available at: http://dx.doi.org/10.1561/2000000039\\x0cReferences\\n\\n187\\n\\n[385] D. Warde-Farley, I. Goodfellow, A. Courville, and Y. Bengi. An empir-\\nical analysis of dropout in piecewise linear networks. In Proceedings of\\nInternational Conference on Learning Representations (ICLR). 2014.\\n\\n[386] M. Welling, M. Rosen-Zvi, and G. Hinton. Exponential family harmo-\\nniums with an application to information retrieval. In Proceedings of\\nNeural Information Processing Systems (NIPS). 2005.\\n\\n[387] C. Weng, D. Yu, M. Seltzer, and J. Droppo. Single-channel mixed speech\\nrecognition using deep neural networks. In Proceedings of International\\nConference on Acoustics Speech and Signal Processing (ICASSP). 2014.\\n[388] J. Weston, S. Bengio, and N. Usunier. Large scale image annotation:\\nLearning to rank with joint word-image embeddings. Machine Learning,\\n81(1):21\\u201335, 2010.\\n\\n[389] J. Weston, S. Bengio, and N. Usunier. Wsabie: Scaling up to large\\nIn Proceedings of International Joint\\n\\nvocabulary image annotation.\\nConference on Arti\\ufb01cial Intelligence (IJCAI). 2011.\\n\\n[390] S. Wiesler, J. Li, and J. Xue. Investigations on hessian-free optimization\\nfor cross-entropy training of deep neural networks. In Proceedings of\\nInterspeech. 2013.\\n\\n[391] M. Wohlmayr, M. Stark, and F. Pernkopf. A probabilistic interac-\\ntion model for multi-pitch tracking with factorial hidden markov model.\\nIEEE Transactions on Audio, Speech, and Language Processing, 19(4),\\nMay 2011.\\n\\n[392] D. Wolpert. Stacked generalization. Neural Networks, 5(2):241\\u2013259,\\n\\n1992.\\n\\n[393] S. J. Wright, D. Kanevsky, L. Deng, X. He, G. Heigold, and H. Li.\\nOptimization algorithms and applications for speech and language pro-\\ncessing. IEEE Transactions on Audio, Speech, and Language Processing,\\n21(11):2231\\u20132243, November 2013.\\n\\n[394] L. Xiao and L. Deng. A geometric perspective of large-margin training\\nof gaussian models. IEEE Signal Processing Magazine, 27(6):118\\u2013123,\\nNovember 2010.\\n\\n[395] X. Xie and S. Seung. Equivalence of backpropagation and contrastive\\nhebbian learning in a layered network. Neural computation, 15:441\\u2013454,\\n2003.\\n\\n[396] Y. Xu, J. Du, L. Dai, and C. Lee. An experimental study on speech\\nenhancement based on deep neural networks. IEEE Signal Processing\\nLetters, 21(1):65\\u201368, 2014.\\n\\nFull text available at: http://dx.doi.org/10.1561/2000000039\\x0c188\\n\\nReferences\\n\\n[397] J. Xue, J. Li, and Y. Gong. Restructuring of deep neural network\\nacoustic models with singular value decomposition. In Proceedings of\\nInterspeech. 2013.\\n\\n[398] S. Yamin, L. Deng, Y. Wang, and A. Acero. An integrative and discrimi-\\nnative technique for spoken utterance classi\\ufb01cation. IEEE Transactions\\non Audio, Speech, and Language Processing, 16:1207\\u20131214, 2008.\\n\\n[399] Z. Yan, Q. Huo, and J. Xu. A scalable approach to using DNN-derived\\nfeatures in GMM-HMM based acoustic modeling for LVCSR. In Pro-\\nceedings of Interspeech. 2013.\\n\\n[400] D. Yang and S. Furui. Combining a two-step CRF model and a joint\\nsource-channel model for machine transliteration.\\nIn Proceedings of\\nAssociation for Computational Linguistics (ACL), pages 275\\u2013280. 2010.\\n[401] K. Yao, D. Yu, L. Deng, and Y. Gong. A fast maximum likelihood non-\\nlinear feature transformation method for GMM-HMM speaker adapta-\\ntion. Neurocomputing, 2013a.\\n\\n[402] K. Yao, D. Yu, F. Seide, H. Su, L. Deng, and Y. Gong. Adaptation of\\ncontext-dependent deep neural networks for automatic speech recogni-\\ntion. In Proceedings of International Conference on Acoustics Speech\\nand Signal Processing (ICASSP). 2012.\\n\\n[403] K. Yao, G. Zweig, M. Hwang, Y. Shi, and D. Yu. Recurrent neural\\nIn Proceedings of Interspeech.\\n\\nnetworks for language understanding.\\n2013.\\n\\n[404] T. Yoshioka and T. Nakatani. Noise model transfer: Novel approach to\\nrobustness against nonstationary noise. IEEE Transactions on Audio,\\nSpeech, and Language Processing, 21(10):2182\\u20132192, 2013.\\n\\n[405] T. Yoshioka, A. Ragni, and M. Gales.\\n\\nInvestigation of unsupervised\\nadaptation of DNN acoustic models with \\ufb01lter bank input.\\nIn Pro-\\nceedings of International Conference on Acoustics Speech and Signal\\nProcessing (ICASSP). 2013.\\n\\n[406] L. Younes. On the convergence of markovian stochastic algorithms with\\nrapidly decreasing ergodicity rates. Stochastics and Stochastic Reports,\\n65(3):177\\u2013228, 1999.\\n\\n[407] D. Yu, X. Chen, and L. Deng. Factorized deep neural networks for adap-\\ntive speech recognition. International Workshop on Statistical Machine\\nLearning for Speech Processing, March 2012b.\\n\\n[408] D. Yu, D. Deng, and S. Wang. Learning in the deep-structured con-\\nditional random \\ufb01elds. Neural Information Processing Systems (NIPS)\\n2009 Workshop on Deep Learning for Speech Recognition and Related\\nApplications, 2009.\\n\\nFull text available at: http://dx.doi.org/10.1561/2000000039\\x0cReferences\\n\\n189\\n\\n[409] D. Yu and L. Deng. Solving nonlinear estimation problems using splines.\\n\\nIEEE Signal Processing Magazine, 26(4):86\\u201390, July 2009.\\n\\n[410] D. Yu and L. Deng. Deep-structured hidden conditional random \\ufb01elds\\nfor phonetic recognition. In Proceedings of Interspeech. September 2010.\\n[411] D. Yu and L. Deng. Accelerated parallelizable neural networks learning\\nalgorithms for speech recognition. In Proceedings of Interspeech. 2011.\\n[412] D. Yu and L. Deng. Deep learning and its applications to signal and\\ninformation processing. IEEE Signal Processing Magazine, pages 145\\u2013\\n154, January 2011.\\n\\n[413] D. Yu and L. Deng. E\\ufb03cient and e\\ufb00ective algorithms for training single-\\nhidden-layer neural networks. Pattern Recognition Letters, 33:554\\u2013558,\\n2012.\\n\\n[414] D. Yu, L. Deng, and G. E. Dahl. Roles of pre-training and \\ufb01ne-tuning in\\ncontext-dependent DBN-HMMs for real-world speech recognition. Neu-\\nral Information Processing Systems (NIPS) 2010 Workshop on Deep\\nLearning and Unsupervised Feature Learning, December 2010.\\n\\n[415] D. Yu, L. Deng, J. Droppo, J. Wu, Y. Gong, and A. Acero. Robust\\nspeech recognition using cepstral minimum-mean-square-error noise\\nsuppressor. IEEE Transactions on Audio, Speech, and Language Pro-\\ncessing, 16(5), July 2008.\\n\\n[416] D. Yu, L. Deng, Y. Gong, and A. Acero. A novel framework and training\\nalgorithm for variable-parameter hidden markov models. IEEE Trans-\\nactions on Audio, Speech and Language Processing, 17(7):1348\\u20131360,\\n2009.\\n\\n[417] D. Yu, L. Deng, X. He, and A. Acero. Large-margin minimum clas-\\nsi\\ufb01cation error training: A theoretical risk minimization perspective.\\nComputer Speech and Language, 22(4):415\\u2013429, October 2008.\\n\\n[418] D. Yu, L. Deng, X. He, and X. Acero. Large-margin minimum classi-\\n\\ufb01cation error training for large-scale speech recognition tasks. In Pro-\\nceedings of International Conference on Acoustics Speech and Signal\\nProcessing (ICASSP). 2007.\\n\\n[419] D. Yu, L. Deng, G. Li, and F. Seide. Discriminative pretraining of deep\\n\\nneural networks. U.S. Patent Filing, November 2011.\\n\\n[420] D. Yu, L. Deng, P. Liu, J. Wu, Y. Gong, and A. Acero. Cross-lingual\\nspeech recognition under runtime resource constraints. In Proceedings\\nof International Conference on Acoustics Speech and Signal Processing\\n(ICASSP). 2009b.\\n\\nFull text available at: http://dx.doi.org/10.1561/2000000039\\x0c190\\n\\nReferences\\n\\n[421] D. Yu, L. Deng, and F. Seide. Large vocabulary speech recognition using\\n\\ndeep tensor neural networks. In Proceedings of Interspeech. 2012c.\\n\\n[422] D. Yu, L. Deng, and F. Seide. The deep tensor neural network with\\napplications to large vocabulary speech recognition. IEEE Transactions\\non Audio, Speech, and Language Processing, 21(2):388\\u2013396, 2013.\\n\\n[423] D. Yu, J.-Y. Li, and L. Deng. Calibration of con\\ufb01dence measures in\\nspeech recognition. IEEE Transactions on Audio, Speech and Language,\\n19:2461\\u20132473, 2010.\\n\\n[424] D. Yu, F. Seide, G. Li, and L. Deng. Exploiting sparseness in deep\\nneural networks for large vocabulary speech recognition. In Proceedings\\nof International Conference on Acoustics Speech and Signal Processing\\n(ICASSP). 2012.\\n\\n[425] D. Yu and M. Seltzer. Improved bottleneck features using pre-trained\\n\\ndeep neural networks. In Proceedings of Interspeech. 2011.\\n\\n[426] D. Yu, M. Seltzer, J. Li, J.-T. Huang, and F. Seide. Feature learning in\\ndeep neural networks \\u2014 studies on speech recognition. In Proceedings\\nof International Conference on Learning Representations (ICLR). 2013.\\n[427] D. Yu, S. Siniscalchi, L. Deng, and C. Lee. Boosting attribute and\\nphone estimation accuracies with deep neural networks for detection-\\nbased speech recognition. In Proceedings of International Conference\\non Acoustics Speech and Signal Processing (ICASSP). 2012.\\n\\n[428] D. Yu, S. Wang, and L. Deng. Sequential labeling using deep-structured\\nconditional random \\ufb01elds. Journal of Selected Topics in Signal Process-\\ning, 4:965\\u2013973, 2010.\\n\\n[429] D. Yu, S. Wang, Z. Karam, and L. Deng. Language recognition using\\ndeep-structured conditional random \\ufb01elds. In Proceedings of Interna-\\ntional Conference on Acoustics Speech and Signal Processing (ICASSP),\\npages 5030\\u20135033. 2010.\\n\\n[430] D. Yu, K. Yao, H. Su, G. Li, and F. Seide. KL-divergence regularized\\ndeep neural network adaptation for improved large vocabulary speech\\nrecognition.\\nIn Proceedings of International Conference on Acoustics\\nSpeech and Signal Processing (ICASSP). 2013.\\n\\n[431] K. Yu, M. Gales, and P. Woodland. Unsupervised adaptation with dis-\\ncriminative mapping transforms. IEEE Transactions on Audio, Speech,\\nand Language Processing, 17(4):714\\u2013723, 2009.\\n\\n[432] K. Yu, Y. Lin, and H. La\\ufb00erty. Learning image representations from\\nthe pixel level via hierarchical sparse coding. In Proceedings Computer\\nVision and Pattern Recognition (CVPR). 2011.\\n\\nFull text available at: http://dx.doi.org/10.1561/2000000039\\x0cReferences\\n\\n191\\n\\n[433] F. Zamora-Mart\\xednez, M. Castro-Bleda, and S. Espa\\xf1a-Boquera. Fast\\nevaluation of connectionist language models. International Conference\\non Arti\\ufb01cial Neural Networks, pages 144\\u2013151, 2009.\\n\\n[434] M. Zeiler. Hierarchical convolutional deep learning in computer vision.\\n\\nPh.D. Thesis, New York University, January 2014.\\n\\n[435] M. Zeiler and R. Fergus. Stochastic pooling for regularization of deep\\nconvolutional neural networks. In Proceedings of International Confer-\\nence on Learning Representations (ICLR). 2013.\\n\\n[436] M. Zeiler and R. Fergus. Visualizing and understanding convolutional\\n\\nnetworks. arXiv:1311.2901, pages 1\\u201311, 2013.\\n\\n[437] M. Zeiler, G. Taylor, and R. Fergus. Adaptive deconvolutional networks\\nfor mid and high level feature learning. In Proceedings of International\\nConference on Computer vision (ICCV). 2011.\\n\\n[438] H. Zen, M. Gales, J. F. Nankaku, and Y. K. Tokuda. Product of\\nexperts for statistical parametric speech synthesis. IEEE Transactions\\non Audio, Speech, and Language Processing, 20(3):794\\u2013805, March 2012.\\n[439] H. Zen, Y. Nankaku, and K. Tokuda. Continuous stochastic feature\\nIEEE Transactions on Audio,\\n\\nmapping based on trajectory HMMs.\\nSpeech, and Language Processings, 19(2):417\\u2013430, February 2011.\\n\\n[440] H. Zen, A. Senior, and M. Schuster. Statistical parametric speech syn-\\nthesis using deep neural networks. In Proceedings of International Con-\\nference on Acoustics Speech and Signal Processing (ICASSP), pages\\n7962\\u20137966. 2013.\\n\\n[441] X. Zhang, J. Trmal, D. Povey, and S. Khudanpur.\\n\\nImproving deep\\nneural network acoustic models using generalized maxout networks. In\\nProceedings of International Conference on Acoustics Speech and Signal\\nProcessing (ICASSP). 2014.\\n\\n[442] X. Zhang and J. Wu. Deep belief networks based voice activity detec-\\ntion. IEEE Transactions on Audio, Speech, and Language Processing,\\n21(4):697\\u2013710, 2013.\\n\\n[443] Z. Zhang, Z. Liu, M. Sinclair, A. Acero, L. Deng, J. Droppo, X. Huang,\\nand Y. Zheng. Multi-sensory microphones for robust speech detection,\\nenhancement and recognition. In Proceedings of International Confer-\\nence on Acoustics Speech and Signal Processing (ICASSP). 2004.\\n\\n[444] Y. Zhao and B. Juang. Nonlinear compensation using the gauss-newton\\nIEEE Transactions on\\n\\nmethod for noise-robust speech recognition.\\nAudio, Speech, and Language Processing, 20(8):2191\\u20132206, 2012.\\n\\nFull text available at: http://dx.doi.org/10.1561/2000000039\\x0c192\\n\\nReferences\\n\\n[445] W. Zou, R. Socher, D. Cer, and C. Manning. Bilingual word embed-\\ndings for phrase-based machine translation. In Proceedings of Empirical\\nMethods in Natural Language Processing (EMNLP). 2013.\\n\\n[446] G. Zweig and P. Nguyen. A segmental CRF approach to large vocab-\\nulary continuous speech recognition. In Proceedings of the Automatic\\nSpeech Recognition and Understanding Workshop (ASRU). 2009.\\n\\nFull text available at: http://dx.doi.org/10.1561/2000000039\\x0c', u'Hindawi Publishing Corporation\\nAdvances in Arti\\ufb01cial Neural Systems\\nVolume 2012, Article ID 107046, 9 pages\\ndoi:10.1155/2012/107046\\n\\nResearch Article\\nSleep Stage Classi\\ufb01cation Using Unsupervised Feature Learning\\n\\nMartin L\\xa8angkvist, Lars Karlsson, and Amy Lout\\ufb01\\n\\nCenter for Applied Autonomous Sensor Systems, \\xa8Orebro University, 701 82 \\xa8Orebro, Sweden\\n\\nCorrespondence should be addressed to Amy Lout\\ufb01, amy.lout\\ufb01@oru.se\\n\\nReceived 17 February 2012; Revised 5 May 2012; Accepted 6 May 2012\\n\\nAcademic Editor: Juan Manuel Gorriz Saez\\n\\nCopyright \\xa9 2012 Martin L\\xa8angkvist et al. This is an open access article distributed under the Creative Commons Attribution\\nLicense, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly\\ncited.\\n\\nMost attempts at training computers for the di\\ufb03cult and time-consuming task of sleep stage classi\\ufb01cation involve a feature\\nextraction step. Due to the complexity of multimodal sleep data, the size of the feature space can grow to the extent that it is\\nalso necessary to include a feature selection step. In this paper, we propose the use of an unsupervised feature learning architecture\\ncalled deep belief nets (DBNs) and show how to apply it to sleep data in order to eliminate the use of handmade features. Using\\na postprocessing step of hidden Markov model (HMM) to accurately capture sleep stage switching, we compare our results to a\\nfeature-based approach. A study of anomaly detection with the application to home environment data collection is also presented.\\nThe results using raw data with a deep architecture, such as the DBN, were comparable to a feature-based approach when validated\\non clinical datasets.\\n\\n1. Introduction\\n\\nOne of the main challenges in sleep stage classi\\ufb01cation is to\\nisolate features in multivariate time-series data which can\\nbe used to correctly identify and thereby automate the\\nannotation process to generate sleep hypnograms. In the\\ncurrent absence of a set of universally applicable features,\\ntypically a two-stage process is required before training\\na sleep stage algorithm, namely, feature extraction and\\nfeature selection [1\\u20139]. In other domains which share similar\\nchallenges, an alternative to using hand-tailored feature\\nrepresentations derived from expert knowledge is to apply\\nunsupervised feature learning techniques, where the feature\\nrepresentations are learned from unlabeled data. This not\\nonly enables the discovery of new useful feature represen-\\ntations that a human expert might not be aware of, which\\nin turn could lead to a better understanding of the sleep\\nprocess and present a way of exploiting massive amounts of\\nunlabeled data.\\n\\nUnsupervised feature learning and in particular deep\\nlearning [10\\u201315] propose ways for training the weight\\nmatrices in each layer in an unsupervised fashion as a pre-\\nprocessing step before training the whole network. This has\\nproven to give good results in other areas such as vision tasks\\n\\n[10], object recognition [16], motion capture data [17],\\nspeech recognition [18], and bacteria identi\\ufb01cation [19].\\n\\nThis work presents a new approach to the automatic\\nsleep staging problem. The main focus is to learn meaningful\\nfeature representations from unlabeled sleep data. A dataset\\nof 25 subjects consisting of electroencephalography (EEG) of\\nbrain activity, electrooculography (EOG) of eye movements,\\nand electromyography (EMG) of skeletal muscle activity\\nis segmented and used to train a deep belief network\\n(DBN), using no prior knowledge. Validation of the learned\\nrepresentations is done by integrating a hidden Markov\\nmodel (HMM) and compare classi\\ufb01cation accuracy with\\na feature-based approach that uses prior knowledge. The\\ninclusion of an HMM serves the purpose of improving upon\\ncapturing a more realistic sleep stage switching, for example,\\nhinders excessive or unlikely sleep stage transitions. It is in\\nthis manner that the knowledge from the human experts\\nis infused into the system. Even though the classi\\ufb01er is\\ntrained using labeled data, the feature representations are\\nlearned from unlabeled data. The architecture of the DBN\\nfollows previous work with unsupervised feature learning for\\nelectroencephalography (EEG) event detection [20].\\n\\nA secondary contribution of\\n\\nthe proposed method\\nleverages the information from the DBN in order to perform\\n\\n\\x0c2\\n\\nAdvances in Arti\\ufb01cial Neural Systems\\n\\nanomaly detection. Particularly, in light of an increasing\\ntrend to streamline sleep diagnosis and reduce the burden\\non health care centers by using at home sleep monitoring\\ntechnologies, anomaly detection is important in order to\\nrapidly assess the quality of the polysomnograph data and\\ndetermine if the patient requires another additional night\\u2019s\\ncollection at home. In this paper, we illustrate how the DBN\\nonce trained on datasets for sleep stage classi\\ufb01cation in the\\nlab can still be applied to data which has been collected at\\nhome to \\ufb01nd particular anomalies such as a loose electrode.\\nFinally, inconsistencies between sleep labs (equipment,\\nelectrode placement), experimental setups (number of sig-\\nnals and categories, subject variations), and interscorer\\nvariability (80% conformance for healthy patients and even\\nless for patients with sleep disorder [9]) make it challenging\\nto compare sleep stage classi\\ufb01cation accuracy to previous\\nworks. Results in [2] report a best result accuracy of around\\n61% for classi\\ufb01cation of 5 stages from a single EEG channel\\nusing GOHMM and AR coe\\ufb03cients as features. Works by\\n[8] achieved 83.7% accuracy using conditional random \\ufb01elds\\nwith six power spectra density features for one EEG signal\\non four human subjects during a 24-hour recording session\\nand considering six stages. Works by [7] achieved 85.6%\\naccuracy on artifact-free, two expert agreement sleep data\\nfrom 47 mostly healthy subjects using 33 features with SFS\\nfeature selection and four separately trained neural networks\\nas classi\\ufb01ers.\\n\\nThe goal of this work is not to replicate the R&K system\\nor improve current state-of-the-art sleep stage classi\\ufb01cation\\nbut rather to explore the advantages of deep learning and\\nthe feasibility of using unsupervised feature learning applied\\nto sleep data. Therefore, the main method of evaluation is\\na comparison with a feature-based shallow model. Matlab\\ncode used in this paper is available at http://aass.oru.se/\\u223cmlt.\\n\\nb1\\n\\nb2\\n\\nb3\\n\\nh1\\n\\nh2\\n\\nh3\\n\\nW21\\n\\nW11\\n\\nv1\\n\\nc1\\n\\nb4\\n\\nh4\\n\\nW24 W34\\n\\nb\\n\\nW\\n\\nc\\n\\nv3\\n\\nc3\\n\\nv2\\n\\nc2\\n\\n(a)\\n\\nOutput\\n\\nRBM 2\\n\\n\\xb7\\xb7\\xb7\\n\\nh2\\n\\nP(h2|v2)\\n\\nP(v2|h2)\\n\\ng\\nn\\ni\\nd\\no\\nc\\nn\\nE\\n\\nRBM 1\\n\\n\\xb7\\xb7\\xb7\\n\\nv2\\n\\n\\xb7\\xb7\\xb7\\n\\nh1\\n\\ng\\nn\\ni\\nd\\no\\nc\\ne\\nD\\n\\nP(h1|v1)\\n\\nP(v1|h1)\\n\\n\\xb7\\xb7\\xb7\\n\\nv1\\n\\nData\\n\\nRecon\\n\\n(b)\\n\\n2. Deep Belief Networks\\n\\nDBN is a probabilistic generative model with deep archi-\\ntecture that searches the parameter space by unsupervised\\ngreedy layerwise training. Each layer consists of a restricted\\nBoltzmann machine (RBM) with visible units, v, and hidden\\nunits, h. There are no visible-visible connections and no\\nhidden-hidden connections. The visible and hidden units\\nhave a bias vector, c and b, respectively. The visible and\\nhidden units are connected by a weight matrix, W, see\\nFigure 1(a). A DBN is formed by stacking a user-de\\ufb01ned\\nnumber of RBMs on top of each other where the output\\nfrom a lower-level RBM is the input to a higher-level RBM,\\nsee Figure 1(b). The main di\\ufb00erence between a DBN and a\\nmultilayer perceptron is the inclusion of a bias vector for the\\nvisible units, which is used to reconstruct the input signal,\\nwhich plays an important role in the way DBNs are trained.\\nA reconstruction of the input can be obtained from the\\nunsupervised pretrained DBN by encoding the input to the\\ntop RBM and then decoding the state of the top RBM back to\\nthe lowest level. For a Bernoulli (visible)-Bernoulli (hidden)\\nRBM, the probability that hidden unit h j is activated given\\n\\nFigure 1: Graphical depiction of (a) RBM and (b) DBN.\\n\\nvisible vector, v, and the probability that visible unit vi is\\nactivated given hidden vector, h, are given by\\n\\n(cid:2)\\n\\nP\\n\\nh j | v\\n\\n(cid:3)\\n\\n=\\n\\n1\\n(cid:4)\\n1 + expb j +\\n\\ni Wi j vi\\n\\nP(vi | h) =\\n\\n1\\n(cid:4)\\n1 + expci+\\n\\n.\\n\\nj Wi j h j\\n\\n(1)\\n\\nThe energy function and the joint distribution for a given\\nvisible and hidden vector are\\n\\nE(v, h) = hTWv + bTh + cTv\\n\\nP(v, h) = 1\\n\\nz expE(v, h).\\n\\n(2)\\n\\nThe parameters W, b, and v are trained to minimize the\\nreconstruction error. An approximation of the gradient of\\n\\n\\x0cAdvances in Arti\\ufb01cial Neural Systems\\n\\n3\\n\\nthe log likelihood of v using contrastive divergence [21] gives\\nthe learning rule for RBM:\\n(cid:5)\\nvih j\\n\\n\\u2202 log P(v)\\n\\nvih j\\n\\n\\u2248\\n\\n\\u2212\\n\\n(cid:6)\\n\\n(cid:5)\\n\\n(cid:6)\\n\\n(3)\\n\\n,\\n\\nrecon\\n\\n\\u2202Wi j\\n\\ndata\\n\\nwhere (cid:4)\\xb7(cid:5) is the average value over all training samples. In\\nthis work, training is performed in three steps: (1) unsuper-\\nvised pretraining of each layer, (2) unsupervised \\ufb01ne-tuning\\nof all layers with backpropagation, and (3) supervised \\ufb01ne-\\ntuning of all layers with backpropagation.\\n\\n3. Experimental Setup\\n\\n3.1. Automatic Sleep Stager. The \\ufb01ve sleep stages that are at\\nfocus are awake, stage 1 (S1), stage 2 (S2), slow wave sleep\\n(SWS), and rapid eye-movement sleep (REM). These stages\\ncome from a uni\\ufb01ed method for classifying an 8 h sleep\\nrecording introduced by Rechtscha\\ufb00en and Kales (R&K)\\n[22]. A graph that shows these \\ufb01ve stages over an entire night\\nis called a hypnogram, and each epoch according to the R&K\\nsystem is either 20 s or 30 s. While the R&K system brings\\nconsensus on terminology, among other advantages [23], it\\nhas been criticized for a number of issues [24]. Even though\\nthe goal in this work is not to replicate the R&K system, its\\nterminology will be used for evaluation of our architecture.\\nEach channel of the data is divided into segments of 1\\nsecond with zero overlap, which is a much higher temporal\\nresolution than the one practiced by the R&K system.\\n\\nWe compare the performance of three experimental\\n\\nsetups as shown in Figure 2.\\n\\n3.1.1. Feat-GOHMM. A Gaussian observation hidden\\nMarkov model (GOHMM) is used on 28 handmade features;\\nsee the appendix for a description of the features used.\\nFeature selection is done by sequential backward selection\\n(SBS), which starts with the full set of features and greedily\\nremoves a feature after each iteration step. A principal\\ncomponent analysis (PCA) with \\ufb01ve principal components is\\nused after feature selection, followed by a Gaussian mixture\\nmodel (GMM) with \\ufb01ve components. The purpose of the\\nPCA is to reduce dimensionality, and the choice of \\ufb01ve\\ncomponents was made since it captured most of the variance\\nin the data, while still being tractable for the GMM step.\\nInitial mean and covariance values\\nfor each GMM\\ncomponent are set to the mean and covariance of annotated\\ndata for each sleep stage. Finally, the output from the GMM\\nis used as input to a hidden Markov model (HMM) [25].\\n\\n3.1.2. Feat-DBN. A 2-layer DBN with 200 hidden units in\\nboth layers and a softmax classi\\ufb01er attached on top is used\\non 28 handmade features. Both layers are pretrained for 300\\nepochs, and the top layer is \\ufb01ne-tuned for 50 epochs. Initial\\nbiases of hidden units are set empirically to \\u22124 to encouraged\\nsparsity [26], which prevents learning trivial or uninteresting\\nfeature representations. Scaling to values between 0 and 1\\nis done by subtracting the mean, divided by the standard\\ndeviation, and \\ufb01nally adding 0.5.\\n\\nfeat-GOHMM\\n\\nfeat-DBN\\n\\nraw-DBN\\n\\nHMM\\n\\nGMM\\n\\nPCA\\n\\nFeature \\nselection\\n\\nFeature \\nextraction\\n\\nHMM\\n\\nDBN\\n\\nFeature \\nextraction\\n\\nHMM\\n\\nDBN\\n\\nRaw data\\n\\nRaw data\\n\\nRaw data\\n\\nFigure 2: Overview of three setups for an automatic sleep stager\\nused in this work. The \\ufb01rst method, feat-GOHMM, is a shallow\\nmethod that uses prior knowledge. The second method, feat-DBN,\\nis a deep architecture that also uses prior knowledge. And, lastly, the\\nthird method, raw-DBN, is a deep architecture that does not use any\\nprior knowledge. See text for more details.\\n\\n3.1.3. Raw-DBN. A DBN with the same parameters as feat-\\nDBN is used on preprocessed raw data. Scaling is done by\\nsaturating the signal at a saturation constant, satchannel, then\\ndivide by 2\\u2217 satchannel, and \\ufb01nally adding 0.5. The saturation\\nconstant was set to satEEG = satEOG = \\xb160 \\u03bcV and satEMG =\\n\\xb140 \\u03bcV. Input consisted of the concatenation of EEG, EOG1,\\nEOG2, and EMG. With window width, w, the visible layer\\nbecomes\\n\\u23a1\\n\\u23a2\\u23a2\\u23a3\\n\\nEMG64\\n1\\n1+w EMG64+w\\n1+w\\n\\nEOG264\\nEOG164\\n1\\n1\\n1+w EOG164+w\\n1+w EOG264+w\\n...\\n\\nEEG64\\n1\\nEEG64+w\\n\\n\\u23a4\\n\\u23a5\\u23a5\\u23a6.\\n\\nv =\\n\\n(4)\\n\\n...\\n\\n...\\n\\n...\\n\\nWith four signals, 1 second window, and 64 samples per\\nsecond, the input dimension is 256.\\n\\n3.2. Anomaly Detection for Home Sleep Data. In this work,\\nanomaly detection is evaluated by training a DBN and\\ncalculating the root mean square error (RMSE) from the\\nreconstructed signal from the DBN and the original signal. A\\nfaulty signal in one channel often a\\ufb00ects other channels for\\nsleep data, such as movement artifacts, blink artifacts, and\\nloose reference or ground electrode. Therefore, a detected\\nfault in one channel should label all channels at that time\\nas faulty.\\n\\nFigure 3 shows data that has been collected at a healthy\\npatient\\u2019s home during sleep. All signals, except EEG2, are\\nnonfaulty prior to a movement artifact at t = 7 s. This\\nmovement a\\ufb00ected the reference electrode or the ground\\nelectrode, resulting in disturbances in all signals for the\\nrest of the night, thereby rendering the signals unusable by\\na clinician. A poorly attached electrode was the cause for\\n\\n\\x0c4\\n\\nEEG1\\n\\nEEG2\\n\\nEOG1\\n\\nEOG2\\n\\nEMG\\n\\n0\\n\\n5\\n\\n10\\n\\n15\\n\\n20\\n\\n25\\n\\n30\\n\\nTime (s)\\n\\nFigure 3: PSG data collected in a home environment. A movement\\noccurs at t = 7 s resulting in one of the electrodes to be misplaced\\na\\ufb00ecting EOG1 and both EEG channels. EOG2 is not properly\\nattached resulting in a faulty signal for the entire night.\\n\\nthe noise in signal EEG2. Previous approaches to artifact\\nrejection in EEG analysis range from simple thresholding\\non abnormal amplitude and/or frequency to more complex\\nstrategies in order to detect individual artefacts [27, 28].\\n\\n4. Experimental Datasets\\n\\nTwo datasets are used in this work. The \\ufb01rst consists of 25\\nacquisitions and is used to train and test the automatic sleep\\nstager. The second consists of 5 acquisitions and is used to\\nvalidate anomaly detection on sleep data collected at home.\\n\\n4.1. Benchmark Dataset. This dataset has kindly been pro-\\nvided by St. Vincent\\u2019s University Hospital and University\\nCollege Dublin, which can be downloaded from PhysioNet\\n[29]. The dataset consists of 25 acquisitions (21 males 4\\nfemales with average age 50, average weight 95 kg, and\\naverage height 173 cm) from subjects with suspected sleep-\\ndisordered breathing. Each acquisition consists of 2 EEG\\nchannels (C3-A2 and C4-A1), 2 EOG channels, and 1 EMG\\nchannel using 10\\u201320 electrode placements system. Only one\\nof the EEG channel (C3-A2) is used in this work. Sample rate\\nis 128 Hz for EEG and 64 Hz for EOG and EMG. Average\\nrecording time is 6.9 hours. Sleep stages are divided into\\nS1: 16.7%, S2: 33.3%, SWS: 12.7%, REM: 14.5%, awake:\\n22.7%, and indeterminate: 0.1%. Scoring was performed by\\none sleep expert.\\n\\nAll signals are preprocessed by notch \\ufb01ltering at 50 Hz\\nin order to cancel out power line disturbances and down-\\nsampled to 64 Hz after being pre\\ufb01ltered with a band-pass\\n\\ufb01lter of 0.3 to 32 Hz for EEG and EOG, and 10 to 32 Hz\\nfor EMG. Each epoch before and after a sleep stage switch is\\nremoved from the training set to avoid possible subsections\\nof mislabeled data within one epoch. This resulted in 20.7%\\nof total training samples to be removed.\\n\\nAdvances in Arti\\ufb01cial Neural Systems\\n\\nA 25 leave-one-out cross-validation is performed. Train-\\ning samples are randomly picked from 24 acquisitions in\\norder to compensate for any class imbalance. A total of\\napproximately 250000 training samples and 50000 training\\nvalidation samples are used for each validation.\\n\\n4.2. Home Sleep Dataset. PSG data of approximately 60\\nhours (5 nights) was collected at a healthy patient\\u2019s home\\nusing a Embla Titanium PSG. A total of 8 electrodes were\\nused: EEG C3, EEG C4, EOG left, EOG right, 2 electrodes for\\nthe EMG channel, reference electrode, and ground electrode.\\nData was collected with a sampling rate of 256 Hz, which\\nwas downsampled to match the sampling rate of the training\\ndata. The signals are preprocessed using the same method as\\nthe benchmark dataset.\\n\\n5. Results\\n\\n5.1. Automatic Sleep Stager. A full\\nleave-one-out cross-\\nvalidation of the 25 acquisitions is performed for the three\\nexperimental setups. The classi\\ufb01cation accuracy and confu-\\nsion matrices for each setup and sleep stage are presented\\nin Tables 1, 2, 3, and 4. Here, the performance of using\\na DBN based approach, either with features or using the\\nraw data, is comparable to the feat-GOHMM. While the\\nbest accuracy was achieved with feat-DBN, followed by raw-\\nDBN and lastly, feat-GOHMM, it is important to examine\\nthe performances individually. Figure 4 shows classi\\ufb01cation\\naccuracy for each subject. The raw-DBN setup gives best, or\\nsecond best, performance in the majority of the sets, with\\nthe exception of subjects 9 and 22. An examination of the\\nperformance when comparing the F1-score for individual\\nsleep stages indicates that S1 is the most di\\ufb03cult stage to\\nclassify and awake and slow wave sleep is the easiest.\\n\\nFor the raw-DBN, it is also possible to analyze the learned\\nfeatures. In Figure 6, the learned features for the \\ufb01rst layer\\nare given. Here, it can clearly be seen that both low and high\\nfrequency features for the EEG and high and low amplitude\\nfeatures for the EMG are included, which to some degree\\ncorrespond to the features which are typically selected in\\nhandmade feature selection methods.\\n\\nSome conclusions from analyzing the selected features\\nfrom the SBS algorithm used in feat-GOHMM can be made.\\nFractal exponent for EEG and entropy for EOG were selected\\nfor all 25 subjects and thus proven to be valuable features.\\nCorrelation between both EOG signals was also among\\nthe top selected features, as well as delta, theta, and alpha\\nfrequencies for EEG. Frequency features for EOG and EMG\\nwere excluded early, which is in accordance to the fact that\\nthese signals do not exhibit valuable information in the\\nfrequency domain [30]. The kurtosis feature was selected\\nmore frequently when it was applied to EMG and less\\nfrequently when it was applied to EEG or EOG. Features of\\nspectral mean for all signals, median for EMG, and standard\\ndeviation for EOG were not frequently selected. See Figure 5\\nfor errors bars for each feature at each sleep stage.\\n\\nIt is worth noting that variations in the number of\\nlayers and hidden units were attempted, and it was found\\n\\n\\x0cAdvances in Arti\\ufb01cial Neural Systems\\n\\nTable 1: Classi\\ufb01cation accuracy and F1-score for the three experimental setups.\\n\\nAccuracy (mean \\xb1 std)\\n\\n63.9 \\xb1 10.8\\n72.2 \\xb1 9.7\\n67.4 \\xb1 12.9\\n\\nAwake\\n0.71\\n0.78\\n0.69\\n\\nS1\\n0.31\\n0.37\\n0.36\\n\\nF1-score\\n\\nS2\\n0.72\\n0.76\\n0.78\\n\\nSWS\\n0.82\\n0.84\\n0.83\\n\\nf eat-GOHMM\\nfeat-DBN\\nraw-DBN\\n\\n5\\n\\nREM\\n0.47\\n0.78\\n0.58\\n\\ny\\nc\\na\\nr\\nu\\nc\\nc\\nA\\n\\n1\\n\\n0.9\\n\\n0.8\\n\\n0.7\\n\\n0.6\\n\\n0.5\\n\\n0.4\\n\\n0.3\\n\\n0.2\\n\\n0.1\\n\\n0\\n\\n1\\n\\n3\\n\\n5\\n\\n7\\n\\n9\\n\\n11\\n\\n13\\n\\n15\\n\\n17\\n\\n19\\n\\n21\\n\\n23\\n\\n25\\n\\nDataset\\n\\nfeat-GOHMM\\nfeat-DBN\\nraw-DBN\\n\\nFigure 4: Classi\\ufb01cation accuracy for 25 testing sets for three setups.\\n\\nTable 2: Confusion matrix for feat-GOHMM.\\n\\nTable 4: Confusion matrix for raw-DBN.\\n\\n%\\n\\nAwake\\nS1\\nS2\\nSWS\\nREM\\n\\n%\\n\\nAwake\\nS1\\nS2\\nSWS\\nREM\\n\\nAwake\\n72.5\\n29.4\\n2.0\\n1.1\\n11.7\\n\\nClassi\\ufb01ed\\n\\nS2\\n3.0\\n25.6\\n71.9\\n9.6\\n29.4\\n\\nS1\\n16.8\\n31.1\\n8.4\\n1.3\\n13.3\\n\\nSWS\\n2.5\\n1.5\\n7.1\\n87.8\\n2.7\\n\\nTable 3: Confusion matrix for feat-DBN.\\n\\nAwake\\n75.8\\n26.0\\n1.0\\n0.4\\n1.9\\n\\nClassi\\ufb01ed\\n\\nS2\\n1.8\\n25.1\\n73.1\\n13.9\\n10.3\\n\\nS1\\n18.2\\n37.6\\n9.8\\n0.1\\n4.0\\n\\nSWS\\n0.2\\n0.7\\n7.2\\n85.5\\n0.1\\n\\nREM\\n5.2\\n12.4\\n10.6\\n0.2\\n42.9\\n\\nREM\\n4.1\\n10.6\\n9.0\\n0.1\\n83.7\\n\\nthat an increase did not signi\\ufb01cantly improve classi\\ufb01cation\\naccuracy. Rather, an increase in either the number of layers\\nor hidden units often resulted in a signi\\ufb01cant increase in\\nsimulation time, and therefore to maintain a reasonable\\ntraining time, the layers and hidden units were kept to a\\n\\n%\\n\\nAwake\\nS1\\nS2\\nSWS\\nREM\\n\\nAwake\\n68.4\\n20.3\\n1.0\\n0.1\\n21.1\\n\\nClassi\\ufb01ed\\n\\nS2\\n2.5\\n24.8\\n76.5\\n11.1\\n11.1\\n\\nS1\\n13.4\\n33.1\\n6.3\\n0.0\\n6.9\\n\\nSWS\\n0.7\\n1.6\\n9.1\\n88.8\\n0.8\\n\\nREM\\n15.1\\n20.2\\n7.1\\n0.0\\n60.1\\n\\nminimum. With the con\\ufb01guration of the three experimental\\nsetups described above and simulations performed on a\\nWindows 7, 64-bit machine with quad-core Intel i5 3.1 GHz\\nCPU with use of a nVIDIA GeForce GTX 470 GPU using\\nGPUmat, simulation time for feat-GOHMM, feat-DBN, and\\nraw-DBN were approximately 10 minutes, 1 hour, and 3\\nhours per dataset, respectively.\\n\\n5.2. Anomaly Detection on Home Sleep Data. A total of \\ufb01ve\\nacquisitions were recorded at a patient\\u2019s home during sleep\\nand manually labeled into faulty or nonfaulty signals. A DBN\\nwith the raw-DBN setup was trained using the benchmark\\ndataset. The root mean square error (RMSE) between the\\nhome sleep data and the reconstructed signal from the\\ntrained DBN for the \\ufb01ve night runs and a close-up for night\\n\\n\\x0c6\\n\\nAdvances in Arti\\ufb01cial Neural Systems\\n\\n1\\n0\\n\\u22121\\n\\na\\nt\\nl\\ne\\nd\\nG\\nE\\nE\\n\\n \\n\\n \\n\\na\\nt\\nl\\ne\\nd\\nG\\nO\\nE\\n\\n1\\n0\\n\\u22121\\n\\n \\n\\na\\nt\\nl\\ne\\nd\\nG\\nM\\nE\\n\\nn\\na\\ni\\nd\\ne\\nm\\nG\\nM\\nE\\n\\n \\n\\nd\\nt\\ns\\n \\nG\\nO\\nE\\n\\n1\\n0\\n\\u22121\\n\\n2\\n1\\n0\\n\\n2\\n1\\n0\\n\\u22121\\n\\nn\\na\\ne\\nm\\n\\n \\n.\\ns\\n \\nG\\nO\\nE\\n\\n1\\n0\\n\\u22121\\n\\n23\\n\\nS1 S2 S3 R W\\n\\n18\\n\\nS1 S2 S3 R W\\n\\n1\\n\\na\\nt\\ne\\nh\\nt\\n \\n\\nG\\nE\\nE\\n\\na\\nt\\ne\\nh\\nt\\n \\n\\nG\\nO\\nE\\n\\na\\nt\\ne\\nh\\nt\\n \\n\\nG\\nM\\nE\\n\\n1\\n\\n0\\n\\u22121\\n\\n1\\n\\n0\\n\\u22121\\n\\n1\\n\\n0\\n\\u22121\\n\\n22\\n\\nS1 S2 S3 R W\\n\\n15\\n\\nS1 S2 S3 R W\\n\\n4\\n\\nS1 S2 S3 R W\\n\\nS1 S2 S3 R W\\n\\n0\\n\\nS1 S2 S3 R W\\n\\n6\\n\\nS1 S2 S3 R W\\n\\n2\\n\\nS1 S2 S3 R W\\n\\nr\\nr\\no\\nc\\n \\nG\\nO\\nE\\n\\n0.5\\n\\u22120.5\\n\\u22121.5\\n\\n1\\n\\n0\\n\\u22121\\n\\n1\\n0\\n\\u22121\\n\\nr\\nt\\nn\\ne\\n \\nG\\nE\\nE\\n\\nn\\na\\ne\\nm\\n\\n \\n.\\ns\\n \\nG\\nM\\nE\\n\\n23\\n\\nS1 S2 S3 R W\\n\\n12\\n\\nS1 S2 S3 R W\\n\\n7\\n\\na\\nh\\np\\nl\\na\\n \\nG\\nE\\nE\\n\\na\\nh\\np\\nl\\na\\n \\nG\\nO\\nE\\n\\na\\nh\\np\\nl\\na\\n \\nG\\nM\\nE\\n\\n \\n\\nt\\nr\\nu\\nk\\nG\\nE\\nE\\n\\nr\\nt\\nn\\ne\\n \\nG\\nO\\nE\\n\\np\\nx\\ne\\n \\n.\\nf\\n \\n\\nG\\nE\\nE\\n\\n1\\n0\\n\\u22121\\n\\n1\\n\\n0\\n\\u22121\\n\\n1\\n0\\n\\u22121\\n\\n1\\n0\\n\\u22121\\n\\n1\\n0\\n\\u22121\\n\\n1\\n0\\n\\u22121\\n\\n13\\n\\nS1 S2 S3 R W\\n\\n9\\n\\nS1 S2 S3 R W\\n\\n6\\n\\n \\n\\na\\nt\\ne\\nb\\nG\\nE\\nE\\n\\n1\\n0\\n\\u22121\\n\\n \\n\\na\\nt\\ne\\nb\\nG\\nO\\nE\\n\\n1\\n0\\n\\u22121\\n\\n \\n\\na\\nt\\ne\\nb\\nG\\nM\\nE\\n\\n1\\n0\\n\\u22121\\n\\n7\\n\\nG\\nE\\nE\\n\\na\\nm\\nm\\na\\ng\\n \\n\\n1\\n0\\n\\u22121\\n\\n9\\n\\nS1 S2 S3 R W\\n\\nS1 S2 S3 R W\\n\\n4\\n\\nS1 S2 S3 R W\\n\\n9\\n\\n \\n\\nG\\nO\\nE\\n\\na\\nm\\nm\\na\\ng\\n\\n \\n\\nG\\nM\\nE\\n\\na\\nm\\nm\\na\\ng\\n\\n \\n\\nt\\nr\\nu\\nk\\nG\\nM\\nE\\n\\n1\\n0\\n\\u22121\\n\\n1\\n0\\n\\u22121\\n\\n1\\n0\\n\\u22121\\n\\n6\\n\\nS1 S2 S3 R W\\n\\n7\\n\\nS1 S2 S3 R W\\n\\n10\\n\\nS1 S2 S3 R W\\n\\nS1 S2 S3 R W\\n\\n4\\n\\n \\n\\nt\\nr\\nu\\nk\\nG\\nO\\nE\\n\\n1\\n\\n0\\n\\u22121\\n\\n4\\n\\nS1 S2 S3 R W\\n\\nS1 S2 S3 R W\\n\\nS1 S2 S3 R W\\n\\nr\\nt\\nn\\ne\\n \\nG\\nM\\nE\\n\\n1\\n0\\n\\u22121\\n\\n15\\n\\nS1 S2 S3 R W\\n\\n2\\n1\\n0\\n\\u22121\\n\\nn\\na\\ne\\nm\\n\\n \\n.\\ns\\n \\nG\\nE\\nE\\n\\n5\\n\\nS1 S2 S3 R W\\n\\n25\\n\\nS1\\n\\nR W\\n\\n25\\n\\nS1 S2 S3 R W\\n\\nS1 S2 S3 R W\\n\\nFigure 5: Error bar of the 28 features. Gray number in background represents how many times that feature was part of best subset from SBS\\nalgorithm (maximum is 25).\\n\\n2 where an electrode falls o\\ufb00 after around 380 minutes can\\nbe seen in Figure 7.\\n\\nInterestingly, attempts on using the feat-GOHMM for\\nsleep stage classi\\ufb01cation on the home sleep dataset resulted\\nin faulty data to be misclassi\\ufb01ed as awake. This could be\\nexplained by the fact that faulty data mostly resembles signals\\nin awake state.\\n\\n6. Discussion\\n\\nIn this work, we have shown that an automatic sleep stager\\ncan be applied to multimodal sleep data without using any\\nhandmade features. We also compared the reconstructed\\nsignal from a trained DBN on data collected in a home\\nenvironment and saw that the RMSE was large where an\\nobvious error had occurred.\\nRegarding the DBN parameter selection, it was noticed\\nthat setting initial biases for the hidden units to \\u22124 was an\\nimportant parameter for achieving good accuracy. A better\\nway of encourage sparsity is to include a sparsity penalty term\\nin the cost objective function [31] instead of making a crude\\nestimation of initial biases for the hidden units. For the raw-\\nDBN setup, it was also crucial to train each layer with a large\\nnumber of epochs and in particular the \\ufb01ne tuning step.\\n\\nWe also noticed a lower performance if sleep stages were\\nnot set to equal sizes in the training set. There was also\\na high variation in the accuracy between patients, even if\\n\\nthey came from the same dataset. Since the DBN will \\ufb01nd\\na generalization that best \\ufb01ts all training examples, a testing\\nset that deviates from the average training set might give poor\\nresults. Since data might di\\ufb00ers greatly between patients, a\\nsingle DBN trained on general sleep data is not specialized\\nenough. The need for a more dynamic system, especially\\none including the transition and emission matrices for the\\nHMM, is made clear when comparing the hypnograms of a\\nhealthy patient and a patient with sleep disordered breathing.\\nFurther, although the HMM provides a simple solution that\\ncaptures temporal properties of sleep data, it makes two\\ncritical assumptions [13]. The \\ufb01rst one is that the next hidden\\nstate can be approximated by a state depending only on the\\nprevious state, and the second one is that observations at\\ndi\\ufb00erent time steps are conditionally independent given a\\nstate sequence. Replacing HMM with conditional random\\n\\ufb01elds (CRFs) could improve accuracy but is still a simplistic\\ntemporal model that does not exploit the power of DBNs\\n[32].\\n\\nWhile a clear advantage of using DBN is the natural\\nway in which it deals with anomalous data, there are some\\nlimitations to the DBN. One limitation is that correlations\\nbetween signals in the input data are not well captured.\\nThis gives a feature-based approach an advantage where, for\\nexample, the correlation between both EOG channels can\\neasily be represented with a feature. This could be solved by\\neither representing the correlation in the input or extending\\nthe DBN to handle such correlations, such as a cRBM [33].\\n\\n\\x0cAdvances in Arti\\ufb01cial Neural Systems\\n\\n7\\n\\n(a)\\n\\n(c)\\n\\n(b)\\n\\n(d)\\n\\nFigure 6: Learned features of layer 1 for (a) EEG, (b) EOG1, (c) EOG2, and (d) EMG. It can be observed that the learned features are of\\nvarious amplitudes and frequencies and some resemble known sleep events such as a K-complex or blink artifacts. Only the \\ufb01rst 100 of the\\n200 features are shown here.\\n\\nRegarding the implemented feat-GOHMM, we have tried\\nour best to get as high accuracy with the setup as possible.\\nIt is almost certain that another set of features, di\\ufb00erent\\nfeature selection algorithm, and/or another classi\\ufb01er could\\noutperform our feat-GOHMM. However, we hope that\\nthis work illustrates the advantages of unsupervised feature\\nlearning, which not only removes the need for domain\\nspeci\\ufb01c expert knowledge, but inherently provides tools for\\nanomaly detection and noise redundancy.\\n\\nIt has been suggested for multimodal signals to train a\\nseparate DBN for each signal \\ufb01rst and then train a top DBN\\nwith concatenated data [34]. This not only could improve\\nclassi\\ufb01cation accuracy, but also provide the ability to single\\nout which signal contains the anomalous signal. Further,\\nthis work has explored clinical data sets in close cooperation\\nwith physicians, and future work will concentrate on the\\napplication for at home monitoring as sleep data is an area\\nwhere unsupervised feature learning is a highly promising\\nmethod for sleep stage classi\\ufb01cation as data is abundant and\\nlabels are costly to obtain.\\n\\nAppendix\\n\\nA. Features\\n\\nA total of 28 features are used in this work.\\n\\nRelative power for signal y in frequency band f\\n\\nis\\n\\ncalculated as\\n\\n(cid:13)\\n\\n(cid:14) =\\n\\nf\\n\\nyPrel\\n\\n(cid:13)\\n\\nyP\\n(cid:4) f5\\nf = f1\\n\\n(cid:14)\\n(cid:13)\\n\\nf\\nyP\\n\\n(cid:14) ,\\n\\nf\\n\\n(A.1)\\n\\nwhere y( f )P is the sum of the absolute power in frequency\\nband f\\nfor signal y. The \\ufb01ve frequency bands used are\\ndelta (0.5\\u20134 Hz), theta (4\\u20138 Hz), alpha (8\\u201313 Hz), beta (13\\u2013\\n20 Hz), and gamma (20\\u201364 Hz).\\n\\nThe median of the absolute value for EMG is calculated\\n\\nas\\n\\nEMGmedian = median\\n\\n\\u239e\\n\\u23a0.\\n\\n|EMG(k)|\\n\\n\\u239b\\n\\u239d N(cid:17)\\nk=1\\n\\n(A.2)\\n\\n\\x0c8\\n\\nAdvances in Arti\\ufb01cial Neural Systems\\n\\n0.5\\n\\n0.45\\n\\n0.4\\n\\n0.35\\n\\n0.3\\n\\n0.25\\n\\n0.2\\n\\n0.15\\n\\n0.1\\n\\n0.05\\n\\n0\\n\\nE\\nS\\nM\\nR\\n\\nEEG\\n\\nEOG1\\n\\nEOG2\\n\\nEEG\\n\\n0.3\\n\\n0.25\\n\\n0.2\\n\\n0.15\\n\\nE\\nS\\nM\\nR\\n\\n0.1\\n\\n0\\n\\n50\\n\\n100\\n\\n150\\n\\n200\\n\\n250\\n\\n300\\n\\n350\\n\\n400\\n\\n450\\n\\n500\\n\\nNight 1\\nNight 2\\nNight 3\\n\\nTime (min)\\n\\nNight 4\\nNight 5\\n\\nFigure 7: RMSE for \\ufb01ve night runs recorded at home (bottom). Color-code of RMSE for night run 2 where the redder areas more anomalous\\nareas of the signal. EOG2 falls o\\ufb00 at around 380 minutes (top).\\n\\nThe eye correlation coe\\ufb03cient for the EOG is calculated as\\n\\n(cid:20)(cid:2)\\n\\n(cid:3)(cid:2)\\n\\n(cid:3)(cid:21)\\n\\nEOGcorr = E\\n\\ny2 \\u2212 \\u03bcy2\\n\\u03c3y2\\nwhere y1 = EOGleft and y2 = EOGright.\\n\\ny1 \\u2212 \\u03bcy1\\n\\u03c3y1\\n\\nThe entropy for a signal y is calculated as\\n\\n,\\n\\n(A.3)\\n\\nyentropy = \\u2212 8(cid:17)\\nk=1\\n\\nnk\\nN ln\\n\\nnk\\nN ,\\n\\n(A.4)\\n\\nwhere N is the number of samples in signal y, and nk is the\\nnumber of samples from y that belongs to the kth bin from a\\nhistogram of y.\\n\\nThe kurtosis for a signal y is calculated as\\n\\nykurtosis = E\\n\\n(cid:23)\\n4\\n\\n(cid:22)\\n\\ny \\u2212 \\u03bc\\n\\u03c3 4\\n\\n,\\n\\n(A.5)\\n\\nNormalization is performed for some features according\\nto [37] and [30]. The absolute median for EMG is normal-\\nized by dividing with the absolute median for the whole EMG\\nsignal.\\n\\nAcknowledgments\\n\\nThe authors are grateful to Professor Walter T. McNicholas\\nof St. Vincents University Hospital, Ireland, and Professor\\nConor Heneghan of University College Dublin, Ireland, for\\nproviding the sleep training data for this study. They would\\nalso like to thank senior physician Lena Leissner and sleep\\ntechnician Meeri Sandelin at the sleep unit of the neuroclinic\\nat \\xa8Orebro University Hospital for their continuous support\\nand expertise. Finally, special thanks to D F Wulsin for\\nwriting and sharing the open-source implementation of\\nDBN for Matlab that was used in this work [20]. This work\\nwas funded by NovaMedTech.\\n\\nwhere \\u03bc and \\u03c3 are the mean and standard deviation, respec-\\ntively, for signal y.\\n\\nThe spectral mean for signal y is calculated as\\n\\nReferences\\n\\nyspectralmean = 1\\nF\\n\\n(cid:13)\\n\\ny\\n\\nf\\n\\n(cid:14)\\nPrel\\n\\n\\xb7 f ,\\n\\nf5(cid:17)\\n\\nf = f1\\n\\n(A.6)\\n\\nwhere F is the sum of the lengths of the 5 frequency bands.\\n\\nFractal exponent [35, 36] for the EEG is calculated as\\nthe negative slope of the linear \\ufb01t of spectral density in the\\ndouble logarithmic graph.\\n\\n[1] K. \\u02c7Su\\u02c7sm\\xb4akov\\xb4aemail and A. Krakovsk\\xb4a, \\u201cDiscrimination ability\\nof individual measures used in sleep stages classi\\ufb01cation,\\u201d\\nArti\\ufb01cial Intelligence in Medicine, vol. 44, no. 3, pp. 261\\u2013277,\\n2008.\\n\\n[2] A. Flexer, G. Gruber, and G. Dor\\ufb00ner, \\u201cA reliable probabilistic\\nsleep stager based on a single EEG signal,\\u201d Arti\\ufb01cial Intelligence\\nin Medicine, vol. 33, no. 3, pp. 199\\u2013207, 2005.\\n\\n[3] L. Johnson, A. Lubin, P. Naitoh, C. Nute, and M. Austin,\\n\\u201cSpectral analysis of the EEG of dominant and non-dominant\\n\\n\\x0cAdvances in Arti\\ufb01cial Neural Systems\\n\\n9\\n\\nalpha subjects during waking and sleeping,\\u201d Electroencephalog-\\nraphy and Clinical Neurophysiology, vol. 26, no. 4, pp. 361\\u2013370,\\n1969.\\n\\n[4] J. Pardey, S. Roberts, L. Tarassenko, and J. Stradling, \\u201cA new\\napproach to the analysis of the human sleep/wakefulness con-\\ntinuum,\\u201d Journal of Sleep Research, vol. 5, no. 4, pp. 201\\u2013210,\\n1996.\\n\\n[5] N. Schaltenbrand, R. Lengelle, M. Toussaint et al., \\u201cSleep stage\\nscoring using the neural network model: comparison between\\nvisual and automatic analysis in normal subjects and patients,\\u201d\\nSleep, vol. 19, no. 1, pp. 26\\u201335, 1996.\\n\\n[6] H. G. Jo, J. Y. Park, C. K. Lee, S. K. An, and S. K. Yoo, \\u201cGenetic\\nfuzzy classi\\ufb01er for sleep stage identi\\ufb01cation,\\u201d Computers in\\nBiology and Medicine, vol. 40, no. 7, pp. 629\\u2013634, 2010.\\n\\n[7] L. Zoubek, S. Charbonnier, S. Lesecq, A. Buguet, and F.\\nChapotot, \\u201cA two-steps sleep/wake stages classi\\ufb01er taking into\\naccount artefacts in the polysomnographic signa,\\u201d in Proceed-\\nings of the 17th World Congress, International Federation of\\nAutomatic Control (IFAC \\u201908), July 2008.\\n\\n[8] G. Luo and W. Min, \\u201cSubject-adaptive real-time sleep stage\\nclassi\\ufb01cation based on conditional random \\ufb01eld,\\u201d in Proceed-\\nings of the American Medical Informatics Association Annual\\nSymposium (AMIA \\u201907), pp. 488\\u2013492, 2007.\\n\\n[9] T. Penzel, K. Kesper, V. Gross, H. F. Becker, and C. Vogelmeier,\\n\\u201cProblems in automatic sleep scoring applied to sleep apnea,\\u201d\\nin Proceedings of the 25th Annual International Conference of\\nthe IEEE Engineering in Medicine and Biology Society (IEEE\\nEMBS \\u201903), pp. 358\\u2013361, September 2003.\\n\\n[10] G. E. Hinton, S. Osindero, and Y. W. Teh, \\u201cA fast learning algo-\\nrithm for deep belief nets,\\u201d Neural Computation, vol. 18, no. 7,\\npp. 1527\\u20131554, 2006.\\n\\n[11] Y. Bengio, P. Lamblin, D. Popovici, and H. Larochelle, \\u201cGreedy\\nlayer-wise training of deep networks,\\u201d in Advances in Neural\\nInformation Processing Systems (NIPS \\u201906), vol. 19, pp. 153\\u2013\\n160, 2006.\\n\\n[12] M. Ranzato, C. Poultney, S. Chopra, and Y. LeCun, \\u201cE\\ufb03cient\\nlearning of sparse representations with an energy-based\\nmodel,\\u201d in Proceedings of the Advances in Neural Information\\nProcessing Systems (NIPS \\u201906), J. Platt, T. Ho\\ufb00man, and B.\\nSch\\xa8olkopf, Eds., MIT Press, 2006.\\n\\n[13] Y. Bengio and Y. LeCun, \\u201cScaling learning algorithms towards\\nAI,\\u201d in Large-Scale Kernel Machines, L. Bottou, O. Chapelle, D.\\nDeCoste, and J. Weston, Eds., MIT Press, 2007.\\n\\n[14] Y. Bengio, \\u201cLearning deep architectures for AI,\\u201d Tech. Rep.\\n\\n1312, Department of IRO, Universite de Montreal, 2007.\\n\\n[15] I. Arel, D. Rose, and T. Karnowski, \\u201cDeep machine learning\\u2014a\\nnew frontier in arti\\ufb01cial intelligence research,\\u201d IEEE Computa-\\ntional Intelligence Magazine, vol. 14, pp. 12\\u201318, 2010.\\n\\n[16] V. Nair and G. E. Hinton, \\u201c3-d object recognition with deep\\nbelief nets,\\u201d in Proceedings of the Advances in Neural Informa-\\ntion Processing Systems (NIPS \\u201906), 2006.\\n\\n[17] G. Taylor, G. E. Hinton, and S. Roweis, \\u201cModeling human\\nmotion using binary latent variables,\\u201d in Proceedings of the\\nAdvances in Neural Information Processing Systems, 2007.\\n\\n[18] N. Jaitly and G. E. Hinton, \\u201cLearning a better representation of\\nspeech sound waves using restricted boltzmann machines,\\u201d in\\nProceedings of the IEEE International Conference on Acoustics,\\nSpeech, and Signal Processing (ICASSP \\u201911), 2011.\\n\\n[19] M. L\\xa8angkvist and A. Lout\\ufb01, \\u201cUnsupervised feature learning\\nfor electronic nose data applied to bacteria identi\\ufb01cation in\\nblood,\\u201d in NIPS Workshop on Deep Learning and Unsupervised\\nFeature Learning, 2011.\\n\\n[20] D. F. Wulsin, J. R. Gupta, R. Mani, J. A. Blanco, and B. Litt,\\n\\u201cModeling electroencephalography waveforms with semi-\\nsupervised deep belief nets: fast classi\\ufb01cation and anomaly\\nmeasurement,\\u201d Journal of Neural Engineering, vol. 8, no. 3,\\nArticle ID 036015, 2011.\\n\\n[21] G. E. Hinton, \\u201cTraining products of experts by minimizing\\ncontrastive divergence,\\u201d Neural Computation, vol. 14, no. 8, pp.\\n1771\\u20131800, 2002.\\n\\n[22] A. Rechtscha\\ufb00en and A. Kales, A Manual of Standardized\\nTerminology, Techniques and Scoring System for Sleep Stages of\\nHuman Subjects, U.S. Government Printing O\\ufb03ce, Washing-\\nton DC, USA, 1968.\\n\\n[23] M. Hirshkowitz, \\u201cStanding on the shoulders of giants: the\\nStandardized Sleep Manual after 30 years,\\u201d Sleep Medicine\\nReviews, vol. 4, no. 2, pp. 169\\u2013179, 2000.\\n\\n[24] S. L. Himanen and J. Hasan, \\u201cLimitations of Rechtscha\\ufb00en and\\nKales,\\u201d Sleep Medicine Reviews, vol. 4, no. 2, pp. 149\\u2013167, 2000.\\n[25] L. R. Rabiner and B. H. Juang, \\u201cAn introduction to hidden\\nmarkov models,\\u201d IEEE ASSP Magazine, vol. 3, no. 1, pp. 4\\u201316,\\n1986.\\n\\n[26] G. E. Hinton, A Practical Guide to Training Restricted Boltz-\\n\\nmann Machines, 2010.\\n\\n[27] S. Charbonnier, L. Zoubek, S. Lesecq, and F. Chapotot, \\u201cSelf-\\nevaluated automatic classi\\ufb01er as a decision-support tool for\\nsleep/wake staging,\\u201d Computers in Biology and Medicine, vol.\\n41, no. 6, pp. 380\\u2013389, 2011.\\n\\n[28] A. Schl\\xa8ogl, C. Keinrath, D. Zimmermann, R. Scherer, R. Leeb,\\nand G. Pfurtscheller, \\u201cA fully automated correction method\\nof EOG artifacts in EEG recordings,\\u201d Clinical Neurophysiology,\\nvol. 118, no. 1, pp. 98\\u2013104, 2007.\\n\\n[29] A. L. Goldberger, L. A. Amaral, L. Glass et al., \\u201cPhysioBank,\\nPhysioToolkit, and PhysioNet: components of a new research\\nresource for complex physiologic signals,\\u201d Circulation, vol.\\n101, no. 23, pp. E215\\u2013220, 2000.\\n\\n[30] L. Zoubek, S. Charbonnier, S. Lesecq, A. Buguet, and F.\\nChapotot, \\u201cFeature selection for sleep/wake stages classi\\ufb01ca-\\ntion using data driven methods,\\u201d Biomedical Signal Processing\\nand Control, vol. 2, no. 3, pp. 171\\u2013179, 2007.\\n\\n[31] G. Huang, H. Lee, and E. Learned-Miller, \\u201cLearning hierar-\\nchical representations for face veri\\ufb01cation with convolutional\\ndeep belief networks,\\u201d in Proceedings of the IEEE Conference on\\nComputer Vision and Pattern Recognition (CVPR \\u201912), 2012.\\n\\n[32] D. Yu, L. Deng, I. Jang, P. Kudumakis, M. Sandler, and K. Kang,\\n\\u201cDeep learning and its applications to signal and information\\nprocessing,\\u201d IEEE Signal Processing Magazine, vol. 28, no. 1, pp.\\n145\\u2013154, 2011.\\n\\n[33] M. Ranzato, A. Krizhevsky, and G. E. Hinton, \\u201cFactored 3-way\\nrestricted boltzmann machines for modeling natural images,\\u201d\\nin Proceedings of the 30th International Conference on Arti\\ufb01cial\\nIntelligence and Statistics, 2010.\\n\\n[34] J. Ngiam, A. Khosla, M. Kim, H. Lee, and A. Y. Ng, \\u201cMulti-\\nmodal deep learning,\\u201d in Proceedings of the 28th International\\nConference on Machine Learning, 2011.\\n\\n[35] A. R. Osborne and A. Provenzale, \\u201cFinite correlation dimen-\\nsion for stochastic systems with power-law spectra,\\u201d Physica D,\\nvol. 35, no. 3, pp. 357\\u2013381, 1989.\\n\\n[36] E. Pereda, A. Gamundi, R. Rial, and J. Gonz\\xb4alez, \\u201cNon-linear\\nbehaviour of human EEG: fractal exponent versus correlation\\ndimension in awake and sleep stages,\\u201d Neuroscience Letters, vol.\\n250, no. 2, pp. 91\\u201394, 1998.\\n\\n[37] T. Gasser, P. Baecher, and J. Moecks, \\u201cTransformations towards\\nthe normal distribution of broad band spectral parameters of\\nthe EEG,\\u201d Electroencephalography and Clinical Neurophysiol-\\nogy, vol. 53, no. 1, pp. 119\\u2013124, 1982.\\n\\n\\x0cSubmit your manuscripts at\\n\\nhttp://www.hindawi.com\\n\\nComputer Games  TechnologyInternational Journal ofHindawi Publishing Corporationhttp://www.hindawi.comVolume 2014Hindawi Publishing Corporationhttp://www.hindawi.comVolume 2014Distributed  Sensor NetworksInternational Journal ofAdvances inFuzzySystemsHindawi Publishing Corporationhttp://www.hindawi.comVolume 2014International Journal ofReconfigurableComputingHindawi Publishing Corporation http://www.hindawi.comVolume 2014Hindawi Publishing Corporationhttp://www.hindawi.comVolume 2014 Applied Computational Intelligence and Soft Computing\\u2009Advances\\u2009in\\u2009Artificial IntelligenceHindawi\\u2009Publishing\\u2009Corporationhttp://www.hindawi.comVolume\\u20092014Advances inSoftware EngineeringHindawi Publishing Corporationhttp://www.hindawi.comVolume 2014Hindawi Publishing Corporationhttp://www.hindawi.comVolume 2014Electrical and Computer EngineeringJournal ofJournal ofComputer Networks and CommunicationsHindawi Publishing Corporationhttp://www.hindawi.comVolume 2014Hindawi Publishing Corporationhttp://www.hindawi.comVolume 2014 Advances in Multimedia International Journal of Biomedical ImagingHindawi Publishing Corporationhttp://www.hindawi.comVolume 2014ArtificialNeural SystemsAdvances inHindawi Publishing Corporationhttp://www.hindawi.comVolume 2014RoboticsJournal ofHindawi Publishing Corporationhttp://www.hindawi.comVolume 2014Hindawi Publishing Corporationhttp://www.hindawi.comVolume 2014Computational Intelligence and NeuroscienceIndustrial EngineeringJournal ofHindawi Publishing Corporationhttp://www.hindawi.comVolume 2014Modelling & Simulation in EngineeringHindawi Publishing Corporation http://www.hindawi.comVolume 2014The Scientific World JournalHindawi Publishing Corporation http://www.hindawi.comVolume 2014Hindawi Publishing Corporationhttp://www.hindawi.comVolume 2014Human-ComputerInteractionAdvances inComputer EngineeringAdvances inHindawi Publishing Corporationhttp://www.hindawi.comVolume 2014\\x0c', u'Zero-Shot Learning Through Cross-Modal Transfer\\n\\nRichard Socher, Milind Ganjoo, Christopher D. Manning, Andrew Y. Ng\\nComputer Science Department, Stanford University, Stanford, CA 94305, USA\\n\\nrichard@socher.org, {mganjoo, manning}@stanford.edu, ang@cs.stanford.edu\\n\\nAbstract\\n\\nThis work introduces a model that can recognize objects in images even if no\\ntraining data is available for the object class. The only necessary knowledge about\\nunseen visual categories comes from unsupervised text corpora. Unlike previous\\nzero-shot learning models, which can only differentiate between unseen classes,\\nour model can operate on a mixture of seen and unseen classes, simultaneously\\nobtaining state of the art performance on classes with thousands of training im-\\nages and reasonable performance on unseen classes. This is achieved by seeing\\nthe distributions of words in texts as a semantic space for understanding what ob-\\njects look like. Our deep learning model does not require any manually de\\ufb01ned\\nsemantic or visual features for either words or images. Images are mapped to be\\nclose to semantic word vectors corresponding to their classes, and the resulting\\nimage embeddings can be used to distinguish whether an image is of a seen or un-\\nseen class. We then use novelty detection methods to differentiate unseen classes\\nfrom seen classes. We demonstrate two novelty detection strategies; the \\ufb01rst gives\\nhigh accuracy on unseen classes, while the second is conservative in its prediction\\nof novelty and keeps the seen classes\\u2019 accuracy high.\\n\\nIntroduction\\n\\n1\\nThe ability to classify instances of an unseen visual class, called zero-shot learning, is useful in sev-\\neral situations. There are many species and products without labeled data and new visual categories,\\nsuch as the latest gadgets or car models, that are introduced frequently. In this work, we show how\\nto make use of the vast amount of knowledge about the visual world available in natural language\\nto classify unseen objects. We attempt to model people\\u2019s ability to identify unseen objects even if\\nthe only knowledge about that object came from reading about it. For instance, after reading the\\ndescription of a two-wheeled self-balancing electric vehicle, controlled by a stick, with which you\\ncan move around while standing on top of it, many would be able to identify a Segway, possibly after\\nbeing brie\\ufb02y perplexed because the new object looks different from previously observed classes.\\nWe introduce a zero-shot model that can predict both seen and unseen classes. For instance, without\\never seeing a cat image, it can determine whether an image shows a cat or a known category from\\nthe training set such as a dog or a horse. The model is based on two main ideas.\\nFig. 1 illustrates the model. First, images are mapped into a semantic space of words that is learned\\nby a neural network model [15]. Word vectors capture distributional similarities from a large, unsu-\\npervised text corpus. By learning an image mapping into this space, the word vectors get implicitly\\ngrounded by the visual modality, allowing us to give prototypical instances for various words. Sec-\\nond, because classi\\ufb01ers prefer to assign test images into classes for which they have seen training\\nexamples, the model incorporates novelty detection which determines whether a new image is on the\\nmanifold of known categories. If the image is of a known category, a standard classi\\ufb01er can be used.\\nOtherwise, images are assigned to a class based on the likelihood of being an unseen category. We\\nexplore two strategies for novelty detection, both of which are based on ideas from outlier detection\\nmethods. The \\ufb01rst strategy prefers high accuracy for unseen classes, the second for seen classes.\\nUnlike previous work on zero-shot learning which can only predict intermediate features or differ-\\nentiate between various zero-shot classes [21, 27], our joint model can achieve both state of the art\\naccuracy on known classes as well as reasonable performance on unseen classes. Furthermore, com-\\npared to related work on knowledge transfer [21, 28] we do not require manually de\\ufb01ned semantic\\n\\n1\\n\\n\\x0cFigure 1: Overview of our cross-modal zero-shot model. We \\ufb01rst map each new testing image into\\na lower dimensional semantic word vector space. Then, we determine whether it is on the manifold\\nof seen images. If the image is \\u2018novel\\u2019, meaning not on the manifold, we classify it with the help of\\nunsupervised semantic word vectors. In this example, the unseen classes are truck and cat.\\nor visual attributes for the zero-shot classes, allowing us to use state-of-the-art unsupervised and\\nunaligned image features instead along with unsupervised and unaligned language corpora.\\n2 Related Work\\nWe brie\\ufb02y outline connections and differences to \\ufb01ve related lines of research. Due to space con-\\nstraints, we cannot do justice to the complete literature.\\nZero-Shot Learning. The work most similar to ours is that by Palatucci et al. [27]. They map fMRI\\nscans of people thinking about certain words into a space of manually designed features and then\\nclassify using these features. They are able to predict semantic features even for words for which\\nthey have not seen scans and experiment with differentiating between several zero-shot classes.\\nHowever, they do not classify new test instances into both seen and unseen classes. We extend their\\napproach to allow for this setup using novelty detection. Lampert et al. [21] construct a set of binary\\nattributes for the image classes that convey various visual characteristics, such as \\u201cfurry\\u201d and \\u201cpaws\\u201d\\nfor bears and \\u201cwings\\u201d and \\u201c\\ufb02ies\\u201d for birds. Later, in section 6.4, we compare our method to their\\nmethod of performing Direct Attribute Prediction (DAP).\\nOne-Shot Learning One-shot learning [19, 20] seeks to learn a visual object class by using very few\\ntraining examples. This is usually achieved by either sharing of feature representations [2], model\\nparameters [12] or via similar context [14]. A recent related work on one-shot learning is that of\\nSalakhutdinov et al. [29]. Similar to their work, our model is based on using deep learning tech-\\nniques to learn low-level image features followed by a probabilistic model to transfer knowledge,\\nwith the added advantage of needing no training data due to the cross-modal knowledge transfer\\nfrom natural language.\\nKnowledge and Visual Attribute Transfer. Lampert et al. and Farhadi et al. [21, 10] were two\\nof the \\ufb01rst to use well-designed visual attributes of unseen classes to classify them. This is different\\nto our setting since we only have distributional features of words learned from unsupervised, non-\\nparallel corpora and can classify between categories that have thousands or zero training images. Qi\\net al. [28] learn when to transfer knowledge from one category to another for each instance.\\nDomain Adaptation. Domain adaptation is useful in situations in which there is a lot of training\\ndata in one domain but little to none in another. For instance, in sentiment analysis one could train a\\nclassi\\ufb01er for movie reviews and then adapt from that domain to book reviews [4, 13]. While related,\\nthis line of work is different since there is data for each class but the features may differ between\\ndomains.\\nMultimodal Embeddings. Multimodal embeddings relate information from multiple sources such\\nas sound and video [25] or images and text. Socher et al. [31] project words and image regions into a\\ncommon space using kernelized canonical correlation analysis to obtain state of the art performance\\nin annotation and segmentation. Similar to our work, they use unsupervised large text corpora to\\n\\n2\\n\\nManifold of known classesautohorsedogtruckNew test image from unknown classcatTraining images\\x0clearn semantic word representations. Their model does require a small amount of training data\\nhowever for each class. Some work has been done on multimodal distributional methods [11, 23].\\nMost recently, Bruni et al. [5] worked on perceptually grounding word meaning and showed that\\njoint models are better able to predict the color of concrete objects.\\n3 Word and Image Representations\\nWe begin the description of the full framework with the feature representations of words and images.\\nDistributional approaches are very common for capturing semantic similarity between words. In\\nthese approaches, words are represented as vectors of distributional characteristics \\u2013 most often their\\nco-occurrences with words in context [26, 9, 1, 32]. These representations have proven very effective\\nin natural language processing tasks such as sense disambiguation [30], thesaurus extraction [24, 8]\\nand cognitive modeling [22].\\nUnless otherwise mentioned, all word vectors are initialized with pre-trained d = 50-dimensional\\nword vectors from the unsupervised model of Huang et al. [15]. Using free Wikipedia text, their\\nmodel learns word vectors by predicting how likely it is for each word to occur in its context. Their\\nmodel uses both local context in the window around each word and global document contex, thus\\ncapturing distributional syntactic and semantic information. For further details and evaluations of\\nthese embeddings, see [3, 7].\\nWe use the unsupervised method of Coates et al. [6] to extract I image features from raw pixels in\\nan unsupervised fashion. Each image is henceforth represented by a vector x \\u2208 RI.\\n4 Projecting Images into Semantic Word Spaces\\nIn order to learn semantic relationships and class membership of images we project the image feature\\nvectors into the d-dimensional, semantic word space F . During training and testing, we consider\\na set of classes Y . Some of the classes y in this set will have available training data, others will\\nbe zero-shot classes without any training data. We de\\ufb01ne the former as the seen classes Ys and the\\nlatter as the unseen classes Yu. Let W = Ws \\u222a Wu be the set of word vectors in Rd for both seen\\nand unseen visual classes, respectively.\\nAll training images x(i) \\u2208 Xy of a seen class y \\u2208 Ys are mapped to the word vector wy correspond-\\ning to the class name. To train this mapping, we train a neural network to minimize the following\\nobjective function :\\n\\nJ(\\u0398) =\\n\\n,\\n\\n(1)\\n\\n(cid:88)\\n\\n(cid:88)\\n\\ny\\u2208Ys\\n\\nx(i)\\u2208Xy\\n\\n(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)wy \\u2212 \\u03b8(2)f\\n\\n(cid:16)\\n\\n\\u03b8(1)x(i)(cid:17)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)2\\n\\nwhere \\u03b8(1) \\u2208 Rh\\xd7I, \\u03b8(2) \\u2208 Rd\\xd7h and the standard nonlinearity f = tanh. We de\\ufb01ne \\u0398 =\\n(\\u03b8(1), \\u03b8(2)). A two-layer neural network is shown to outperform a single linear mapping in the\\nexperiments section below. The cost function is trained with standard backpropagation and L-BFGS.\\nBy projecting images into the word vector space, we implicitly extend the semantics with a visual\\ngrounding, allowing us to query the space, for instance for prototypical visual instances of a word.\\nFig. 2 shows a visualization of the 50-dimensional semantic space with word vectors and images\\nof both seen and unseen classes. The unseen classes are cat and truck. The mapping from 50 to 2\\ndimensions was done with t-SNE [33]. We can observe that most classes are tightly clustered around\\ntheir corresponding word vector while the zero-shot classes (cat and truck for this mapping) do not\\nhave close-by vectors. However, the images of the two zero-shot classes are close to semantically\\nsimilar classes (such as in the case of cat, which is close to dog and horse but is far away from car\\nor ship). This observation motivated the idea for \\ufb01rst detecting images of unseen classes and then\\nclassifying them to the zero-shot word vectors.\\n5 Zero-Shot Learning Model\\nIn this section we \\ufb01rst give an overview of our model and then describe each of its components.\\nIn general, we want to predict p(y|x), the conditional probability for both seen and unseen classes\\ny \\u2208 Ys \\u222a Yu given an image from the test set x \\u2208 Xt. To achieve this we will employ the semantic\\nvectors to which these images have been mapped to f \\u2208 Ft.\\nBecause standard classi\\ufb01ers will never predict a class that has no training examples, we introduce\\na binary novelty random variable which indicates whether an image is in a seen or unseen class\\n\\n3\\n\\n\\x0cFigure 2: T-SNE visualization of the semantic word space. Word vector locations are highlighted\\nand mapped image locations are shown both for images for which this mapping has been trained and\\nunseen images. The unseen classes are cat and truck.\\nV \\u2208 {s, u}. Let Xs be the set of all feature vectors for training images of seen classes and Fs their\\ncorresponding semantic vectors. We similarly de\\ufb01ne Fy to be the semantic vectors of class y. We\\npredict a class y for a new input image x and its mapped semantic vector f via:\\n\\np(y|x, Xs, Fs, W, \\u03b8) =\\n\\nP (y|V, x, Xs, Fs, W, \\u03b8)P (V |x, Xs, Fs, W, \\u03b8).\\n\\n(cid:88)\\n\\nV \\u2208{s,u}\\n\\nMarginalizing out the novelty variable V allows us to \\ufb01rst distinguish between seen and unseen\\nclasses. Each type of image can then be classi\\ufb01ed differently. The seen image classi\\ufb01er can be a\\nstate of the art softmax classi\\ufb01er while the unseen classi\\ufb01er can be a simple Gaussian discriminator.\\n5.1 Strategies for Novelty Detection\\nWe now consider two strategies for predicting whether an image is of a seen or unseen class. The\\nterm P (V = u|x, Xs, Fs, W, \\u03b8) is the probability of an image being in an unseen class. An image\\nfrom an unseen class will not be very close to the existing training images but will still be roughly\\nin the same semantic region. For instance, cat images are closest to dogs even though they are not\\nas close to the dog word vector as most dog images are. Hence, at test time, we can use outlier\\ndetection methods to determine whether an image is in a seen or unseen class.\\nWe compare two strategies for outlier detection. Both are computed on the manifold of training\\nimages that were mapped to the semantic word space. The \\ufb01rst method is relatively liberal in its\\nassessment of novelty. It uses simple thresholds on the marginals assigned to each image under iso-\\nmetric, class-speci\\ufb01c Gaussians. The mapped points of seen classes are used to obtain this marginal.\\nFor each seen class y \\u2208 Ys, we compute P (x|Xy, wy, Fy, \\u03b8) = P (f|Fy, wy) = N (f|wy, \\u03a3y). The\\nGaussian of each class is parameterized by the corresponding semantic word vector wy for its mean\\nand a covariance matrix \\u03a3y that is estimated from all the mapped training points with that label. We\\nrestrict the Gaussians to be isometric to prevent over\\ufb01tting. For a new image x, the outlier detector\\nthen becomes the indicator function that is 1 if the marginal probability is below a certain threshold\\nTy for all the classes:\\n\\nP (V = u|f, Xs, W, \\u03b8) := 1{\\u2200y \\u2208 Ys : P (f|Fy, wy) < Ty}\\n\\nWe provide an experimental analysis for various thresholds T below. The thresholds are selected\\nto make at least some fraction of the vectors from training images above threshold, that is, to be\\nclassi\\ufb01ed as a seen class. Intuitively, smaller thresholds result in fewer images being labeled as\\nunseen. The main drawback of this method is that it does not give a real probability for an outlier.\\n\\n4\\n\\n  airplaneautomobilebirdcatdeerdogfroghorseshiptruckcatautomobiletruckfrogshipairplanehorsebirddogdeer\\x0cAn alternative would be to use the method of [17] to obtain an actual outlier probability in an unsu-\\npervised way. Then, we can obtain the conditional class probability using a weighted combination\\nof classi\\ufb01ers for both seen and unseen classes (described below). Fig. 2 shows that many unseen\\nimages are not technically outliers of the complete data manifold. Hence this method is very con-\\nservative in its assignment of novelty and therefore preserves high accuracy for seen classes.\\nWe need to slightly modify the original approach since we distinguish between training and test\\nsets. We do not want to use the set of all test images since they would then not be considered\\noutliers anymore. The modi\\ufb01ed version has the same two parameters: k = 20, the number of\\nnearest neighbors that are considered to determine whether a point is an outlier and \\u03bb = 3, which\\ncan be roughly seen as a multiplier on the standard deviation. The larger it is, the more a point has\\nto deviate from the mean in order to be considered an outlier.\\nFor each point f \\u2208 Ft, we de\\ufb01ne a context set C(f ) \\u2286 Fs of k nearest neighbors in the training set\\nof seen categories. We can compute the probabilistic set distance pdist of each point x to the points\\nin C(f ):\\n\\n(cid:115)(cid:80)\\n\\npdist\\u03bb(f, C(f )) = \\u03bb\\n\\nq\\u2208C(f ) d(f, q)2\\n\\n|C(f )|\\n\\n,\\n\\nwhere d(f, q) de\\ufb01nes some distance function in the word space. We use Euclidean distances. Next\\nwe de\\ufb01ne the local outlier factor:\\n\\nlof\\u03bb(f ) =\\n\\npdist\\u03bb(f, C(f ))\\n\\nEq\\u223cC(f )[pdist\\u03bb(f, C(q))]\\n\\n\\u2212 1.\\n\\nLarge lof values indicate increasing outlierness. In order to obtain a probability, we next de\\ufb01ne a\\nnormalization factor Z that can be seen as a kind of standard deviation of lof values in the training\\nset of seen classes:\\n\\n(cid:113)Eq\\u223cFs[(lof(q))2].\\n(cid:19)(cid:27)\\n(cid:26)\\n(cid:18) lof\\u03bb(f )\\n\\nZ\\u03bb(Fs)\\n\\nNow, we can de\\ufb01ne the Local Outlier Probability:\\n\\nZ\\u03bb(Fs) = \\u03bb\\n\\nLoOP (f ) = max\\n\\n0, erf\\n\\n,\\n\\n(2)\\n\\nwhere erf is the Gauss Error function. This probability can now be used to weigh the seen and unseen\\nclassi\\ufb01ers by the appropriate amount given our belief about the outlierness of a new test image.\\n5.2 Classi\\ufb01cation\\nIn the case where V = s, i.e.\\nthe point is considered to be of a known class, we can use any\\nprobabilistic classi\\ufb01er for obtaining P (y|V = s, x, Xs). We use a softmax classi\\ufb01er on the original\\nI-dimensional features. For the zero-shot case where V = u we assume an isometric Gaussian\\ndistribution around each of the novel class word vectors and assign classes based on their likelihood.\\n6 Experiments\\nFor most of our experiments we utilize the CIFAR-10 dataset [18]. The dataset has 10 classes, each\\nwith 5,000 32 \\xd7 32 \\xd7 3 RGB images. We use the unsupervised feature extraction method of Coates\\nand Ng [6] to obtain a 12,800-dimensional feature vector for each image. For word vectors, we use\\na set of 50-dimensional word vectors from the Huang dataset [15] that correspond to each CIFAR\\ncategory. During training, we omit two of the 10 classes and reserve them for zero-shot analysis.\\nThe remaining categories are used for training.\\nIn this section we \\ufb01rst analyze the classi\\ufb01cation performance for seen classes and unseen classes\\nseparately. Then, we combine images from the two types of classes, and discuss the trade-offs\\ninvolved in our two unseen class detection strategies. Next, the overall performance of the entire\\nclassi\\ufb01cation pipeline is summarized and compared to another popular approach by Lampert et al.\\n[21]. Finally, we run a few additional experiments to assess quality and robustness of our model.\\n6.1 Seen and Unseen Classes Separately\\nFirst, we evaluate the classi\\ufb01cation accuracy when presented only with images from classes that\\nhave been used in training. We train a softmax classi\\ufb01er to label one of 8 classes from CIFAR-10\\n(2 are reserved for zero-shot learning). In this case, we achieve an accuracy of 82.5% on the set of\\n\\n5\\n\\n\\x0cFigure 4: Comparison of accuracies for images from previously seen and unseen categories when\\nunseen images are detected under the (a) Gaussian threshold model, (b) LoOP model. The average\\naccuracy on all images is shown in (c) for both models. We also show a line corresponding to the\\nsingle accuracy achieved in the Bayesian pipeline. In these examples, the zero-shot categories are\\n\\u201ccat\\u201d and \\u201ctruck\\u201d.\\nclasses excluding cat and truck, which closely matches the SVM-based classi\\ufb01cation results in the\\noriginal Coates and Ng paper [6] that used all 10 classes.\\nWe now focus on classi\\ufb01cation between only two zero-shot classes. In this case, the classi\\ufb01cation is\\nbased on isometric Gaussians which amounts to simply comparing distances between word vectors\\nof unseen classes and an image mapped into semantic space. In this case, the performance is good\\nif there is at least one seen class similar to the zero-shot class. For instance, when cat and dog are\\ntaken out from training, the resulting zero-shot classi\\ufb01cation does not work well because none of the\\nother 8 categories is similar enough to both images to learn a good semantic distinction. On the other\\nhand, if cat and truck are taken out, then the cat vectors can be mapped to the word space thanks to\\nsimilarities to dogs and trucks can be distinguished thanks to car, yielding better performance.\\nFig. 3 shows the accuracy achieved in distin-\\nguishing images belonging to various combina-\\ntions of zero-shot classes. We observe, as ex-\\npected, that the maximum accuracy is achieved\\nwhen choosing semantically distinct categories.\\nFor instance, frog-truck and cat-truck do very\\nwell. The worst accuracy is obtained when cat\\nand dog are chosen instead. From the \\ufb01gure we\\nsee that for certain combinations of zero-shot\\nclasses, we can achieve accuracies up to 90%.\\n6.2\\nof Novelty Detectors on Average Accuracy\\nOur next area of investigation is to determine\\nthe average performance of the classi\\ufb01er for\\nthe overall dataset that includes both seen and\\nunseen images. We compare the performance\\nwhen each image is passed through either of the two novelty detectors which decide with a certain\\nprobability (in the second scenario) whether an image belongs to a class that was used in training.\\nDepending on this choice, the image is either passed through the softmax classi\\ufb01er for seen category\\nimages, or assigned to the class of the nearest semantic word vector for unseen category images.\\nFig. 4 shows the accuracies for test images for different choices made by the two scenarios for\\nnovelty detection. The test set includes an equal number of images from each category, with 8\\ncategories having been seen before, and 2 being new. We plot the accuracies of the two types\\nof images separately for comparison. Firstly, at the left extreme of the curve, the Gaussian unseen\\nimage detector treats all of the images as unseen, and the LoOP model takes the probability threshold\\nfor an image being unseen to be 0. At this point, with all unseen images in the test set being treated\\nas such, we achieve the highest accuracies, at 90% for this zero-shot pair. Similarly, at the other\\nextreme of the curve, all images are classi\\ufb01ed as belonging to a seen category, and hence the softmax\\nclassi\\ufb01er for seen images gives the best possible accuracy for these images.\\n\\nFigure 3: Visualization of classi\\ufb01cation accuracy\\nachieved for unseen images, for different choices\\nof zero-shot classes selected before training.\\n\\nIn\\ufb02uence\\n\\n6\\n\\n00.20.40.60.8100.10.20.30.40.50.60.70.80.91(a) Gaussian modelFraction of points classified as unseenAccuracy00.20.40.60.8100.10.20.30.40.50.60.70.80.91(b) LoOP modelOutlier probability thresholdAccuracy00.20.40.60.810.10.20.30.40.50.60.70.8(c) ComparisonFraction unseen/outlier thresholdAccuracy  GaussianLoOP0.58667seen classesseen classesunseen classesunseen classes0.6557cat\\u2212dogplane\\u2212autoauto\\u2212deerdeer\\u2212shipcat\\u2212truck00.10.20.30.40.50.60.70.80.91Pair of zero\\u2212shot classes usedZero\\u2212shot accuracy\\x0cBetween the extremes, the curves for unseen image accuracies and seen image accuracies fall and\\nrise at different rates. Since the Gaussian model is liberal in designating an image as belonging to an\\nunseen category, it treats more of the images as unseen, and hence we continue to get high unseen\\nclass accuracies along the curve. The LoOP model, which tries to detect whether an image could\\nbe regarded as an outlier for each class, does not assign very high outlier probabilities to zero-shot\\nimages due to a large number of them being spread on inside the manifold of seen images (see Fig. 2\\nfor a 2-dimensional visualization of the originally 50-dimensional space). Thus, it continues to treat\\nthe majority of images as seen, leading to high seen class accuracies. Hence, the LoOP model can\\nbe used in scenarios where one does not want to degrade the high performance on classes from the\\ntraining set but allow for the possibility of unseen classes.\\nWe also see from Fig. 4 (c) that since most images in the test set belong to previously seen categories,\\nthe LoOP model, which is conservative in assigning the unseen label, gives better overall accuracies\\nthan the Gaussian model. In general, we can choose an acceptable threshold for seen class accuracy\\nand achieve a corresponding unseen class accuracy. For example, at 70% seen class accuracy in the\\nGaussian model, unseen classes can be classi\\ufb01ed with accuracies of between 30% to 15%, depending\\non the class. Random chance is 10%.\\n6.3 Combining predictions for seen and unseen classes\\nThe \\ufb01nal step in our experiments is to perform the full Bayesian pipeline as de\\ufb01ned by Equation 2.\\nWe obtain a prior probability of an image being an outlier. The LoOP model outputs a probability\\nfor the image instance being an outlier, which we use directly. For the Gaussian threshold model, we\\ntune a cutoff fraction for log probabilities beyond which images are classi\\ufb01ed as outliers. We assign\\nprobabilities 0 and 1 to either side of this threshold. We show the horizontal lines corresponding to\\nthe overall accuracy for the Bayesian pipeline on Figure 4.\\n6.4 Comparison to attribute-based classi\\ufb01cation\\nTo establish a context for comparing our model performance, we also run the attribute-based classi-\\n\\ufb01cation approach outlined by Lampert et al. [21]. We construct an attribute set of 25 attributes high-\\nlighting different aspects of the CIFAR-10 dataset, with certain aspects dealing with animal-based\\nattributes, and others dealing with vehicle-based attributes. We train each binary attribute classi\\ufb01er\\nseparately, and use the trained classi\\ufb01ers to construct attribute labels for unseen classes. Finally,\\nwe use MAP prediction to determine the \\ufb01nal output class. The table below shows a summary of\\nresults. Our overall accuracies for both models outperform the attribute-based model.\\n\\nBayesian pipeline (Gaussian)\\nBayesian pipeline (LoOP)\\nAttribute-based (Lampert et al.)\\n\\n74.25%\\n65.31%\\n45.25%\\n\\nIn general, an advantage of our approach is the ability to adapt to a domain quickly, which is dif\\ufb01cult\\nin the case of the attribute-based model, since appropriate attribute types need to be carefully picked.\\n6.5 Novelty detection in original feature space\\nThe analysis of novelty detectors in 6.2 involves calcula-\\ntion in the word space. As a comparison, we perform the\\nsame experiments with the Gaussian model in the origi-\\nnal feature space. In the mapped space, we observe that\\nof the 100 images assigned the highest probability of be-\\ning an outlier, 12% of those images are false positives. On\\nthe other hand, in the original feature space, the false pos-\\nitive rate increases to 78%. This is intuitively explained\\nby the fact that the mapping function gathers extra seman-\\ntic information from the word vectors it is trained on, and\\nimages are able to cluster better around these assumed\\nGaussian centroids. In the original space, there is no se-\\nmantic information, and the Gaussian centroids need to\\nbe inferred from among the images themselves, which are\\nnot truly representative of the center of the image space\\nfor their classes.\\n6.6 Extension to\\nCIFAR-100 and Analysis of Deep Semantic Mapping\\nSo far, our tests were on the CIFAR-10 dataset. We\\nnow describe results on the more challenging CIFAR-100\\n\\nFigure 5: Comparison of accuracies\\nfor images from previously seen and\\nunseen categories\\nthe modi\\ufb01ed\\nCIFAR-100 dataset, after training the\\nsemantic mapping with a one-layer net-\\nwork and two-layer network.\\nThe\\ndeeper mapping function performs bet-\\nter.\\n\\nfor\\n\\n7\\n\\n00.20.40.60.8100.20.40.60.81Fraction of points classified as seenAccuracy  1\\u2212layer NN2\\u2212layer NNunseen accuraciesseen accuracies\\x0cdataset [18], which consists of 100 classes, with 500 32 \\xd7 32 \\xd7 3 RGB images in each class. We\\nremove 4 categories for which no vector representations were available in our vocabulary. We then\\ncombined the CIFAR-10 dataset to get a set of 106 classes. Six zero-shot classes were chosen: \\u2018for-\\nest\\u2019, \\u2018lobster\\u2019, \\u2018orange\\u2019, \\u2018boy\\u2019, \\u2018truck\\u2019, and \\u2018cat\\u2019. As before, we train a neural network to map the\\nvectors into semantic space. With this setup, we get a peak non-zero-shot accuracy of 52.7%, which\\nis almost near the baseline on 100 classes [16]. When all images are labeled as zero shot, the peak\\naccuracy for the 6 unseen classes is 52.7%, where chance would be at 16.6%.\\nBecause of the large semantic space corresponding to 100 classes, the proximity of an image to\\nits appropriate class vector is dependent on the quality of the mapping into semantic space. We\\nhypothesize that in this scenario a two layer neural network as described in Sec. 4 will perform\\nbetter than a single layer or linear mapping. Fig. 5 con\\ufb01rms this hypothesis. The zero-shot accuracy\\nis 10% higher with a 2 layer neural net compared to a single layer with 42.2%.\\n\\n6.7 Zero-Shot Classes with Distractor Words\\n\\nWe would like zero-shot images to be classi-\\n\\ufb01ed correctly when there are a large number\\nof unseen categories to choose from. To eval-\\nuate such a setting with many possible but in-\\ncorrect unseen classes we create a set of dis-\\ntractor words. We compare two scenarios. In\\nthe \\ufb01rst, we add random nouns to the semantic\\nspace. In the second, much harder, setting we\\nadd the k nearest neighbors of a word vector.\\nWe then evaluate classi\\ufb01cation accuracy with\\neach new set. For the zero-shot class cat and\\ntruck, the nearest neighbors distractors include\\nrabbit, kitten and mouse, among others.\\nThe accuracy does not change much if random\\ndistractor nouns are added. This shows that the\\nsemantic space is spanned well and our zero-\\nshot learning model is quite robust. Fig. 6\\nshows the classi\\ufb01cation accuracies for the second scenario. Here, accuracy drops as an increas-\\ning number of semantically related nearest neighbors are added to the distractor set. This is to be\\nexpected because there are not enough related categories to accurately distinguish very similar cat-\\negories. After a certain number, the effect of a new distractor word is small. This is consistent with\\nour expectation that a certain number of closely-related semantic neighbors would distract the clas-\\nsi\\ufb01er; however, beyond that limited set, other categories would be further away in semantic space\\nand would not affect classi\\ufb01cation accuracy.\\n\\nFigure 6: Visualization of the zero-shot classi\\ufb01-\\ncation accuracy when distractor words from the\\nnearest neighbor set of a given category are also\\npresent.\\n\\n7 Conclusion\\nWe introduced a novel model for jointly doing standard and zero-shot classi\\ufb01cation based on deep\\nlearned word and image representations. The two key ideas are that (i) using semantic word vector\\nrepresentations can help to transfer knowledge between modalities even when these representations\\nare learned in an unsupervised way and (ii) that our Bayesian framework that \\ufb01rst differentiates novel\\nunseen classes from points on the semantic manifold of trained classes can help to combine both\\nzero-shot and seen classi\\ufb01cation into one framework. If the task was only to differentiate between\\nvarious zero-shot classes we could obtain accuracies of up to 90% with a fully unsupervised model.\\n\\nAcknowledgments\\nRichard is partly supported by a Microsoft Research PhD fellowship. The authors gratefully acknowledge\\nthe support of the Defense Advanced Research Projects Agency (DARPA) Deep Exploration and Filtering of\\nText (DEFT) Program under Air Force Research Laboratory (AFRL) prime contract no. FA8750-13-2-0040,\\nthe DARPA Deep Learning program under contract number FA8650-10-C-7020 and NSF IIS-1159679. Any\\nopinions, \\ufb01ndings, and conclusions or recommendations expressed in this material are those of the authors and\\ndo not necessarily re\\ufb02ect the view of DARPA, AFRL, or the US government.\\n\\n8\\n\\n0102030400.20.30.40.50.60.70.80.91Number of distractor wordsAccuracy  Neighbors of catNeighbors of truck\\x0cReferences\\n[1] M. Baroni and A. Lenci. Distributional memory: A general framework for corpus-based semantics.\\n\\nComputational Linguistics, 36(4):673\\u2013721, 2010.\\n\\n[2] E. Bart and S. Ullman. Cross-generalization: learning novel classes from a single example by feature\\n\\nreplacement. In CVPR, 2005.\\n\\n[3] Y. Bengio, R. Ducharme, P. Vincent, and C. Janvin. A neural probabilistic language model. J. Mach.\\n\\nLearn. Res., 3, March 2003.\\n\\n[4] J. Blitzer, M. Dredze, and F. Pereira. Biographies, Bollywood, Boom-boxes and Blenders: Domain\\n\\nAdaptation for Sentiment Classi\\ufb01cation. In ACL, 2007.\\n\\n[5] E. Bruni, G. Boleda, M. Baroni, and N. Tran. Distributional semantics in technicolor. In ACL, 2012.\\n[6] A. Coates and A. Ng. The Importance of Encoding Versus Training with Sparse Coding and Vector\\n\\nQuantization . In ICML, 2011.\\n\\n[7] R. Collobert and J. Weston. A uni\\ufb01ed architecture for natural language processing: deep neural networks\\n\\nwith multitask learning. In ICML, 2008.\\n\\n[8] J. Curran. From Distributional to Semantic Similarity. PhD thesis, University of Edinburgh, 2004.\\n[9] K. Erk and S. Pad\\xb4o. A structured vector space model for word meaning in context. In EMNLP, 2008.\\n[10] A. Farhadi, I. Endres, D. Hoiem, and D. Forsyth. Describing objects by their attributes. In CVPR, 2009.\\n[11] Y. Feng and M. Lapata. Visual information in semantic representation. In HLT-NAACL, 2010.\\n[12] M. Fink. Object classi\\ufb01cation from a single example utilizing class relevance pseudo-metrics. In NIPS,\\n\\n2004.\\n\\n[13] X. Glorot, A. Bordes, and Y. Bengio. Domain adaptation for Large-Scale sentiment classi\\ufb01cation: A deep\\n\\nlearning approach. In ICML, 2011.\\n\\n[14] D. Hoiem, A.A. Efros, and M. Herbert. Geometric context from a single image. In ICCV, 2005.\\n[15] E. H. Huang, R. Socher, C. D. Manning, and A. Y. Ng.\\n\\nImproving Word Representations via Global\\n\\nContext and Multiple Word Prototypes. In ACL, 2012.\\n\\n[16] Yangqing Jia, Chang Huang, and T. Darrell. Beyond spatial pyramids: Receptive \\ufb01eld learning for pooled\\nimage features. In Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on, pages\\n3370 \\u20133377, june 2012.\\n\\n[17] H. Kriegel, P. Kr\\xa8oger, E. Schubert, and A. Zimek. LoOP: local Outlier Probabilities. In Proceedings of\\n\\nthe 18th ACM conference on Information and knowledge management, CIKM \\u201909, 2009.\\n\\n[18] Alex Krizhevsky. Learning Multiple Layers of Features from Tiny Images. Master\\u2019s thesis, Computer\\n\\nScience Department, University of Toronto, 2009.\\n\\n[19] R.; Perona L. Fei-Fei; Fergus. One-shot learning of object categories. TPAMI, 28, 2006.\\n[20] B. M. Lake, J. Gross R. Salakhutdinov, and J. B. Tenenbaum. One shot learning of simple visual concepts.\\n\\nIn Proceedings of the 33rd Annual Conference of the Cognitive Science Society, 2011.\\n\\n[21] C. H. Lampert, H. Nickisch, and S. Harmeling. Learning to Detect Unseen Object Classes by Between-\\n\\nClass Attribute Transfer. In CVPR, 2009.\\n\\n[22] T. K. Landauer and S. T. Dumais. A solution to Plato\\u2019s problem: the Latent Semantic Analysis theory of\\n\\nacquisition, induction and representation of knowledge. Psychological Review, 104(2):211\\u2013240, 1997.\\n\\n[23] C.W. Leong and R. Mihalcea. Going beyond text: A hybrid image-text approach for measuring word\\n\\nrelatedness. In IJCNLP, 2011.\\n\\n[24] D. Lin. Automatic retrieval and clustering of similar words.\\n\\n768\\u2013774, 1998.\\n\\nIn Proceedings of COLING-ACL, pages\\n\\n[25] J. Ngiam, A. Khosla, M. Kim, J. Nam, H. Lee, and A.Y. Ng. Multimodal deep learning. In ICML, 2011.\\n[26] S. Pado and M. Lapata. Dependency-based construction of semantic space models. Computational Lin-\\n\\nguistics, 33(2):161\\u2013199, 2007.\\n\\n[27] M. Palatucci, D. Pomerleau, G. Hinton, and T. Mitchell. Zero-shot learning with semantic output codes.\\n\\nIn NIPS, 2009.\\n\\n[28] Guo-Jun Qi, C. Aggarwal, Y. Rui, Q. Tian, S. Chang, and T. Huang. Towards cross-category knowledge\\n\\npropagation for learning visual concepts. In CVPR, 2011.\\n\\n[29] A. Torralba R. Salakhutdinov, J. Tenenbaum. Learning to learn with compound hierarchical-deep models.\\n\\nIn NIPS, 2012.\\n\\n[30] H. Sch\\xa8utze. Automatic word sense discrimination. Computational Linguistics, 24:97\\u2013124, 1998.\\n\\n9\\n\\n\\x0c[31] R. Socher and L. Fei-Fei. Connecting modalities: Semi-supervised segmentation and annotation of images\\n\\nusing unaligned text corpora. In CVPR, 2010.\\n\\n[32] P. D. Turney and P. Pantel. From frequency to meaning: Vector space models of semantics. Journal of\\n\\nArti\\ufb01cial Intelligence Research, 37:141\\u2013188, 2010.\\n\\n[33] L. van der Maaten and G. Hinton. Visualizing data using t-SNE. Journal of Machine Learning Research,\\n\\n2008.\\n\\n10\\n\\n\\x0c', u'4\\n1\\n0\\n2\\n\\n \\n\\nv\\no\\nN\\n0\\n1\\n\\n \\n\\n \\n \\n]\\n\\nG\\nL\\n.\\ns\\nc\\n[\\n \\n \\n\\n1\\nv\\n9\\n3\\n5\\n2\\n\\n.\\n\\n1\\n1\\n4\\n1\\n:\\nv\\ni\\nX\\nr\\na\\n\\nUnifying Visual-Semantic Embeddings with\\n\\nMultimodal Neural Language Models\\n\\nRyan Kiros, Ruslan Salakhutdinov, Richard S. Zemel\\n\\nUniversity of Toronto\\n\\nCanadian Institute for Advanced Research\\n\\n{rkiros, rsalakhu, zemel}@cs.toronto.edu\\n\\nAbstract\\n\\nInspired by recent advances in multimodal learning and machine translation, we\\nintroduce an encoder-decoder pipeline that learns (a): a multimodal joint embed-\\nding space with images and text and (b): a novel language model for decoding\\ndistributed representations from our space. Our pipeline effectively uni\\ufb01es joint\\nimage-text embedding models with multimodal neural language models. We in-\\ntroduce the structure-content neural language model that disentangles the structure\\nof a sentence to its content, conditioned on representations produced by the en-\\ncoder. The encoder allows one to rank images and sentences while the decoder\\ncan generate novel descriptions from scratch. Using LSTM to encode sentences,\\nwe match the state-of-the-art performance on Flickr8K and Flickr30K without\\nusing object detections. We also set new best results when using the 19-layer Ox-\\nford convolutional network. Furthermore we show that with linear encoders, the\\nlearned embedding space captures multimodal regularities in terms of vector space\\narithmetic e.g. *image of a blue car* - \"blue\" + \"red\" is near images of red cars.\\nSample captions generated for 800 images are made available for comparison.\\n\\n1\\n\\nIntroduction\\n\\nGenerating descriptions for images has long been regarded as a challenging perception task integrat-\\ning vision, learning and language understanding. One not only needs to correctly recognize what\\nappears in images but also incorporate knowledge of spatial relationships and interactions between\\nobjects. Even with this information, one then needs to generate a description that is relevant and\\ngrammatically correct. With the recent advances made in deep neural networks, tasks such as object\\nrecognition and detection have made signi\\ufb01cant breakthroughs in only a short time. The task of\\ndescribing images is one that now appears tractable and ripe for advancement. Being able to append\\nlarge image databases with accurate descriptions for each image would signi\\ufb01cantly improve the\\ncapabilities of content-based image retrieval systems. Moreover, systems that can describe images\\nwell, could in principle, be \\ufb01ne-tuned to answer questions about images also.\\nThis paper describes a new approach to the problem of image caption generation, casted into the\\nframework of encoder-decoder models. For the encoder, we learn a joint image-sentence embedding\\nwhere sentences are encoded using long short-term memory (LSTM) recurrent neural networks [1].\\nImage features from a deep convolutional network are projected into the embedding space of the\\nLSTM hidden states. A pairwise ranking loss is minimized in order to learn to rank images and their\\ndescriptions. For decoding, we introduce a new neural language model called the structure-content\\nneural language model (SC-NLM). The SC-NLM differs from existing models in that it disentangles\\nthe structure of a sentence to its content, conditioned on distributed representations produced by the\\nencoder. We show that sampling from an SC-NLM allows us to generate realistic image captions,\\nsigni\\ufb01cantly improving over the generated captions produced by [2]. Furthermore, we argue that\\nthis combination of approaches naturally \\ufb01ts into the experimentation framework of [3], that is, a\\ngood encoder can be used to rank images and captions while a good decoder can be used to generate\\nnew captions from scratch. Our approach effectively uni\\ufb01es image-text embedding models (encoder\\n\\n1\\n\\n\\x0cFigure 1: Sample generated captions. The bottom row shows different error cases. Additional results\\ncan be found at http://www.cs.toronto.edu/~rkiros/lstm_scnlm.html\\n\\nphase) [4, 5, 6] with multimodal neural language models (decoder phase) [2] [7]. Furthermore, our\\nmethod builds on analogous approaches being used in machine translation [8, 9, 10, 11].\\nWhile the application focus of our work is on image description generation and ranking, we also\\nqualitatively analyse properties of multimodal vector spaces learned using images and sentences. We\\nshow that using a linear sentence encoder, linguistic regularities [12] also carry over to multimodal\\nvector spaces. For example, *image of a blue car* - \"blue\" + \"red\" results in a vector that is near\\nimages of red cars. We qualitatively examine several types of analogies and structures with PCA\\nprojections. Consequently, even with a global image-sentence training objective the encoder can still\\nbe used to retrieve locally (e.g. individual words). This is analogous to pairwise ranking methods\\nused in machine translation [13, 14].\\n\\n1.1 Multimodal representation learning\\n\\nA large body of work has been done on learning multimodal representations of images and text.\\nPopular approaches include learning joint image-word embeddings [4, 5] as well as embedding\\nimages and sentences into a common space [6, 15]. Our proposed pipeline makes direct use of\\nthese ideas. Other approaches to multimodal learning include the use of deep Boltzmann machines\\n[16], log-bilinear neural language models [2], autoencoders [17], recurrent neural networks [7] and\\ntopic-models [18]. Several bi-directional approaches to ranking images and captions have also been\\nproposed, based off of kernel CCA [3], normalized CCA [19] and dependency tree recursive net-\\nworks [6]. From an architectural standpoint, our encoder-decoder model is most similar to [20], who\\nproposed a two-step embedding and generation procedure for semantic parsing.\\n\\n1.2 Generating descriptions of images\\n\\nWe group together approaches to generation into three types of methods, each described here in\\nmore detail:\\nTemplate-based methods. Template-based methods involve \\ufb01lling in sentence templates, such as\\ntriplets, based on the results of object detections and spatial relationships [21, 22, 23, 24, 25]. While\\n\\n2\\n\\n\\x0cFigure 2: Encoder: A deep convolutional network (CNN) and long short-term memory recurrent\\nnetwork (LSTM) for learning a joint image-sentence embedding. Decoder: A new neural language\\nmodel that combines structure and content vectors for generating words one at a time in sequence.\\n\\nthese approaches can produce accurate descriptions, they are often more \\u2018robotic\\u2019 in nature and do\\nnot generalize to the \\ufb02uidity and naturalness of captions written by humans.\\nComposition-based methods. These approaches aim to harness existing image-caption databases\\nby extracting components of related captions and composing them together to generate novel de-\\nscriptions [26, 27]. The advantage of these approaches are that they allow for a much broader\\nand more expressive class of captions that are more \\ufb02uent and human-like then template-based ap-\\nproaches.\\nNeural network methods. These approaches aim to generate descriptions by sampling from condi-\\ntional neural language models. The initial work in this area, based off of multimodal neural language\\nmodels [2], generated captions by conditioning on feature vectors from the output of a deep con-\\nvolutional network. These ideas were recently extended to multimodal recurrent networks with\\nsigni\\ufb01cant improvements [7]. The methods described in this paper produce descriptions that at least\\nqualitatively on par with current state-of-the-art composition-based methods [27].\\nDescription generation systems have been plagued with issues of evaluation. While Bleu and Rouge\\nhave been used in the past, [3] has argued that such automated evaluation methods are unreliable\\nand do not match human judgements. These authors instead proposed that the problem of ranking\\nimages and captions can be used as a proxy for generation. Since any generation system requires a\\nscoring function to access how well a caption and image match, optimizing this task should naturally\\ncarry over to an improvement in generation. Many recent methods have since used this approach\\nfor evaluation. None the less, the question on how to transfer improvements on ranking to gen-\\nerating new descriptions remained. We argue that encoder-decoder methods naturally \\ufb01t into this\\nexperimentation framework. That is, the encoder gives us a way to rank images and captions and\\ndevelop good scoring functions, while the decoder can use the representations learned to optimize\\nthe scoring functions as a way of generating and scoring new descriptions.\\n\\n1.3 Encoder-decoder methods for machine translation\\n\\nOur proposed pipeline, while new to caption generation, has already experienced several successes in\\nNeural Machine Translation (NMT). The goal of NMT is to develop an end-to-end translation system\\nwith a large neural network, as opposed to using a neural network as an additional feature function\\nto an existing phrase-based system. NMT methods are based on the encoder-decoder principle.\\nThat is, an encoder is used to map an English sentence to a distributed vector. A decoder is then\\nconditioned on this vector to generate a French translation from the source text. Current methods\\ninclude using a convolutional encoder and RNN decoder [8], RNN encoder and RNN decoder [9, 10]\\nand LSTM encoder with LSTM decoder [11]. While still a young research area, these methods have\\nalready achieved performance on par with strong phrase-based systems and have improved on the\\nstart-of-the-art when used for rescoring.\\nWe argue that it is natural to think of image caption generation as a translation problem. That is,\\nour goal is to translate an image into a description. This point of view has also been used by [28]\\nand allows us to make use of existing ideas in the machine translation literature. Furthermore, there\\nis a natural correspondence between the concept of scoring functions (how well does a caption and\\nimage match) and alignments (which parts of a description correspond to which parts of an image)\\nthat can naturally be exploited for generating descriptions.\\n\\n3\\n\\n\\x0c2 An encoder-decoder model for ranking and generation\\n\\nIn this section we describe our image caption generation pipeline. We \\ufb01rst review LSTM RNNs\\nwhich are used for encoding sentences, followed by how to learn multimodal distributed represen-\\ntations. We then review log-bilinear neural language models [29], multiplicative neural language\\nmodels [30] and then introduce our structure-content neural language model.\\n\\n2.1 Long short-term memory RNNs\\n\\nLong short-term memory [1] is a recurrent neural network that incorporates a built in memory cell\\nto store information and exploit long range context. LSTM memory cells are surrounded by gat-\\ning units for the purpose of reading, writing and reseting information. LSTMs have been used to\\nachieve state-of-the-art performance in several tasks such as handwriting recognition [31], sequence\\ngeneration [32] speech recognition [33] and machine translation [11] among others. Dropout [34]\\nstrategies have also been proposed to prevent over\\ufb01tting in deep LSTMs. [35]\\nLet Xt denote a matrix of training instances at time t.\\nIn our case, Xt is used to denote a\\nmatrix of word representations for the t-th word of each sentence in the training batch. Let\\n(It, Ft, Ct, Ot, Mt) denote the input, forget, cell, output and hidden states of the LSTM at time\\nstep t. The LSTM architecture in this work is implemented using the following equations:\\n\\nIt = \\u03c3(Xt \\xb7 Wxi + Mt\\u22121 \\xb7 Whi + Ct\\u22121 \\xb7 Wci + bi)\\nFt = \\u03c3(Xt \\xb7 Wxf + Mt\\u22121 \\xb7 Whf + Ct\\u22121 \\xb7 Wcf + bf )\\nCt = Ft \\u2022 Ct\\u22121 + It \\u2022 tanh(Xt \\xb7 Wxc + Mt\\u22121 \\xb7 Whc + bc)\\nOt = \\u03c3(Xt \\xb7 Wxo + Mt\\u22121 \\xb7 Who + Ct \\xb7 Wco + bo)\\nMt = Ot \\u2022 tanh(Ct)\\n\\n(1)\\n(2)\\n(3)\\n(4)\\n(5)\\nwhere (\\u03c3) denotes the sigmoid activation function, (\\xb7) indicates matrix multiplication and (\\u2022) indi-\\ncates component-wise multiplication. 1\\n\\n2.2 Multimodal distributed representations\\n\\nSuppose for training we are given image-description pairs each corresponding to an image and a\\ndescription that correctly describes the image. Images are represented as the top layer (before the\\nsoftmax) of a convolutional network trained on the ImageNet classi\\ufb01cation task [36].\\nLet D be the dimensionality of an image feature vector (e.g. 4096 for AlexNet [36]), K the di-\\nmensionality of the embedding space and let V be the number of words in the vocabulary. Let\\nWI \\u2208 RK\\xd7D and WT \\u2208 RK\\xd7V be the image embedding matrix and word embedding matri-\\nces, respectively. Given an image description S = {w1, . . . , wN} with words w1, . . . , wN , 2 let\\n{w1, . . . , wN}, wi \\u2208 RK, i = 1, . . . , n denote the corresponding word representations to words\\nw1, . . . , wN (entries in the matrix WT ). The representation of a sentence v is the hidden state of\\nthe LSTM at time step N (i.e. the vector mt). We note that other approaches for computing sentence\\nrepresentations for image-text embeddings have been proposed, including dependency tree RNNs\\n[6] and bags of dependency parses [15]. Let q \\u2208 RD denote an image feature vector (for the image\\ncorresponding to description S) and let x = WI \\xb7 q \\u2208 RK be the image embedding. We de\\ufb01ne a\\nscoring function s(x, v) = x \\xb7 v, where x and v are \\ufb01rst scaled to have unit norm (making s equiv-\\nalent to cosine similarity). Let \\u03b8 denote all the parameters to be learned (WI and all the LSTM\\nweights) 3. We optimize the following pairwise ranking loss:\\n\\nmax{0, \\u03b1 \\u2212 s(x, v) + s(x, vk)} +\\n\\nmax{0, \\u03b1 \\u2212 s(v, x) + s(v, xk)}\\n\\n(6)\\n\\n(cid:88)\\n\\n(cid:88)\\n\\n(cid:88)\\n\\n(cid:88)\\n\\nmin\\n\\n\\u03b8\\n\\nx\\n\\nk\\n\\nv\\n\\nk\\n\\nwhere vk is a contrastive (non-descriptive) sentence for image embedding x, and vice-versa with xk.\\nFor all of our experiments, we initialize the word embeddings WT to be pre-computed K = 300\\ndimensional vectors learned using a continuous bag-of-words model [37]. The contrastive terms are\\nchosen randomly from the training set and resampled every epoch.\\n\\n1For additional details on LSTM: http://people.idsia.ch/~juergen/rnn.html.\\n2As a slight abuse of notation, we refer to wi as both a word and an index into the word embedding matrix.\\n3We keep the word embedding matrix WT \\ufb01xed.\\n\\n4\\n\\n\\x0c2.3 Log-bilinear neural language models\\n\\nThe log-bilinear language model (LBL) [29] is a deterministic model that may be viewed as a feed-\\nforward neural network with a single linear hidden layer. Each word w in the vocabulary is repre-\\nsented as a K-dimensional real-valued vector w \\u2208 RK, as in the case of the encoder. Let R denote\\na V \\xd7 K matrix of word representation vectors 4 where V is the vocabulary size. Let (w1, . . . wn\\u22121)\\nbe a tuple of n\\u2212 1 words where n\\u2212 1 is the context size. The LBL model makes a linear prediction\\nof the next word representation as\\n\\n(7)\\nwhere C(i), i = 1, . . . , n \\u2212 1 are K \\xd7 K context parameter matrices. Thus, \\u02c6r is the predicted\\nrepresentation of wn. The conditional probability P (wn = i|w1:n\\u22121) of wn given w1, . . . , wn\\u22121 is\\n\\nC(i)wi,\\n\\n\\u02c6r =\\n\\ni=1\\n\\nn\\u22121(cid:88)\\n\\nP (wn = i|w1:n\\u22121) =\\n\\n,\\n\\n(8)\\n\\n(cid:80)V\\n\\nexp(\\u02c6rT ri + bi)\\nj=1 exp(\\u02c6rT rj + bj)\\n\\nwhere b \\u2208 RV is a bias vector. Learning is done with stochastic gradient descent.\\n\\n2.4 Multiplicative neural language models\\nSuppose now we are given a vector u \\u2208 RK from the multimodal vector space, which has an\\nassociation with a word sequence S = {w1, . . . , wN}. For example, u may be the embedded\\nrepresentation of an image whose description is given by S. A multiplicative neural language model\\n[30] models the distribution P (wn = i|w1:n\\u22121, u) of a new word wn given context from the previous\\nwords and the vector u. A multiplicative model has the additional property that the word embedding\\nmatrix is instead replaced with a tensor T \\u2208 RV \\xd7K\\xd7G where G is the number of slices. Given u,\\ni=1 uiT (i) i.e. word\\nrepresentations with respect to u are computed as a linear combination of slices weighted by each\\ncomponent ui of u. Here, the number of slices G is equal to K, the dimensionality of u.\\nIt is often unnecessary to use a fully unfactored tensor. As in e.g. [38, 39], we re-represent T in\\nterms of three matrices Wf k \\u2208 RF\\xd7K, Wf d \\u2208 RF\\xd7G and Wf v \\u2208 RF\\xd7V , such that\\n\\nwe can compute a word representation matrix as a function of u as T u =(cid:80)G\\n\\nT u = (Wf v)(cid:62) \\xb7 diag(Wf du) \\xb7 Wf k\\n\\n(9)\\nwhere diag(\\xb7) denotes the matrix with its argument on the diagonal. These matrices are parametrized\\nby a pre-chosen number of factors F . In [30], the conditioning vector u is referred to as an attribute\\nand using a third-order model of words allows one to model conditional similarity: how meanings\\nof words change as a function of the attributes they\\u2019re conditioned on.\\nLet E = (Wf k)(cid:62)Wf v denote a \\u2018folded\\u2019 K \\xd7 V matrix of word embeddings. Given the context\\nw1, . . . , wn\\u22121, the predicted next word representation \\u02c6r is given by:\\n\\n\\u02c6r =\\n\\nC(i)E(:, wi),\\n\\n(10)\\n\\nn\\u22121(cid:88)\\n\\ni=1\\n\\nwhere E(:, wi) denotes the column of E for the word representation of wi and C(i), i = 1, . . . , n\\u22121\\nare K \\xd7 K context matrices. Given a predicted next word representation \\u02c6r, the factor outputs\\nare f = (Wf k\\u02c6r) \\u2022 (Wf du), where \\u2022 is a component-wise product. The conditional probability\\nP (wn = i|w1:n\\u22121, u) of wn given w1, . . . , wn\\u22121 and u can be written as\\n\\nP (wn = i|w1:n\\u22121, u) =\\n\\nexp(cid:0)(Wf v(:, i))(cid:62)f + bi\\n(cid:1)\\nj=1 exp(cid:0)(Wf v(:, j))(cid:62)f + bj\\n(cid:80)V\\n\\n(cid:1) ,\\n\\nwhere Wf v(:, i) denotes the column of Wf v corresponding to word i. In contrast to the log-bilinear\\nmodel, the matrix of word representations R from before is replaced with the factored tensor T that\\nwe have derived. We compared the multiplicative model against an additive variant [2] and found on\\nlarge datasets, such as the SBU Captioned Photo dataset [40], the multiplicative variant signi\\ufb01cantly\\noutperforms its additive counterpart. Thus, the SC-NLM is derived from the multiplicative variant.\\n\\n4Note that this is a different matrix then that used by the encoder. We use the same vocabulary throughout\\n\\nboth models.\\n\\n5\\n\\n\\x0c(a) Multiplicative NLM\\n\\n(b) Structure-content NLM\\n\\n(c) SC-NLM prediction\\n\\nFigure 3: Left: multiplicative neural language model. Middle: Structure-content neural language\\nmodel (SC-NLM). Right: The prediction problem of an SC-NLM.\\n\\n2.5 Structure-content neural language models\\n\\nWe now describe the structure-content neural language model. Suppose that, along with a de-\\nscription S = {w1, . . . , wN}, we are also given a sequence of word-speci\\ufb01c structure variables\\nT = {t1, . . . , tN}. Throughout our experiments, each ti corresponds to the part-of-speech for word\\nwi, although other possibilities can be used instead. Given an embedding u (the content vector), our\\ngoal is to model the distribution P (wn = i|w1:n\\u22121, tn:n+k, u) from previous word context w1:n\\u22121\\nand forward structure context tn:n+k, where k is the forward context size. Figure 3 gives an illus-\\ntration of the model and prediction problem. Intuitively, the structure variables help guide the model\\nduring the generation phrase and can be thought of as a soft template to help avoid the model from\\ngenerating grammatical nonsense. Note that this model shares a resemblance with the NNJM of [41]\\nfor machine translation, where the previous word context are predicted words in the target language,\\nand the forward context are words in the source language.\\nOur model can be interpreted as a multiplicative neural language model but where the attribute\\nvector is no longer u but instead an additive function of u and the structure variables T . Let\\n{tn, . . . , tn+k}, ti \\u2208 RK, i = n, . . . , n + k be embedding vectors for the structure variables T .\\nThese are obtained from a learned lookup table in the same way as words are. We introduce a se-\\nquence of G \\xd7 G structure context matrices T(i), i = n, . . . , n + k which play the same role as the\\nword context matrices C(i). Let Tu denote a G \\xd7 K context matrix for the multimodal vector u.\\nThe attribute vector \\u02c6u of combined structure and content information is computed as\\n\\n(cid:34)(cid:32)n+k(cid:88)\\n\\n(cid:33)\\n\\n(cid:35)\\n\\n\\u02c6u =\\n\\nT(i)ti\\n\\n+ T(u)u + b\\n\\n(11)\\n\\ni=n\\n\\n+\\n\\nwhere [\\xb7]+ = max{\\xb7, 0} is a ReLU non-linearity and b is a bias vector. The vector \\u02c6u now plays the\\nsame role as the vector u for the multiplicative model previously described and the remainder of the\\nmodel remains unchanged. Our experiments use G = K = 300 and factors F = 100.\\nThe SC-NLM is trained on a large collection of image descriptions (e.g. Flickr30K). There are\\nseveral choices available for representing the conditioning vectors u. One choice would be to use\\nthe embedding of the corresponding image. An alternative choice, which is the approach we take, is\\nto condition on the embedding vector for the description S computed with the LSTM. The advantage\\nof this approach is that the SC-NLM can be trained purely on text alone. This allows us to make\\nuse of large amounts of monolingual text (e.g. non image captions) to improve the quality of the\\nlanguage model. Since the embedding vectors of S share a joint space with the image embeddings,\\nwe can also condition the SC-NLM on image embeddings (e.g. at test time, when no description\\nis available) after the model has been trained. This is a signi\\ufb01cant advantage over a conditional\\nlanguage model that explicitly requires image-caption pairs for training and highlights the strength\\nof a multimodal encoding space.\\nDue to space limitations, we leave the full details of our caption generation procedure to the supple-\\nmentary material.\\n\\n6\\n\\n\\x0cModel\\nRandom Ranking\\nSDT-RNN [6]\\n\\u2020 DeViSE [5]\\n\\u2020 SDT-RNN [6]\\nDeFrag [15]\\n\\u2020 DeFrag [15]\\nm-RNN [7]\\nOur model\\nOur model (OxfordNet)\\n\\nFlickr8K\\nImage Annotation\\n\\nImage Search\\n\\nR@1 R@5 R@10 Med r R@1 R@5 R@10 Med r\\n500\\n0.1\\n29\\n4.5\\n4.8\\n29\\n25\\n6.0\\n32\\n5.9\\n15\\n12.6\\n15\\n14.5\\n13.5\\n14\\n10\\n18.0\\n\\n1.1\\n28.6\\n27.3\\n34.0\\n27.3\\n44.0\\n48.5\\n45.7\\n55.0\\n\\n1.0\\n29.0\\n29.6\\n31.7\\n26.5\\n42.5\\n42.4\\n43.7\\n51.5\\n\\n0.6\\n18.0\\n16.5\\n22.7\\n19.2\\n32.9\\n37.2\\n36.2\\n40.9\\n\\n631\\n32\\n28\\n23\\n34\\n14\\n11\\n13\\n8\\n\\n0.1\\n6.1\\n5.9\\n6.6\\n5.2\\n9.7\\n11.5\\n10.4\\n12.5\\n\\n0.5\\n18.5\\n20.1\\n21.6\\n17.6\\n29.6\\n31.0\\n31.0\\n37.0\\n\\nTable 1: Flickr8K experiments. R@K is Recall@K (high is good). Med r is the median rank (low is good).\\nBest results overall are bold while best results without OxfordNet features are underlined. A \\u2020 infront of the\\nmethod indicates that object detections were used along with single frame features.\\n\\n3 Experiments\\n\\n3.1\\n\\nImage-sentence ranking\\n\\nOur main quantitative results is to establish the effectiveness of using an LSTM sentence encoder\\nfor ranking image and descriptions. We perform the same experimental procedure as done by [15]\\non the Flickr8K [3] and Flickr30K [42] datasets. These datasets come with 8,000 and 30,000 images\\nrespectively with each image annotated using 5 sentences by independent annotators. As with [15],\\nwe did not do any explicit text preprocessing. We used two convolutional network architectures\\nthe Toronto ConvNet 5 as well as the 19-layer\\nfor extracting 4096 dimensional image features:\\nOxfordNet [43] which \\ufb01nished 2nd place in the ILSVRC 2014 classi\\ufb01cation competition. Following\\nthe protocol of [15], 1000 images are used for validation, 1000 for testing and the rest are used for\\ntraining. Evaluation is performed using Recall@K, namely the mean number of images for which\\nthe correct caption is ranked within the top-K retrieved results (and vice-versa for sentences). We\\nalso report the median rank of the closest ground truth result from the ranked list. We compare our\\nresults to each of the following methods:\\nDeViSE. The deep visual semantic embedding model [5] was proposed as a way of performing zero-\\nshot object recognition and was used as a baseline by [15]. In this model, sentences are represented\\nas the mean of their word embeddings and the objective function optimized matches ours.\\nSDT-RNN. The semantic dependency tree recursive neural network [6] is used to learn sentence\\nrepresentations for embedding into a joint image-sentence space. The same objective is used.\\nDeFrag. Deep fragment embeddings [15] were proposed as an alternative to embedding full-frame\\nimage features and take advantage of object detections from the R-CNN [44] detector. Descriptions\\nare represented as a bag of dependency parses. Their objective incorporates both a global and\\nfragment objectives, for which their global objective matches ours.\\nm-RNN. The multimodal recurrent neural network [7] is a recently proposed method that uses per-\\nplexity as a bridge between modalities, as \\ufb01rst introduced by [2]. Unlike all other methods, the\\nm-RNN does not use a ranking loss and instead optimizes the log-likelihood of predicting the next\\nword in a sequence conditioned on an image.\\nOur LSTMs use 1 layer with 300 units and weights initialized uniformly from [-0.08, 0.08]. The\\nmargin \\u03b1 was set to \\u03b1 = 0.2, which we found performed well on both datasets. Training is done\\nusing stochastic gradient descent with an initial learning rate of 1 and was exponentially decreased.\\nWe used minibatch sizes of 40 on Flickr8K and 100 on Flickr30K. No momentum was used. The\\nsame hyperparameters are used for the OxfordNet experiments.\\n\\n3.1.1 Results\\n\\nTables 1 and 2 illustrate our results on Flickr8K and Flickr30K respectively. The performance of\\nour model is comparable to that of the m-RNN. For some metrics we outperform or match existing\\nresults while on others m-RNN outperforms our model. The m-RNN does not learn an explicit em-\\nbedding between images and sentences and relies on perplexity as a means of retrieval. Methods that\\n\\n5https://github.com/TorontoDeepLearning/convnet\\n\\n7\\n\\n\\x0cFlickr30K\\n\\nImage Annotation\\n\\nImage Search\\n\\nModel\\nRandom Ranking\\n\\u2020 DeViSE [5]\\n\\u2020 SDT-RNN [6]\\n\\u2020 DeFrag [15]\\n\\u2020 DeFrag + Finetune CNN [15]\\nm-RNN [7]\\nOur model\\nOur model (OxfordNet)\\n\\nR@1 R@5 R@10 Med r R@1 R@5 R@10 Med r\\n500\\n0.1\\n25\\n4.5\\n9.6\\n16\\n14\\n14.2\\n13\\n16.4\\n16\\n18.4\\n13\\n14.8\\n23.0\\n8\\n\\n1.0\\n32.7\\n41.1\\n44.2\\n44.5\\n41.5\\n46.3\\n56.5\\n\\n1.1\\n29.2\\n41.1\\n51.3\\n54.7\\n50.9\\n50.9\\n62.9\\n\\n0.5\\n21.9\\n29.8\\n30.8\\n31.4\\n31.2\\n34.0\\n42.0\\n\\n0.6\\n18.1\\n29.8\\n37.7\\n40.2\\n40.2\\n39.2\\n50.7\\n\\n631\\n26\\n16\\n10\\n8\\n10\\n10\\n5\\n\\n0.1\\n6.7\\n8.9\\n10.2\\n10.3\\n12.6\\n11.8\\n16.8\\n\\nTable 2: Flickr30K experiments. R@K is Recall@K (high is good). Med r is the median rank (low is good).\\nBest results overall are bold while best results without OxfordNet features are underlined. A \\u2020 infront of the\\nmethod indicates that object detections were used along with single frame features.\\n\\nlearn explicit embedding spaces have a signi\\ufb01cant speed advantage over perplexity-based retrieval\\nmethods, since retrieval is easily done with a single matrix multiply of stored embedding vectors\\nfrom the dataset with the query vector. Thus explicit embedding methods are much better suited for\\nscaling to large datasets.\\nPerhaps more interestingly is the fact that both our method and the m-RNN outperform existing\\nmodels that integrate object detections. This is contradictory to [6], where recurrent networks are the\\nworst performing models. This highlights the effectiveness of LSTM cells for encoding dependen-\\ncies across descriptions and learning meaningful distributed sentence representations. Integrating\\nobject detections into our framework should almost surely improve performance as well as allow for\\ninterpretable retrievals, as in the case of DeFrag.\\nUsing image features from the OxfordNet model results in a signi\\ufb01cant performance boost across\\nall metrics, giving new state-of-the-art numbers on these evaluation tasks.\\n\\n3.2 Multimodal linguistic regularities\\n\\nWord embeddings learned with skip-gram [37] or neural language models [45] were shown by [12]\\nto exhibit linguistic regularities that allow these models to perform analogical reasoning. For in-\\nstance, \"man\" is to \"woman\" as \"king\" is to ? can be answered by \\ufb01nding the closest vector to\\n\"king\" - \"man\" + \"woman\". A natural question we ask is whether multimodal vector spaces exhibit\\nthe same phenomenon. Would *image of a blue car* - \"blue\" + \"red\" be near images of red cars?\\n\\nSuppose that we train an embedding model with a linear encoder, namely v =(cid:80)N\\n\\ni=1 wi for word\\nvectors wi and sentence vector v (where both v and the image embedding are normalized to unit\\nlength). Using our example above, let vblue, vred and vcar denote the word embeddings for blue, red\\nand car respectively. Let Ibcar and Ircar denote embeddings of images with blue and red cars. After\\ntraining a linear encoder, the model has the property that vblue + vcar \\u2248 Ibcar and vred + vcar \\u2248\\nIrcar. It follows that\\n\\n(12)\\n(13)\\n(14)\\nThus given a query image q, a negative word wn and a positive word wp (all with unit norm), we\\nseek an image x\\u2217 such that:\\n\\nvcar \\u2248 Ibcar \\u2212 vblue\\nvred + vcar \\u2248 Ibcar \\u2212 vblue + vred\\nIrcar \\u2248 Ibcar \\u2212 vblue + vred\\n\\nx\\u2217 = argmax\\n\\nx\\n\\n(q \\u2212 wn + wp)(cid:62)x\\n(cid:107)q \\u2212 wn + wp(cid:107)\\n\\n(15)\\n\\nThe supplementary material contains qualitative evidence that the above holds for several types\\nof regularities and images. 6 In our examples, we consider retrieving the top-4 nearest images.\\nOccasionally we observed that a poor result would be obtained within the top-4 among good results.\\nWe found a simple strategy for removing these cases is to \\ufb01rst retrieve the top N nearest images,\\nthen re-sort these based on their distance to the mean of the N images.\\nIt is worth noting that these kinds of regularities are not well observed with an LSTM encoder, since\\nsentences are no longer just a sum of their words. The linear encoder is roughly equivalent to the\\n\\n6For this model we \\ufb01netune the word representations.\\n\\n8\\n\\n\\x0cDeViSE baselines in tables 1 and 2, which perform signi\\ufb01cantly worse for retrieval than an LSTM\\nencoder. So while these regularities are interesting the learned multimodal vector space is not well\\napt for ranking sentences and images.\\n\\n3.3\\n\\nImage caption generation\\n\\nWe generated image descriptions for roughly 800 images from the SBU captioned photo dataset [40].\\nThese are the same images used to display results by the current state-of-the-art composition based\\napproach, TreeTalk [27]. 7 Our LSTM encoder and SC-NLM decoder were trained by concatenating\\nthe Flickr30K dataset with the recently released Microsoft COCO dataset [46], which combined\\ngive us over 100,000 images and over 500,000 descriptions for training. The SBU dataset contains 1\\nmillion images each with a single description and was used by [27] for training their model. While\\nthe SBU dataset is larger, the annotated descriptions are noisier and more personalized.\\nThe generated results can be found at http://www.cs.toronto.edu/~rkiros/lstm_\\nscnlm.html 8. For each image we show the original caption, the nearest neighbour sentence\\nfrom the training set, the top-5 generated samples from our model and the best generated result from\\nTreeTalk. The nearest neighbour sentence is displayed to demonstrate that our model has not simply\\nlearned to copy the training data. Our generated descriptions are arguably the nicest ones to date.\\n\\n4 Discussion\\n\\nWhen generating a description, it is often the case that only a small region is relevant at any given\\ntime. We are developing an attention-based model that jointly learns to align parts of captions to\\nimages and use these alignments to determine where to attend next, thus dynamically modifying the\\nvectors used for conditioning the decoder. We also plan on experimenting with LSTM decoders as\\nwell as deep and bidirectional LSTM encoders.\\n\\nAcknowledgments\\n\\nWe would like to thank Nitish Srivastava for assistance with his ConvNet package as well as prepar-\\ning the Oxford convolutional network. We also thank the anonymous reviewers from the NIPS 2014\\ndeep learning workshop for their comments and suggestions.\\n\\nReferences\\n[1] Sepp Hochreiter and J\\xfcrgen Schmidhuber. Long short-term memory. Neural computation,\\n\\n1997.\\n\\n[2] Ryan Kiros, Richard S Zemel, and Ruslan Salakhutdinov. Multimodal neural language models.\\n\\nICML, 2014.\\n\\n[3] Micah Hodosh, Peter Young, and Julia Hockenmaier. Framing image description as a ranking\\n\\ntask: Data, models and evaluation metrics. JAIR, 2013.\\n\\n[4] Jason Weston, Samy Bengio, and Nicolas Usunier. Large scale image annotation: learning to\\n\\nrank with joint word-image embeddings. Machine learning, 2010.\\n\\n[5] Andrea Frome, Greg S Corrado, Jon Shlens, Samy Bengio, Jeffrey Dean, and Tomas Mikolov\\n\\nMarcAurelio Ranzato. Devise: A deep visual-semantic embedding model. NIPS, 2013.\\n\\n[6] Richard Socher, Q Le, C Manning, and A Ng. Grounded compositional semantics for \\ufb01nding\\n\\nand describing images with sentences. In TACL, 2014.\\n\\n[7] Junhua Mao, Wei Xu, Yi Yang, Jiang Wang, and Alan L Yuille. Explain images with multi-\\n\\nmodal recurrent neural networks. arXiv preprint arXiv:1410.1090, 2014.\\n\\n[8] Nal Kalchbrenner and Phil Blunsom. Recurrent continuous translation models. In EMNLP,\\n\\n2013.\\n\\n[9] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. EMNLP, 2014.\\n\\n7http://ilp-cky.appspot.com/generation\\n8These results use features from the Toronto ConvNet.\\n\\n9\\n\\n\\x0c[10] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by\\n\\njointly learning to align and translate. arXiv preprint arXiv:1409.0473, 2014.\\n\\n[11] Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural\\n\\nnetworks. NIPS, 2014.\\n\\n[12] Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig. Linguistic regularities in continuous space\\n\\nword representations. In NAACL-HLT, 2013.\\n\\n[13] Karl Moritz Hermann and Phil Blunsom. Multilingual distributed representations without word\\n\\nalignment. ICLR, 2014.\\n\\n[14] Karl Moritz Hermann and Phil Blunsom. Multilingual models for compositional distributional\\n\\nsemantics. In ACL, 2014.\\n\\n[15] Andrej Karpathy, Armand Joulin, and Li Fei-Fei. Deep fragment embeddings for bidirectional\\n\\nimage sentence mapping. NIPS, 2014.\\n\\n[16] Nitish Srivastava and Ruslan Salakhutdinov. Multimodal learning with deep boltzmann ma-\\n\\nchines. In NIPS, 2012.\\n\\n[17] Jiquan Ngiam, Aditya Khosla, Mingyu Kim, Juhan Nam, Honglak Lee, and Andrew Ng. Mul-\\n\\ntimodal deep learning. In ICML, 2011.\\n\\n[18] Yangqing Jia, Mathieu Salzmann, and Trevor Darrell. Learning cross-modality similarity for\\n\\nmultinomial data. In ICCV, 2011.\\n\\n[19] Yunchao Gong, Liwei Wang, Micah Hodosh, Julia Hockenmaier, and Svetlana Lazebnik. Im-\\nproving image-sentence embeddings using large weakly annotated photo collections. In ECCV.\\n2014.\\n\\n[20] Phil Blunsom, Nando de Freitas, Edward Grefenstette, Karl Moritz Hermann, et al. A deep\\n\\narchitecture for semantic parsing. In ACL 2014 Workshop on Semantic Parsing, 2014.\\n\\n[21] Girish Kulkarni, Visruth Premraj, Sagnik Dhar, Siming Li, Yejin Choi, Alexander C Berg,\\nand Tamara L Berg. Baby talk: Understanding and generating simple image descriptions. In\\nCVPR, 2011.\\n\\n[22] Ali Farhadi, Mohsen Hejrati, Mohammad Amin Sadeghi, Peter Young, Cyrus Rashtchian, Julia\\nHockenmaier, and David Forsyth. Every picture tells a story: Generating sentences from\\nimages. In ECCV. 2010.\\n\\n[23] Siming Li, Girish Kulkarni, Tamara L Berg, Alexander C Berg, and Yejin Choi. Composing\\n\\nsimple image descriptions using web-scale n-grams. In CONLL, 2011.\\n\\n[24] Yezhou Yang, Ching Lik Teo, Hal Daum\\xe9 III, and Yiannis Aloimonos. Corpus-guided sentence\\n\\ngeneration of natural images. In EMNLP, 2011.\\n\\n[25] Margaret Mitchell, Xufeng Han, Jesse Dodge, Alyssa Mensch, Amit Goyal, Alex Berg, Kota\\nYamaguchi, Tamara Berg, Karl Stratos, and Hal Daum\\xe9 III. Midge: Generating image descrip-\\ntions from computer vision detections. In EACL, 2012.\\n\\n[26] Polina Kuznetsova, Vicente Ordonez, Alexander C Berg, Tamara L Berg, and Yejin Choi.\\n\\nCollective generation of natural image descriptions. ACL, 2012.\\n\\n[27] Polina Kuznetsova, Vicente Ordonez, Tamara L. Berg, and Yejin Choi. Treetalk : Composition\\n\\nand compression of trees for image descriptions. TACL, 2014.\\n\\n[28] Marcus Rohrbach, Wei Qiu, Ivan Titov, Stefan Thater, Manfred Pinkal, and Bernt Schiele.\\n\\nTranslating video content to natural language descriptions. In ICCV, 2013.\\n\\n[29] Andriy Mnih and Geoffrey Hinton. Three new graphical models for statistical language mod-\\n\\nelling. In ICML, pages 641\\u2013648, 2007.\\n\\n[30] Ryan Kiros, Richard S Zemel, and Ruslan Salakhutdinov. A multiplicative model for learning\\n\\ndistributed text-based attribute representations. NIPS, 2014.\\n\\n[31] Alex Graves, Marcus Liwicki, Santiago Fern\\xe1ndez, Roman Bertolami, Horst Bunke, and J\\xfcr-\\ngen Schmidhuber. A novel connectionist system for unconstrained handwriting recognition.\\nTPAMI, 2009.\\n\\n[32] Alex Graves. Generating sequences with recurrent neural networks.\\n\\narXiv:1308.0850, 2013.\\n\\narXiv preprint\\n\\n[33] Alex Graves, Navdeep Jaitly, and Abdel-rahman Mohamed. Hybrid speech recognition with\\n\\ndeep bidirectional lstm. In IEEE Workshop on ASRU, 2013.\\n\\n10\\n\\n\\x0c[34] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\n\\nnov. Dropout: A simple way to prevent neural networks from over\\ufb01tting. JMLR, 2014.\\n\\n[35] Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals. Recurrent neural network regularization.\\n\\narXiv preprint arXiv:1409.2329, 2014.\\n\\n[36] Alex Krizhevsky, Ilya Sutskever, and Geoff Hinton. Imagenet classi\\ufb01cation with deep convo-\\n\\nlutional neural networks. In NIPS, 2012.\\n\\n[37] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Ef\\ufb01cient estimation of word\\n\\nrepresentations in vector space. arXiv preprint arXiv:1301.3781, 2013.\\n\\n[38] Roland Memisevic and Geoffrey Hinton. Unsupervised learning of image transformations. In\\n\\nCVPR, pages 1\\u20138, 2007.\\n\\n[39] Alex Krizhevsky, Geoffrey E Hinton, et al. Factored 3-way restricted boltzmann machines for\\n\\nmodeling natural images. In AISTATS, pages 621\\u2013628, 2010.\\n\\n[40] Vicente Ordonez, Girish Kulkarni, and Tamara L Berg. Im2text: Describing images using 1\\n\\nmillion captioned photographs. In NIPS, 2011.\\n\\n[41] Jacob Devlin, Rabih Zbib, Zhongqiang Huang, Thomas Lamar, Richard Schwartz, and John\\nMakhoul. Fast and robust neural network joint models for statistical machine translation. ACL,\\n2014.\\n\\n[42] Peter Young Alice Lai Micah Hodosh and Julia Hockenmaier. From image descriptions to vi-\\nsual denotations: New similarity metrics for semantic inference over event descriptions. TACL,\\n2014.\\n\\n[43] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale\\n\\nimage recognition. arXiv preprint arXiv:1409.1556, 2014.\\n\\n[44] Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. Rich feature hierarchies for\\n\\naccurate object detection and semantic segmentation. CVPR, 2014.\\n\\n[45] Yoshua Bengio, R\\xe9jean Ducharme, Pascal Vincent, and Christian Janvin. A neural probabilistic\\n\\nlanguage model. JMLR, 2003.\\n\\n[46] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan,\\nPiotr Doll\\xe1r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. arXiv\\npreprint arXiv:1405.0312, 2014.\\n\\n11\\n\\n\\x0c5 Supplementary material: Additional experimentation and details\\n\\n5.1 Multimodal linguistic regularities\\n\\n(a) Simple cases\\n\\n(b) Colors\\n\\n(c) Image structure\\n\\n(d) Sanity check\\n\\nFigure 4: Multimodal vector space arithmetic. Query images were downloaded online and retrieved\\nimages are from the SBU dataset.\\n\\n(a) Colors\\n\\n(b) Weather\\n\\nFigure 5: PCA projection of the 300-dimensional word and image representations for (a) cars and\\ncolors and (b) weather and temperature.\\n\\nFigure 4 illustrates sample results using a model trained on the SBU dataset. All queries were\\ndownloaded online and retrieved images are from the SBU images used for training. What is of\\ninterest to note is that the resulting images depend highly on the image used for the query. For\\nexample, searching for the word \\u2018night\\u2019 retrieves arbitrary images taken at night. On the other\\nhand, an image with a building predominantly as its focus will return night images when \\u2018day\\u2019 is\\n\\n12\\n\\n\\x0csubtracted and \\u2018night\\u2019 is added. A similar phenomenon occurs with the example of cats, bowls and\\nboxes. As additional visualizations, we computed PCA projections of cars and their corresponding\\ncolors as well as images and the weather occurrences in Figure 5. These results give us strong\\nevidence for the regularities apparent in multimodal vector spaces trained with linear encoders. Of\\ncourse, sensible results are only likely to be obtained if (a) the content of the image is correctly\\nrecognized, (b) the subtraction word is relevant to the image and (c) an image exists that is sensible\\nfor the corresponding query.\\n\\n5.2\\n\\nImage description generation\\n\\nThe SC-NLM was trained on the concatenation of training sentences from both Flickr30K and Mi-\\ncrosoft COCO. Given an image, we \\ufb01rst map it into the multimodal space. From this embedding,\\nwe de\\ufb01ne 2 sets of candidate conditioning vectors to the SC-NLM:\\nImage embedding. The embedded image itself. Note that the SC-NLM was not trained with images\\nbut can be conditioned on images since the embedding space is multimodal.\\ntop-N nearest words and sentences. After \\ufb01rst computing the image embedding, we obtain the\\ntop-N nearest neighbour words and training sentences using cosine similarity. These retrievals are\\ntreated as a \\u2018bag of concepts\\u2019 for which we compute an embedding vector as the mean of each\\nconcept. All of our results use N = 5.\\nAlong with the candidate conditioning vectors, we also compute candidate POS sequences used by\\nthe SC-NLM. For this, we obtain a set of all POS sequences from the training set whose lengths\\nwere between 4 and 12, inclusive. Captions are generated by \\ufb01rst sampling a conditioning vector,\\nnext sampling a POS sequence, then computing a MAP estimate from the SC-NLM. We generate\\na large list of candidate descriptions (1000 for each image in our results) and rank these candidates\\nusing a scoring function. Our scoring function consists of two feature functions:\\nTranslation model. The candidate description is embedded into the multimodal space using the\\nLSTM. We then compute a translation score as the cosine similarity between the image embedding\\nand the embedding of the candidate description. This scores how relevant the content of the candi-\\ndate is to the image. We also augment to this score a multiplicative penalty to non-stopwords that\\nappear too frequently in the description. 9\\nLanguage model. We trained a Kneser-Ney trigram model on a large corpus and compute the log-\\nprobability of the candidate under the model. This scores how reasonable of an English sentence is\\nthe candidate.\\nThe total score of a caption is then the weighted sum of the translation and language models. Due to\\nthe challenge of quantitatively evaluating generated descriptions, we tuned the weights by hand on\\nqualitative results alone. All of the candidate descriptions are ranked by their scores, and the top-5\\ncaptions are returned.\\n\\n9For instance, given an image of a car, we would want a candidate to be ranked low if each noun in the\\n\\ndescription was \\u2018car\\u2019.\\n\\n13\\n\\n\\x0c', u'RECENT ADVANCES IN DEEP LEARNING FOR SPEECH RESEARCH AT MICROSOFT  \\n\\n \\n\\nLi Deng, Jinyu Li, Jui-Ting Huang, Kaisheng Yao, Dong Yu, Frank Seide, Michael L. Seltzer, Geoff Zweig, \\n\\nXiaodong He, Jason Williams, Yifan Gong, and Alex Acero \\n\\nMicrosoft Corporation, One Microsoft Way, Redmond, WA 98052, USA \\n\\n \\n\\nABSTRACT \\n\\n \\nDeep  learning  is  becoming  a  mainstream  technology  for  speech \\nrecognition  at  industrial  scale.  In  this  paper,  we  provide  an \\noverview of the  work by Microsoft speech researchers since 2009 \\nin this area, focusing on more recent advances which shed light to \\nthe  basic  capabilities  and  limitations  of  the  current  deep  learning \\ntechnology.  We  organize  this  overview  along  the  feature-domain \\nand  model-domain  dimensions  according  to  the  conventional \\napproach  to  analyzing  speech  systems.  Selected  experimental \\nresults, including speech recognition and related applications such \\nas  spoken  dialogue  and  language  modeling,  are  presented  to \\ndemonstrate  and  analyze  the  strengths  and  weaknesses  of  the \\ntechniques described in the paper. Potential improvement of these \\ntechniques and future research directions are discussed.  \\n Index  Terms\\u2014  deep  learning,  neural  network,  multilingual, \\nspeech recognition, spectral features, convolution, dialogue    \\n \\n\\n1. INTRODUCTION \\n\\nincreasingly \\n\\n \\nFor  many  years,  speech  recognition \\ntechnology  has  been \\ndominated  by  a  \\u201cshallow\\u201d  architecture  using  many  Gaussians  in \\nthe  mixtures  associated  with  HMM  states  to  represent  acoustic \\nvariability  in  the  speech  signal.  Since  2009,  in  collaboration  with \\nresearchers at University of Toronto and other organizations, we at \\nMicrosoft  have  developed  deep  learning  technology  that  has \\nsuccessfully  replaced  Gaussian  mixtures  for  speech  recognition \\nand  feature  coding  at  an \\nlarger  scale  (e.g., \\n[24][19][53][39][7][8][44][54][13][56][30][48]).  In  this  paper,  we \\nprovide an overview of this body of work, with emphasis on more \\nrecent experiments which shed light onto the understanding of the \\nbasic  capabilities  and  limitations  of  the  current  deep  learning \\ntechnology for speech recognition and related applications. \\n     The organization of this paper is as follows. In Sections 2-5, we \\nfocus  on  several  aspects  of  deep  learning  in  the  feature-domain \\nwith the theme of how deep models can enable the effective use of \\nprimitive, \\ninformation-rich  spectral  features.  The  remaining \\nsections are focused on the model-domain implementation of deep \\nlearning  and  on  two  application  areas  beyond  acoustic  modeling \\nfor  speech  recognition.  Representative  experimental  results  are \\nshown to facilitate the analysis on the strengths and weaknesses of \\nthe techniques we have developed and illustrated in this paper.  \\n \\n\\n2.  BACK TO PRIMITIVE SPECTRAL FEATURES  \\n\\n \\nDeep  learning,  sometimes  referred  as  representation  learning  or \\n(unsupervised)  feature  learning  [3]  sets  an  important  goal  of \\nautomatic  discovery  of  powerful  features  from  raw  input  data \\nindependent  of  application  domains.  For  speech  feature  learning \\nand  for  speech  recognition,  this  goal  is  condensed  to  the  use  of \\nprimitive spectral [26]or possibly waveform [46] features.  \\n\\nthe  past  30  years  or  so, \\n\\nlargely  \\u201chand-crafted\\u201d \\n     Over \\ntransformations  of  speech  spectrogram  have  led  to  significant \\naccuracy  improvements  in  the  Gaussian  mixture  model  (GMM) \\nbased HMM systems, despite the known loss of information from \\nthe  raw  speech  data.  The  most  successful  transformation  is  the \\nnon-adaptive  cosine  transform,  which  gave  rise  to  Mel-frequency \\ncepstral  coefficients  (MFCC)  and  the  related  PLP  features.  The \\ncosine  transform  approximately  de-correlates  feature  components, \\nwhich is important for the use of diagonal GMMs. However, when \\nGMMs  are  replaced  by  deep  learning  models  such  as  deep  neural \\nnets (DNN), deep belief nets (DBN), or deep autoencoders (DAE), \\nsuch  de-correlation  become  irrelevant  due  to  the  very  strength  of \\nthe deep learning methods in modeling data correlation. Our early \\nwork [19] demonstrated such strength and in particular the benefit \\nof  spectrograms  over  MFCCs  in  effective  coding  of  bottleneck \\nspeech features using DAE in an unsupervised manner. Subsequent \\nwork carried out at Stanford [40] generalized the use of DAE from \\nsingle  modality  of  speech  to  bimodal  speech  and  visual  features. \\nThis  success  partly  inspired  the  mixed-band  and  multilingual \\nDNNs to be described in Section 4. \\n     More  recent  experiments  at  Microsoft  demonstrate  noticeably \\nlower  speech  recognition  errors  using  large-scale  DNNs  when \\nmoving  from  MFCCs  back  to  more  primitive  filter-bank  features \\n(i.e., a Mel-scaled spectrogram with no cosine transforms). Table 1 \\nis a summary of these experiments, where the DNN-HMM speech \\nrecognizer  in  a  voice  search  task  makes  use  of  72  hours  of  audio \\ntraining  data  with  over  26  million  frames.  The  relative  error  rate \\nreduction  going  from  MFCC  to  filter-banks  shown  in  Table  1  is \\ncomparable  to  that  which  we  also  observed  for  the  TIMIT  phone \\nrecognition task. Note the use of raw FFT features has not resulted \\nin even lower errors, suggesting that current DNN training cannot \\nautomatically  learn  Mel-like  filter  weights.  The  same  difficulty  is \\nalso  found  for  learning  or  improving  delta-like  features  as  shown \\nin the bottom two rows of Table 1. \\n \\n\\nTable 1: Comparing MFCC with filter-bank features \\n\\nSystems (Features: static+\\u0394+\\u0394\\u0394) \\nBest GMM-HMM (MFCCs; fMPE+BMMI) \\nDNN (MFCCs) \\nDNN (256 log FFT bins) \\nDNN (29 log filter-banks) \\nDNN (40 log filter-banks) \\n    -Static 40-log-filter-banks only (11-frames) \\n    -Static 40-log-filter-banks only (19-frames) \\n\\nWord error rate \\n\\n34.7% \\n31.6% \\n32.3% \\n30.1% \\n29.9% \\n31.1% \\n30.5% \\n\\n      \\n     One  advantage  of  MFCC  is  its  automatic  normalization  (after \\nremoving C0) of power variation arising from different microphone \\ngains  associated  with  different  data  sets.  When  spectral  (or  time-\\ndomain)  features  are  used,  each  feature  component  is  subject  to \\nsuch variation [46]. However, when the data sets are obtained from \\nthe same source in training the DNN system (as is the case for the \\n\\n\\x0ctask of Table 1), similar  error rates are obtained with and without \\napplying a sentence-level spectral feature normalization procedure \\n(30.0%  vs.  30.1%).  But  when  the  data  sets  are  from  diverse \\nsources, we observed that the  application of feature normalization \\nprocedures  has  reduced  the  error  rate  from  24.5%  to  23.7%. \\nEffective  online  normalization  of  features,  however, \\nis  a \\ncumbersome  process  in  practical  speech  recognition  scenarios. \\nThis  raises  a  need  for  improving  the  current  DNN  method  in \\nhandling  amplitude  or  power  variation  across  all  spectral  feature \\ncomponents.  One  potential  solution  is  the  use  of  rectified  linear \\ninstead of sigmoid hidden units. \\n \\n\\n3.  CONVOLUTION ON SPECTRAL FEATURES \\n\\n \\n\\ntypical  speech \\n\\nCompared  with  MFCCs,  \\u201craw\\u201d  spectral  features  not  only  retain \\nmore information (including possibly redundant or irrelevant one), \\nbut  also  enable  the  use  of  convolution  and  pooling  operations  to \\nrepresent  and  handle  some \\ninvariance  and \\nvariability  ---  e.g.,  vocal  tract  length  differences  across  speakers, \\ndistinct  speaking  styles  causing  formant  undershoot  or  overshoot, \\netc. --- expressed explicitly in the frequency domain.  \\n     As a baseline, we explored a primitive convolutional neural net \\n(CNN)  [1]  where  the  pooling  configuration  is  fixed.  The  larger \\npooling  size  enforces  a  greater  degree  of  invariance  to  frequency \\nshifts  while  also  running  a  greater  risk  of  confusion  among \\ndifferent speech sounds with similar formant frequencies. Based on \\ndetailed  error  analysis,  we  have  developed  a  strategy  for  trading \\nbetween  invariance  and  discrimination.  This  strategy  reduces  the \\nTIMIT  core  test  set\\u2019s  phone  recognition  error  rate  to  19.7%  from \\n20.4%.  After  regularizing  the  CNN  using  a  variant  of  the \\n\\u201cdropout\\u201d  technique  [25],  the  error  rate  drops  further  to  18.7%. \\nNote  all  the  above  error  analysis  and  the  interpretation  of  the \\nconvolution  and  pooling  operations  in  the  CNN  have  been  made \\npossible  after  the  change  from  the  use  of  MFCC  to  spectral \\nfeatures.  Details  of  this  new  deep  CNN  and  error  analysis  are \\nprovided in [17]. \\n\\n \\n\\n4. LEARNING MULTI-TASK FEATURES \\n\\n \\n\\nFrom  its  very  original  motivation,  deep  learning  or  representation \\nlearning algorithms are designed to make them especially powerful \\nin multi-task scenarios that would benefit from universal or shared \\nfeature  representations  in  the  intermediate  layer(s)  of  the  deep \\narchitecture; e.g., [40]. In this section, we present and analyze two \\nsets  of  speech  recognition  experiments  to  demonstrate  that  the \\nDNN  is  a  universal  learner  that effectively  handles  heterogeneous \\ndata from different acoustic sources and languages.  \\n \\n4.1 Mixed-Band DNN \\n\\nIn this set of experiments, we design the filter-banks in such a \\nway that the narrowband (8-kHz) data are treated as wideband (16-\\nkHz) data with half of the feature dimensions missing. We use the \\nsame filter-bank design that is described and used in [20]. For the \\n8-kHz  data,  the  upper  filter  banks  are  padded  with  0\\u2019s  in  the \\nmultitask  architecture  shown  in  Figure  1a.  The  common  layers \\nextract  features  that  correlate  with  both  the  narrowband  and \\nwideband data. \\n\\n     Experiments  have  been  carried  out  on  a  large-scale  speech \\nrecognition  task,  with  the  results  summarized  in  Table  2;  see \\ndetails  in  [34].  The  use  of  additional  narrowband  data,  which  is \\nvery different but highly correlated with wideband data (of primary \\n\\n \\n\\n \\n\\nFigure 1: a) left: DNN training/testing with mixed-band acoustic \\ndata with16-kHz and 8-kHz sampling rates; b) right: Illustrative \\narchitecture for multilingual DNN  \\n \\nbusiness  interest  to  us),  has  reduced  the  error  rate  from  30.0%  to \\n28.3%, amounting to 5.7% relative error reduction with the number \\nof test words being 26,757. In our group\\u2019s previous work, we made \\nseveral attempts to exploit narrowband data (plentiful from earlier \\ntelephone-based  applications)  to  benefit  training  the  wideband \\nspeech  models  in  the  GMM-HMM  framework  without  success. \\nSwitching to the DNN created a quick success. \\n \\nTable 2: DNN performance on wideband and narrowband test sets \\n(a multitask-learning setup) using mixed-bandwidth training data. \\n\\nTraining Data \\n\\nTest WER \\n(Wideband) \\n\\nTest WER \\n\\n(Narrowband) \\n\\nWideband only \\nNarrowband only \\nWideband+Narrowband \\n\\n30.0% \\n\\n- \\n\\n28.3% \\n\\n71.2% \\n29.0% \\n29.3% \\n\\n \\n\\n4.2 Multi-Lingual DNN \\n\\nMultilingual  speech  recognition  is  of  high  practical  value.    It \\nhas a long history of research, making use of many sources of prior \\nknowledge  including  phonology  and  speech  production  [43][16] \\nand  of  model  adaptation  [35]  or  neural  net  initialization  [49][51]. \\nHowever, given the very nature of  multitask  machine learning (as \\nreviewed in [11]), multilingual speech recognition is best suited for \\nthe  DNN  where  the  intermediate  hidden  layer(s)  is  expected  to \\nprovide  universal  representations  across  multiple \\nlanguages\\u2019 \\nacoustic data that are highly correlated. \\n\\nWe  developed  and  experimentally  evaluated  the  multilingual \\nDNN architecture shown in Figure 1b. It has the input and hidden \\nlayers shared by all languages, but separate output layers are made \\nspecific  to  each  language.  In  the  training  phase,  the  multilingual \\nDNN  is  exposed  to  the  training  acoustic  data  from  all  languages. \\nGiven  a  training  data  point,  regardless  of  the  language,  all  shared \\nDNN  parameters  are  updated,  but  we  learn  only  the  top-layer \\nweights  corresponding  to  the  correct  language.  After  the  training, \\nthe  entire  DNN  except  the  top  layer  can  be  considered  as  the \\nfeature extractor shared across all languages. \\n\\n Using  this  language-universal  feature  extractor,  we  readily \\nconstruct a powerful monolingual DNN for any target language as \\nfollows.  First,  the top  layer  and its  connection  to  the  hidden  layer \\ntrained  previously  are  discarded.  Then,  a  new \\nlayer \\ncorresponding  to  the  target  language\\u2019s  senone  set  is  built  and  the \\n\\ntop \\n\\n\\x0cweights  to  the  language-universal  hidden  layer  are  trained  using \\nthe limited training data from the target language. \\n     The  multilingual  DNN  has  been  evaluated  on  a  Microsoft-\\ninternal speech recognition task. In the training set, we used French \\n(FRA),  German  (DEU),  Spanish  (ESP),  and  Italian  (ITA)  as  the \\nresource-rich languages, with 138 hours, 195 hours, 63 hours, and \\n93 hours of  speech  data,  respectively.  In  Table  3,  the  final  WERs \\nare compared on a FRA test set for two monolingual FRA DNNs: \\none is trained using only FRA data and the other extracted from the \\nmultilingual  (FRA+DEU+ESP+ITA)  DNN.  The  latter  DNN  gives \\n3.5% fewer errors than the former DNN. \\n\\nTable 3: Comparing DNN word error rates on a resource-rich \\ntask (FRA training data=138 hrs) w. & wo other languages \\nWER on FRA \\n\\nSpeech Recognizers \\nDNN trained with only FRA data \\nDNN trained with FRA + DEU + ESP+ ITA \\n\\n28.1% \\n27.1% \\n\\n \\n     For  cross-lingual  evaluation  of  the  multilingual  DNN,  we \\nused  9  hours  of  training  data  from  U.S.  English  (ENU)  as  the \\nresource-limited  target  language,  with  typical  results  presented  in \\nTable  4.    Retraining  only  the  top  layer  gives  lower  errors  than \\nretraining all layers due to the data sparsity in ENU. Adding three \\nmore  source  languages  in  training  further  reduces  recognition \\nerrors.  We  see  that  the  multilingual  DNN  provides  an  effective \\nstructure  for \\nlearnt  from  multiple \\nlanguages to the DNN for a resource-limited target language due to \\nphonetic information sharing. \\n\\ninformation \\n\\ntransferring \\n\\nTable  4:  Comparing  DNN  word  error  rates  on  a  resource-\\nlimited task (ENU training data=9 hrs) w. & wo other languages.  \\n\\nSpeech Recognizers \\nDNN trained with only ENU data \\n    +FRA, retrain all layers with ENU \\nor +FRA, retrain the top layer with ENU \\nor +FRA+ DEU+ ESP+ITA, retrain top layer \\n\\nWER on ENU \\n\\n30.9% \\n30.6% \\n27.3% \\n25.3% \\n\\n \\n\\n5. NOISE-ROBUST INTERNAL FEATURES \\n\\n \\n\\nA  main  benefit  of  the  DNN  as  the  acoustic  model  is  its  ability  to \\ndiscover representations that are stable with respect to variations in \\nthe  training  data.  One  significant  source  of  such  variations  is \\nenvironmental  noise.  In  order  to  evaluate  the  noise-robustness  of \\nDNN-based acoustic models, we performed a series of experiments \\nusing Aurora 4, a medium-vocabulary corpus based on WSJ0.    \\n\\nThe  results  in  Table  5  compare  the  performance  of  four \\nsystems on the Aurora 4 task. The first is the baseline GMM-HMM \\nsystem  with  no  compensation.  The  second  system  [21]  represents \\nthe  state  of  the  art  in  noise  robustness  for  HMM-based  speech \\nrecognition,  combining  MPE  discriminative  training  and  noise-\\nadaptive  training  (e.g.,  [31][12])  to  compensate  for  noise  and \\nchannel  mismatch.  The  third  system  uses  a  log-linear  model  with \\nfeatures derived from HMM likelihoods [41]. The final system is a \\nDNN-HMM with 7 hidden layers and 2000 hidden units per layer. \\nThis  system  uses  no  explicit  noise  compensation  algorithm.  The \\nDNN-HMM  significantly  outperforms  the  other  systems.  In \\naddition,  the  DNN-HMM  result  was  obtained  in  a  single  pass, \\nwhile  the  previous  two  systems  require  multiple  passes  for \\nadaptation.  These \\ninherent \\nrobustness  of  the  hidden-layer  features  in  the  DNN  to  unwanted \\nvariability from noise and channel mismatch.  \\n\\nresults  clearly  demonstrate \\n\\nthe \\n\\nTable  5:  Word  error  rate  (%)  for  all  four  test  sets  (A,  B,  C, \\n\\nand D) of the Aurora 4 task. DNN outperforms GMM systems \\n\\n \\n\\nA \\n\\nB \\n\\nC \\n\\nD \\n\\nAVG \\n\\nGMM-HMM (Baseline) \\nGMM (MPE+VAT)  \\n\\n12.5  18.3 \\n7.2  12.8 \\n\\nGMM + Deriv. Kernels \\nDNN (7x2000) \\n. \\n\\n \\n\\n7.4 \\n5.6 \\n\\n12.6 \\n8.8 \\n\\n20.5 \\n11.5 \\n\\n10.7 \\n8.9 \\n\\n31.9 \\n19.7 \\n\\n19.0 \\n20.0 \\n\\n23.9 \\n15.3 \\n\\n14.8 \\n13.4 \\n\\n \\n\\n6. DNN ADAPTATION \\n\\nAdapting  DNN  acoustic  models  is  more  difficult  than  adapting \\nGMMs.  We  have  recently  investigated  the  affine  transformation \\nand  several  of  its  variants  for  adaptation  of  the  top  hidden  layer \\n[52].  The  feature-space  discriminative  linear  regression  (fDLR) \\nmethod [2] with an affine transformation on the input layer is also \\nevaluated.  We  have  implemented  stochastic  gradient  descent \\n(SGD)  and  batch  update  methods  for  the  above  adaptation \\ntechniques.  Both  implementations  lead  to  significant  reduction  of \\nword error rates on top of a baseline DNN system.  Shown in Table \\n6,  on  a  large  vocabulary  speech  recognition  task,  a  SGD \\nimplementation  of  the  fDLR  and  the  top  softmax  layer  adaptation \\nis  shown  to  reduce  word  errors  by  17%  and  14%,  respectively, \\ncompared to the baseline DNN performance. Using a batch update \\nfor adapting the softmax layer reduces recognition errors by 10%.  \\n     We have recently developed a KL-distance based regularization \\nmethod  [33]  to  improve  robustness  of  the  DNN  system  under  the \\ncondition  of  a  small  number  of  adaptation  utterances  [55].  As \\nshown in Table 7, on a large vocabulary system, the method shows \\n6%  to  20%  relative  error  reductions  using  5  to  200  supervised \\nadaptation  utterances  compared  with  the  baseline  DNN.  (For  the \\nunsupervised case, the improvement is somewhat less.)   \\n \\n\\nTable 6: DNN adaptation using SGD and batch implementations \\n\\nWERR (%) \\n\\n \\n- \\n\\nWER \\n43.6% \\n34.1% \\n29.4% \\n28.5% \\n30.9% \\n\\nSpeech Recognition Systems \\nGMM-HMM \\nDNN \\nDNN + AdaptSoftMax (SGD) \\nDNN + fDLR (SGD) \\nDNN + AdaptSoftMax (batch) \\n \\nTable 7: Word error rates for varying number (200, 50, and 5) of \\nadaptation utterances. DNN baseline error rate 34.1%.  \\nAdaptation Methods \\nfDLR  \\nKL-regularization  \\n \\n\\n13.9 \\n16.8 \\n9.3 \\n\\n30.4% \\n28.4% \\n\\n36.5% \\n32.1% \\n\\n200 \\n\\n28.5% \\n27.5% \\n\\n50 \\n\\n5 \\n\\n7. RECURRENT NETWORKS FOR LANGAUGE \\n\\nMODELING \\n\\nIn  the  approach  described  here,  we  explore  the  capability  of  a \\nneural net to combine and exploit information of diverse types, and \\napply it to the task of language modeling. This approach has been \\nproposed in dialog systems with a feed-forward net [57] and more \\nrecently  for  recurrent  nets  [47][37].  In  all  these  approaches  the \\nbasic idea is to augment the input to the network with information \\nabove-and-beyond  the  immediate  word  history.  In  [37],  we \\npropose  the  architecture  of  Fig.  2,  which  adds  a  side-channel  of \\ninformation to the basic recurrent network of [38]. By providing a \\nside-channel  consisting  of  a  slowly  changing  Latent  Semantic \\nAnalysis  (LSA)  representation  of  the  preceding  text  in  the  Penn \\nTreebank data, we improved perplexity over a Kneser-Ney 5-gram \\n\\n\\x0cmodel  with  a  cache  from  126  to 110  \\u2013  to  our  knowledge  the  best \\nsingle-model  perplexity  reported  for  this  dataset.  Interestingly, \\nthese gains hold up even after interpolating a standard recurrent net \\nwith a cache model, indicating that the context-dependent recurrent \\nnet  is  indeed  exploiting  the  topic  information  of  the  LSA  vector, \\nand not just implementing a cache. In subsequent experiments, we \\nhave  found  that  this  architecture  is  able  to  use  other  sources  of \\ninformation  as  well;  for  example,  in  initial  experiments  with  a \\nvoice-search  application,  conditioning  on  a \\nlatent-semantic \\nanalysis  representation  of  a  user\\u2019s  history  has  reduced  the \\nperplexity of a language model from 135 to 122. \\n\\nTable  8:  Goal  tracking  accuracy  for  five  slots  using  a  baseline \\nmaximum entropy model and a DSN.  Experiments were done on a \\nfixed corpus of dialogs with real users.   \\n\\n \\n\\nBus route \\n\\nOrigin location \\n\\nDestination location \\n\\nDate \\nTime \\n\\nBaseline \\n\\n58.0% \\n56.4% \\n66.5% \\n83.9% \\n63.1% \\n\\nDSN \\n58.1% \\n57.1% \\n65.4% \\n84.6% \\n62.5% \\n\\n    \\n     The  input  to  the  dialog  state  tracking  component  of  the  full \\ndialogue system comes from the speech understanding component. \\nWe have also explored the use of various versions of deep learning \\nmodels  for  this  task,  with  highly  promising  results  reported  in \\n[15][50]. \\n\\n9. SUMMARY AND DISCUSSION \\n\\nThis paper provides selected samples of our recent experiments on \\napplying  deep  learning  methods  to  advancing  speech  technology \\nand  related  applications,  including  feature  extraction,  acoustic \\nmodeling, language modeling, speech understanding, and dialogue \\nstate estimation. \\n\\n      Figure 2: Recurrent neural network with side-channel information \\n\\n \\n\\n \\n\\n8. STATE TRACKING FOR SPOKEN DIALOGUE \\n\\nWe  have  also  begun  applying  deep  learning  to  spoken  dialogue \\nsystems, specifically the component of dialog state tracking.  The \\nobjective  of  dialog  state  tracking  is  to  assign  a  probability  of \\ncorrectness to user goals at time t given the history of the dialogue \\nfrom            ,  and  historical  information  about  the  user.    For \\nexample,  in  a  bus  timetable  application,  a  user  goal  is  the  user\\u2019s \\nlocation, intended destination, and desired arrival or departure date \\nand  time,  the  dialogue  history  includes  everything  the  system  has \\nasked  so  far  and  all  of  the  spoken  language  understanding  results \\nobserved,  and  the  historical  information  about  the  user  includes \\nwhich  locations  they  have  asked  for  in  the  past.    In  practice, \\nprobabilities  are  assigned  to  the  subset  of  most  promising  goals, \\nand  also  to  a  special  class  that  indicates  that  none  of  the  goals  is \\ncorrect.  Deep networks are attractive here because there are many \\ninteractions  among  features  that  predict  the  correctness  of  a  user \\ngoal. \\n     One  way  of  framing  the  dialogue  state  tracking  problem  is  to \\nconstruct  a  binary  classifier  that  scores  candidate  user  goals  as \\ncorrect  or  incorrect  in  isolation;  normalizing  the  scores  yields  a \\ndistribution  over  all  goals.    Following  this  approach,  we  recently \\nexplored  the  application  of  the  deep  stacking  network  or  DSN \\n[13][14] to this task. Our initial experiments show its performance \\nis  on  par  with  state-of-the-art  classifiers.  Table  8  summarizes  the \\npreliminary  results  using  a  slightly  tuned  DSN  on  a  corpus  of \\ndialogs from the spoken dialog challenge 2010 [5] and 2011-2012, \\nwhere  the  percent  accuracy  indicates  how  often  the  correct  user \\ngoal was identified.  Results are similar to our strongest baseline -- \\na  tuned,  highly  optimized  maximum  entropy  classifier.    In  future \\nwork  we  plan  to  conduct  an  evaluation  in  an  end-to-end  dialog \\nsystem, and to tackle the technical challenge of instance-dependent \\nsizes  of  the  classes  and  feature  dimensions  by  incorporating \\nstructure into the deep learning architectures. \\n\\nA  major  theme  we  adopt  in  writing  this  overview  goes  to  the \\nvery core of deep learning --- automatic learning of representations \\nin  place  of  hand-tuned  feature  engineering.  To  this  end,  we \\npresented  experimental  evidence  that  spectrogram  features  of \\nspeech are superior to  MFCC with DNN, in contrast to the earlier \\nlong-standing practice with  GMM-HMMs. New improvements on \\nDNN  architectures  and  learning  are  needed  to  push  the  features \\neven further back to the raw level of acoustic measurements. \\n     Our and other\\u2019s work over past few years has demonstrated that \\ndeep  learning  is  a  powerful  technology;  e.g.  on  the  Switchboard \\nASR task the word error rate has reduced sharply from 23% in the \\nGMM-HMM  system  as  prior  art  to  as  low  as  13%  currently \\n[32][48].  Our  future  work  on  deep  learning  research  is  directed \\ntowards three largely orthogonal directions: 1) more effective deep \\narchitectures and learning algorithms, including enhancing recently \\ndeveloped techniques  (e.g., [4][9][17][42]); 2) scaling deep model \\ntraining  with  increasingly  larger  data  sets  [6][10][48][29];  and  3) \\nextending  the  applications  of  deep  learning  models  to  other  areas \\nof  speech  and  language  processing,  and  beyond  (e.g.,  preliminary \\nand  promising  applications  to  speech  synthesis  [36],  end-to-end \\nspeech  understanding  and \\nrecognition \\nconfidence measure [28], and information retrieval [18]).  \\nACKNOWLEDGEMENTS:  Some  experiments  discussed  in  this \\npaper were carried out with contributions from our summer interns: \\nA. Metallinou, O. Abdelhamid, and T. Mikolov.  \\n \\n\\ntranslation \\n\\n[22][58], \\n\\nREFERENCES \\n\\n[1]  O. Abdel-Hamid and A. Mohamed, H. Jiang, and G. Penn. \\u201cApplying \\nconvolutional neural network concepts to hybrid NN-HMM model for \\nspeech recognition,\\u201d ICASSP, 2012. \\n\\n[2]  V.  Abrash,  H.  Franco,  A.  Sankar,  and  M.  Cohen,  \"Connectionist \\n\\nspeaker normalization and adaptation,\\u201d Eurospeech, 1995. \\n\\n[3]  Y. Bengio. \\u201cRepresentation learning: A review and new perspectives,\\u201d \\n\\nIEEE Trans. PAMI, special issue Learning Deep Architectures, 2013. \\n[4]  Y.  Bengio,  N.  Boulanger,  and  R.  Pascanu.  \\u201cAdvances  in  optimizing \\n\\nrecurrent networks,\\u201d ICASSP, 2013. \\n\\n[5]  A.  Black,  et  al,  \\u201cSpoken  dialog  challenge  2010:  Comparison  of  live \\n\\nand control test results,\\u201d SIGdial Workshop, 2011. \\n\\n[6]  X.  Chen,  A.  Eversole,  G.  Li,  D.  Yu,  and  F.  Seide,  \\u201cPipelined  back-\\ncontext-dependent  deep  neural  networks,\\u201d \\n\\npropagation \\nfor \\nInterspeech, 2012. \\n\\n\\x0c[7]  G.  Dahl,  D.  Yu,  L.  Deng,  and  A.  Acero,  \\u201cLarge  vocabulary \\ncontinuous speech recognition with context-dependent DBN-HMMs,\\u201d \\nICASSP, 2011. \\n\\n[8]  G.  Dahl,  D.  Yu,  L.  Deng,  and  A.  Acero,  \\u201cContext-dependent  pre-\\n\\ntrained deep neural networks for large vocabulary speech recognition,\\u201d \\nIEEE Trans. Audio, Speech, Lang. Proc., vol. 20, pp. 30\\u201342, 2012. \\n\\n[9]  G.  Dahl,  T.  Sainath,  and  G.  Hinton.  \\u201cImproving  DNNs  for  LVCSR \\n\\nusing RELU and dropout,\\u201d ICASSP, 2013. \\n\\n[10]  J. Dean et al., \\u201cLarge scale distributed deep networks,\\u201d NIPS, 2012. \\n[11]  L.  Deng  and  X.  Li.  \\u201cMachine  learning  paradigms  for  speech \\nrecognition:  An  overview,\\u201d  IEEE  Trans.  Audio,  Speech  &  Lang. \\nProc., Vol. 21, No. 5, May 2013. \\n\\n[12]  L.  Deng,  A.  Acero,  M.  Plumpe,  and  X.  Huang.  \\u201cLarge-vocabulary \\nspeech  recognition  under  adverse  acoustic  environments,\\u201d  Proc. \\nICSLP, 2000. \\n\\n[33]  X.  Li  and  J.  Bilmes,  \\u201cRegularized  adaptation  of  discriminative \\n\\nclassifiers,\\u201d ICASSP, 2006. \\n\\n[34]  J. Li, D. Yu, J. -T. Huang, and Y. Gong, \\u201cImproving wideband speech \\nrecognition  using  mixed-bandwidth  training  data  in  CD-DNN-\\nHMM,\\u201d IEEE SLT, 2012. \\n\\n[35]  H. Lin, L. Deng, D. Yu, Y. Gong, A. Acero, and C.-H. Lee, \\u201cA study \\non  multilingual  acoustic  modeling  for  large  vocabulary  ASR,\\u201d \\nICASSP, 2009. \\n\\n[36]  Z.  Ling,  L.  Deng,  and  D.  Yu.  \\u201cModeling  spectral  envelopes  using \\nrestricted  Boltzmann  machines  for  statistical  Parametric  speech \\nsynthesis,\\u201d ICASSP, 2013.  \\n\\n[37]  T.  Mikolov  and  G.  Zweig,  \\u201cContext  Dependent  Recurrent  Neural \\n\\nNetwork Language Model,\\u201d Proc. SLT, 2012. \\n\\n[38]  T.  Mikolov,  M.  Karafiat,  J.  Cernocky,  and  S.Khudanpur,  \\u201cRecurrent \\n\\nneural network based language model,\\u201d Interspeech, 2010. \\n\\n[13]  L.  Deng,  D.  Yu,  and  J.  Platt.  \\u201cScalable  stacking  and  learning  for \\n\\n[39]  A. Mohamed, D.Yu, L. Deng, \\u201cInvestigation of full-sequence training \\n\\nbuilding deep architectures,\\u201d ICASSP, 2012. \\n\\nof deep belief networks for speech recognition,\\u201d Interspeech,2010. \\n\\n[14]  L.  Deng  and  D.  Yu.  \\u201cDeep  convex  net:  A  scalable  architecture  for \\n\\n[40]  J.  Ngiam,  A.  Khosla,  M.  Kim,  J.  Nam,  H.  Lee,  and  A.  Ng, \\n\\nspeech pattern classification,\\u201d Interspeech, 2011. \\n\\n\\u201cMultimodal deep learning,\\u201d ICML, 2011. \\n\\n[15]  L.  Deng,  G.  Tur,  X.  He,  and  D.  Hakkani-Tur,  \\u201cUse  of  kernel  deep \\nconvex  networks  and  end-to-end  learning  for  spoken  language \\nunderstanding,\\u201d IEEE SLT, 2012. \\n\\n[16]  L.  Deng.  \\u201cIntegrated-multilingual  speech  recognition using universal \\nphonological  features  in  a  functional  speech  production  model,\\u201d \\nICASSP, 1997. \\n\\n[17]  L.  Deng,  O.  Abdel-Hamid  and  D.  Yu.  \\u201cA  deep  convolutional  neural \\nnetwork  using  heterogeneous  pooling  for  trading  acoustic  invariance \\nwith phonetic confusion,\\u201d ICASSP, 2013. \\n\\n[41]  A.  Ragni  and  M.  Gales,  \\u201cDerivative  kernels  for  noise  robust  ASR\\u201d, \\n\\nProc. ASRU, 2011.  \\n\\n[42]  T.  Sainath,  A.  Mohamed,  B.  Kingsbury,  B.  Ramabhadran,  \\u201cDeep \\n\\nconvolutional neural networks for LVCSR,\\u201d ICASSP, 2013. \\n\\n[43]  T.  Schultz  and  A.  Waibel,  \\u201cMultilingual  and  cross-lingual  speech \\nrecognition,\\u201d  DARPA  Workshop  on  Broadcast  News  Transcription \\nand Understanding, 1998. \\n\\n[44]  F.  Seide,  G.  Li,  and  D.  Yu,  \\u201cConversational  speech  transcription \\n\\nusing context-dependent deep neural networks,\\u201d Interspeech 2011. \\n\\n[18]  L. Deng, X. He, and J. Gao. \\u201cDeep stacking networks for information \\n\\n[45]  M. L. Seltzer, D. Yu, and Y. Wang, \\u201cAn investigation of deep neural \\n\\nretrieval,\\u201d ICASSP, 2013. \\n\\n[19]  L. Deng, M. Seltzer, D. Yu, A. Acero, A. Mohamed, and G. Hinton, \\n\\u201cBinary  coding  of  speech  spectrograms  using  a  deep  auto-encoder,\\u201d \\nInterspeech, 2010. \\n\\n[20]  X.  Fan,  M.  Seltzer,  J.  Droppo,  H.  Malvar,  and  A.  Acero,  \\u201cJoint \\nencoding  of  the  waveform  and  speech  recognition  features  using  a \\ntransform codec,\\u201d ICASSP, 2011. \\n\\n[21]  F.  Flego  and  M.  Gales,  \\u201cFactor  analysis  based  VTS  and  JUD  noise \\nestimation  and  compensation,\\u201d  Cambridge  University,  Tech.  Rep. \\nCUED/FINFENG/TR653, 2011. \\n\\n[22]  X.  He,  L.  Deng,  D.  Hakkani-Tur,  G.  Tur,  \\u201cMulti-style  adaptive \\ntraining  for  robust  cross-lingual  spoken  language  understanding,\\u201d \\nICASSP, 2013. \\n\\n[23]  G.  Heigold,  V.  Vanhoucke,  A.  Senior,  P.  Nguyen,  M.  Ranzato,  M. \\nDevin,  and  J.  Dean.  \\u201cMultilingual  acoustic  models  using  distributed \\ndeep neural networks,\\u201d ICASSP, 2013. \\n\\n[24]  G.  Hinton,  L.  Deng,  D.  Yu,  G.  Dahl,  A.  Mohamed,  N.  Jaitly,  A. \\nSenior,  V.  Vanhoucke,  P.  Nguyen,  T.  Sainath    and  B.  Kingsbury, \\n\\u201cDeep neural networks for acoustic modeling in speech recognition,\\u201d \\nIEEE Sig. Proc. Mag., vol. 29, 2012. \\n\\n[25]  G.  Hinton,  N.  Srivastava,  A.  Krizhevsky,  I.  Sutskever,  &  R. \\nSalakhutdinov.  \\u201cImproving  neural  networks  by  preventing  co-\\nadaptation of feature detectors,\\u201d arXiv: 1207.0580v1, 2012. \\n\\n[26]  H.  Hermansky.  \\u201cSpeech  recognition  from  spectral  dynamics,\\u201d \\n\\nSadhana (Indian Academy of Sciences), 2011, pp. 729-744. \\n\\n[27]  J.  -T.  Huang,  J.  Li,  D.  Yu,  L.  Deng,  and  Y.  Gong.  \\u201cCross-language \\nknowledge  transfer  using  multilingual  deep  neural  network  with \\nshared hidden layers\\u201d ICASSP, 2013. \\n\\n[28]  P.  Huang,  K.  Kumar,  C.  Liu,  Y.  Gong,  and  L.  Deng.  \\u201cPredicting \\nspeech recognition confidence using deep learning with word identity \\nand score features,\\u201d ICASSP, 2013. \\n\\n[29]  P. Huang, L. Deng, M. Hasegawa-Johnson, X. He. \\u201cRandom features \\n\\nfor kernel deep convex networks,\\u201d ICASSP, 2013. \\n\\n[30]  B.  Hutchinson,  L.  Deng,  and  D.  Yu,  \\u201cTensor  deep  stacking \\n\\nnetworks,\\u201d IEEE Trans. PAMI, 2013, to appear. \\n\\n[31]  O.  Kalinli,  M.  L.  Seltzer,  J.  Droppo,  A.  Acero,  \\u201cNoise  adaptive \\ntraining for robust automatic speech recognition\\u201d, IEEE Trans. Audio, \\nSpeech & Lang. Proc., vol. 18, no. 8, pp. 1889-1901, 2010. \\n\\nnetworks for noise robust speech recognition,\\u201d ICASSP, 2013. \\n\\n[46]  H.  Sheikhzadeh  and  L.  Deng.  \\u201cWaveform-based  speech  recognition \\nusing  hidden  filter  models:  Parameter  Selection  and  sensitivity  to \\npower normalization,\\u201d IEEE Trans. Speech & Audio Proc., Vol.2, pp. \\n80-91, 1994. \\n\\n[47]  Y.  Shi,  P.  Wiggers  and  C.M.  Jonker,  \\u201cTowards  recurrent  neural \\nnetwork  language  models  with  linguistic  and  contextual  features,\\u201d \\nInterspeech, 2012. \\n\\n[48]  H.  Su,  G.  Li,  D.  Yu,  and  F.  Seide,  \\u201cError  back  propagation  for \\nfor \\n\\ntraining  of  context-dependent  deep  networks \\n\\nsequence \\nconversational speech transcription,\\u201d ICASSP, 2013.  \\n\\n[49]  P.  Swietojanski,  A.  Ghoshal,  and  S.  Renals,  \"Unsupervised  cross-\\nlingual knowledge transfer in DNN-based LVCSR,\" Proc. SLT, 2012. \\n[50]  G.  Tur,  L.  Deng,  D.  Hakkani-Tur,  and  X.  He,  \\u201cTowards  deeper \\nunderstanding:  Deep  convex  networks  for  semantic  utterance \\nclassification,\\u201d ICASSP, 2012. \\n\\n[51]  N.  Vu,  W.  Breiter,  F.  Metze,  and  T.  Schultz,  \\u201cAn  investigation  on \\ninitialization  schemes  for  multilayer  perceptron \\ntraining  using \\nmultilingual data and their effect on ASR performance,\\u201d Interspeech, \\n2012. \\n\\n[52]  K. Yao, D. Yu, F. Seide, H. Su, L.  Deng, and Y. Gong, \\u201cAdaptation \\nof  context-dependent  deep  neural  networks  for  automatic  speech \\nrecognition,\\u201d IEEE SLT, 2012. \\n\\n[53]  D.  Yu,  L.  Deng, and  G.  Dahl,  \\u201cRoles  of pre-training  and  fine-tuning \\nin context-dependent DBN-HMMs for real-world speech recognition,\\u201d \\nNIPS Workshop on Deep Learning, 2010. \\n\\n[54]  D.  Yu,  F.  Seide,  G.  Li,  and  L.  Deng,  \\u201cExploiting  sparseness in deep \\nneural  networks  for  large  vocabulary  speech  recognition,\\u201d  ICASSP, \\n2012, pp. 4409\\u20134412. \\n\\n[55]  D.  Yu,  K.  Yao,  H.  Su,  G.  Li  and  F.  Seide,  \\u201cKL-divergence \\nregularized  deep  neural  network  adaptation  for  improved  large \\nvocabulary speech recognition,\\u201d ICASSP 2013. \\n\\n[56]  D. Yu, L. Deng, and F. Seide. \\u201cThe deep tensor neural network with \\napplications  to  large  vocabulary  speech  recognition,\\u201d  IEEE  Trans \\nAudio, Speech, & Lang. Proc. vol. 21, no. 2, pp. 388-396, Feb, 2013. \\n\\n[57]  F.  Zamora-Martinez,  S.  Espana-Boquera,  M.J.  Castro-Bleda,  and  R. \\nDe-Mori,  \\u201cCache  neural  network  language  models  based  on  long-\\ndistance dependencies for a spoken dialog system,\\u201d ICASSP, 2012. \\n\\n[32]  B. Kingsbury, T.  Sainath, and H. Soltau, \\u201cScalable minimum Bayes \\nrisk  training  of  DNN  acoustic  models  using  distributed  Hessian-free \\noptimization,\\u201d Interspeech, 2012. \\n\\n[58]  Y. Zhang, L. Deng, X. He, and A. Acero. \\u201cA novel decision function \\nand the associated decision-feedback learning for speech translation,\\u201d \\nICASSP, 2011. \\n\\n\\x0c', u'Modeling Spatial-Temporal Clues in a Hybrid Deep\\n\\nLearning Framework for Video Classi\\ufb01cation\\n\\n\\u2020\\nZuxuan Wu, Xi Wang, Yu-Gang Jiang\\n\\n, Hao Ye, Xiangyang Xue\\n\\nSchool of Computer Science, Shanghai Key Lab of Intelligent Information Processing,\\n\\nFudan University, Shanghai, China\\n\\n{zxwu, xwang10, ygj, haoye10, xyxue}@fudan.edu.cn\\n\\n5\\n1\\n0\\n2\\n\\n \\nr\\np\\nA\\n7\\n\\n \\n\\n \\n \\n]\\n\\nV\\nC\\n.\\ns\\nc\\n[\\n \\n \\n\\n1\\nv\\n1\\n6\\n5\\n1\\n0\\n\\n.\\n\\n4\\n0\\n5\\n1\\n:\\nv\\ni\\nX\\nr\\na\\n\\nABSTRACT\\nClassifying videos according to content semantics is an im-\\nportant problem with a wide range of applications. In this\\npaper, we propose a hybrid deep learning framework for\\nvideo classi\\ufb01cation, which is able to model static spatial\\ninformation, short-term motion, as well as long-term tem-\\nporal clues in the videos. Speci\\ufb01cally, the spatial and the\\nshort-term motion features are extracted separately by two\\nConvolutional Neural Networks (CNN). These two types of\\nCNN-based features are then combined in a regularized fea-\\nture fusion network for classi\\ufb01cation, which is able to learn\\nand utilize feature relationships for improved performance.\\nIn addition, Long Short Term Memory (LSTM) networks\\nare applied on top of the two features to further model\\nlonger-term temporal clues. The main contribution of this\\nwork is the hybrid learning framework that can model sev-\\neral important aspects of the video data. We also show\\nthat (1) combining the spatial and the short-term motion\\nfeatures in the regularized fusion network is better than di-\\nrect classi\\ufb01cation and fusion using the CNN with a softmax\\nlayer, and (2) the sequence-based LSTM is highly comple-\\nmentary to the traditional classi\\ufb01cation strategy without\\nconsidering the temporal frame orders. Extensive experi-\\nments are conducted on two popular and challenging bench-\\nmarks, the UCF-101 Human Actions and the Columbia Con-\\nsumer Videos (CCV). On both benchmarks, our framework\\nachieves to-date the best reported performance: 91.3% on\\nthe UCF-101 and 83.5% on the CCV.\\n\\nCategories and Subject Descriptors\\nH.3.1 [Information Storage and Retrieval]: Content\\nAnalysis and Indexing\\u2014Indexing methods; I.5.2 [Pattern\\nRecognition]: Design Methodology\\u2014Classi\\ufb01er design and\\nevaluation\\n\\nKeywords\\nVideo Classi\\ufb01cation, Deep Learning, CNN, LSTM, Fusion.\\n\\n1.\\n\\nINTRODUCTION\\n\\nVideo classi\\ufb01cation based on contents like human actions\\nor complex events is a challenging task that has been ex-\\ntensively studied in the research community. Signi\\ufb01cant\\n\\n\\u2020Corresponding author.\\n\\nprogress has been achieved in recent years by designing var-\\nious features, which are expected to be robust to intra-class\\nvariations and discriminative to separate di\\ufb00erent classes.\\nFor example, one can utilize traditional image-based features\\nlike the SIFT [26] to capture the static spatial information in\\nvideos. In addition to the static frame based visual features,\\nmotion is a very important clue for video classi\\ufb01cation, as\\nmost classes containing object movements like the human\\nactions require the motion information to be reliably recog-\\nnized. For this, a very popular feature is the dense trajecto-\\nries [44], which tracks densely sampled local frame patches\\nover time and computes several traditional features based\\non the trajectories.\\n\\nIn contrast to the hand-crafted features, there is a grow-\\ning trend of learning robust feature representations from raw\\ndata with deep neural networks. Among the many existing\\nnetwork structures, Convolutional Neural Networks (CNN)\\nhave demonstrated great success on various tasks, including\\nimage classi\\ufb01cation [21, 38, 34], image-based object localiza-\\ntion [7], speech recognition [5], etc. For video classi\\ufb01cation,\\nJi et al. [13] and Karparthy et al. [18] extended the CNN\\nto work on the temporal dimension by stacking frames over\\ntime. Recently, Simonyan et al. [33] proposed a two-stream\\nCNN approach, which uses two CNNs on static frames and\\noptical \\ufb02ows respectively to capture the spatial and the mo-\\ntion information. It focuses only on short-term motion as\\nthe optical \\ufb02ows are computed in very short time windows.\\nWith this approach, similar or slightly better performance\\nthan the hand-crafted features like [44] has been reported.\\nThese existing works, however, are not able to model the\\nlong-term temporal clues in the videos. As aforementioned,\\nthe two-stream CNN [33] uses stacked optical \\ufb02ows com-\\nputed in short time windows as inputs, and the order of\\nthe optical \\ufb02ows is fully discarded in the learning process\\n(cf. Section 3.1). This is not su\\ufb03cient for video classi\\ufb01ca-\\ntion, as many complex contents can be better identi\\ufb01ed by\\nconsidering the temporal order of short-term actions. Take\\n\\u201cbirthday\\u201d event as an example\\u2014it usually involves several\\nsequential actions, such as \\u201cmaking a wish\\u201d, \\u201cblowing out\\ncandles\\u201d and \\u201ceating cakes\\u201d.\\n\\nTo address the above limitation, this paper proposes a hy-\\nbrid deep learning framework for video classi\\ufb01cation, which\\nis able to harness not only the spatial and short-term mo-\\ntion features, but also the long-term temporal clues. In or-\\nder to leverage the temporal information, we adopt a Re-\\ncurrent Neural Networks (RNN) model called Long Short\\nTerm Memory (LSTM), which maps the input sequences\\nto outputs using a sequence of hidden states and incorpo-\\n\\n\\x0cFigure 1: An overview of the proposed hybrid deep learning framework for video classi\\ufb01cation. Given an\\ninput video, two types of features are extracted using the CNN from spatial frames and short-term stacked\\nmotion optical \\ufb02ows respectively. The features are separately fed into two sets of LSTM networks for long-\\nterm temporal modeling (Left and Right). In addition, we also employ a regularized feature fusion network\\nto perform video-level feature fusion and classi\\ufb01cation (Middle). The outputs of the sequence-based LSTM\\nand the video-level feature fusion network are combined to generate the \\ufb01nal prediction. See texts for more\\ndiscussions.\\n\\nrates memory units that enable the network to learn when\\nto forget previous hidden states and when to update hidden\\nstates with new information. In addition, many approaches\\nfuse multiple features in a very \\u201cshallow\\u201d manner by either\\nconcatenating the features before classi\\ufb01cation or averaging\\nthe predictions of classi\\ufb01ers trained using di\\ufb00erent features\\nseparately.\\nIn this work we integrate the spatial and the\\nshort-term motion features in a deep neural network with\\ncarefully designed regularizations to explore feature correla-\\ntions. This method can perform video classi\\ufb01cation within\\nthe same network and further combining its outputs with\\nthe predictions of the LSTMs can lead to very competitive\\nclassi\\ufb01cation performance.\\n\\nFigure 1 gives an overview of the proposed framework.\\nSpatial and short-term motion features are \\ufb01rst extracted\\nby the two-stream CNN approach [33], and then input into\\nthe LSTM for long-term temporal modeling. Average pool-\\ning is adopted to generate video-level spatial and motion\\nfeatures, which are fused by the regularized feature fusion\\nnetwork. After that, outputs of the sequence-based LSTM\\nand the video-level feature fusion network are combined as\\nthe \\ufb01nal predictions. Notice that, in contrast to the cur-\\nrent framework, alternatively one may train a fusion net-\\nwork to combine the frame-level spatial and motion features\\n\\ufb01rst and then use a single set of LSTM for temporal mod-\\neling. However, in our experiments we have observed worse\\nresults using this strategy. The main reason is that learn-\\ning dimension-wise feature correlations in the fusion network\\nrequires strong and reliable supervision, but we only have\\nvideo-level class labels which are not necessarily always re-\\nlated to the frame semantics. In other words, the imprecise\\nframe-level labels populated from the video annotations are\\n\\ntoo noisy to learn a good fusion network. The main contri-\\nbutions of this work are summarized as follows:\\n\\n\\u2022 We propose an end-to-end hybrid deep learning frame-\\nwork for video classi\\ufb01cation, which can model not only\\nthe short-term spatial-motion patterns but also the\\nlong-term temporal clues with variable-length video se-\\nquences as inputs.\\n\\n\\u2022 We adopt the LSTM to model long-term temporal\\nclues on top of both the spatial and the short-term\\nmotion features. We show that both features work\\nwell with the LSTM, and the LSTM based classi\\ufb01ers\\nare very complementary to the traditional classi\\ufb01ers\\nwithout considering the temporal frame orders.\\n\\n\\u2022 We fuse the spatial and the motion features in a regu-\\nlarized feature fusion network that can explore feature\\ncorrelations and perform classi\\ufb01cation. The network is\\ncomputationally e\\ufb03cient in both training and testing.\\n\\u2022 Through an extensive set of experiments, we demon-\\nstrate that our proposed framework outperforms sev-\\neral alternative methods with clear margins. On the\\nwell-known UCF-101 and CCV benchmarks, we attain\\nto-date the best performance.\\n\\nThe rest of this paper is organized as follows. Section 2\\nreviews related works. Section 3 describes the proposed hy-\\nbrid deep learning framework in detail. Experimental results\\nand comparisons are discussed in Section 4, followed by con-\\nclusions in Section 5.\\n\\nVideo-levelFeature PoolingInput VideoFinal PredictionSpatial FramesLSTMLSTMLSTMLSTMLSTMLSTMLSTMLSTMLSTMLSTMSpatial CNNSpatial CNNSpatial CNNSpatial CNNSpatial CNN1sy2sy3sy1sT\\uf02dysTyStacked Motion Optical FlowLSTMLSTMLSTMLSTMLSTMLSTMLSTMLSTMLSTMLSTMMotion CNNMotion CNNMotion CNNMotion CNNMotion CNN1my2my3my1mT\\uf02dymTyEsWEmWE\\uf03d\\uf06cVideo-levelFeature PoolingFusion Layer\\x0c2. RELATED WORKS\\n\\nVideo classi\\ufb01cation has been a longstanding research topic\\nin multimedia and computer vision. Successful classi\\ufb01cation\\nsystems rely heavily on the extracted video features, and\\nhence most existing works focused on designing robust and\\ndiscriminative features. Many video representations were\\nmotivated by the advances in image domain, which can be\\nextended to utilize the temporal dimension of the video data.\\nFor instance, Laptev [23] extended the 2D Harris corner de-\\ntector [10] into 3D space to \\ufb01nd space-time interest points.\\nKlaser et al. proposed HOG3D by extending the idea of inte-\\ngral images for fast descriptor computation [19]. Wang et al.\\nreported that dense sampling at regular positions in space\\nand time outperforms the detected sparse interest points on\\nvideo classi\\ufb01cation tasks [45]. Partly inspired by this \\ufb01nding,\\nthey further proposed the dense trajectory features, which\\ndensely sample local patches from each frame at di\\ufb00erent\\nscales and then track them in a dense optical \\ufb02ow \\ufb01eld over\\ntime [44]. This method has demonstrated very competitive\\nresults on major benchmark datasets. In addition, further\\nimprovements may be achieved by using advantageous fea-\\nture encoding methods like the Fisher Vectors [29] or adopt-\\ning feature normalization strategies, such as RootSift [1] and\\nPower Norm [32]. Note that these spatial-temporal video de-\\nscriptors only capture local motion patterns within a very\\nshort period, and popular descriptor quantization methods\\nlike the bag-of-words entirely destroy the temporal order in-\\nformation of the descriptors.\\n\\nTo explore the long-term temporal clues, graphical mod-\\nels have been popularly used, such as hidden Markov mod-\\nels (HMM), Bayesian Networks (BN), Conditional Random\\nFields (CRF), etc. For instance, Li et al. proposed to replace\\nthe hidden states in HMMs with visualizable salient poses\\nestimated by Gaussian Mixture Models [24], and Tang et al.\\nintroduced latent variables over video frames to discover the\\nmost discriminative states of an event based on a variable\\nduration HMM [39]. Zeng et al. exploited multiple types\\nof domain knowledge to guide the learning of a Dynamic\\nBN for action recognition [51].\\nInstead of using directed\\ngraphical models like the HMM and BN, undirected graphi-\\ncal models have also been adopted. Vail et al. employed the\\nCRF for activity recognition in [41]. Wang et al. proposed\\na max-margin hidden CRF for action recognition in videos,\\nwhere a human action is modeled as a global root template\\nand a constellation of several \\u201cparts\\u201d [46].\\n\\nMany related works have investigated the fusion of mul-\\ntiple features, which is often e\\ufb00ective for improving classi\\ufb01-\\ncation performance. The most straightforward and popular\\nways are early fusion and late fusion. Generally, the early\\nfusion refers to fusion at the feature level, such as feature\\nconcatenation or linear combination of kernels of individual\\nfeatures. For example, in [53], Zhang et al. computed non-\\nlinear kernels for each feature separately, and then fused the\\nkernels for model training. The fusion weights can be manu-\\nally set or automatically estimated by multiple kernel learn-\\ning (MKL) [2]. For the late fusion methods, independent\\nclassi\\ufb01ers are \\ufb01rst trained using each feature separately, and\\noutputs of the classi\\ufb01ers are then combined. In [50], Ye et\\nal. proposed a robust late fusion approach to fuse multiple\\nclassi\\ufb01cation outputs by seeking a shared low-rank latent\\nmatrix, assuming that noises may exist in the predictions of\\nsome classi\\ufb01ers, which can possibly be removed by using the\\nlow-rank matrix.\\n\\nBoth early and late fusion fail to explore the correlations\\nshared by the features and hence are not ideal for video\\nclassi\\ufb01cation.\\nIn this paper we employ a regularized neu-\\nral network tailored feature fusion and classi\\ufb01cation, which\\ncan automatically learn dimension-wise feature correlations.\\nSeveral studies are related. In [15, 12], the authors proposed\\nto construct an audio-visual joint codebook for video clas-\\nsi\\ufb01cation, in order to discover and model the audio-visual\\nfeature correlations. There are also studies on using neural\\nnetworks for feature fusion. In [37], the authors employed\\ndeep Boltzmann machines to learn a fused representation\\nof images and texts. In [28], a deep denoised auto-encoder\\nwas used for cross-modality and shared representation learn-\\ning. Very recently, Wu et al. [47] presented an approach us-\\ning regularizations in neural networks to exploit feature and\\nclass relationships. The fusion approach in this work di\\ufb00ers\\nin the following. First, instead of using the traditional hand-\\ncrafted features as inputs, we adopt CNN features trained\\nfrom both static frames and motion optical \\ufb02ows. Second\\nand very importantly, the formulation in the regularized fea-\\nture fusion network has a much lower complexity compared\\nwith that of [47].\\n\\nResearchers have attempted to apply the RNN to model\\nthe long-term temporal information in videos. Venugopalan\\net al. [43] proposed to translate videos to textual sentences\\nwith the LSTM through transferring knowledge from image\\ndescription tasks. Ranzato et al. [30] introduced a genera-\\ntive model with the RNN to predict motions in videos. In\\nthe context of video classi\\ufb01cation, Donahua et al. adopted\\nthe LSTM to model temporal information [6] and Srivas-\\ntava et al. designed an encoder-decoder RNN architecture\\nto learn feature representations in an unsupervised man-\\nner [36]. To model motion information, both works adopted\\noptical \\ufb02ow \\u201cimages\\u201d between nearby frames as the inputs of\\nthe LSTM. In contrast, our approach adopts stacked opti-\\ncal \\ufb02ows. Stacked \\ufb02ows over a short time period can better\\nre\\ufb02ect local motion patterns, which are found to be able to\\nproduce better results. In addition, our framework incorpo-\\nrates video-level predictions with the feature fusion network\\nfor signi\\ufb01cantly improved performance, which was not con-\\nsidered in these existing works.\\n\\nBesides the above discussions of related studies on feature\\nfusion and temporal modeling with the RNN, several rep-\\nresentative CNN-based approaches for video classi\\ufb01cation\\nshould also be covered here. The image-based CNN features\\nhave recently been directly adopted for video classi\\ufb01cation,\\nextracted using o\\ufb00-the-shelf models trained on large-scale\\nimage datasets like the ImageNet [11, 31, 52]. For instance,\\nJain et al. [11] performed action recognition using the SVM\\nclassi\\ufb01er with such CNN features and achieved top results\\nin the 2014 THUMOS action recognition challenge [16]. A\\nfew works have also tried to extend the CNN to exploit the\\nmotion information in videos. Ji et al. [13] and Karparthy et\\nal. [18] extended the CNN by stacking visual frames in \\ufb01xed-\\nsize time windows and using spatial-temporal convolutions\\nfor video classi\\ufb01cation. Di\\ufb00erently, the two-stream CNN ap-\\nproach by Simonyan et al. [33] applies the CNN separately\\non visual frames (the spatial stream) and stacked optical\\n\\ufb02ows (the motion stream). This approach has been found\\nto be more e\\ufb00ective, which is adopted as the basis of our\\nproposed framework. However, as discussed in Section 1,\\nall these approaches [13, 18, 33] can only model short-term\\nmotion, not the long-term temporal clues.\\n\\n\\x0c3. METHODOLOGY\\n\\nIn this section, we describe the key components of the\\nproposed hybrid deep learning framework shown in Figure 1,\\nincluding the CNN-based spatial and short-term motion fea-\\ntures, the long-term LSTM-based temporal modeling, and\\nthe video-level regularized feature fusion network.\\n3.1 Spatial and Motion CNN Features\\n\\nConventional CNN architectures take images as the inputs\\nand consist of alternating convolutional and pooling layers,\\nwhich are further topped by a few fully-connected (FC) lay-\\ners. To extract the spatial and the short-term motion fea-\\ntures, we adopt the recent two-stream CNN approach [33].\\nInstead of using stacked frames in short time windows like [13,\\n18], this approach decouples the videos into spatial and mo-\\ntion1 streams modeled by two CNNs separately. Figure 2\\ngives an overview. The spatial stream is built on sampled in-\\ndividual frames, which is exactly the same as the CNN-based\\nimage classi\\ufb01cation pipeline and is suitable for capturing the\\nstatic information in videos like scene backgrounds and basic\\nobjects. The motion counterpart operates on top of stacked\\noptical \\ufb02ows. Speci\\ufb01cally, optical \\ufb02ows (displacement vector\\n\\ufb01elds) are computed between each pair of adjacent frames,\\nand the horizontal and vertical components of the displace-\\nment vectors can form two optical \\ufb02ow images. Instead of\\nusing each individual \\ufb02ow image as the input of the CNN, it\\nwas reported that stacked optical \\ufb02ows over a time window\\nare better due to the ability of modeling the short-term mo-\\ntion. In other words, the input of the motion stream CNN\\nis a 2L-channel stacked optical \\ufb02ow image, where L is the\\nnumber of frames in the window. The two CNNs produce\\nclassi\\ufb01cation scores separately using a softmax layer and the\\nscores are linearly combined as the \\ufb01nal prediction.\\n\\nLike many existing works on visual classi\\ufb01cation using the\\nCNN features [31], we adopt the output of the \\ufb01rst FC layer\\nof the two CNNs as the spatial and the short-term motion\\nfeatures.\\n3.2 Temporal Modeling with LSTM\\n\\nDuring the training process of the spatial and the motion\\nstream CNNs, each sweep through the network takes one\\nvisual frame or one stacked optical \\ufb02ow image, and the tem-\\nporal order of the frames is fully discarded. To model the\\nlong-term dynamic information in video sequences, we lever-\\nage the LSTM model, which has been successfully applied to\\nspeech recognition [8], image captioning [6], etc. LSTM is a\\ntype of RNN with controllable memory units and is e\\ufb00ective\\nin many long range sequential modeling tasks without suf-\\nfering from the \\u201cvanishing gradients\\u201d e\\ufb00ect like traditional\\nRNNs. Generally, LSTM recursively maps the input repre-\\nsentations at the current time step to output labels via a\\nsequence of hidden states, and thus the learning process of\\nLSTM should be in a sequential manner (from left to right\\nin the two sets of LSTM of Figure 1). Finally, we can obtain\\na prediction score at each time step with a softmax trans-\\nformation using the hidden states from the last layer of the\\nLSTM.\\n\\nMore formally, given a sequence of feature representations\\n(x1, x2, . . . , xT ), an LSTM maps the inputs to an output\\n\\n1Note that the authors of the original paper [33] used the name\\n\\u201ctemporal stream\\u201d. We call it \\u201cmotion stream\\u201d as it only captures\\nshort-term motion, which is di\\ufb00erent from the long-term temporal\\nmodeling in our proposed framework.\\n\\nFigure 2: The framework of the two-stream CNN.\\nOutputs of the \\ufb01rst fully-connected layer in the two\\nCNNs (outlined) are used as the spatial and the\\nshort-term motion features for further processing.\\n\\nFigure 3: The structure of an LSTM unit.\\n\\nsequence (y1, y2, . . . , yT ) by computing activations of the\\nunits in the network with the following equations recursively\\nfrom t = 1 to t = T :\\n\\nit = \\u03c3(Wxixt + Whiht\\u22121 + Wcict\\u22121 + bi),\\nft = \\u03c3(Wxf xt + Whf ht\\u22121 + Wcf ct\\u22121 + bf ),\\nct = ftct\\u22121 + it tanh(Wxcxt + Whcht\\u22121 + bc),\\not = \\u03c3(Wxoxt + Whoht\\u22121 + Wcoct + bo),\\nht = ot tanh(ct),\\n\\nwhere xt, ht are the input and hidden vectors with the sub-\\nscription t denoting the t-th time step, it, ft, ct, ot are re-\\nspectively the activation vectors of the input gate, forget\\ngate, memory cell and output gate, W\\u03b1\\u03b2 is the weight ma-\\ntrix between \\u03b1 and \\u03b2 (e.g., Wxi is weight matrix from the\\ninput xt to the input gate it), b\\u03b1 is the bias term of \\u03b1 and\\n\\u03c3 is the sigmoid function de\\ufb01ned as \\u03c3(x) = 1\\n1+e\\u2212x . Figure 3\\nvisualizes the structure of an LSTM unit.\\n\\nThe core idea behind the LSTM model is a built-in mem-\\nory cell that stores information over time to explore long-\\nrange dynamics, with non-linear gate units governing the\\ninformation \\ufb02ow into and out of the cell. As we can see\\nfrom the above equations, the current frame xt and the pre-\\nvious hidden states ht\\u22121 are used as inputs of four parts at\\nthe t-th time step. The memory cell aggregates information\\nfrom two sources: the previous cell memory unit ct\\u22121 multi-\\nplied by the activation of the forget gate ft and the squashed\\ninputs regulated with the input gate\\u2019s activation it, the com-\\nbination of which enables the LSTM to learn to forget in-\\nformation from previous states or consider new information.\\n\\nSpatial stream CNNIndividualframeMotion stream CNNStacked optical flowScorefusion\\xd7Input GateOutput GateForget Gate\\xd7\\xd7Cell1t\\uf02dhtxtitftotcth\\x0cIn addition, the output gate ot controls how much informa-\\ntion from the memory cell is passed to the hidden states\\nht for the following time step. With the explicitly control-\\nlable memory units and di\\ufb00erent functional gates, LSTM\\ncan explore long-range temporal clues with variable-length\\ninputs. As a neural network, the LSTM model can be easily\\ndeepened by stacking the hidden states from a layer l \\u2212 1 as\\ninputs of the next layer l.\\n\\nLet us now consider a model of K layers, the feature vector\\nxt at the t-th time step is fed into the \\ufb01rst layer of the\\nLSTM together with the hidden state h1\\nt\\u22121 in the same layer\\nobtained from the last time step to produce an updated h1\\nt ,\\nwhich will then be used as the inputs of the following layer.\\nDenote fW as the mapping function from the inputs to the\\nhidden states, the transition from layer l \\u2212 1 to layer l can\\nbe written as:\\n\\n(cid:26) fW (hl\\u22121\\n\\nt\\n\\nfW (xt, hl\\n\\nhl\\n\\nt =\\n\\nt\\u22121),\\n\\n, hl\\nt\\u22121),\\n\\nl > 1;\\nl = 1.\\n\\nIn order to obtain the prediction scores for a total of C\\nclasses at a time step t, the outputs from last layer of the\\nLSTM are sent to a softmax layer estimating probabilities\\nas:\\n\\n(cid:80)\\n\\nprobc =\\n\\nexp(wc\\n\\nT hK\\n\\nt + bc)\\n\\nc(cid:48)\\u2208C exp(wc(cid:48) T hK\\n\\nt + bc(cid:48) )\\n\\n,\\n\\nwhere probc, wc and bc are respectively the probability pre-\\ndiction, the corresponding weight vector and the bias term\\nof the c-th class. Such an LSTM network can be trained with\\nthe Back-Propagation Through Time (BPTT) algorithm [9],\\nwhich \\u201cunrolls\\u201d the model into a feed forward neural net and\\nback-propagates to determine the optimal weights.\\n\\n1, xs\\n\\n1 , xm\\n\\n2, . . . , xs\\n\\n2 , . . . , xm\\n1, ys\\n\\nAs shown in Figure 1, we adopt two LSTM models for tem-\\nporal modeling. With the two-stream CNN pipeline for fea-\\nture extraction, we have a spatial feature set (xs\\nT )\\nand a motion feature set (xm\\nT ). The learning\\nprocess leads to a set of predictions (ys\\n2, . . . , ys\\nT ) for the\\nspatial part and another set (ym\\nT ) for the mo-\\ntion part. For both LSTM models, we adopt the last step\\noutput yT as the video-level prediction scores, since the out-\\nputs at the last time step are based on the consideration of\\nthe entire sequence. We empirically observe that the last\\nstep outputs are better than pooling predictions from all\\nthe time steps.\\n3.3 Regularized Feature Fusion Network\\n\\n2 , . . . , ym\\n\\n1 , ym\\n\\nGiven both the spatial and the motion features, it is easy\\nto understand that correlations may exist between them\\nsince both are computed on the same video (e.g., person-\\nrelated static visual features and body motions). A good\\nfeature fusion method is supposed to be able to take advan-\\ntages of the correlations, while also can maintain the unique\\ncharacteristics to produce a better fused representation. In\\norder to explore this important problem rather than using\\nthe simple late fusion as [33], we employ a regularized fea-\\nture fusion neural network, as shown in the middle part of\\nFigure 1. First, average pooling is adopted to aggregate the\\nframe-level CNN features into video-level representations,\\nwhich are used as the inputs of the fusion network. The\\ninput features are non-linearly mapped to another layer and\\nthen fused in a feature fusion layer, where we apply regular-\\nizations in the learning of the network weights.\\n\\nt=1 xs\\n\\nn,t \\u2208 Rds and xm\\n\\nn = (cid:80)T\\n\\nn = (cid:80)T\\n\\nDenote N as the total number of training videos with\\nboth the spatial and the motion representations. For the\\nn, xm\\nn-th sample, it can be written as a 3-tuple as (xs\\nn , yn),\\nn,t \\u2208 Rdm\\nt=1 xm\\nwhere xs\\nrepresent the averaged spatial and motion features respec-\\ntively, and yn is the corresponding label of the n-th sample.\\nFor the ease of discussion, let us consider a degenerated\\ncase \\ufb01rst, where only one feature is available. Denote g(\\xb7)\\nas the mapping of the neural network from inputs to out-\\nputs. The objective of network training is to minimize the\\nfollowing empirical loss:\\n\\n(cid:107)g(xi) \\u2212 yi(cid:107)2 + \\u03bb1\\u03a6(W),\\n\\n(1)\\n\\nN(cid:88)\\n\\ni=1\\n\\nmin\\nW\\n\\n(cid:13)(cid:13)(cid:13)WE(cid:13)(cid:13)(cid:13)2,1\\n\\nwhere the \\ufb01rst term measures the discrepancy between the\\noutput g(xi) and the ground-truth label yi, and the sec-\\nond term is usually a Frobenius norm based regularizer to\\nprevent over-\\ufb01tting.\\n\\nWe now move on to discuss the case of fusion and predic-\\ntion with two features. Note that the approach can be easily\\nextended to support more than two input features. Specif-\\nically, we use a fusion layer (see Figure 1) to absorb the\\nspatial and temporal features into a fused representation.\\nTo exploit the correlations of the features, we regularize the\\nfusion process with a structural (cid:96)21 norm, which is de\\ufb01ned\\nij. Incorporating the (cid:96)21 norm in\\nthe standard deep neural network formulation, we arrive at\\nthe following optimization problem:\\n\\nas (cid:107)W(cid:107)2,1 = (cid:80)\\n\\n(cid:113)(cid:80)\\n\\nj w2\\n\\ni\\n\\n\\u03bb2\\n2\\n\\nL + \\u03bb1\\u03a6(W) +\\n\\nwhere L = (cid:80)N\\n\\nmin\\nW\\ni=1 (cid:107)g(xs\\n\\n(2)\\nm] \\u2208\\nRP\\xd7D denotes the concatenated weight matrix from the Eth\\nlayer (i.e., the last layer of feature abstraction in Figure 1),\\nD = ds + dm and P is the dimension of the fusion layer.\\n\\ni ) \\u2212 yi(cid:107)2, WE = [WE\\n\\ns , WE\\n\\ni , xm\\n\\n,\\n\\nDi\\ufb00erent from the objective in Equation (1), here we have\\nan additional (cid:96)21 norm that is used for exploring feature cor-\\nrelations in the E-th layer. The term (cid:107)WE(cid:107)2,1 computes the\\n2-norm of the weight values across di\\ufb00erent features in each\\ndimension. Therefore, the regularization attains minimum\\nwhen WE contains the smallest number of non-zero rows,\\nwhich is the discriminative information shared by distinct\\nfeatures. That is to say, the (cid:96)21 norm encourages the matrix\\nWE to be row sparse, which leads to similar zero/nonzero\\npatterns of the columns of the matrix WE. Hence it en-\\nforces di\\ufb00erent features to share a subset of hidden neurons,\\nre\\ufb02ecting the feature correlations.\\n\\nHowever, in addition to seeking for the correlations shared\\namong features, the unique discriminative information should\\nalso be preserved so that the complementary information can\\nbe used for improved classi\\ufb01cation performance. Thus, we\\nadd an additional regularizer to Equation (2) as following:\\n\\n(cid:13)(cid:13)(cid:13)WE(cid:13)(cid:13)(cid:13)2,1\\n\\n\\u03bb2\\n2\\n\\n(cid:13)(cid:13)(cid:13)WE(cid:13)(cid:13)(cid:13)1,1\\n\\nL + \\u03bb1\\u03a6(W) +\\n\\n+ \\u03bb3\\n\\nmin\\nW\\n\\n(3)\\nThe term (cid:107)WE(cid:107)1,1 can be regarded as a complement of the\\n(cid:107)WE(cid:107)2,1 norm. It provides the robustness of the (cid:96)21 norm\\nfrom sharing incorrect features among di\\ufb00erent representa-\\ntions, and thus allows di\\ufb00erent representations to emphasize\\ndi\\ufb00erent hidden neurons.\\n\\n.\\n\\nAlthough the regularizer terms in Equation (3) are all\\nconvex functions, the optimization problem in Equation (3)\\n\\n\\x0cis nonconvex due to the nonconvexity of the sigmoid func-\\ntion. Below, we discuss the optimization strategy using the\\ngradient descent method in two cases:\\n\\n1. For the E-th layer, our objective function has four valid\\nterms: the empirical loss, the (cid:96)2 regularizer \\u03a6(W), and\\ntwo nonsomooth structural regularizers, i.e., the (cid:96)21\\nand (cid:96)11 terms. Note that simply using the gradient\\ndescent method is not optimal due to the two nons-\\nmooth terms. We propose to optimize the E-th layer\\nusing a proximal gradient descent method, which splits\\nthe objective function into two parts:\\n\\np = L + \\u03bb1\\u03a6(W),\\n\\n(cid:13)(cid:13)(cid:13)WE(cid:13)(cid:13)(cid:13)2,1\\n\\nq =\\n\\n\\u03bb2\\n2\\n\\n(cid:13)(cid:13)(cid:13)WE(cid:13)(cid:13)(cid:13)1,1\\n\\n,\\n\\n+ \\u03bb3\\n\\nwhere p is a smooth function and q is a nonsmooth\\nfunction. Thus, the update of the i-th iteration is for-\\nmulated as:\\n\\n(WE)(i) = Proxq((WE)(i) \\u2212 \\u2207p((WE)(i))),\\n\\nwhere Prox is a proximal operator de\\ufb01ned as:\\n(cid:107)W \\u2212 V(cid:107) + q(V ).\\n\\nProxq(W) = arg min\\nV\\n\\nThe proximal operator on the combination of (cid:96)21/(cid:96)11\\nnorm ball can be computed analytically as:\\n\\n1 \\u2212 \\u03bb2\\n\\nUr\\xb7,\\u2200r = 1,\\xb7\\xb7\\xb7 , P,\\n\\nWE\\n\\nr\\xb7 =\\n\\n(cid:107)Ur\\xb7(cid:107)2\\n\\n(4)\\nwhere Ur\\xb7 = [|Vr\\xb7| \\u2212 \\u03bb3]+\\xb7sign[Vr\\xb7], and Wr\\xb7, Ur\\xb7, Vr\\xb7\\ndenote the r-th row of matrix W, U and V, respec-\\ntively. Readers may refer to [49] for a detailed proof\\nof a similar analytical solution.\\n\\n(cid:18)\\n\\n(cid:19)\\n\\n2. For all the other layers, the objective function in Equa-\\ntion 3 only contains the \\ufb01rst two valid terms, which are\\nboth smooth. Thus, we can directly apply the gradient\\ndescent method as in [3]. Denote Gl as the gradient of\\nWl, the weight matrix of the lth layer is updated as:\\n\\nWl = Wl \\u2212 \\u03b7Gl.\\n\\n(5)\\n\\nThe only additional computation cost for training our reg-\\nularized feature fusion network is to compute the proximal\\noperator in the E-th layer. The complexity of the analytical\\nsolution in Equation (4) is O(P \\xd7 D). Therefore, the pro-\\nposed proximal gradient descent method can quickly train\\nthe network with a\\ufb00ordable computational cost. When in-\\ncorporating more features, our formulation can be computed\\ne\\ufb03ciently with linearly increased cost, while cubic opera-\\ntions are required by the approach of [47] to reach a similar\\ngoal. In sum, the above optimization is incorporated into\\nthe conventional back propagation procedure, as described\\nin Algorithm 1.\\n3.4 Discussion\\n\\nThe approach described above has the capability of model-\\ning static spatial, short-term motion and long-term temporal\\nclues, which are all very important in video content analysis.\\nOne may have noticed that the proposed hybrid deep learn-\\ning framework contains several components that are sepa-\\nrately trained. Joint training is feasible but not adopted\\nin this current framework for the following reason. The\\n\\nInput : xs\\n\\nn and xm\\n\\nn : the spatial and motion CNN\\n\\nfeatures of the n-th video sample;\\nyn: the label of the n-th video sample;\\nrandomly initialized weight matrices W;\\n\\n1 begin\\n2\\n\\nfor epoch \\u2190 1 to M do\\n\\n3\\n\\n4\\n\\n5\\n\\n6\\n\\n7\\n\\n8\\n\\n9\\n\\nGet the prediction error with feedforward\\npropagation;\\nfor l \\u2190 L to 1 do\\n\\nEvaluate the gradients and update the\\nweight matrices using Equation (5);\\nif\\n\\nl == E then\\nEvaluate the proximal operator\\naccording to Equation (4);\\n\\nend\\n\\nend\\n\\nend\\n\\n10\\n11 end\\n\\nAlgorithm 1: The training procedure of the regular-\\nized feature fusion network.\\n\\njoint training process is more complex and existing works\\nexploring this aspect indicate that the performance gain is\\nnot very signi\\ufb01cant. In [6], the authors jointly trained the\\nLSTM with a CNN for feature extraction, which only im-\\nproved the performance on a benchmark dataset from 70.5%\\nto 71.1%. Besides, an advantage of separate training is that\\nthe framework is more \\ufb02exible, where a component can be\\nreplaced easily without the need of re-training the entire\\nframework. For instance, more discriminative CNN models\\nlike the GoogLeNet [38] and deeper RNN models [4] can be\\nused to replace the CNN and LSTM parts respectively.\\n\\nIn addition, as mentioned in Section 1, there could be\\nalternative frameworks or models with similar capabilities.\\nThe main contribution of this work is to show that such a\\nhybrid framework is very suitable for video classi\\ufb01cation. In\\naddition to showing the e\\ufb00ectiveness of the LSTM and the\\nregularized feature fusion network, we also show that the\\ncombination of both in the hybrid framework can lead to\\nsigni\\ufb01cant improvements, particularly for long videos that\\ncontain rich temporal clues.\\n\\n4. EXPERIMENTS\\n4.1 Experimental Setup\\n4.1.1 Datasets\\nWe adopt two popular datasets to evaluate the proposed\\n\\nhybrid deep learning framework.\\n\\nUCF-101 [35]. The UCF-101 dataset is one of the most\\npopular action recognition benchmarks. It consists of 13,320\\nvideo clips of 101 human actions (27 hours in total). The 101\\nclasses are divided into \\ufb01ve groups: Body-Motion, Human-\\nHuman Interactions, Human-Object Interactions, Playing\\nMusical Instruments and Sports. Following [16], we conduct\\nevaluations using 3 train/test splits, which is currently the\\nmost popular setting in using this dataset. Results are mea-\\nsured by classi\\ufb01cation accuracy on each split and we report\\nthe mean accuracy over the three splits.\\n\\nColumbia Consumer Videos (CCV) [17]. The CCV\\ndataset contains 9,317 YouTube videos annotated accord-\\n\\n\\x0cing to 20 classes, which are mainly events like \\u201cbasketball\\u201d,\\n\\u201cgraduation ceremony\\u201d, \\u201cbirthday party\\u201d and \\u201cparade\\u201d. We\\nfollow the convention de\\ufb01ned in [17] to use a training set of\\n4,659 videos and a test set of 4,658 videos. The performance\\nis evaluated by average precision (AP) for each class, and we\\nreport the mean AP (mAP) as the overall measure.\\n\\nImplementation Details\\n\\n4.1.2\\nFor feature extraction network structure, we adopt the\\nVGG 19 [34] and the CNN M [33] to extract the spatial and\\nthe motion CNN features, respectively. The two networks\\ncan achieve 7.5% [34] and 13.5% [33] top-5 error rates on\\nthe ImageNet ILSVRC-2012 validation set respectively. The\\nspatial CNN is \\ufb01rst pre-trained with the ILSVRC-2012 train-\\ning set with 1.2 million images and then \\ufb01ne-tuned using the\\nvideo data, which is observed to be better than training from\\nscratch. The motion CNN is trained from scratch as there\\nis no o\\ufb00-the-shelf training set in the required form. In addi-\\ntion, simple data augmentation methods like cropping and\\n\\ufb02ipping are utilized following [33].\\n\\nThe CNN models are trained using mini-batch stochas-\\ntic gradient descent with a momentum \\ufb01xed to 0.9. In the\\n\\ufb01ne-tuning case of the spatial CNN, the rate starts from\\n10\\u22123 and decreases to 10\\u22124 after 14K iterations, then to\\n10\\u22125 after 20K iterations. This setting is similar to [33], but\\nwe start from a smaller rate of 10\\u22123 instead of 10\\u22122. For\\nthe motion CNN, we set the learning rate to 10\\u22122 initially,\\nand reduce it to 10\\u22123 after 100K iterations, then to 10\\u22124\\nafter 200K iterations. Our implementation is based on the\\npublicly available Ca\\ufb00e toolbox [14] with modi\\ufb01cations to\\nsupport parallel training with multiple GPUs in a server.\\n\\nFor temporal modeling, we adopt two layers in the LSTM\\nfor both the spatial and the motion features. Each LSTM\\nhas 1,024 hidden units in the bottom layer and 512 hidden\\nunits in the other layer. The network weights are learnt us-\\ning a parallel implementation of the BPTT algorithm with a\\nmini-batch size of 10. In addition, the learning rate and mo-\\nmentum are set to 10\\u22124 and 0.9 respectively. The training\\nis stopped after 150K iterations for both datasets.\\n\\nFor the regularized feature fusion network, we use four\\nlayers of neurons as illustrated in the middle of Figure 1.\\nSpeci\\ufb01cally, we \\ufb01rst use one layer with 200 neurons for the\\nspatial and motion feature to perform feature abstraction\\nseparately, and then one layer with 200 neurons for feature\\nfusion with the proposed regularized structural norms. Fi-\\nnally, the fused features are used to build a logistic regression\\nmodel in the last layer for video classi\\ufb01cation. We set the\\nlearning rate to 0.7 and \\ufb01x \\u03bb1 to 3 \\xd7 10\\u22125 in order to pre-\\nvent over-\\ufb01tting. In addition, we tune \\u03bb2 and \\u03bb3 in the same\\nrange as \\u03bb1 using cross-validation.\\n\\n4.1.3 Compared Approaches\\nTo validate the e\\ufb00ectiveness of our approach, we com-\\npare with the following baseline or alternative methods: (1)\\nTwo-stream CNN. Our implementation produces similar\\noverall results with the original work [33]. We also report the\\nresults of the individual spatial-stream and motion-stream\\nCNN models, namely Spatial CNN and Motion CNN,\\nrespectively; (2) Spatial LSTM, which refers to the LSTM\\ntrained with the spatial CNN features; (3) Motion LSTM,\\nthe LSTM trained with the motion CNN features; (4) SVM-\\nbased Early Fusion (SVM-EF). \\u03c72-kernel is computed\\nfor each video-level CNN feature and then the two kernels\\n\\nare averged for classi\\ufb01cation; (5) SVM-based Late Fu-\\nsion (SVM-LF). Separate SVM classi\\ufb01ers are trained for\\neach video-level CNN feature and the prediction outputs are\\naveraged; (6) Multiple Kernel Learning (SVM-MKL),\\nwhich combines the two features with the (cid:96)p-norm MKL [20]\\nby \\ufb01xing p = 2; (7) Early Fusion with Neural Net-\\nworks (NN-EF), which concatenates the two features into\\na long vector and then use a neural network for classi\\ufb01ca-\\ntion; (8) Late Fusion with Neural Networks (NN-LF),\\nwhich deploys a separate neural network for each feature\\nand then uses the average output scores as the \\ufb01nal pre-\\ndiction; (9) Multimodal Deep Boltzmann Machines\\n(M-DBM) [28, 37], where feature fusion is performed using\\na neural network in a free manner without regularizations;\\n(10) RDNN [47], which also imposes regularizations in a\\nneural network for feature fusion, using a formulation that\\nhas a much higher complexity than our approach.\\n\\nThe \\ufb01rst three methods are a part of the proposed frame-\\nwork, which are evaluated as baselines to better understand\\nthe contribution of each individual component. The last\\nseven methods focus on fusing the spatial and the motion\\nfeatures (outputs of the \\ufb01rst fully-connected layer of the\\nCNN models) for improved classi\\ufb01cation performance. We\\ncompare our regularized fusion network with all the seven\\nmethods.\\n4.2 Results and Discussions\\n\\n4.2.1 Temporal Modeling\\nWe \\ufb01rst evaluate the LSTM to investigate the signi\\ufb01cance\\nof leveraging the long-term temporal clues for video classi-\\n\\ufb01cation. The results and comparisons are summarized in\\nTable 1. The upper two groups in the table compare the\\nLSTM models with the two-stream CNN, which performs\\nclassi\\ufb01cation by pooling video-level representations without\\nconsidering the temporal order of the frames. On UCF-101,\\nthe Spatial LSTM is better than the spatial stream CNN,\\nwhile the result of the Motion LSTM is slightly lower than\\nthat of the motion stream CNN. It is interesting to see that,\\non the spatial stream, the LSTM is even better than the\\nstate-of-the-art CNN, indicating that the temporal informa-\\ntion is very important for human action modeling, which is\\nfully discarded in the spatial stream CNN. Since the mecha-\\nnism of the LSTM is totally di\\ufb00erent, these results are fairly\\nappealing because it is potentially very complementary to\\nthe video-level classi\\ufb01cation based on feature pooling.\\n\\nOn the CCV dataset, the LSTM models produce lower\\nperformance than the CNN models on both streams. The\\nreasons are two-fold. First, since the average duration of the\\nCCV videos (80 seconds) is around 10 times longer than that\\nof the UCF-101 and the contents in CCV are more complex\\nand noisy, the LSTM might be a\\ufb00ected by the noisy video\\nsegments that are irrelevant to the major class of a video.\\nSecond, some classes like \\u201cwedding reception\\u201d and \\u201cbeach\\u201d do\\nnot contain clear temporal order information (see Figure 4),\\nfor which the LSTM can hardly capture helpful clues.\\n\\nWe now assess the performance of combining the LSTM\\nand the CNN models to study whether they are complemen-\\ntary, by fusing the outputs of the two types of models trained\\non the two streams. Note that the fusion method adopted\\nhere is the simple late average fusion, which uses the average\\nprediction scores of di\\ufb00erent models. More advanced fusion\\nmethods will be evaluated in the next subsection.\\n\\n\\x0cSpatial CNN\\nSpatial LSTM\\nMotion CNN\\nMotion LSTM\\n\\nCNN + LSTM (Spatial)\\nCNN + LSTM (Motion)\\n\\nCNN + LSTM (Spatial & Motion)\\n\\nUCF-101\\n\\nCCV\\n75.0%\\n80.1%\\n43.3%\\n83.3%\\n58.9%\\n77.5%\\n54.7%\\n76.6%\\n77.9%\\n84.0%\\n70.9%\\n81.4%\\n90.1% 81.7%\\n\\nTable 1: Performance of the LSTM and the CNN\\nmodels trained with the spatial and the short-term\\nmotion features respectively on UCF-101 and CCV.\\n\\u201c+\\u201d indicates model fusion, which simply uses the\\naverage prediction scores of di\\ufb00erent models.\\n\\nSpatial SVM\\nMotion SVM\\n\\nSVM-EF\\nSVM-LF\\n\\nSVM-MKL\\n\\nNN-EF\\nNN-LF\\nM-DBM\\n\\nTwo-stream CNN\\n\\nRDNN\\n\\nNon-regularized Fusion Network\\n\\nRegularized Fusion Network\\n\\nUCF-101\\n\\nCCV\\n74.4%\\n78.6%\\n57.9%\\n78.2%\\n75.3%\\n86.6%\\n74.9%\\n85.3%\\n75.4%\\n86.8%\\n75.6%\\n86.5%\\n75.2%\\n85.1%\\n75.3%\\n86.9%\\n75.8%\\n86.2%\\n75.9%\\n88.1%\\n87.0%\\n75.4%\\n88.4% 76.2%\\n\\nTable 2: Performance comparison on UCF-101 and\\nCCV, using various fusion approaches to combine\\nthe spatial and the short-term motion clues.\\n\\nResults are reported in the bottom three rows of Table 1.\\nWe observe signi\\ufb01cant improvements from model fusion on\\nboth datasets. On UCF-101, the fusion leads to an absolute\\nperformance gain of around 1% compared with the best sin-\\ngle model for the spatial stream, and a gain of 4% for the\\nmotion stream. On CCV, the improvements are more sig-\\nni\\ufb01cant, especially for the motion stream where an absolute\\ngain of 12% is observed. These results con\\ufb01rm the fact that\\nthe long-term temporal clues are highly complementary to\\nthe spatial and the short-term motion features. In addition,\\nthe fusion of all the CNN and the LSTM models trained\\non the two streams attains the highest performance on both\\ndatasets: 90.1% and 81.7% on UCF-101 and CCV respec-\\ntively, showing that the spatial and the short-term motion\\nfeatures are also very complementary. Therefore, it is im-\\nportant to incorporate all of them into a successful video\\nclassi\\ufb01cation system.\\n\\n4.2.2 Feature Fusion\\nNext, we compare our regularized feature fusion network\\nwith the alternative fusion methods, using both the spa-\\ntial and the motion CNN features. The results are pre-\\nsented in Table 2, which are divided into four groups. The\\n\\ufb01rst group reports the performance of individual features ex-\\ntracted from the \\ufb01rst fully connected layer of the CNN mod-\\nels, classi\\ufb01ed by SVM classi\\ufb01ers. This is reported to study\\n\\nthe gain from the SVM based fusion methods, as shown in\\nthe second group of results. The individual feature results\\nusing the CNN, i.e., the Spatial CNN and the Motion CNN,\\nare already reported in Table 1. The third group of results\\nin Table 2 are based on the alternative neural network fusion\\nmethods. Note that NN-EF and NN-LF take the features\\nfrom the CNN models and perform fusion and classi\\ufb01ca-\\ntion using separate neural networks, while the two-stream\\nCNN approach performs classi\\ufb01cation using the CNN di-\\nrectly with a late score fusion (Figure 2). Finally, the last\\ngroup contains the results of the proposed fusion network.\\n\\nAs can be seen, the SVM based fusion methods can greatly\\nimprove the results on UCF-101. On CCV, the gain is con-\\nsistent but not very signi\\ufb01cant, indicating that the short-\\nterm motion is more important for modeling the human ac-\\ntions with clearer motion patterns and less noises. SVM-\\nMKL is only slightly better than the simple early and late\\nfusion methods, which is consistent with observations in re-\\ncent works on visual recognition [42].\\n\\nOur proposed regularized feature fusion network (the last\\nrow in Table 2) is consistently better than the alternative\\nneural network based fusion methods shown in the third\\ngroup of the table. In particular, the gap between our re-\\nsults and that of the M-DBM and the two-stream CNN con-\\n\\ufb01rms that using regularizations in the fusion process is help-\\nful. Compared with the RDNN, our formulation produces\\nslightly better results but with a much lower complexity as\\ndiscussed earlier.\\n\\nIn addition, as the proposed formulation contains two\\nstructural norms, to directly evaluate the contribution of\\nthe norms, we also report a baseline using the same net-\\nwork structure without regularization (\\u201cnon-regularized fu-\\nsion network\\u201d in the table), which is similar to the M-DBM\\napproach in its design but di\\ufb00ers slightly in network struc-\\ntures. We see that adding regularizations in the same net-\\nwork can improve 1.4% on UCF-101 and 0.8% on CCV.\\n\\n4.2.3 The Hybrid Framework\\nFinally, we discuss the results of the entire hybrid deep\\nlearning framework by further combining results from the\\ntemporal LSTM and the regularized fusion network. The\\nprediction scores from these networks are fused linearly with\\nweights estimated by cross-validation. As shown in the last\\nrow of Table 3, we achieve very strong performance on both\\ndatasets: 91.3% on UCF-101 and 83.5% on CCV. The per-\\nformance of the hybrid framework is clearly better than\\nthat of the Spatial LSTM and the Motion LSTM (in Ta-\\nble 1). Compared with the Regularized Fusion Network (in\\nTable 2), adding the long-term temporal modeling in the\\nhybrid framework improves 2.9% on UCF-101 and 7.3% on\\nCCV, which are fairly signi\\ufb01cant considering the di\\ufb03culties\\nof the two datasets. In contrast to the fusion result in the\\nlast row of Table 1, the gain of the proposed hybrid frame-\\nwork comes from the use of the regularized fusion, which\\nagain veri\\ufb01es the e\\ufb00ectiveness of our fusion method.\\n\\nTo better understand the contributions of the key compo-\\nnents in the hybrid framework, we further report the per-\\nclass performance on CCV in Figure 4. We see that, al-\\nthough the performance of the LSTM is clearly lower, fusing\\nit with the video-level predictions by the regularized fusion\\nnetwork can signi\\ufb01cantly improve the results for almost all\\nthe classes. This is a bit surprising because some classes\\ndo not seem to require temporal information to be recog-\\n\\n\\x0cFigure 4: Per-class performance on CCV, using the Spatial and Motion LSTM, the Regularized Fusion\\nNetwork, and their combination, i.e., the Hybrid Deep Learning Framework.\\n\\nUCF-101\\nDonahue et al. [6]\\n\\nSrivastava et al. [36]\\n\\nWang et al. [44]\\nTran et al. [40]\\n\\nSimonyan et al. [33]\\n\\nLan et al. [22]\\nZha et al. [52]\\n\\nOurs\\n\\nCCV\\nXu et al. [48]\\nYe et al. [50]\\n\\n82.9%\\n84.3%\\n85.9% Jhuo et al. [12]\\nMa et al. [27]\\n86.7%\\n88.0%\\nLiu et al. [25]\\n89.1% Wu et al. [47]\\n89.6%\\n91.3%\\n\\n/\\n\\nOurs\\n\\n60.3%\\n64.0%\\n64.0%\\n63.4%\\n68.2%\\n70.6%\\n\\n83.5%\\n\\nFigure 5: Two example videos of class \\u201ccat\\u201d in the\\nCCV dataset with similar temporal clues over time.\\n\\nnized. After checking into some of the videos we \\ufb01nd that\\nthere could be helpful clues which can be modeled, even for\\nobject-related classes like \\u201ccat\\u201d and \\u201cdog\\u201d. For instance, as\\nshown in Figure 5, we observe that quite a number of \\u201ccat\\u201d\\nvideos contain only a single cat running around on the \\ufb02oor.\\nThe LSTM network may be able to capture this clue, which\\nis helpful for classi\\ufb01cation.\\n\\nE\\ufb03ciency. In addition to achieving the superior classi\\ufb01-\\ncation performance, our framework also enjoys high compu-\\ntational e\\ufb03ciency. We summarize the average testing time\\nof a UCF-101 video (duration: 8 seconds) as follows. The ex-\\ntraction of the frames and the optical \\ufb02ows takes 3.9 seconds,\\nand computing the CNN-based spatial and short-term mo-\\ntion features requires 9 seconds. Prediction with the LSTM\\nand the regularized fusion network needs 2.8 seconds. All\\nthese are evaluated on a single NVIDIA Telsa K40 GPU.\\n\\n4.2.4 Comparison with State of the Arts\\nIn this subsection, we compare with several state-of-the-\\nart results. As shown in Table 3, our hybrid deep learn-\\ning framework produces the highest performance on both\\ndatasets. On the UCF-101, many works with competitive\\nresults are based on the dense trajectories [44, 52], while\\nour approach fully relies on the deep learning techniques.\\nCompared with the original result of the two-stream CNN\\nin [33], our framework is better with the additional fusion\\nand temporal modeling functions, although it is built on our\\nimplementation of the CNN models which are slightly worse\\nthan that of [33] (our two-stream CNN result is 86.2%).\\nNote that a gain of even just 1% on the widely adopted UCF-\\n101 dataset is generally considered as a signi\\ufb01cant progress.\\n\\nTable 3: Comparison with state-of-the-art results.\\nOur approach produces to-date the best reported\\nresults on both datasets.\\n\\nIn addition, the recent works in [6, 36] also adopted the\\nLSTM to explore the temporal clues for video classi\\ufb01cation\\nand reported promising performance. However, our LSTM\\nresults are not directly comparable as the input features are\\nextracted by di\\ufb00erent neural networks.\\n\\nOn the CCV dataset, all the recent approaches relied on\\nthe joint use of multiple features by developing new fusion\\nmethods [48, 50, 12, 27, 25, 47]. Our hybrid deep learning\\nframework is signi\\ufb01cantly better than all of them.\\n\\n5. CONCLUSIONS\\n\\nWe have proposed a novel hybrid deep learning framework\\nfor video classi\\ufb01cation, which is able to model static visual\\nfeatures, short-term motion patterns and long-term tempo-\\nral clues.\\nIn the framework, we \\ufb01rst extract spatial and\\nmotion features with two CNNs trained on static frames\\nand stacked optical \\ufb02ows respectively. The two types of fea-\\ntures are used separately as inputs of the LSTM network\\nfor long-term temporal modeling. A regularized fusion net-\\nwork is also deployed to combine the two features on video-\\nlevel for improved classi\\ufb01cation. Our hybrid deep learning\\nframework integrating both the LSTM and the regularized\\nfusion network produces very impressive performance on two\\nwidely adopted benchmark datasets. Results not only verify\\nthe e\\ufb00ectiveness of the individual components of the frame-\\nwork, but also demonstrate that the frame-level temporal\\nmodeling and the video-level fusion and classi\\ufb01cation are\\nhighly complementary, and a big leap of performance can\\nbe attained by combining them.\\n\\nbasketballbaseballsoccerice skatingskiingswimmingbikingcatdogbirdgraduationbirthdaywedding receptionwedding ceremonywedding dancemusic perfnon-music perfparadebeachplayground0.00.20.40.60.81.0average precisionSpatial LSTMRegularized Feature FusionMotion LSTMHybrid Deep Learning Framework\\x0cAlthough deep learning based approaches have been suc-\\ncessful in addressing many problems, e\\ufb00ective network ar-\\nchitectures are urgently needed for modeling sequential data\\nlike the videos. Several researchers have recently explored\\nthis direction. However, compared with the progress on im-\\nage classi\\ufb01cation, the achieved performance gain on video\\nclassi\\ufb01cation over the traditional hand-crafted features is\\nmuch less signi\\ufb01cant. Our work in this paper represents\\none of the few studies showing very strong results. For fu-\\nture work, further improving the capability of modeling the\\ntemporal dimension of videos is of high priority.\\nIn addi-\\ntion, audio features which are known to be useful for video\\nclassi\\ufb01cation can be easily incorporated into our framework.\\n\\n6. REFERENCES\\n\\n[1] R. Arandjelovic and A. Zisserman. Three things everyone\\nshould know to improve object retrieval. In CVPR, 2012.\\n\\n[2] F. R. Bach, G. R. Lanckriet, and M. I. Jordan. Multiple kernel\\nlearning, conic duality, and the smo algorithm. In ICML, 2004.\\n\\n[3] Y. Bengio. Practical recommendations for gradient-based\\n\\ntraining of deep architectures. In Neural Networks: Tricks of\\nthe Trade. Springer, 2012.\\n\\n[4] J. Chung, \\xb8C. G\\xa8ul\\xb8cehre, K. Cho, and Y. Bengio. Gated feedback\\n\\nrecurrent neural networks. CoRR, 2015.\\n\\n[5] G. E. Dahl, D. Yu, L. Deng, and A. Acero. Context-Dependent\\n\\nPre-Trained Deep Neural Networks for Large-Vocabulary\\nSpeech Recognition. IEEE TASLP, 2012.\\n\\n[6] J. Donahue, L. A. Hendricks, S. Guadarrama, M. Rohrbach,\\n\\nS. Venugopalan, K. Saenko, and T. Darrell. Long-term\\nrecurrent convolutional networks for visual recognition and\\ndescription. CoRR, 2014.\\n\\n[7] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature\\n\\nhierarchies for accurate object detection and semantic\\nsegmentation. In CVPR, 2014.\\n\\n[8] A. Graves, A. Mohamed, and G. E. Hinton. Speech recognition\\n\\nwith deep recurrent neural networks. In ICASSP, 2013.\\n\\n[9] A. Graves and J. Schmidhuber. Framewise phoneme\\n\\nclassi\\ufb01cation with bidirectional lstm and other neural network\\narchitectures. Neural Networks, 2005.\\n\\n[10] C. Harris and M. J. Stephens. A combined corner and edge\\n\\ndetector. In Alvey Vision Conference, 1988.\\n\\n[11] M. Jain, J. van Gemert, and C. G. M. Snoek. University of\\namsterdam at thumos challenge 2014. In ECCV THUMOS\\nChallenge Workshop, 2014.\\n\\n[12] I.-H. Jhuo, G. Ye, S. Gao, D. Liu, Y.-G. Jiang, D. T. Lee, and\\n\\nS.-F. Chang. Discovering joint audio-visual codewords for video\\nevent detection. Machine Vision and Applications, 2014.\\n\\n[13] S. Ji, W. Xu, M. Yang, and K. Yu. 3d convolutional neural\\n\\nnetworks for human action recognition. In ICML, 2010.\\n[14] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long,\\n\\nR. Girshick, S. Guadarrama, and T. Darrell. Ca\\ufb00e:\\nConvolutional architecture for fast feature embedding. In ACM\\nMultimedia, 2014.\\n\\n[15] W. Jiang, C. Cotton, S.-F. Chang, D. Ellis, and A. Loui.\\nShort-term audio-visual atoms for generic video concept\\nclassi\\ufb01cation. In ACM Multimedia, 2009.\\n\\n[16] Y.-G. Jiang, J. Liu, A. Roshan Zamir, G. Toderici, I. Laptev,\\n\\nM. Shah, and R. Sukthankar. THUMOS challenge: Action\\nrecognition with a large number of classes.\\nhttp://crcv.ucf.edu/THUMOS14/, 2014.\\n\\n[17] Y.-G. Jiang, G. Ye, S.-F. Chang, D. Ellis, and A. C. Loui.\\n\\nConsumer video understanding: A benchmark database and an\\nevaluation of human and machine performance. In ACM\\nICMR, 2011.\\n\\n[18] A. Karpathy, G. Toderici, S. Shetty, T. Leung, R. Sukthankar,\\n\\nand L. Fei-Fei. Large-scale video classi\\ufb01cation with\\nconvolutional neural networks. In CVPR, 2014.\\n\\n[19] A. Klaser, M. Marsza(cid:32)lek, and C. Schmid. A spatio-temporal\\n\\ndescriptor based on 3d-gradients. In BMVC, 2008.\\n\\n[20] M. Kloft, U. Brefeld, S. Sonnenburg, and A. Zien. Lp-norm\\nmultiple kernel learning. The Journal of Machine Learning\\nResearch, 2011.\\n\\n[21] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet\\nclassi\\ufb01cation with deep convolutional neural networks. In\\nNIPS, 2012.\\n\\n[22] Z. Lan, M. Lin, X. Li, A. G. Hauptmann, and B. Raj. Beyond\\n\\ngaussian pyramid: Multi-skip feature stacking for action\\nrecognition. CoRR, 2014.\\n\\n[23] I. Laptev. On space-time interest points. IJCV,\\n\\n64(2/3):107\\u2013123, 2007.\\n\\n[24] W. Li, Z. Zhang, and Z. Liu. Expandable data-driven graphical\\n\\nmodeling of human actions based on salient postures. IEEE\\nTCSVT, 2008.\\n\\n[25] D. Liu, K.-T. Lai, G. Ye, M.-S. Chen, and S.-F. Chang.\\n\\nSample-speci\\ufb01c late fusion for visual category recognition. In\\nCVPR, 2013.\\n\\n[26] D. G. Lowe. Distinctive image features from scale-invariant\\n\\nkeypoints. IJCV, 2004.\\n\\n[27] A. J. Ma and P. C. Yuen. Reduced analytic dependency\\n\\nmodeling: Robust fusion for visual recognition. IJCV, 2014.\\n\\n[28] J. Ngiam, A. Khosla, M. Kim, J. Nam, H. Lee, and A. Ng.\\n\\nMultimodal deep learning. In ICML, 2011.\\n\\n[29] D. Oneata, J. Verbeek, C. Schmid, et al. Action and event\\nrecognition with \\ufb01sher vectors on a compact feature set. In\\nICCV, 2013.\\n\\n[30] M. Ranzato, A. Szlam, J. Bruna, M. Mathieu, R. Collobert,\\n\\nand S. Chopra. Video (language) modeling: a baseline for\\ngenerative models of natural videos. CoRR, 2014.\\n\\n[31] A. S. Razavian, H. Azizpour, J. Sullivan, and S. Carlsson. CNN\\n\\nfeatures o\\ufb00-the-shelf: an astounding baseline for recognition.\\nCoRR, 2014.\\n\\n[32] J. S\\xb4anchez, F. Perronnin, T. Mensink, and J. Verbeek. Image\\n\\nclassi\\ufb01cation with the \\ufb01sher vector: Theory and practice.\\nIJCV, 2013.\\n\\n[33] K. Simonyan and A. Zisserman. Two-stream convolutional\\n\\nnetworks for action recognition in videos. In NIPS, 2014.\\n[34] K. Simonyan and A. Zisserman. Very deep convolutional\\nnetworks for large-scale image recognition. CoRR, 2014.\\n\\n[35] K. Soomro, A. R. Zamir, and M. Shah. UCF101: A dataset of\\n\\n101 human actions classes from videos in the wild. CoRR, 2012.\\n\\n[36] N. Srivastava, E. Mansimov, and R. Salakhutdinov.\\n\\nUnsupervised learning of video representations using LSTMs.\\nCoRR, 2015.\\n\\n[37] N. Srivastava and R. Salakhutdinov. Multimodal learning with\\n\\ndeep boltzmann machines. In NIPS, 2012.\\n\\n[38] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov,\\n\\nD. Erhan, V. Vanhoucke, and A. Rabinovich. Going Deeper\\nwith Convolutions. CoRR, 2014.\\n\\n[39] K. Tang, L. Fei-Fei, and D. Koller. Learning latent temporal\\n\\nstructure for complex event detection. In CVPR, 2012.\\n\\n[40] D. Tran, L. Bourdev, R. Fergus, L. Torresani, and M. Paluri.\\n\\nC3d: Generic features for video analysis. CoRR, 2014.\\n\\n[41] D. L. Vail, M. M. Veloso, and J. D. La\\ufb00erty. Conditional\\nrandom \\ufb01elds for activity recognition. In AAAMS, 2007.\\n\\n[42] A. Vedaldi, V. Gulshan, M. Varma, and A. Zisserman. Multiple\\n\\nkernels for object detection. In ICCV, 2009.\\n\\n[43] S. Venugopalan, H. Xu, J. Donahue, M. Rohrbach, R. J.\\n\\nMooney, and K. Saenko. Translating videos to natural language\\nusing deep recurrent neural networks. CoRR, 2014.\\n\\n[44] H. Wang and C. Schmid. Action recognition with improved\\n\\ntrajectories. In ICCV, 2013.\\n\\n[45] H. Wang, M. M. Ullah, A. Klaser, I. Laptev, and C. Schmid.\\n\\nEvaluation of local spatio-temporal features for action\\nrecognition. In BMVC, 2009.\\n\\n[46] Y. Wang and G. Mori. Max-margin hidden conditional random\\n\\n\\ufb01elds for human action recognition. In CVPR, 2009.\\n\\n[47] Z. Wu, Y.-G. Jiang, J. Wang, J. Pu, and X. Xue. Exploring\\n\\ninter-feature and inter-class relationships with deep neural\\nnetworks for video classi\\ufb01cation. In ACM Multimedia, 2014.\\n\\n[48] Z. Xu, Y. Yang, I. Tsang, N. Sebe, and A. Hauptmann. Feature\\nweighting via optimal thresholding for video analysis. In ICCV,\\n2013.\\n\\n[49] H. Yang, M. R. Lyu, and I. King. E\\ufb03cient online learning for\\n\\nmultitask feature selection. ACM SIGKDD, 2013.\\n\\n[50] G. Ye, D. Liu, I.-H. Jhuo, and S.-F. Chang. Robust late fusion\\n\\nwith rank minimization. In CVPR, 2012.\\n\\n[51] Z. Zeng and Q. Ji. Knowledge based activity recognition with\\n\\ndynamic bayesian network. In ECCV, 2010.\\n\\n[52] S. Zha, F. Luisier, W. Andrews, N. Srivastava, and\\n\\nR. Salakhutdinov. Exploiting image-trained cnn architectures\\nfor unconstrained video classi\\ufb01cation. CoRR, 2015.\\n\\n[53] J. Zhang, M. Marsza(cid:32)lek, S. Lazebnik, and C. Schmid. Local\\nfeatures and kernels for classi\\ufb01cation of texture and object\\ncategories: A comprehensive study. IJCV, 2007.\\n\\n\\x0c', u'DEEP LEARNING FOR ROBUST FEATURE GENERATION\\n\\nIN AUDIOVISUAL EMOTION RECOGNITION\\nYelin Kim, Honglak Lee, and Emily Mower Provost \\u2217\\n\\nElectrical Engineering and Computer Science, Ann Arbor, Michigan, USA\\n\\n{yelinkim, honglak, emilykmp}@umich.edu\\n\\nUniversity of Michigan\\n\\nABSTRACT\\n\\nAutomatic emotion recognition systems predict high-level affective\\ncontent from low-level human-centered signal cues. These systems\\nhave seen great improvements in classi\\ufb01cation accuracy, due in part\\nto advances in feature selection methods. However, many of these\\nfeature selection methods capture only linear relationships between\\nfeatures or alternatively require the use of labeled data. In this paper\\nwe focus on deep learning techniques, which can overcome these\\nlimitations by explicitly capturing complex non-linear feature inter-\\nactions in multimodal data. We propose and evaluate a suite of Deep\\nBelief Network models, and demonstrate that these models show im-\\nprovement in emotion classi\\ufb01cation performance over baselines that\\ndo not employ deep learning. This suggests that the learned high-\\norder non-linear relationships are effective for emotion recognition.\\nIndex Terms\\u2014 emotion classi\\ufb01cation, deep learning, multi-\\nmodal features, unsupervised feature learning, deep belief networks\\n\\n1. INTRODUCTION\\n\\nEmotion recognition is the process of predicting the high-level af-\\nfective content of an utterance from the low-level signal cues pro-\\nduced by a speaker. This process is complicated by the inherent\\nmultimodality of human emotion expression (e.g., facial and vocal\\nexpression). This multimodality is characterized by complex high-\\ndimensional and non-linear cross-modal interactions [1]. Previous\\nresearch has demonstrated the bene\\ufb01t of using multimodal data in\\nemotion recognition tasks and has identi\\ufb01ed various techniques for\\ngenerating robust multimodal features [2\\u20136]. However, although ef-\\nfective, these techniques do not take advantage of the complex non-\\nlinear relationship that exists between the modalities of interest, or\\nalternatively require the use of labeled data. In this work, we apply\\ndeep learning techniques to provide robust features for audio-visual\\nemotion recognition.\\n\\nEmotion recognition accuracy relies heavily on the ability to\\ngenerate representative features. However, this is a very challenging\\nproblem. Emotion states do not have explicit temporal boundaries\\nand emotion expression patterns often vary across individuals [7].\\nThis problem is further complicated by the high dimensionality of\\nthe audio-visual feature space. Consequently, accurate modeling\\ngenerally requires a reduction of the original input feature space.\\nThis reduction is commonly accomplished using feature selection,\\na method that identi\\ufb01es a subset of the initial features that provide\\nenhanced classi\\ufb01cation accuracy [8]. However, it is not yet clear\\nwhether it is more advantageous to select a subset of emotionally\\n\\u2217This work is supported by the National Science Foundation (NSF RI\\n\\n1217183)\\n\\nrelevant features or to capture the complex interactions across all fea-\\ntures considered. In this paper, we demonstrate the effectiveness of\\nDeep Belief Networks (DBN) for multimodal emotion feature gen-\\neration. We learn multi-layered DBNs that capture the non-linear\\ndependencies of audio-visual features while reducing the dimension-\\nality of the feature space.\\n\\nThere has been a substantial body of work on feature repre-\\nsentation, extraction, and selection methods in the emotion recog-\\nnition \\ufb01eld in the last decade. Our work is motivated by the dis-\\ncovery of methods for learning multiple layers of adaptive features\\nusing DBNs [9]. Research has demonstrated that deep networks\\ncan effectively generate discriminative features that approximate the\\ncomplex non-linear dependencies between features in the original\\nset. These deep generative models have been applied to speech\\nand language processing, as well as emotion recognition tasks [10\\u2013\\n12]. In speech processing, Ngiam et al. [13] proposed and evaluated\\ndeep networks to learn audio-visual features from spoken letters. In\\nemotion recognition, Brueckner et al. [14] found that the use of a\\nRestricted Boltzmann Machine (RBM) prior to a two-layer neural\\nnetwork with \\ufb01ne-tuning could signi\\ufb01cantly improve classi\\ufb01cation\\naccuracy in the Interspeech automatic likability classi\\ufb01cation chal-\\nlenge [15]. The work by Stuhlsatz et al. [16] took a different ap-\\nproach for learning acoustic features in speech emotion recognition\\nusing Generalized Discriminant Analysis (GerDA) based on Deep\\nNeural Networks (DNNs). While the present study is related to re-\\ncent approaches in multi-modal deep learning and the application of\\ndeep learning techniques to emotion data, it focuses on non-linear\\naudio-visual feature learning for emotion, which has not been exten-\\nsively explored in the emotion recognition domain.\\n\\nIn the current work we present a suite of DBN models to inves-\\ntigate audio-visual feature learning in the emotion domain. We com-\\npare two methodologies: (1) unsupervised feature learning (DBN)\\nand (2) secondary supervised feature selection. We \\ufb01rst build an\\nunsupervised two-layer DBN, enforcing multi-modal learning as in-\\ntroduced by [13]. We augment this DBN with two types of feature\\nselection (FS): 1) before DBN training to assess the bene\\ufb01t of fea-\\nture learning exclusively from an emotionally-salient subset of the\\noriginal features and 2) after DBN training to assess the advantage\\nof reducing the learned feature space in a supervised context. We\\ncompare this to the performance of a three-layer DBN model. Our\\nbaseline is a Support Vector Machine that uses subsets of the origi-\\nnal feature space selected using supervised and unsupervised feature\\nselection. The results provide important insight into feature learning\\nmethods for multimodal emotion data.\\n\\nThe results show that the DBN models outperform the baseline\\nmodels. Further, our results demonstrate that the three-layer DBN\\noutperforms the two-layer DBN models for emotionally subtle data.\\n\\n\\x0cThis suggests that unsupervised feature learning can be used in lieu\\nof supervised feature selection for this data type. In addition, the\\nrelative performance improvement of the three-layer model for sub-\\ntle emotions suggests that these complex feature relationships are\\nparticularly important for identifying subtle emotional cues. This is\\nan important \\ufb01nding given the challenges inherent in and need for\\nrecognizing emotions elicited in realistic scenarios [17].\\n\\n2. RELATED WORK\\n\\n2.1. Feature Selection in Emotion Recognition\\n\\nIn this section, we discuss the feature selection techniques that are\\nused extensively in emotion research including: Forward Selection,\\nInformation Gain (IG), and Principal Component Analysis (PCA).\\nThese techniques are either supervised (forward selection and IG)\\nor use representations based on the linear dependencies between the\\noriginal features (PCA).\\n\\nForward feature selection is a greedy algorithm that sequen-\\ntially selects features that increase the overall classi\\ufb01cation accuracy.\\nThis method has been widely used in many machine learning ap-\\nplications, including emotion recognition tasks [18]. Although this\\nmethod can identify a subset of good features for classi\\ufb01cation, it\\nmay not be suitable if there are groups of features with complex\\nrelationships due to the greedy nature of the approach. IG based fea-\\nture selection methods are also commonly used in emotion recogni-\\ntion [19,20]. This method ranks features by calculating the reduction\\nin the entropy of class labels given knowledge of each feature. In\\ngeneral, however, it does not search for feature interactions. Further-\\nmore, both forward selection and IG methods require labeled data\\nduring the feature selection process.\\n\\nPCA and its variants (e.g., Principal Feature Analysis, or PFA\\n[21]) are broadly used in the emotion recognition literature [22\\u201324].\\nPCA \\ufb01nds a linear projection of the base feature set to a new fea-\\nture space where the new features are uncorrelated. The feature set\\ncan be reduced to retain a majority of the variance in the original\\nfeature space. Although this unsupervised method has been widely\\nused in many emotion applications, the limitation is in its linear pro-\\njection of the base features, which tends to obscure the emotion con-\\ntent [25]. PFA is an extension of PCA. It clusters the data in the PCA\\nspace and returns \\ufb01nal features closest to the center of each cluster.\\nThis results in a feature set that maintains an approximation of the\\nvariance of the original set, while minimizing correlations between\\nfeatures. We leverage IG for our proposed deep learning feature se-\\nlection methods, and IG and PFA for the baseline models.\\n\\n2.2. Unsupervised Feature Learning and Deep Learning\\n\\nDeep learning techniques (See [9] for a survey) have become in-\\ncreasingly popular in various communities including speech and\\nlanguage processing [10\\u201312] and vision processing [26\\u201330]. This\\nprogress has been facilitated by the recent discovery of more effec-\\ntive learning algorithms for constructing DBNs in an unsupervised\\ncontext, for example exploiting single-layer building blocks such\\nas Restricted Boltzmann Machines (RBMs) [31]. DBNs [32] learn\\nhierarchical representation from data and can be effectively con-\\nstructed by greedily training and stacking multiple RBMs.\\nRBMs are undirected graphical models that represent the density\\nof input data v \\u2208 RD (referred to as \\u201cvisible units\\u201d) using binary\\nlatent variables h \\u2208 {0, 1}K (referred to as \\u201chidden units\\u201d).\\nIn\\nthe RBM, there are no connections between units in the same layer,\\nwhich makes it easy to compute the conditional probabilities.\\n\\nIn this work, we use Gaussian RBMs that employ real-valued\\nvisible units for training the \\ufb01rst layer of the DBNs. We use\\nBernoulli-Bernoulli RBMs that employ binary visible and hidden\\nunits for training the deeper layers. In a Gaussian RBM, the joint\\nprobability distribution and energy function of v and h is as follows:\\n(1)\\n\\nP (v, h) =\\n\\nE(v, h) =\\n\\n1\\nZ\\n\\nexp(\\u2212E(v, h))\\n1\\n2\\u03c32\\n\\n(cid:88)\\n(cid:32)(cid:88)\\n\\nv2\\ni\\n\\ni\\n\\n\\u2212 1\\n\\u03c32\\n\\ni\\n\\ncivi +\\n\\n(cid:88)\\n\\nj\\n\\nbjhj +\\n\\n(cid:88)\\n\\ni,j\\n\\nviWijhj\\n\\n(cid:33) (2)\\n\\nwhere c \\u2208 RD and b \\u2208 RK are the biases for visible and hidden\\nunits, respectively, W \\u2208 RD\\xd7K are weights between visible units\\nand hidden units, \\u03c3 is a hyper-parameter, and Z is a normalization\\nconstant. The conditional probability distributions of the Gaussian\\nRBM are as follows:\\n\\n(3)\\n\\n(4)\\n\\n(5)\\n\\nP (hj = 1|v) = sigmoid\\n\\n(cid:33)\\n\\nWijvi + bj)\\n\\n(cid:33)\\n\\n(cid:88)\\n\\ni\\n\\n1\\n\\u03c32 (\\n\\n(cid:32)\\n(cid:88)\\n\\nj\\n\\n(cid:32)\\n\\nP (vi|h) = N\\n\\nvi;\\n\\nWijhj + ci, \\u03c32\\n\\nThe posteriors of the hidden units given visible units (Equation 3)\\nform the generated features used in the classi\\ufb01cation framework.\\nThe parameters of the RBM (W, b, c) are learned using contrastive\\ndivergence as in [33]. We use sparsity regularization [26] to penal-\\nize a deviation of expected activation of the hidden units from a low\\n\\ufb01xed level p. Given a training set {v(1), ..., v(m)}, we include a reg-\\nularization penalty of the form:\\n\\n(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)p \\u2212 1\\n\\nm\\n\\nK(cid:88)\\n\\nj=1\\n\\nm(cid:88)\\n\\nE(cid:104)\\n\\nl=1\\n\\nj |v(l)(cid:105)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)2\\n\\nh(l)\\n\\n\\u03bb\\n\\nwhere E[\\xb7] is the conditional expectation given the data, \\u03bb is a reg-\\nularization parameter, and p is a constant that speci\\ufb01es the target\\nactivation of the hidden unit hj [26].\\n\\n3. DATA\\n\\n3.1. IEMOCAP Data\\nIn this work, we use the Interactive Emotional Dyadic Motion Cap-\\nture (IEMOCAP) Database [34]. This database contains both mo-\\ntion capture markers and audio data from \\ufb01ve pairs of actors (male-\\nfemale). The subjects\\u2019 facial movements were recorded using 53 in-\\nfrared facial markers. The actors performed from scripted scenes and\\nimprovised scenarios. This method of collection allowed for both\\ncontrol over the affective content and naturalistic speaking styles.\\n\\nThe data were evaluated using categorical and dimensional la-\\nbels (only categorical labels are used in this study). The categorical\\nground truth of the data was labeled by at least three evaluators. In\\nthis work, we only consider utterances with labels from the follow-\\ning set: Angry, Happy, Neutral, Sad. We use three types of utter-\\nances in this paper: (1) prototypical data (complete agreement on\\nthe affective state from evaluators), (2) non-prototypical data (ma-\\njority agreement), and (3) a combined set of these two data types.\\nThere are 1430 utterances in prototypical data (Angry: 284, Happy:\\n707, Neutral: 123, Sad: 316 utterances) and 1588 utterances in non-\\nprototypical data (Angry: 316, Happy: 498, Neutral: 455, Sad: 319\\nutterances), resulting in 3018 utterances in the combined set.\\n\\n\\x0c3.2. Audio-Visual Feature Extraction\\nThe original audio features include both prosodic and spectral fea-\\ntures, such as pitch, energy and mel-frequency \\ufb01lter banks (MFBs).\\nMFBs have been shown to be better discriminative features than\\nmel-frequency cepstral coef\\ufb01cients (MFCCs) in emotion recogni-\\ntion [35]. The original video features are based on Facial Animation\\nParameters (FAP), part of the MPEG-4 standard. FAPs describe the\\nmovement of the face using distances between particular points on\\nthe face. They have been widely used to capture facial expressions\\nin the emotion recognition literature. The subset is chosen to include\\nemotionally meaningful movements (e.g., eye squint, smile, etc.).\\n\\nThe \\ufb01nal features are statistical functionals of the raw audio-\\nvisual features. These include mean, variance, lower and upper\\nquantiles, and quantile range, giving a total of 685 features. Of\\nthese 685 features, 145 are auditory features and 540 are video fea-\\ntures. The features are normalized on a per-speaker basis to mitigate\\nspeaker variation [20].\\n\\n4. PROPOSED METHOD\\n\\n4.1. Cross-Validation and Performance Evaluation\\nWe use leave-one-speaker-out cross validation to ensure that the\\nmodels are not overtraining to the affective styles of a particular\\nspeaker. We pre-train the DBN models (unsupervised) and search\\nfor the best hyper-parameters including: sparsity parameters and the\\nnumber of \\ufb01nal output nodes. We select our hyper-parameters using\\ncross validation over the training data. We \\ufb01x the number of hidden\\nnodes of the two-layer DBNs, the sigma parameter for the \\ufb01rst-layer\\nGaussian RBMs, and the L2 regularization parameter (Section 4.3).\\nWe select the best hyper-parameters for each data type: prototypical,\\nnon-prototypical, and combined.\\n\\nWe evaluate the performance of the baseline and DBN systems\\nusing Unweighted Accuracy (UA). UA is an average of the recall\\nfor each emotion class [17]. The unweighted accuracy better re\\ufb02ects\\noverall accuracy in the presence of class imbalance.\\n\\n4.2. Baseline Models\\nOur baseline models are two SVMs with radial basis function (RBF)\\nkernels. The SVMs do not use features generated via deep learn-\\ning techniques. The SVMs have radial basis function (RBF) ker-\\nnels and are implemented using the Matlab Bioinformatics Toolkit.\\nWe train four emotion-speci\\ufb01c binary SVMs in a self-vs.-other ap-\\nproach. The \\ufb01nal emotion class label is assigned by identifying the\\nmodel in which the test point is maximally far from the hyperplane\\nas in as in [20].\\n\\nBoth models employ feature selection. The \\ufb01rst uses IG [36]\\nand the second uses PFA [21] feature selection (a supervised and\\nunsupervised feature selection technique, respectively).\\nIG is ap-\\nplied to each emotion class, resulting in four sets of emotion-speci\\ufb01c\\nfeatures. Each emotion-speci\\ufb01c SVM uses the associated emotion-\\nspeci\\ufb01c feature subset. The number of features is chosen over {60,\\n120, 180} for each data type.\\n\\nWe optimized the baselines using leave-one-subject-out cross-\\nvalidation for each data type (prototypical, non-prototypical, and\\ncombined data). The parameters include the number of selected fea-\\ntures using IG and PFA, the value of the box constraint (C=1) for the\\nsoft margin in the SVM, and the scaling factor (sigma=8) in the RBF\\nkernel.\\n\\nWe also compare our results with the maximal accuracy achieved\\nfrom a previous work of Metallinou et al. [37], which utilizes the\\n\\n(a)\\n\\n(c)\\n\\n(b)\\n\\n(d)\\n\\nFig. 1. Illustration of proposed models: (a) DBN2, (b) FS-DBN2,\\n(c) DBN2-FS, and (d) DBN3.\\n\\nsame IEMOCAP database as our work and introduces a decision-\\nlevel Bayesian fusion over models using face, voice, and head\\nmovement cues. Although Metallinou\\u2019s work used a different subset\\nof the IEMOCAP database, this comparison supports the strong\\nperformance of our proposed method.\\n\\n4.3. Deep Belief Network Models\\nWe experiment with four different DBN models in order to explore\\ndifferent non-linear dependencies between audio and motion-capture\\nfeatures. We also assess the utility of feature selection methods in\\nthese deep architectures (Figure 1).\\n\\nOur basic DBN is a two-layer model and is a building block for\\nthe other variants. It learns the audio and video features separately\\nin the \\ufb01rst hidden layer. The learned features from the \\ufb01rst layer\\nare concatenated and used as the input to the second hidden layer as\\nintroduced in [13]. We call this the DBN2 model (Figure 1(a)).\\n\\nThe other three DBN models are based on DBN2. Two involve\\nfeature selection and one is a three-layer DBN model. The two-\\nlayer models use supervised feature selection (IG) either prior to or\\npost the unsupervised pre-training. The three-layer model reduces\\nthe feature dimensionality using a third RBM layer, invoking unsu-\\npervised feature learning. Thus, the three-layer model captures ad-\\nditional high-order non-linear dependencies of all features, whereas\\nthe models employing supervised feature selection use only emo-\\ntionally salient features. The variants are de\\ufb01ned as follows:\\n\\n\\u2022 FS-DBN2 is a two-layer DBN with feature selection prior to\\n\\nthe training of the DBN2 model (Figure 1(b)).\\n\\n\\u2022 DBN2-FS is a two-layer DBN with feature selection on the\\n\\n\\ufb01nal RBM output nodes (Figure 1(c)).\\n\\n\\u2022 DBN3 is a three-layer DBN that stacks an additional RBM\\non the second-layer RBM output nodes of the DBN2 model\\n(Figure 1(d)).\\n\\nThe number of hidden units in the \\ufb01rst layer is approximately\\n1.5x overcomplete for each audio feature (300 units from 145 audio\\nfeatures) and video feature (700 units from 540 video features), re-\\nsulting in 1000 concatenated \\ufb01rst layer hidden units. The number of\\nsecond hidden units is \\ufb01xed at 200 for DBN2, DBN2-FS, and DBN3.\\n\\n... ... Audio Input ... ... Video Input ... ... ... Audio Input ... ... Video Input ... ... ... Feature  Selection ... ... Audio Input ... ... Video Input ... ... Feature  Selection ... ... Audio Input ... ... Video Input ... ... RBM \\x0cmodels and maximal UAs of the baseline models (73.38% with IG)\\nis 0.40%. The baseline models themselves achieve differing lev-\\nels of accuracy; the IG baseline outperforms the PFA baseline by\\n3.36%. This may suggest that in emotionally clear utterances, super-\\nvised feature selection (emotion-speci\\ufb01c IG) is preferable to unsu-\\npervised feature selection (PFA). The accuracy of the DBN3 model\\nindicates that unsupervised feature learning can achieve compara-\\nble performance to supervised feature selection for emotionally clear\\nutterances. Further, the DBN3 outperforms unsupervised feature se-\\nlection (PFA baseline) by 3.76%, highlighting the potential impor-\\ntance of feature learning rather than unsupervised feature reduction\\nfor emotionally clear data.\\n\\nThe deep learning method performs comparably to the previous\\nwork of Metallinou et al. [37], 62.42%. Direct comparisons are not\\npossible due to differences in the data subsets considered.\\n\\n6. CONCLUSIONS\\n\\nIn this work, we investigate the utility of deep learning techniques\\nfor unsupervised feature learning in audio-visual emotion recogni-\\ntion. Our results demonstrate that DBNs can be used to generate\\naudio-visual features for emotion classi\\ufb01cation, even in an unsuper-\\nvised context. The comparison of the classi\\ufb01cation performances\\nbetween the baseline and the proposed DBN models demonstrate\\nthat it is important to retain complex non-linear feature relationships\\n(using deep learning techniques) in emotion classi\\ufb01cation tasks. The\\nstrongest performance gain is observed in the non-prototypical data.\\nThis is important in applications of automatic emotion recognition\\nsystems where emotional subtlety is common.\\n\\nIn our future work, we will investigate the comparative advan-\\ntage of deep learning techniques with additional emotion corpora.\\nWe will also investigate deep modeling in the context of dynamic\\nfeature generation. Finally, the visualization of complex dependen-\\ncies between either features or weights between hidden nodes of the\\nDBNs may open a new gateway for the interpretation of audio-visual\\nemotion data.\\n\\n7. ACKNOWLEDGEMENTS\\n\\nThanks to Kihyuk Sohn, David Escott, Krithika Swaminatham, and\\nNattavut Yampikulsakul for many helpful discussions.\\n\\nTable 1. Unweighted classi\\ufb01cation accuracy (%) for combined, non-\\nprototypical, and prototypical data\\n\\nBaseline\\n\\nIG\\n\\nPFA DBN2 DBN2-\\n\\nProposed DBNs\\nDBN3\\n\\nFS\\n66.12\\n56.97\\n72.96\\n\\n65.71\\n57.70\\n73.78\\n\\nFS-\\nDBN2\\n65.89\\n56.07\\n72.77\\n\\nCombined 64.42 64.45 65.25\\n55.81 55.99 56.89\\nNon-Prot\\nProt\\n73.38 70.02 70.46\\n\\nFor FS-DBN2, the number of second hidden units is \\ufb01xed to 150 be-\\ncause the number of visible units is smaller compared to the other\\nthree DBN models.\\n\\nThe sparseness parameters are selected using leave-one-speaker-\\nout cross-validation, while all other parameters (including hidden\\nlayer size and weight regularization) are kept \\ufb01xed (See Section 4.1\\nfor details). Since the number of video features is larger than the\\nnumber of audio features, we select the sparsity parameters of bias\\nfor audio data and video data over {0.1, 0.2} and {0.02, 0.1}, re-\\nspectively. Also, the sparsity parameters of \\u03bb are selected over {2,\\n6, 10} for audio features, while \\u03bb sparsity parameters are \\ufb01xed at\\n5 for video features. Our preliminary results demonstrated that the\\n\\u03bb value for the video features did not noticeably affect the results.\\nThe number of features selected at the \\ufb01nal level (DBN2-FS) and\\nthe number of hidden units at the \\ufb01nal level (DBN3) are selected\\nover {50, 100, 150}.\\n\\nFor FS-DBN2, a total of 100 audio features and 200 video fea-\\ntures are chosen using IG. We \\ufb01rst pre-train a sparse RBM with 100-\\n200 nodes for the audio features and 200-600 nodes for the video\\nfeatures. We select the sparsity parameters of bias over {0.1, 0.5}\\nfor each RBM. \\u03bb is \\ufb01xed as 5. Next, we concatenate the learned fea-\\ntures and pre-train a \\ufb01rst layer of DBN with 800 output nodes and\\nthe second layer with 150 nodes (Bernoulli-Bernoulli).\\n\\nThe output of each DBN is classi\\ufb01ed using the same SVM struc-\\n\\nture used in the baseline (Section 4.2).\\n\\n5. RESULTS AND DISCUSSION\\n\\nA summary of the emotion classi\\ufb01cation results can be seen in Ta-\\nble 1. The DBN models for the combined data achieve UAs rang-\\ning from 65.25% (DBN2) to 66.12% (DBN2-FS). All DBN models\\noutperform the baseline models (the two baseline models perform\\ncomparably). The performance gap between the maximal UAs of\\nproposed models and the PFA baseline is 1.67%.\\n\\nThe DBN models for the non-prototypical data achieve accu-\\nracies ranging from 56.70% (FS-DBN2) to 57.70% (DBN3). All\\nDBNs outperform the baseline models (which again perform com-\\nparably). The performance gaps between the UAs of proposed mod-\\nels and baseline models range from 1.71% to 1.89%. We obtain a\\nslight performance gain when using DBN3 compared to both DBN2-\\nFS and FS-DBN2 for subtle or non-prototypical utterances (0.73%\\nand 1.63% increase, respectively). This result is important given\\nthat the DBN3 model does not use any labeled data (unsupervised\\nfeature learning), whereas the FS-DBN2 model learns a new set of\\nfeatures from a previously identi\\ufb01ed subset of emotionally salient\\nfeatures and the DBN2-FS invokes feature selection at the output.\\nThis demonstrates that we can effectively use unsupervised feature\\nlearning, rather than supervised feature selection, for emotion recog-\\nnition, even for emotionally subtle utterances (non-prototypical).\\n\\nThe DBN models for the prototypical data achieve accuracies\\nranging from 70.46% (DBN2) to the maximum of 73.78% (DBN3).\\nThe performance gap between the maximal UAs of the proposed\\n\\n\\x0c8. REFERENCES\\n\\n[1] G.W. Taylor, G.E. Hinton, and S.T. Roweis, \\u201cModeling human motion\\nusing binary latent variables,\\u201d Advances in neural information process-\\ning systems, vol. 19, pp. 1345, 2007.\\n\\n[2] C. Busso, Z. Deng, S. Yildirim, M. Bulut, C.-M. Lee, A. Kazemzadeh,\\nS. Lee, U. Neumann, and S. Narayanan, \\u201cAnalysis of emotion recogni-\\ntion using facial expressions, speech and multimodal information,\\u201d in\\nProceedings of the 6th international conference on Multimodal inter-\\nfaces. ACM, 2004, pp. 205\\u2013211.\\n\\n[3] D. Ververidis and C. Kotropoulos, \\u201cFast and accurate sequential \\ufb02oat-\\ning forward feature selection with the bayes classi\\ufb01er applied to speech\\nemotion recognition,\\u201d Signal Processing, vol. 88, no. 12, pp. 2956\\u2013\\n2970, 2008.\\n\\n[4] T. Vogt and E. Andr\\xb4e, \\u201cComparing feature sets for acted and sponta-\\nneous speech in view of automatic emotion recognition,\\u201d in IEEE In-\\nternational Conference on Multimedia and Expo (ICME). IEEE, 2005,\\npp. 474\\u2013477.\\n\\n[5] M. Wimmer, B. Schuller, D. Arsic, G. Rigoll, and B. Radig, \\u201cLow-\\nlevel fusion of audio and video feature for multi-modal emotion recog-\\nnition,\\u201d in 3rd International Conference on Computer Vision Theory\\nand Applications. VISAPP, 2008, vol. 2, pp. 145\\u2013151.\\n\\n[6] M. Pantic, G. Caridakis, E. Andr\\xb4e, J. Kim, K. Karpouzis, and S. Kol-\\nlias, \\u201cMultimodal emotion recognition from low-level cues,\\u201d Emotion-\\nOriented Systems, pp. 115\\u2013132, 2011.\\n\\n[7] C.N. Anagnostopoulos, T. Iliou, and I. Giannoukos, \\u201cFeatures and clas-\\nsi\\ufb01ers for emotion recognition from speech: a survey from 2000 to\\n2011,\\u201d Arti\\ufb01cial Intelligence Review, pp. 1\\u201323, 2012.\\n\\n[8] M. El Ayadi, M.S. Kamel, and F. Karray, \\u201cSurvey on speech emotion\\nrecognition: Features, classi\\ufb01cation schemes, and databases,\\u201d Pattern\\nRecognition, vol. 44, no. 3, pp. 572\\u2013587, 2011.\\n\\n[9] Y. Bengio, \\u201cLearning deep architectures for AI,\\u201d Foundations and\\n\\nTrends in Machine Learning, vol. 2, no. 1, pp. 1\\u2013127, 2009.\\n\\n[10] N. Morgan,\\n\\n\\u201cDeep and wide: Multiple layers in automatic speech\\nrecognition,\\u201d Audio, Speech, and Language Processing, IEEE Trans-\\nactions on, vol. 20, no. 1, pp. 7\\u201313, 2012.\\n\\n[11] A. Mohamed, G.E. Dahl, and G. Hinton, \\u201cAcoustic modeling using\\ndeep belief networks,\\u201d Audio, Speech, and Language Processing, IEEE\\nTransactions on, vol. 20, no. 1, pp. 14\\u201322, 2012.\\n\\n[12] G. Sivaram and H. Hermansky,\\n\\n\\u201cSparse multilayer perceptron for\\nphoneme recognition,\\u201d Audio, Speech, and Language Processing, IEEE\\nTransactions on, vol. 20, no. 1, pp. 23\\u201329, 2012.\\n\\n[13] J. Ngiam, A. Khosla, M. Kim, J. Nam, H. Lee, and A.Y. Ng, \\u201cMulti-\\nmodal deep learning,\\u201d in Proceedings of the 28th International Confer-\\nence on Machine Learning (ICML), 2011, pp. 689\\u2013696.\\n\\n[14] R. Brueckner and B Schuller, \\u201cLikability classi\\ufb01cation - a not so deep\\n\\nneural network approach,\\u201d in Proceedings of INTERSPEECH, 2012.\\n\\n[15] B. Schuller, S. Steidl, A. Batliner, E. N\\xa8oth, A. Vinciarelli, F. Burkhardt,\\nR. van Son, F. Weninger, F. Eyben, T. Bocklet, et al., \\u201cThe interspeech\\n2012 speaker trait challenge,\\u201d Interspeech, Portland, Oregon, 2012.\\n\\n[16] A. Stuhlsatz, C. Meyer, F. Eyben, T. ZieIke, G. Meier, and B. Schuller,\\n\\u201cDeep neural networks for acoustic emotion recognition: raising the\\nbenchmarks,\\u201d in Acoustics, Speech and Signal Processing (ICASSP),\\n2011 IEEE International Conference on. IEEE, 2011, pp. 5688\\u20135691.\\n[17] B. Schuller, S. Steidl, and A. Batliner, \\u201cThe interspeech 2009 emotion\\n\\nchallenge,\\u201d in Proc. Interspeech, 2009, pp. 312\\u2013315.\\n\\n[18] C.M. Lee and S.S. Narayanan, \\u201cToward detecting emotions in spoken\\ndialogs,\\u201d Speech and Audio Processing, IEEE Transactions on, vol. 13,\\nno. 2, pp. 293\\u2013303, 2005.\\n\\n[19] T. Polzehl, S. Sundaram, H. Ketabdar, M. Wagner, and F. Metze, \\u201cEmo-\\ntion classi\\ufb01cation in childrens speech using fusion of acoustic and lin-\\nguistic features,\\u201d Proceedings of INTERSPEECH-2009, Brighton, UK,\\npp. 340\\u2013343, 2009.\\n\\n[20] E. Mower, MJ. Mataric, and SS. Narayanan, \\u201cA framework for au-\\ntomatic human emotion classi\\ufb01cation using emotion pro\\ufb01les,\\u201d Audio,\\nSpeech, and Language Processing, IEEE Transactions on, vol. 19, no.\\n5, pp. 1057\\u20131070, 2011.\\n\\n[21] Y. Lu, I. Cohen, XS. Zhou, and Q. Tian, \\u201cFeature selection using prin-\\ncipal feature analysis,\\u201d in Proceedings of the 15th international confer-\\nence on Multimedia. ACM, 2007, pp. 301\\u2013304.\\n\\n[22] S. Steidl, M. Levit, A. Batliner, E. N\\xa8oth, and H. Niemann, \\u201cOf all\\nthings the measure is man: Automatic classi\\ufb01cation of emotions and\\ninter-labeler consistency,\\u201d in Proc. ICASSP, 2005, vol. 1, pp. 317\\u2013320.\\n[23] A. Metallinou, C. Busso, S. Lee, and S. Narayanan, \\u201cVisual emotion\\nrecognition using compact facial representations and viseme informa-\\ntion,\\u201d in Acoustics Speech and Signal Processing (ICASSP), 2010 IEEE\\nInternational Conference on. IEEE, 2010, pp. 2474\\u20132477.\\n\\n[24] M. W\\xa8ollmer, A. Metallinou, F. Eyben, B. Schuller, and S. Narayanan,\\n\\u201cContext-sensitive multimodal emotion recognition from speech and\\nfacial expression using bidirectional lstm modeling,\\u201d in Proc. of IN-\\nTERSPEECH Conference, 2010, pp. 2362\\u20132365.\\n\\n[25] C. Busso and S.S. Narayanan, \\u201cInterrelation between speech and fa-\\ncial gestures in emotional utterances: a single subject study,\\u201d Audio,\\nSpeech, and Language Processing, IEEE Transactions on, vol. 15, no.\\n8, pp. 2331\\u20132347, 2007.\\n\\n[26] H. Lee, C. Ekanadham, and A. Ng, \\u201cSparse deep belief net model for\\nvisual area v2,\\u201d Advances in neural information processing systems,\\nvol. 20, pp. 873\\u2013880, 2008.\\n\\n[27] Y. Tang and C. Eliasmith, \\u201cDeep networks for robust visual recog-\\nnition,\\u201d in International Conference on Machine Learning. Citeseer,\\n2010, vol. 28.\\n\\n[28] H. Lee, R. Grosse, R. Ranganath, and A. Y. Ng, \\u201cUnsupervised learn-\\ning of hierarchical representations with convolutional deep belief net-\\nworks,\\u201d Communications of the ACM, vol. 54, no. 10, pp. 95\\u2013103,\\n2011.\\n\\n[29] K. Sohn, D.Y. Jung, H. Lee, and A.O. Hero,\\n\\n\\u201cEf\\ufb01cient learning\\nof sparse, distributed, convolutional feature representations for object\\nrecognition,\\u201d in Computer Vision (ICCV), 2011 IEEE International\\nConference on. IEEE, 2011, pp. 2643\\u20132650.\\n\\n[30] A. Krizhevsky, I. Sutskever, and G.E. Hinton, \\u201cImagenet classi\\ufb01ca-\\ntion with deep convolutional neural networks,\\u201d in Advances in Neural\\nInformation Processing Systems 25, 2012, pp. 1106\\u20131114.\\n\\n[31] P. Smolensky, \\u201cInformation processing in dynamical systems: Founda-\\ntions of harmony theory,\\u201d Parallel distributed processing: Explorations\\nin the microstructure of cognition, vol. 1, pp. 194\\u2013281, 1986.\\n\\n[32] G.E. Hinton, S. Osindero, and Y.-W. Teh, \\u201cA fast learning algorithm for\\ndeep belief nets,\\u201d Neural Computation, vol. 18, no. 7, pp. 1527\\u20131554,\\n2006.\\n\\n[33] G.E. Hinton, \\u201cTraining products of experts by minimizing contrastive\\ndivergence,\\u201d Neural computation, vol. 14, no. 8, pp. 1771\\u20131800, 2002.\\n[34] C. Busso, M. Bulut, C.C. Lee, A. Kazemzadeh, E. Mower, S. Kim, J.N.\\nChang, S. Lee, and S.S. Narayanan, \\u201cIemocap: Interactive emotional\\ndyadic motion capture database,\\u201d Language resources and evaluation,\\nvol. 42, no. 4, pp. 335\\u2013359, 2008.\\n\\n[35] C. Busso, S. Lee, and S. Narayanan, \\u201cUsing neutral speech models\\nfor emotional speech analysis,\\u201d Proceedings of Interspeech 2007, pp.\\n2225\\u20132228, 2007.\\n\\n[36] W. Duch, J. Biesiada, T. Winiarski, K. Grudzinski, and K. Grabczewski,\\n\\u201cFeature selection based on information theory \\ufb01lters,\\u201d in Neural Net-\\nworks and Soft Computing: Proceedings of the Sixth International Con-\\nference on Neural Networks and Soft Computing, Zakopane, Poland,\\nJune 11-15, 2002. Physica Verlag, 2003, vol. 1, p. 173.\\n\\n[37] A. Metallinou, S. Lee, and S. Narayanan, \\u201cDecision level combination\\nof multiple modalities for recognition and analysis of emotional ex-\\npression,\\u201d in Acoustics Speech and Signal Processing (ICASSP), 2010\\nIEEE International Conference on. IEEE, 2010, pp. 2462\\u20132465.\\n\\n\\x0c', u'Improving Neural Networks with Dropout\\n\\nby\\n\\nNitish Srivastava\\n\\nA thesis submitted in conformity with the requirements\\n\\nfor the degree of Master of Science\\n\\nGraduate Department of Computer Science\\n\\nUniversity of Toronto\\n\\nc(cid:13) Copyright 2013 by Nitish Srivastava\\n\\n\\x0cAbstract\\n\\nImproving Neural Networks with Dropout\\n\\nNitish Srivastava\\n\\nMaster of Science\\n\\nGraduate Department of Computer Science\\n\\nUniversity of Toronto\\n\\n2013\\n\\nDeep neural nets with a huge number of parameters are very powerful machine learning systems. How-\\n\\never, over\\ufb01tting is a serious problem in such networks. Large networks are also slow to use, making it\\n\\ndi\\ufb03cult to deal with over\\ufb01tting by combining many di\\ufb00erent large neural nets at test time. Dropout\\n\\nis a technique for addressing this problem. The key idea is to randomly drop units (along with their\\n\\nconnections) from a neural network during training. This prevents the units from co-adapting too much.\\n\\nDropping units creates thinned networks during training. The number of possible thinned networks is\\n\\nexponential in the number of units in the network. At test time all possible thinned networks are com-\\n\\nbined using an approximate model averaging procedure. Dropout training followed by this approximate\\n\\nmodel combination signi\\ufb01cantly reduces over\\ufb01tting and gives major improvements over other regulariza-\\n\\ntion methods. In this work, we describe models that improve the performance of neural networks using\\n\\ndropout, often obtaining state-of-the-art results on benchmark datasets.\\n\\nii\\n\\n\\x0cContents\\n\\n1 Introduction\\n\\n2 Dropout with feed forward neural nets\\n\\n2.1 Model Description . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n2.2 Learning dropout nets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n\\n2.3 Pretraining dropout nets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n\\n2.4 Classi\\ufb01cation Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n\\n2.5 Comparison with Bayesian methods.\\n\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n\\n2.6 Comparison with standard regularizers.\\n\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n\\n1\\n\\n3\\n\\n3\\n4\\n\\n4\\n\\n4\\n\\n9\\n\\n9\\n\\n2.7 E\\ufb00ect on features.\\n\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\\n\\n2.8 E\\ufb00ect on sparsity. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\\n\\n2.9 E\\ufb00ect of dropout rate.\\n\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\\n\\n2.10 E\\ufb00ect of data set size.\\n\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\\n\\n2.11 Monte-Carlo model averaging vs. weight scaling.\\n\\n. . . . . . . . . . . . . . . . . . . . . . . 13\\n\\n3 Dropout with Boltzmann Machines\\n\\n15\\n\\n3.1 Dropout RBMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\\n\\n3.2 Learning Dropout RBMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\\n\\n3.3 E\\ufb00ect on features . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\\n\\n3.4 E\\ufb00ect on sparsity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\\n\\n4 Marginalizing dropout\\n\\n18\\n\\n4.1 Linear Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\\n\\n4.2 Logistic regression and deep networks\\n\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\\n\\n5 Conclusions\\n\\nBibliography\\n\\n20\\n\\n21\\n\\niii\\n\\n\\x0cChapter 1\\n\\nIntroduction\\n\\nNeural networks are powerful computational models that are being used extensively for solving problems\\n\\nin vision, speech, natural language processing and many other areas.\\n\\nIn spite of many successes, neural networks still su\\ufb00er from a major weakness. The presence of non-\\n\\nlinear hidden layers makes deep networks very expressive models which are therefore prone to severe\\n\\nover\\ufb01tting. A typical neural net training procedure involves early stopping to prevent this. Several\\n\\nregularization schemes have also been proposed to prevent over\\ufb01tting. These methods combined with\\n\\nlarge datasets have made it possible to apply neural networks for solving machine learning problems\\n\\nin several domains. However, over\\ufb01tting still remains a major challenge to overcome when it comes to\\n\\ntraining extremely large neural networks or working in domains which o\\ufb00er very small amounts of data.\\n\\nModel combination typically improves the performance of machine learning models. Averaging the\\n\\npredictions of several models is most helpful when the individual models are di\\ufb00erent from each other\\n\\nand each model is fast to train and use at test time. However, large neural networks are hard to train and\\n\\nslow to use at test time. In order to make them di\\ufb00erent they must either have di\\ufb00erent hyperparameters\\n\\nor be trained on di\\ufb00erent data. This often makes it impractical to train many large networks and average\\n\\nall their predictions at test time.\\n\\n\\u201cDropout\\u201d is a technique that aims to address both these concerns. The term \\u201cdropout\\u201d refers to\\n\\ndropping out units (hidden and visible) in a neural network. By dropping a unit out, we mean removing\\n\\nit from the network, along with all its incoming and outgoing edges. The choice of which units to drop\\n\\nis random. In the simplest case, each unit is retained with a \\ufb01xed probability p, where p can be chosen\\n\\nbased on the particular problem by a validation set (a typical value is p = 0.5). Dropping out is done\\n\\nindependently for each hidden unit and for each training case. Thus, applying dropout to a neural\\n\\nnetwork amounts to sub-sampling a \\u201cthinned\\u201d neural network from it. A neural net with n units, can\\nbe seen as a collection of 2n possible thinned neural networks. These networks all share weights so that\\nthe total number of parameters is still O(n2), or less.\\n\\nFor large n, each time a training case is presented, it is likely to use a new thinned network. So\\ntraining a neural network with dropout can be seen as training a collection of 2n thinned networks with\\nmassive weight sharing, where each thinned network gets trained very rarely, if at all.\\n\\nWhen the model is being used at test time, it is not feasible to explicitly average the predictions from\\n\\nexponentially many thinned models. However, a very simple approximate averaging method works well.\\n\\nThe idea is to use a single neural net at test time without dropout. The weights of this test network\\n\\n1\\n\\n\\x0cChapter 1.\\n\\nIntroduction\\n\\n2\\n\\nare scaled versions of the weights of the thinned networks used during training. The weights are scaled\\n\\nsuch that for any given input to a hidden unit the expected output (under the distribution used to drop\\n\\nunits at training time) is the same as the output at test time. So, if a unit is retained with probability\\n\\np, this amounts to multiplying the outgoing weights of that unit by p. With this approximate averaging\\nmethod, 2n networks with shared weights can be combined into a single neural network to be used at\\ntest time. Training a network with dropout and using the approximate averaging method at test time\\n\\nleads to signi\\ufb01cantly lower generalization error on a wide variety of classi\\ufb01cation problems.\\n\\nDropout can also be interpreted as a way of regularizing a neural network by adding noise to its\\n\\nhidden units. This idea has previously been used in the context of Denoising Autoencoders [26, 27]\\n\\nwhere noise is added to the inputs of an autoencoder and the target is kept noise-free. Our work extends\\n\\nthis idea by dropping units in the hidden layers too and performing appropriate weight scaling at test\\n\\ntime. Compared to the 5% noise that typically works best for DAEs, it is usually optimal to drop out\\n\\n20% of input units and 50% of the hidden units to obtain the most bene\\ufb01t from dropout.\\n\\nA motivation for this method comes from a theory of role of sex in evolution [10]. Sexual reproduction\\n\\ninvolves taking half the genes of one parent and half of the other and combining them to produce an\\n\\no\\ufb00spring. The asexual alternative involves creating an o\\ufb00spring with a copy of the parent\\u2019s genes. It\\n\\nseems plausible that asexual reproduction is a better optimizer of \\ufb01tness which is widely believed to\\n\\nbe the criterion for natural selection, i.e., successful organisms would be able to create more copies\\n\\nof successful organisms. Sexual reproduction seems to be downgrading the genes by pairing up two\\n\\nrandomly chosen halves. However, sexual reproduction is the way most advanced organisms evolved.\\n\\nOne explanation is that the criteria for natural selection may not be individual \\ufb01tness but rather mix-\\n\\nability of genes. The ability of genes to be able to work well with another random set of genes makes\\n\\nthem more robust. Since a gene cannot rely on an exact partner to be present at all times, it must\\n\\nlearn to do something useful on its own without relying on a partner to make up for its shortcomings.\\n\\nSimilarly, hidden units in a neural network trained with dropout must learn to work with a randomly\\n\\nchosen sample of other units. This makes each hidden unit more robust and drives it towards creating\\n\\nuseful features on its own without relying on other hidden units to correct its mistakes. Preventing\\n\\nco-adaptation in this manner improves neural networks.\\n\\nThe idea of dropout is not limited to feed forward neural nets. It can be more generally applied\\n\\nto graphical models such as Boltzmann Machines. The chapters that follow explore di\\ufb00erent aspects of\\n\\ndropout in detail, apply it to di\\ufb00erent problems and compare it with other forms of regularization and\\n\\nmodel combination.\\n\\n\\x0cChapter 2\\n\\nDropout with feed forward neural\\n\\nnets\\n\\nThis chapter describes training and testing methods to be used when dropout is applied to feed forward\\n\\nneural nets.\\n\\n2.1 Model Description\\n\\nThis section describes the dropout neural network model. Consider a neural network with L hidden\\nlayers. Let l \\u2208 {1, . . . , L} index the hidden layers of the network. Let z(l) denote the vector of inputs\\ninto layer l, y(l) denote the vector of outputs from layer l (y(0) = x is the input). W (l) and b(l) are the\\nweights and biases at layer l. The feed forward operation of a neural network can be described as (for\\nl \\u2208 {0, . . . , L \\u2212 1})-\\n\\nz(l+1) = W (l+1)yl + b(l+1)\\n\\ny(l+1) = f (z(l+1))\\n\\nwhere f is any activation function. With dropout, the feed forward operation becomes -\\n\\nr(l)\\ni\\n\\n\\u223c Bernoulli(p)\\n\\n(cid:101)y(l) = r(l) \\u2217 y(l)\\nz(l+1) = W (l+1)(cid:101)yl + b(l+1)\\n\\ny(l+1) = f (z(l+1))\\n\\n(2.1)\\n\\n(2.2)\\n\\n(2.3)\\n\\n(2.4)\\n\\n(2.5)\\n\\n(2.6)\\n\\nHere r(l) is a vector of Bernoulli random variables each of which has probability p of being 1. This\\nvector is sampled for each layer and multiplied element-wise with the outputs of that layer, y(l), to create\\n\\nthe thinned outputs (cid:101)y(l). The thinned outputs are then used as input to the next layer. For learning,\\n\\nthe derivatives of the loss function are backpropagated through the thinned network.\\n\\nAt test time, the weights are scaled as W (l)\\n\\ntest = pW (l). The resulting neural network is run without\\n\\ndropout.\\n\\n3\\n\\n\\x0cChapter 2. Dropout with feed forward neural nets\\n\\n4\\n\\n2.2 Learning dropout nets\\n\\nDropout neural networks can be trained with stochastic gradient descent. Dropout is done separately\\n\\nfor each training case in every minibatch. Dropout can be used with any activation function and our\\n\\nexperiments with logistic, tanh and recti\\ufb01ed linear units yielded similar results though requiring di\\ufb00erent\\n\\namounts of training time (recti\\ufb01ed linear units were fastest to train). Several methods that have been\\n\\nused to improve stochastic gradient descent with standard neural networks such as momentum, decaying\\n\\nlearning rates and L2 weight decay are useful for dropout neural networks as well.\\n\\nOne particular form of regularization was found to be especially useful for dropout - constraining\\n\\nthe norm of the incoming weight vector at each hidden unit to be upper bounded by a \\ufb01xed constant c.\\nIn other words, if wi represents the vector of weights incident on hidden unit i, the neural network was\\noptimized under the constraint ||wi||2 \\u2264 c. This constraint was imposed during optimization by scaling\\nwi to lie on a ball of radius c, if it ever violated the constraint. This is kind of regularization is also called\\nmax-norm regularization and has been previously used in the context of collaborative \\ufb01ltering [20].\\n\\nThe constant c is a tuneable hyperparameter, which can be determined using a validation set. Al-\\n\\nthough dropout alone gives signi\\ufb01cant improvements, optimizing under this constraint, coupled with a\\n\\nlarge decaying learning rates and high momentum provides a signi\\ufb01cant boost over just using dropout.\\n\\nOne explanation of this fact is that constraining the weight vector to lie inside a ball of \\ufb01xed radius makes\\n\\nit possible to use a huge learning rate without the possibility of weights blowing up. The noise provided\\n\\nby dropout then allows the optimization process to explore di\\ufb00erent regions of the weight space that it\\n\\nwould have otherwise not encountered. As the learning rate decays, the optimization takes shorter steps\\n\\nand gradually trades o\\ufb00 exploration with exploitation and \\ufb01nally settles into a minimum.\\n\\n2.3 Pretraining dropout nets\\n\\nNeural networks can be pretrained using stacks of RBMs [6], autoencoders [27] or Deep Boltzmann\\n\\nMachines [17]. This pretraining followed by \\ufb01netuning with backpropagation has been shown to give\\n\\nsigni\\ufb01cant performance boosts over \\ufb01netuning from random initializations in certain cases. Pretraining\\n\\nis also an e\\ufb00ective way of making use of unlabeled data.\\n\\nDropout nets can also be pretrained using these techniques. The procedure is identical to standard\\n\\npretraining [6] except with a small modi\\ufb01cation - the weights obtained from pretraining should be scaled\\n\\nup by a factor of 1/p. The reason is similar to that for scaling down the weights by a factor of p\\n\\nwhen testing (maintaining the same expected output at each unit). Compared to learning from random\\n\\ninitializations, \\ufb01netuning from pretrained weights typically requires a smaller learning rate so that the\\n\\ninformation in the pretrained weights is not entirely lost.\\n\\n2.4 Classi\\ufb01cation Results\\n\\nThe above training and test procedure was applied to several datasets. The best results were consistently\\n\\nobtained when dropout was used. These datasets range over a variety of domains and tasks -\\n\\n\\u2022 MNIST is a standard toy dataset of handwritten digits.\\n\\u2022 TIMIT is a standard speech benchmark for clean speech recognition.\\n\\n\\x0cChapter 2. Dropout with feed forward neural nets\\n\\n5\\n\\n\\u2022 SVHN consists of images of house numbers collected by Google Street View.\\n\\u2022 Reuters-RCV1 is a collection of Reuters newswire articles.\\n\\u2022 Flickr-1M consists of multimodal data (1 million images and tags).\\n\\u2022 Alternate Splicing dataset consists of biochemistry data for genes.\\n\\nFor completeness, we also report results obtained by other researchers who have used dropout. These\\n\\ninclude CIFAR-10 [7] and ImageNet-1K [9]. This section describes the results along with the neural net\\n\\narchitectures used to obtain these results. All datasets are publicly available and all except TIMIT and\\nWSJ are free. The code for reproducing these results can be obtained from http://www.cs.toronto.\\nedu/~nitish/dropout. The implementation is GPU-based and uses cudamat [11]. Convolutional neural\\nnet implementation is based on kernels from cuda-convnet used for obtaining results in [9].\\n\\n2.4.1 Results on MNIST\\n\\nMethod\\n2 layer NN [19]\\nSVM gaussian kernel\\nDropout\\nDropout + weight\\nnorm constraint\\nDBN + \\ufb01netuning\\nDBN + dropout \\ufb01ne-\\ntuning\\nDBM + \\ufb01netuning\\nDBM + dropout\\n\\ufb01netuning\\n\\nUnit Type Error %\\n\\nLogistic\\n\\n-\\n\\nReLU\\nReLU\\n\\nLogistic\\nLogistic\\n\\nLogistic\\nLogistic\\n\\n1.60\\n1.4\\n1.25\\n1.05\\n\\n1.18\\n0.92\\n\\n0.96\\n0.79\\n\\nFigure 2.1: Comparison of training methods\\n\\n.\\n\\nFigure 2.2: Test error for di\\ufb00erent architectures\\n\\n.\\n\\nMNIST is a collection of 28\\xd728 pixel handwritten digit images. There are 60,000 training and 10,000\\ntest images. A validation set consisting of 10,000 images was held out from the training set. No input\\n\\npreprocessing was done. No spatial information or input distortions was used.\\n\\nClassi\\ufb01cation experiments were done with networks of many di\\ufb00erent architectures. Fig. 2.2 shows\\n\\nthe test error curves obtained for some of these. All of these used recti\\ufb01ed linear units.\\n\\nFig 2.1 compares the test classi\\ufb01cation results obtained by several di\\ufb00erent methods and their exten-\\n\\nsions using dropout. The pretrained dropout nets use logistic units and all other networks use recti\\ufb01ed\\n\\nlinear units. The best performance without unsupervised pretraining for the permutation invariant set-\\n\\nting using neural nets is 1.60% [19]. Adding dropout reduced the error to 1.25% and adding weight norm\\n\\nconstraints further reduced that to 1.05%. Pretrained dropout nets also improved the performance for\\n\\nDeep Belief Nets and Deep Boltzmann Machines. DBM pretrained dropout nets achieve a test error of\\n\\n0.79% which is state-of-the-art for the permutation invariant setting.\\n\\n02000004000006000008000001000000Number of weight updates0.81.01.21.41.61.82.02.2Classification Error %Dropout 1000 units 2 layersDropout 1000 units 3 layersDropout 1000 units 4 layersDropout 2000 units 2 layersDropout 2000 units 3 layersDropout 2000 units 4 layers\\x0cChapter 2. Dropout with feed forward neural nets\\n\\n6\\n\\n2.4.2 Results on SVHN\\n\\nThe Street View House Numbers (SVHN) Dataset [14] consists of real-world images of house numbers\\n\\nobtained from Google Street View. The part of the dataset that we use in our experiments consists\\nof 32 \\xd7 32 pixel color images centered on a digit in a house number. Fig. 2.3 shows some examples of\\nimages from this dataset. The task is to identify the digit in the center of the image.\\n\\nFigure 2.3: Samples of images from the Street View House Numbers (SVHN) dataset.\\n\\nFor this dataset, dropout was applied in convolutional neural networks. The network consists of three\\n\\nconvolutional layers each followed by a max-pooling layer. The convolutional layers have 64, 64 and 128\\n\\ufb01lters respectively. Each convolutional layer has a 5 \\xd7 5 receptive \\ufb01eld applied with a stride of 1 pixel.\\nThe max pooling layers pool a 3 \\xd7 3 region and are applied at strides of 2 pixels. The convolutional\\nlayers are followed by two fully connected hidden layers having 3072 and 2048 units respectively. All\\n\\nunits use the recti\\ufb01ed linear activation function. Dropout was applied to all the layers of the network\\n\\nwith the probability of retaining the unit being p = (0.9, 0.9, 0.9, 0.5, 0.5, 0.5) for the di\\ufb00erent layers of\\n\\nthe network (going from input to convolutional layers to fully connected layers). These hyperparameters\\n\\nwere tuned using a validation set. In addition, the weight norm constraint was used for hidden units\\n\\nin the fully-connected layers. Besides the test set, the SVHN dataset consists of a standard labelled\\n\\ntraining set and another set of labelled examples that are easy. The validation set was constructed by\\n\\ntaking examples from both the sets. Two-thirds of it were taken from the standard set (400 per class)\\n\\nand one-third from the extra set (200 per class), a total of 6000 samples. This same process is used\\n\\nin [18]. The inputs were RGB pixels normalized to have zero mean and unit variance.\\n\\nTable. 2.1 compares the results obtained by using dropout with other methods. Dropout leads to a\\n\\nmore than 35% relative improvement over the best previously published results. It bridges the distance\\n\\nto human-level performance by more than half. The additional gain in performance obtained by adding\\n\\ndropout in the convolutional layers besides doing dropout in the fully connected layers suggests that the\\n\\nutility of dropout is not limited to densely connected neural networks but can be more generally applied\\n\\nto other specialized architectures.\\n\\n\\x0cChapter 2. Dropout with feed forward neural nets\\n\\n7\\n\\nTable 2.1: Results on the Street View House Numbers dataset.\\n\\nMethod\\nBinary Features (WDCH) [14]\\nHOG [14]\\nStacked Sparse Autoencoders [14]\\nKMeans [14]\\nMulti-stage Conv Net with average pooling [18]\\nMulti-stage Conv Net + L2 pooling [18]\\nMulti-stage Conv Net + L4 pooling + padding [18]\\nConv Net + max-pooling\\nConv Net + max pooling + dropout in fully connected layers\\nConv Net + max pooling + dropout in all layers\\nConv Net + max pooling + dropout in all layers + input translations\\nHuman Performance\\n\\nError %\\n\\n36.7\\n15.0\\n10.3\\n9.4\\n9.06\\n5.36\\n4.90\\n3.95\\n3.02\\n2.78\\n2.68\\n2.0\\n\\n2.4.3 Results on TIMIT\\n\\nTIMIT is a speech dataset with recordings from 680 speakers covering 8 major dialects of American\\n\\nEnglish reading ten phonetically-rich sentences in a controlled noise-free environment. It has been used\\n\\nto benchmark many speech recognition systems. Table. 2.2 compares dropout neural nets against some\\n\\nof them. The open source Kaldi toolkit [16] was used to preprocess the data into log-\\ufb01lter banks and to\\n\\nget labels for speech frames. Dropout neural networks were trained on windows of 21 frames to predict\\n\\nthe label of the central frame. No speaker dependent operations were performed. A 6-layer dropout net\\n\\ngives a phone error rate of 23.4%. This is already a very good performance on this dataset. Dropped\\n\\nfurther improves it to 21.8%. Similarly, a 4-layer pretrained dropout net improves the phone error rate\\n\\nfrom 22.7% to 19.7%.\\n\\nTable 2.2: Phone error rate on the TIMIT core test set.\\n\\nMethod\\nNeural Net (6 layers) [12]\\nDropout Neural Net (6 layers)\\nDBN-pretrained Neural Net (4 layers)\\nDBN-pretrained Neural Net (6 layers) [12]\\nDBN-pretrained Neural Net (8 layers) [12]\\nmcRBM-DBN-pretrained Neural Net (5 layers) [2]\\nDBN-pretrained Neural Net (4 layers) + dropout\\nDBN-pretrained Neural Net (8 layers) + dropout\\n\\nPhone Error Rate%\\n\\n23.4\\n21.8\\n22.7\\n22.4\\n20.7\\n20.5\\n19.7\\n19.7\\n\\n2.4.4 Results on Reuters-RCV1\\n\\nReuters-RCV1 is a collection of newswire articles from Reuters. We created a subset of this dataset\\n\\nconsisting of 402,738 articles and a vocabulary of 2000 most commonly used words after removing stop\\n\\nwords. The subset was created so that the articles belong to 50 disjoint categories. The task is to identify\\n\\nthe category that a document belongs to. The data was split into equal sized training and test sets.\\n\\nA neural net with 2 hidden layers of 2000 units each obtained an error rate of 31.05%. Adding\\n\\ndropout reduced the error marginally to 29.62%.\\n\\n\\x0cChapter 2. Dropout with feed forward neural nets\\n\\n8\\n\\n2.4.5 Results on Flickr-1M\\n\\nOften real-world data consists of multiple modalities - photographs on the web (images and text), videos\\n\\n(images and sound), sensory perception (images, sound, touch, internal feedbacks). Multimodal data\\n\\nraises interesting machine learning problems such as fusing multiple modalities into a joint representa-\\n\\ntion and inferring missing modalities conditioned on observed ones. Recent e\\ufb00orts have been made in\\n\\ncomputer vision [4] and deep learning [15, 22, 21].\\n\\nThe Flickr-1M dataset [8] consists of 1 million pairs of images and tags (text attributed to the\\n\\nimages by users) obtained from the social photography website Flickr. 25,000 pairs are labelled into\\n\\n38 overlapping topics. The other 975,000 image-text pairs are unlabeled. The task is to identify the\\n\\ntopics to which the labelled pairs belongs. Applying dropout to this dataset seeks to demonstrate two\\n\\nideas. Firstly, the use of unlabeled data to pretrain dropout neural networks and secondly, to show the\\n\\napplicability of dropout to the much less studied domain of multimodal data.\\n\\nTable 2.3: Results on the Flickr-1M dataset.\\n\\nMethod\\nLDA [8]\\nSVM [8]\\nDBN [22]\\nAutoencoder (based on [15])\\nDBM [22]\\nMultiple Kernel Learning SVMs [4]\\nDBN with dropout \\ufb01netuning\\nDBM with dropout \\ufb01netuning\\n\\nMean Average Precision % Precision at 50\\n\\n0.492\\n0.475\\n0.599\\n0.600\\n0.609\\n0.623\\n0.628\\n0.632\\n\\n0.754\\n0.758\\n0.867\\n0.875\\n0.873\\n\\n-\\n\\n0.891\\n0.895\\n\\nTable. 2.3 compares the pretrained dropout neural networks with other models. The evaluation\\n\\nmetrics are Mean Average Precision and Precision at 50. Mean Average Precision is the mean over all\\n\\n38 topics of the recall-weighted precision for each topic. Precision at 50 is the mean over all 38 topics\\n\\nof the precision at a recall of 50 data points. The labelled set was split as 10K-5K-10K for training,\\n\\nvalidation and testing respectively. The unlabeled data was used for training DBN and DBM models as\\n\\ndescribed in [22]. The discriminative model pretrained by a DBN has more than 10 million parameters.\\n\\nThe DBM model, after being unrolled as described in [17] has around 16 million parameters. However,\\n\\nthe training set is only 10,000 in size. This makes it hard to discriminatively \\ufb01netune the models without\\n\\ncausing over\\ufb01tting. However, when dropout is applied, over\\ufb01tting is drastically reduced. Dropout with\\n\\npretrained models achieves state-of-the-art results, outperforming the best previously published results\\n\\non this dataset that were obtained with an Multiple Kernel Learning based SVM model [4]. It is also\\n\\ninteresting to note that the MKL model used over 30,000 standard computer vision features while our\\n\\nmodel used 3857 features only.\\n\\n2.4.6 Results on ImageNet\\n\\nImageNet-1K is a collection of over 1 million images categorized into 1000 labels. The system that\\n\\nwas used to obtain state-of-the-art results on this dataset in the ILSVRC-2012 competition [9] used\\n\\nconvolutional neural networks trained with dropout. The model achieved a top-5 error rate of 15.3%\\n\\nand won the competition by a massive margin (The second best entry stood at 26.2%).\\n\\n\\x0cChapter 2. Dropout with feed forward neural nets\\n\\n9\\n\\n2.5 Comparison with Bayesian methods.\\n\\nDropout can be seen as a way of doing an approximate equally-weighted averaging of exponentially\\n\\nmany models. On the other hand, Bayesian neural networks [13] are the proper way of doing model\\n\\naveraging over a continuum of neural network models with appropriate weights. Unfortunately, Bayesian\\n\\nneural nets are slow to train and di\\ufb03cult to scale to very large neural nets. It is also expensive to get\\n\\npredictions from many large nets at test time. On the other hand, dropout neural nets are much faster\\n\\nto train and use at test time. However, Bayesian neural nets are extremely useful for solving problems in\\n\\ndomains where data is scarce such as medical diagnosis, genetics, drug discovery and other bio-chemical\\n\\napplications. In this section we report experiments that compare Bayesian neural nets with dropout\\n\\nneural nets for small datasets where Bayesian neural networks are known to perform well and obtain\\n\\nstate-of-the-art results. These datasets are mostly characterized by having a large number of dimensions\\n\\nrelative to the number of examples.\\n\\n2.5.1 Predicting tissue-regulated alternative splicing\\n\\nAlternative splicing is a signi\\ufb01cant cause of cellular diversity in mammalian tissues. Predicting the oc-\\n\\ncurrence of alternate splicing in certain tissues under di\\ufb00erent conditions is important for understanding\\n\\nmany human diseases. The alternative splicing dataset consists of data for 3665 cassette exons, 1014\\n\\nRNA features and 4 tissue types derived from 27 mouse tissues. Given the RNA features, the task is\\n\\nto predict the probability of three splicing related events that biologists care about. See [29] for a full\\n\\nexposition. The evaluation metric is Code Quality which is a measure of the negative KL divergence\\n\\nbetween the target and predicted probability distributions (Higher is better).\\n\\nTable 2.4: Results on the Alternative Splicing Dataset.\\n\\nMethod\\nNeural Network (early stopping) [29]\\nRegression, PCA [29]\\nSVM, PCA [29]\\nNeural Network (dropout)\\nBayesian Neural Network [29]\\n\\nCode Quality (bits)\\n\\n440\\n463\\n487\\n567\\n623\\n\\nA two layer network with 1024 units in each layer was trained on this dataset. A value of p = 0.5\\n\\nwas used for the hidden layer and p = 0.7 for the input layer. Results were averaged across the same\\n\\n5 folds used in [29]. Table. 2.4 compares dropout neural nets with other models trained on this data.\\n\\nThis experiment suggests that dropout improves the performance of neural networks signi\\ufb01cantly but not\\n\\nenough to match the performance of Bayesian neural networks. The dropout neural networks outperform\\n\\nSVMs and standard neural nets trained with early stopping. It is interesting to note that the dropout\\n\\nnets are very large (1000s of hidden units) compared to a few tens of units in the Bayesian network.\\n\\n2.6 Comparison with standard regularizers.\\n\\nSeveral regularization methods have been proposed for preventing over\\ufb01tting in neural networks. These\\n\\ninclude L2 weight decay (more generally Tikhonov regularization [24]), lasso [23] and KL-sparsity reg-\\n\\nularization which minimizes the KL-divergence between the distribution of hidden unit activations and\\n\\n\\x0cChapter 2. Dropout with feed forward neural nets\\n\\n10\\n\\na target Bernoulli distribution. Another regularization involves putting an upper bound on the norm\\n\\nof the incoming weight vector at each hidden unit. Dropout can be seen as another way of regularizing\\n\\nneural networks. In this section we compare dropout with some of these regularization methods.\\n\\nThe MNIST dataset is used to compare these regularizers. The same network architecture (784-\\n\\n1024-1024-2048-10) was used for all the methods. Table. 2.5 shows the results. The KL-sparsity method\\n\\nused a target sparsity of 0.1 at each layer of the network. It is easy to see that dropout leads to less\\n\\ngeneralization error. An important observation is that weight norm regularization signi\\ufb01cantly improves\\n\\nthe results obtained by dropout alone.\\n\\nTable 2.5: Comparison of di\\ufb00erent regularization methods on MNIST\\n\\nMethod\\nL2\\nL1 (towards the end of training)\\nKL-sparsity\\nMax-norm\\nDropout\\nDropout + Max-norm\\n\\nMNIST Classi\\ufb01cation error %\\n\\n1.62\\n1.60\\n1.55\\n1.35\\n1.25\\n1.05\\n\\n2.7 E\\ufb00ect on features.\\n\\nIn a standard neural network, each parameter individually tries to change so that it reduces the \\ufb01nal loss\\n\\nfunction, given what all other units are doing. This conditioning may lead to complex co-adaptations\\n\\nwhich cause over\\ufb01tting since these co-adaptations do not generalize. We hypothesize that for each hidden\\n\\nunit, dropout prevents co-adaptation by making the presence of other hidden units unreliable. Therefore,\\n\\nno hidden unit can rely on other units to correct its mistakes and must perform well in a wide variety\\n\\nof di\\ufb00erent contexts provided by the other hidden units. The experimental results discussed in previous\\n\\nsections lend credence to this hypothesis. To observe this e\\ufb00ect directly, we look at the features learned\\n\\nby neural networks trained on visual tasks with and without dropout.\\n\\nFig. 2.4a shows features learned by an autoencoder with a single hidden layer of 256 recti\\ufb01ed linear\\n\\nunits without dropout. Fig. 2.4b shows the features learned by an identical autoencoder which used\\n\\ndropout in the hidden layer with p = 0.5.\\n\\nIt is apparent that the features shown in Fig. 2.4a have\\n\\nco-adapted in order to produce good reconstructions. Each hidden unit on its own does not seem to be\\n\\ndetecting a meaningful feature. On the other hand, in Fig. 2.4b, the features seem to detect edges and\\n\\nspots in di\\ufb00erent parts of the image.\\n\\n2.8 E\\ufb00ect on sparsity.\\n\\nA curious side-e\\ufb00ect of doing dropout training is that the activations of the hidden units become sparse,\\n\\neven when no sparsity inducing regularizers are present. Thus, dropout leads to sparser representations.\\n\\nTo observe this e\\ufb00ect, we take the autoencoders trained in the previous section and look at the histogram\\n\\nof hidden unit activations on a random mini-batch taken from the test set. We also look at the histogram\\n\\nof mean hidden unit activations over the minibatch. Fig. 2.5a and Fig. 2.5b show the histograms for the\\n\\ntwo models. For the dropout autoencoder, we do not scale down the weights since that would obviously\\n\\n\\x0cChapter 2. Dropout with feed forward neural nets\\n\\n11\\n\\n(a) Without dropout\\n\\n(b) Dropout with p = 0.5.\\n\\nFigure 2.4: Features learned on MNIST with one hidden layer autoencoders having 256 recti\\ufb01ed linear\\nunits.\\n\\nincrease the sparsity by making the weights smaller. To ensure a fair comparison, the weights used to\\n\\nobtain the histogram were the same as the ones learned during training.\\n\\n(a) Without dropout\\n\\n(b) Dropout with p = 0.5.\\n\\nFigure 2.5: E\\ufb00ect of dropout on sparsity: In each panel, the \\ufb01gure on the left shows a histogram of the\\nmean activation of hidden units in a randomly chosen test minibatch. The \\ufb01gure on the right shows a\\nhistogram of the activations on the same minibatch.\\n\\nIn Fig. 2.5a, there are many more hidden units that are in a non-zero state compared to those in\\n\\nFig. 2.5b, as seen by the signi\\ufb01cant mass away from zero. The mean activation of hidden units is close\\n\\nto 2.0 for the autoencoder without dropout but drops to around 0.5 when dropout is used.\\n\\n\\x0cChapter 2. Dropout with feed forward neural nets\\n\\n12\\n\\n2.9 E\\ufb00ect of dropout rate.\\n\\nDropout has a tune-able hyperparameter p (the probability of retaining a hidden unit in the network).\\n\\nIn this section, the e\\ufb00ect of varying this hyperparameter is explored. The comparison is done in two\\n\\nsituations -\\n\\n1. The number of hidden units is held constant.\\n\\n2. The expected number of hidden units that will be retained is held constant.\\n\\nIn the \\ufb01rst case, all the nets have the same architecture at test time but they are trained with di\\ufb00erent\\n\\namounts of dropout. In our experiment we use a 784-2048-2048-2048-10 architecture. The inputs were\\n\\nnot thinned. Fig. 2.6a shows the test error obtained as a function of p. It can be observed that the\\nperformance is insensitive to the value of p if 0.4 \\u2264 p \\u2264 0.8, but rises sharply for small value of p. This\\nis to be expected because for the same number of hidden units, having a small p means very few units\\nwill turn on during training. It can be seen that this has lead to under\\ufb01tting since the training error is\\n\\nalso high.\\n\\nTherefore, a more fair comparison is the second case in which the quantity pn is held constant where\\n\\nn is the number of hidden units in any particular layer. This means that networks that have small p\\n\\nwill have larger number of hidden units. This ensures that the expected number of units that will be\\n\\npresent after dropout is same. However, the test networks will be of di\\ufb00erent sizes. In our experiments,\\n\\npn = 256 for the \\ufb01rst two hidden layers and pn = 512 for the last hidden layer. Fig. 2.6b shows the\\n\\ntest error obtained as a function of p. We notice that the magnitude of errors for small values of p has\\n\\nreduced compared to Fig. 2.6a. Values of p that are close to 0.6 seem to perform best for this choice of\\n\\npn but our usual default value of 0.5 is close to optimal.\\n\\n(a) Keeping n \\ufb01xed.\\n\\n(b) Keeping pn \\ufb01xed.\\n\\nFigure 2.6: E\\ufb00ect of changing dropout rates on MNIST.\\n\\n2.10 E\\ufb00ect of data set size.\\n\\nOne test of a good regularizer is that it should make it possible to train models with a large number of\\n\\nparameters even on small datasets. This section explores the e\\ufb00ect of changing the dataset size when\\n\\ndropout is used with feed forward networks. Huge neural networks trained in the standard way over\\ufb01t\\n\\n0.00.20.40.60.81.0Probability of retaining a unit (p)0.00.51.01.52.02.53.03.5Classification Error %Test ErrorTraining Error0.00.20.40.60.81.0Probability of retaining a unit (p)0.00.51.01.52.02.53.0Classification Error %Test ErrorTraining Error\\x0cChapter 2. Dropout with feed forward neural nets\\n\\n13\\n\\nmassively on small datasets. To see if dropout can help, we run classi\\ufb01cation experiments on MNIST\\n\\nand vary the amount of data given to the network.\\n\\nFigure 2.7: E\\ufb00ect of varying dataset size.\\n\\nThe results of these experiments are shown in Fig. 2.7. The network was given datasets of size 100,\\n\\n500, 1K, 5K, 10K and 50K randomly sampled without replacement from the MNIST training set. The\\n\\nsame network architecture (784-1024-1024-2048-10) was used for all datasets. Dropout with p = 0.5 was\\n\\nperformed at all the hidden layers and p = 0.8 at the input layer. It can be observed that for extremely\\nsmall datasets (100, 500) dropout does not give any improvements. The model has enough parameters\\n\\nthat it can over\\ufb01t on the training data, even with all the noise coming from dropout. As the size of\\n\\nthe dataset is increased, the gain from doing dropout increases up to a point and then declines. This\\n\\nsuggests that for any given architecture and dropout rate, there is a \\u201csweet spot\\u201d corresponding to some\\n\\namount of data that is large enough to not be memorized in spite of the noise but not so large that\\n\\nover\\ufb01tting is not a problem anyways.\\n\\n2.11 Monte-Carlo model averaging vs. weight scaling.\\n\\nThe test time procedure that was proposed is to do an approximate model combination by scaling down\\n\\nthe weights of the trained neural network. Another expensive but reasonable way of averaging the models\\nis to sample k neural nets using dropout for each test case and average their predictions. As k \\u2192 \\u221e, this\\nMonte-Carlo model average gets close to the true model average. Finite values of k are also expected\\n\\nto give reasonable results. It is interesting to compare the performance of this method with the weight\\n\\nscaling method that has been used till now.\\n\\nWe again use the MNIST dataset and do classi\\ufb01cation by averaging the predictions of k randomly\\n\\nsampled neural networks. Fig. 2.8 shows the test error rate obtained for di\\ufb00erent values of k. This is\\n\\ncompared with the error obtained using the weight scaling method (shown as a horizontal line). It can\\n\\nbe seen that around k = 50, the Monte-Carlo method becomes as good as the approximate method.\\n\\nThereafter, the Monte-Carlo method is slightly better than the approximate method but well within one\\n\\nstandard deviation of it. This suggests that the weight scaling method is a fairly good approximation\\n\\nof the true model average.\\n\\n102103104105Dataset size051015202530Classification Error %With dropoutWithout dropout\\x0cChapter 2. Dropout with feed forward neural nets\\n\\n14\\n\\nFigure 2.8: Monte-Carlo model averaging vs. weight scaling.\\n\\n020406080100120Number of samples used for Monte-Carlo averaging (k)1.001.051.101.151.201.251.301.35Test Classification error %Monte-Carlo Model AveragingApproximate averaging by weight scaling\\x0cChapter 3\\n\\nDropout with Boltzmann Machines\\n\\nThe core idea behind dropout is to sample smaller sub-models from a large model, train them and\\n\\nthen combine them at test time. This idea can be generalized beyond feed forward networks. In this\\n\\nchapter, we explore dropout when applied to Restricted Boltzmann Machines. For clarity of exposition,\\n\\nwe describe dropout for hidden units only. Extending dropout to visible units is straightforward.\\n\\n3.1 Dropout RBMs\\n\\nConsider an RBM with visible units v \\u2208 {0, 1}D and hidden units h \\u2208 {0, 1}F . It de\\ufb01nes the following\\nprobability distribution\\n\\nP (h, v; \\u03b8) =\\n\\n1\\nZ(\\u03b8)\\n\\nexp(v(cid:62)W h + a(cid:62)h + b(cid:62)v)\\n\\nWhere \\u03b8 = (W, a, b) represents the model parameters and Z is the partition function.\\nDropout RBMs are RBMs augmented with a vector of binary random variables r \\u2208 {0, 1}F . Each\\nrandom variable rj takes the value 1 with probability p, independent of others. If rj takes the value 1,\\nthe hidden unit hj is retained, otherwise it is dropped from the model. The joint distribution de\\ufb01ned\\nby a Dropout RBM can be expressed as-\\n\\nP (r, h, v; p, \\u03b8) = P (r; p)P (h, v|r; \\u03b8)\\nprj (1 \\u2212 p)1\\u2212rj\\n\\nP (r; p) =\\n\\nF(cid:89)\\n\\nP (h, v|r; \\u03b8) =\\n\\nj=1\\n\\n1\\n\\nZ(cid:48)(\\u03b8, r)\\n\\nexp(v(cid:62)W h + a(cid:62)h + b(cid:62)v)\\n\\ng(hj, rj) = 1(rj = 1) + 1(rj = 0)1(hj = 0)\\n\\n(3.1)\\n\\nF(cid:89)\\n\\nj=1\\n\\ng(hj, rj)\\n\\nZ(cid:48)(\\u03b8, r) is the normalization constant. g(hj, rj) imposes the constraint that if rj = 0, hj must be 0.\\n\\n15\\n\\n\\x0cChapter 3. Dropout with Boltzmann Machines\\n\\n16\\n\\nThe distribution over h, conditioned on v and r is factorial\\n\\nF(cid:89)\\n\\nP (h|r, v) =\\n\\nP (hj|rj, v)\\n\\nj=1\\n\\nP (hj = 1|rj, v) = 1(rj = 1)\\u03c3\\n\\nbj +\\n\\n(cid:32)\\n\\n(cid:33)\\n\\nWijvi\\n\\n(cid:88)\\n\\ni\\n\\nThe distribution over v conditioned on h is same as that of an RBM-\\n\\nP (v|h) =\\n\\nP (vi = 1|h) = \\u03c3\\n\\nP (vi|h)\\n\\nD(cid:89)\\n\\uf8eb\\uf8edai +\\n\\ni=1\\n\\n(cid:88)\\n\\n\\uf8f6\\uf8f8\\n\\nWijhj\\n\\nConditioned on r, the distribution over {v, h} is same as the distribution that an RBM would impose,\\nexcept that the units for which rj = 0 are dropped from h. Therefore, the Dropout RBM model can be\\nseen as a mixture of exponentially many RBMs with shared weights each using a di\\ufb00erent subset of h.\\n\\nj\\n\\n3.2 Learning Dropout RBMs\\n\\nLearning algorithms developed for RBMs such as Contrastive Divergence [5] can be directly applied for\\nlearning Dropout RBMs. The only di\\ufb00erence is that r is \\ufb01rst sampled and only the hidden units that\\n\\nare retained are used for training. Similar to dropout neural networks, a di\\ufb00erent r is sampled for each\\n\\ntraining case in every minibatch. In our experiments, we use CD-1 for training dropout RBMs.\\n\\n3.3 E\\ufb00ect on features\\n\\nDropout in feed forward networks improved the quality of features by reducing co-adaptations. This\\n\\nsection explores whether this e\\ufb00ect transfers to Dropout RBMs as well.\\n\\nFig. 3.1a shows features learned by a binary RBM with 256 hidden units. Fig. 3.1b shows features\\n\\nlearned by a dropout RBM with the same number of hidden units. Features learned by the dropout\\n\\nRBM appear qualitatively di\\ufb00erent in the sense that they seem to capture features that are coarser\\n\\ncompared to the sharply de\\ufb01ned stroke-like features in the standard RBM. There seem to be very few\\n\\ndead units in the dropout RBM relative to the standard RBM.\\n\\n3.4 E\\ufb00ect on sparsity\\n\\nNext, we investigate the e\\ufb00ect of dropout RBM training on sparsity of the hidden unit activations.\\n\\nFig. 3.2a shows the histograms of hidden unit activations and their means on a test mini-batch after\\n\\ntraining an RBM. Fig. 3.2b shows the same for dropout RBMs. The histograms clearly indicate that\\n\\nthe dropout RBMs learn much sparser representations than standard RBMs even when no additional\\n\\nsparsity inducing regularizer is present.\\n\\n\\x0cChapter 3. Dropout with Boltzmann Machines\\n\\n17\\n\\n(a) Without dropout\\n\\n(b) Dropout with p = 0.5.\\n\\nFigure 3.1: Features learned on MNIST by 256 hidden unit RBMs.\\n\\n(a) Without dropout\\n\\n(b) Dropout with p = 0.5.\\n\\nFigure 3.2: E\\ufb00ect of dropout on sparsity: In each panel, the \\ufb01gure on the left shows a histogram of the\\nmean activation of hidden units in a randomly chosen test minibatch. The \\ufb01gure on the right shows a\\nhistogram of the activations on the same minibatch.\\n\\n\\x0cChapter 4\\n\\nMarginalizing dropout\\n\\nDropout can be seen as a way of adding noise to the states of hidden units in a neural network. In this\\n\\nchapter, we explore the class of models that arise as a result of marginalizing this noise. These models\\n\\ncan be seen as deterministic versions of dropout. In contrast to regular (\\u201cMonte-Carlo\\u201d) dropout, these\\n\\nmodels do not need random bits and it is possible to get gradients for the marginalized loss functions.\\n\\nIn this chapter, we brie\\ufb02y explore these models.\\n\\nMarginalization in the context of denoising autoencoders has been explored previously [1, 25]. De-\\n\\nterministic algorithms have been proposed that try to learn models that are robust to feature deletion\\n\\nat test time [3].\\n\\n4.1 Linear Regression\\n\\nFirst we explore a very simple case of applying dropout to the classical problem of linear regression. Let\\nX \\u2208 RN\\xd7D be a data matrix of N data points. y \\u2208 RN be a vector of targets. Linear regression tries\\nto \\ufb01nd a w \\u2208 RD that minimizes\\n\\n||y \\u2212 Xw||2\\n\\nWhen the input X is dropped out such that any input dimension is retained with probability p, the\\ninput can be expressed as R \\u2217 X where R \\u2208 {0, 1}N\\xd7D is a random matrix with Rij \\u223c Bernoulli(p) and\\n\\u2217 denotes an element-wise product. Marginalizing the noise, the objective function becomes\\n\\nminimize\\n\\nw\\n\\nE\\nR\\u223cBernoulli(p)\\n\\nThis reduces to\\n\\n(cid:2)||y \\u2212 (R \\u2217 X)w||2(cid:3)\\n\\nminimize\\n\\nw\\n\\n||y \\u2212 pXw||2 + p(1 \\u2212 p)||\\u0393w||2\\n\\nwhere \\u0393 = (diag(X(cid:62)X))1/2. Therefore, dropout with linear regression is equivalent, in expectation, to\\nridge regression with a particular form for \\u0393. This form of \\u0393 essentially scales the weight cost for weight\\nwi by the standard deviation of the ith dimension of the data.\\n\\nAnother interesting way to look at this objective is to absorb the factor of p into w. This leads to\\n\\n18\\n\\n\\x0cChapter 4. Marginalizing dropout\\n\\n19\\n\\nthe following form\\n\\n||y \\u2212 X(cid:101)w||2 +\\n\\n||\\u0393(cid:101)w||2\\n\\n1 \\u2212 p\\np\\n\\nminimize\\n\\nWhere (cid:101)w = pw. This makes the dependence of the regularization constant on p explicit. For p close\\n\\nw\\n\\nto 1, all the inputs are retained and the regularization constant is small. As more dropout is done (by\\n\\ndecreasing p), the regularization constant grows larger.\\n\\n4.2 Logistic regression and deep networks\\n\\nFor logistic regression and deep neural nets, it is hard to obtain a closed form marginalized model.\\nHowever, Wang [28] showed that in the context of dropout applied to logistic regression, the correspond-\\n\\ning marginalized model can be trained approximately. Under reasonable assumptions, the distributions\\n\\nover the inputs to the logistic unit and over the gradients of the marginalized model are Gaussian.\\n\\nTheir means and variances can be computed e\\ufb03ciently. This approximate marginalization outperforms\\n\\nMonte-Carlo dropout in terms of training time and generalization performance.\\n\\nHowever, the assumptions involved in this technique become successively weaker as more layers are\\n\\nadded and it would be interesting to see if this same technique can be directly extended to deeper\\n\\nnetworks.\\n\\n\\x0cChapter 5\\n\\nConclusions\\n\\nDropout is a technique for improving neural networks by reducing over\\ufb01tting. The main idea is to\\n\\nprevent co-adaptation of hidden units. Dropout improves performance of neural nets in a wide variety\\n\\nof application domains including object classi\\ufb01cation, digit recognition, speech recognition, document\\n\\nclassi\\ufb01cation and analysis of bio-medical data. This suggests that dropout as a technique is quite\\n\\ngeneral and not speci\\ufb01c to any domain. It has been used in models that achieve state-of-the-art results\\n\\non ImageNet and SVHN.\\n\\nThe central idea of dropout is to take a large model that over\\ufb01ts easily and repeatedly sample and\\n\\ntrain smaller sub-models from it. Since all the sub-models share parameters with the large model, this\\n\\nprocess trains the large model which is then used at test time. We demonstrated that this idea works\\n\\nin the context of feed forward neural networks. This idea can be extended to Restricted Boltzmann\\nMachines and other graphical models which can be seen as composed of exponentially many sub-models\\n\\nwith shared weights.\\n\\nMarginalized versions of dropout models may o\\ufb00er some of the bene\\ufb01ts of dropout training without\\n\\nhaving to deal with noise. These models are an interesting direction for future work.\\n\\n20\\n\\n\\x0cBibliography\\n\\n[1] Minmin Chen, Zhixiang Xu, Kilian Weinberger, and Fei Sha. Marginalized denoising autoencoders\\n\\nfor domain adaptation. In John Langford and Joelle Pineau, editors, Proceedings of the 29th Inter-\\n\\nnational Conference on Machine Learning (ICML-12), ICML \\u201912, pages 767\\u2013774. ACM, New York,\\n\\nNY, USA, July 2012.\\n\\n[2] G.E. Dahl, M. Ranzato, A. Mohamed, and GE Hinton. Phone recognition with the mean-covariance\\n\\nrestricted boltzmann machine. Advances in Neural Information Processing Systems, 23:469\\u2013477,\\n\\n2010.\\n\\n[3] A. Globerson and S. Roweis. Nightmare at test time: robust learning by feature deletion.\\n\\nIn\\n\\nProceedings of the 23rd international conference on Machine learning, pages 353\\u2013360. ACM, 2006.\\n\\n[4] M. Guillaumin, J. Verbeek, and C. Schmid. Multimodal semi-supervised learning for image classi-\\n\\n\\ufb01cation. In Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on, pages\\n\\n902 \\u2013909, june 2010.\\n\\n[5] G. E. Hinton, S. Osindero, and Y. Teh. A fast learning algorithm for deep belief nets. Neural\\n\\nComputation, 18:1527\\u20131554, 2006.\\n\\n[6] Geo\\ufb00rey Hinton and Ruslan Salakhutdinov. Reducing the dimensionality of data with neural net-\\n\\nworks. Science, 313(5786):504 \\u2013 507, 2006.\\n\\n[7] Geo\\ufb00rey E. Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.\\n\\nImproving neural networks by preventing co-adaptation of feature detectors. CoRR, abs/1207.0580,\\n\\n2012.\\n\\n[8] Mark J. Huiskes, Bart Thomee, and Michael S. Lew. New trends and ideas in visual concept\\n\\ndetection: the MIR \\ufb02ickr retrieval evaluation initiative. In Multimedia Information Retrieval, pages\\n\\n527\\u2013536, 2010.\\n\\n[9] Alex Krizhevsky, Ilya Sutskever, and Geo\\ufb00 Hinton. Imagenet classi\\ufb01cation with deep convolutional\\n\\nneural networks. In P. Bartlett, F.C.N. Pereira, C.J.C. Burges, L. Bottou, and K.Q. Weinberger,\\neditors, Advances in Neural Information Processing Systems 25, pages 1106\\u20131114. 2012.\\n\\n[10] Adi Livnat, Christos Papadimitriou, Nicholas Pippenger, and Marcus W. Feldman. Sex, mixability,\\n\\nand modularity. Proceedings of the National Academy of Sciences, 107(4):1452\\u20131457, 2010.\\n\\n[11] Volodymyr Mnih. Cudamat: a CUDA-based matrix class for python. Technical Report UTML TR\\n\\n2009-004, Department of Computer Science, University of Toronto, November 2009.\\n\\n21\\n\\n\\x0cBibliography\\n\\n22\\n\\n[12] A. Mohamed, G. Dahl, and G. Hinton. Acoustic modeling using deep belief networks. Audio,\\n\\nSpeech, and Language Processing, IEEE Transactions on, (99):1\\u20131, 2010.\\n\\n[13] Radford M. Neal. Bayesian Learning for Neural Networks. Springer-Verlag New York, Inc., Secau-\\n\\ncus, NJ, USA, 1996.\\n\\n[14] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y. Ng. Reading\\ndigits in natural images with unsupervised feature learning. In NIPS Workshop on Deep Learning\\n\\nand Unsupervised Feature Learning 2011, 2011.\\n\\n[15] Jiquan Ngiam, Aditya Khosla, Mingyu Kim, Juhan Nam, Honglak Lee, and Andrew Y. Ng. Mul-\\n\\ntimodal deep learning. In International Conference on Machine Learning (ICML), Bellevue, USA,\\n\\nJune 2011.\\n\\n[16] Daniel Povey, Arnab Ghoshal, Gilles Boulianne, Lukas Burget, Ondrej Glembek, Nagendra Goel,\\n\\nMirko Hannemann, Petr Motlicek, Yanmin Qian, Petr Schwarz, Jan Silovsky, Georg Stemmer, and\\n\\nKarel Vesely. The kaldi speech recognition toolkit. In IEEE 2011 Workshop on Automatic Speech\\n\\nRecognition and Understanding. IEEE Signal Processing Society, December 2011. IEEE Catalog\\n\\nNo.: CFP11SRW-USB.\\n\\n[17] Ruslan Salakhutdinov and Geo\\ufb00rey Hinton. Deep Boltzmann machines.\\n\\nIn Proceedings of the\\n\\nInternational Conference on Arti\\ufb01cial Intelligence and Statistics, volume 5, pages 448\\u2013455, 2009.\\n\\n[18] Pierre Sermanet, Soumith Chintala, and Yann LeCun. Convolutional neural networks applied to\\n\\nhouse numbers digit classi\\ufb01cation.\\n\\nIn International Conference on Pattern Recognition (ICPR\\n\\n2012), 2012.\\n\\n[19] P.Y. Simard, D. Steinkraus, and J.C. Platt. Best practices for convolutional neural networks applied\\n\\nto visual document analysis. In Proceedings of the Seventh International Conference on Document\\n\\nAnalysis and Recognition, volume 2, pages 958\\u2013962, 2003.\\n\\n[20] Nathan Srebro and Adi Shraibman. Rank, trace-norm and max-norm. In Proceedings of the 18th\\n\\nannual conference on Learning Theory, COLT\\u201905, pages 545\\u2013560, Berlin, Heidelberg, 2005. Springer-\\n\\nVerlag.\\n\\n[21] Nitish Srivastava and Ruslan Salakhutdinov. Multimodal learning with deep belief nets. ICML\\n\\n2012 Representation Learning Workshop, 2012.\\n\\n[22] Nitish Srivastava and Ruslan Salakhutdinov. Multimodal learning with deep boltzmann machines.\\n\\nIn P. Bartlett, F.C.N. Pereira, C.J.C. Burges, L. Bottou, and K.Q. Weinberger, editors, Advances\\n\\nin Neural Information Processing Systems 25, pages 2231\\u20132239. 2012.\\n\\n[23] Robert Tibshirani. Regression shrinkage and selection via the lasso. J. Roy. Statist. Soc. Ser. B,\\n\\n58(1):267\\u2013288, 1996.\\n\\n[24] Andrey Nikolayevich Tikhonov. On the stability of inverse problems. Doklady Akademii Nauk\\n\\nSSSR, 39(5):195\\u2013198, 1943.\\n\\n[25] Laurens van der Maaten, M. Chen, S. Tyree, and Kilian Weinberger. Learning with marginalized\\n\\ncorrupted features. In Proceedings of the International Conference on Machine Learning, In Press.\\n\\n\\x0cBibliography\\n\\n23\\n\\n[26] Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol. Extracting and\\n\\ncomposing robust features with denoising autoencoders. In Proceedings of the 25th international\\n\\nconference on Machine learning, ICML \\u201908, pages 1096\\u20131103, New York, NY, USA, 2008. ACM.\\n\\n[27] Pascal Vincent, Hugo Larochelle, Isabelle Lajoie, Yoshua Bengio, and Pierre-Antoine Manzagol.\\n\\nStacked denoising autoencoders: Learning useful representations in a deep network with a local\\n\\ndenoising criterion. Journal of Machine Learning Research, 11:3371\\u20133408, December 2010.\\n\\n[28] Sida Wang. Fast dropout training for logistic regression. In NIPS workshop on log-linear models,\\n\\n2012.\\n\\n[29] Hui Yuan Xiong, Yoseph Barash, and Brendan J. Frey. Bayesian prediction of tissue-regulated\\n\\nsplicing using rna sequence and cellular context. Bioinformatics, 27(18):2554\\u20132562, 2011.\\n\\n\\x0c', u'Deep Fragment Embeddings for Bidirectional Image\\n\\nSentence Mapping\\n\\nAndrej Karpathy\\n\\nArmand Joulin\\n\\nDepartment of Computer Science, Stanford University, Stanford, CA 94305, USA\\n\\n{karpathy,ajoulin,feifeili}@cs.stanford.edu\\n\\nLi Fei-Fei\\n\\nAbstract\\n\\nWe introduce a model for bidirectional retrieval of images and sentences through\\na deep, multi-modal embedding of visual and natural language data. Unlike pre-\\nvious models that directly map images or sentences into a common embedding\\nspace, our model works on a \\ufb01ner level and embeds fragments of images (ob-\\njects) and fragments of sentences (typed dependency tree relations) into a com-\\nmon space. We then introduce a structured max-margin objective that allows our\\nmodel to explicitly associate these fragments across modalities. Extensive exper-\\nimental evaluation shows that reasoning on both the global level of images and\\nsentences and the \\ufb01ner level of their respective fragments improves performance\\non image-sentence retrieval tasks. Additionally, our model provides interpretable\\npredictions for the image-sentence retrieval task since the inferred inter-modal\\nalignment of fragments is explicit.\\n\\nIntroduction\\n\\n1\\nThere is signi\\ufb01cant value in the ability to associate natural language descriptions with images. De-\\nscribing the contents of images is useful for automated image captioning and conversely, the ability\\nto retrieve images based on natural language queries has immediate image search applications. In\\nparticular, in this work we are interested in training a model on a set of images and their associated\\nnatural language descriptions such that we can later rank a \\ufb01xed set of withheld sentences given an\\nimage query, and vice versa.\\nThis task is challenging because it requires detailed understanding of the content of images, sen-\\ntences and their inter-modal correspondence. Consider an example sentence query, such as \\u201cA dog\\nwith a tennis ball is swimming in murky water\\u201d (Figure 1). In order to successfully retrieve a corre-\\nsponding image, we must accurately identify all entities, attributes and relationships present in the\\nsentence and ground them appropriately to a complex visual scene.\\nOur primary contribution is in formulating a structured, max-margin objective for a deep neural net-\\nwork that learns to embed both visual and language data into a common, multimodal space. Unlike\\nprevious work that embeds images and sentences, our model breaks down and embeds fragments of\\nimages (objects) and fragments of sentences (dependency tree relations [1]) in a common embed-\\nding space and explicitly reasons about their latent, inter-modal correspondences. Extensive empir-\\nical evaluation validates our approach. In particular, we report dramatic improvements over state of\\nthe art methods on image-sentence retrieval tasks on Pascal1K [2], Flickr8K [3] and Flickr30K [4]\\ndatasets. We make our code publicly available.\\n2 Related Work\\nImage Annotation and Image Search. There is a growing body of work that associates images and\\nsentences. Some approaches focus on the direction of describing the contents of images, formulated\\neither as a task of mapping images to a \\ufb01xed set of sentences written by people [5, 6], or as a task of\\nautomatically generating novel captions [7, 8, 9, 10, 11, 12]. More closely related to our motivation\\nare methods that allow natural bi-drectional mapping between the two modalities. Socher and Fei-\\nFei [13] and Hodosh et al.\\n[3] use Kernel Canonical Correlation Analysis to align images and\\nsentences, but their method is not easily scalable since it relies on computing kernels quadratic in\\n\\n1\\n\\n\\x0cFigure 1: Our model takes a dataset of\\nimages and their sentence descriptions\\nand learns to associate their fragments.\\nIn images, fragments correspond to ob-\\nject detections and scene context. In sen-\\ntences, fragments consist of typed de-\\npendency tree [1] relations.\\n\\nnumber of images and sentences. Farhadi et al. [5] learn a common meaning space, but their method\\nis limited to representing both images and sentences with a single triplet of (object, action, scene).\\nZitnick et al. [14] use a Conditional Random Field to reason about spatial relationships in cartoon\\nscenes and their relation to natural language descriptions. Finally, joint models of language and\\nperception have also been explored in robotics settings [15].\\nMultimodal Representation Learning. Our approach falls into a general category of learning\\nfrom multi-modal data. Several probabilistic models for representing joint multimodal probability\\ndistributions over images and sentences have been developed, using Deep Boltzmann Machines [16],\\nlog-bilinear models [17], and topic models [18, 19]. Ngiam et al. [20] described an autoencoder\\nthat learns audio-video representations through a shared bottleneck layer. More closely related to\\nour task and approach is the work of Frome et al.\\n[21], who introduced a model that learns to\\nmap images and words to a common semantic embedding with a ranking cost. Adopting a similar\\napproach, Socher et al.\\n[22] described a Dependency Tree Recursive Neural Network that puts\\nentire sentences into correspondence with visual data. However, these methods reason about the\\nimage only on the global level using a single, \\ufb01xed-sized representation from the top layer of a\\nConvolutional Neural Network as a description for the entire image. In contrast, our model explicitly\\nreasons about objects that make up a complex scene.\\nNeural Representations for Images and Natural Language. Our model is a neural network\\nthat is connected to image pixels on one side and raw 1-of-k word representations on the other.\\nThere have been multiple approaches for learning neural representations in these data domains. In\\nComputer Vision, Convolutional Neural Networks (CNNs) [23] have recently been shown to learn\\npowerful image representations that support state of the art image classi\\ufb01cation [24, 25, 26] and\\nobject detection [27, 28]. In language domain, several neural network models have been proposed\\nto learn word/n-gram representations [29, 30, 31, 32, 33, 34], sentence representations [35] and\\nparagraph/document representations [36].\\n\\n3 Proposed Model\\nLearning and Inference. Our task is to retrieve relevant images given a sentence query, and con-\\nversely, relevant sentences given an image query. We train our model on a set of N images and N\\ncorresponding sentences that describe their content (Figure 2). Given this set of correspondences,\\nwe learn the weights of a neural network with a structured loss to output a high score when a com-\\npatible image-sentence pair is fed through the network, and low score otherwise. Once the training is\\ncomplete, all training data is discarded and the network is evaluated on a withheld set of images and\\nsentences. The evaluation scores all image-sentence pairs in the test set, sorts the images/sentences\\nin order of decreasing score and records the location of a ground truth result in the list.\\nFragment Embeddings. Our core insight is that images are complex structures that are made\\nup of multiple entities that the sentences make explicit references to. We capture this intuition\\ndirectly in our model by breaking down both images and sentences into fragments and reason about\\ntheir alignment. In particular, we propose to detect objects as image fragments and use sentence\\ndependency tree relations [1] as sentence fragments (Figure 2).\\nObjective. We will compute the representation of both image and sentence fragments with a neural\\nnetwork, and interpret the top layer as high-dimensional vectors embedded in a common multi-\\nmodal space. We will think of the inner product between these vectors as a fragment compatibility\\nscore, and compute the global image-sentence score as a \\ufb01xed function of the scores of their respec-\\ntive fragments. Intuitively, an image-sentence pair will obtain a high global score if the sentence\\nfragments can each be con\\ufb01dently matched to some fragment in the image. Finally, we will learn\\nthe weights of the neural networks such that the true image-sentence pairs achieve a score higher\\n(by a margin) than false image-sentence pairs.\\n\\n2\\n\\n\\x0cFigure 2: Computing the Fragment and image-sentence similarities. Left: CNN representations (green) of\\ndetected objects are mapped to the fragment embedding space (blue, Section 3.2). Right: Dependency tree\\nrelations in the sentence are embedded (Section 3.1). Our model interprets inner products (shown as boxes)\\nbetween fragments as a similarity score. The alignment (shaded boxes) is latent and inferred by our model\\n(Section 3.3.1). The image-sentence similarity is computed as a \\ufb01xed function of the pairwise fragment scores.\\n\\nWe \\ufb01rst describe the neural networks that compute the Image and Sentence Fragment embeddings.\\nThen we discuss the objective function, which is composed of the two aforementioned objectives.\\n3.1 Dependency Tree Relations as Sentence Fragments\\nWe would like to extract and represent the set of visually identi\\ufb01able entities described in a sentence.\\nFor instance, using the example in Figure 2, we would like to identify the entities (dog, child)\\nand characterise their attributes (black, young) and their pairwise interactions (chasing). Inspired\\nby previous work [5, 22] we observe that a dependency tree of a sentence provides a rich set of\\ntyped relationships that can serve this purpose more effectively than individual words or bigrams.\\nWe discard the tree structure in favor of a simpler model and interpret each relation (edge) as an\\nindividual sentence fragment (Figure 2, right shows 5 example dependency relations). Thus, we\\nrepresent every word using 1-of-k encoding vector w using a dictionary of 400,000 words and map\\nevery dependency triplet (R, w1, w2) into the embedding space as follows:\\n\\n(cid:18)\\n\\n(cid:20) Wew1\\n\\n(cid:21)\\n\\n(cid:19)\\n\\n+ bR\\n\\n.\\n\\nWR\\n\\ns = f\\n\\nWew2\\n\\n(1)\\nHere, We is a d \\xd7 400, 000 matrix that encodes a 1-of-k vector into a d-dimensional word vector\\nrepresentation (we use d = 200). We \\ufb01x We to weights obtained through an unsupervised objective\\ndescribed in Huang et al. [34]. Note that every relation R can have its own set of weights WR and\\nbiases bR. We \\ufb01x the element-wise nonlinearity f (.) to be the Recti\\ufb01ed Linear Unit (ReLU), which\\ncomputes x \\u2192 max(0, x). The size of the embedded space is cross-validated, and we found that\\nvalues of approximately 1000 generally work well.\\n3.2 Object Detections as Image Fragments\\nSimilar to sentences, we wish to extract and describe the set of entities that images are composed of.\\nInspired by prior work [7], as a modeling assumption we observe that the subject of most sentence\\ndescriptions are attributes of objects and their context in a scene. This naturally motivates the use of\\nobjects and the global context as the fragments of an image. In particular, we follow Girshick et al.\\n[27] and detect objects in every image with a Region Convolutional Neural Network (RCNN). The\\nCNN is pre-trained on ImageNet [37] and \\ufb01netuned on the 200 classes of the ImageNet Detection\\nChallenge [38]. We use the top 19 detected locations and the entire image as the image fragments\\nand compute the embedding vectors based on the pixels Ib inside each bounding box as follows:\\n\\nv = Wm[CNN\\u03b8c(Ib)] + bm,\\n\\n(2)\\n\\nwhere CNN(Ib) takes the image inside a given bounding box and returns the 4096-dimensional\\nactivations of the fully connected layer immediately before the classi\\ufb01er. The CNN architecture is\\nidentical to the one described in Girhsick et al. [27]. It contains approximately 60 million parameters\\n\\u03b8c and closely resembles the architecture of Krizhevsky et al [25].\\n3.3 Objective Function\\nWe are now ready to formulate the objective function. Recall that we are given a training set of N\\nimages and corresponding sentences. In the previous sections we described parameterized functions\\nthat map every sentence and image to a set of fragment vectors {s} and {v}, respectively. All\\nparameters of our model are contained in these two functions. As shown in Figure 2, our model\\n\\n3\\n\\n\\x0cFigure 3: The two objectives for a\\nbatch of 2 examples. Left: Rows rep-\\nresent fragments vi, columns sj. Ev-\\nery square shows an ideal scenario of\\ni sj) in the MIL ob-\\nyij = sign(vT\\njective. Red boxes are yij = \\u22121.\\nYellow indicates members of posi-\\ntive bags that happen to currently\\nbe yij = \\u22121. Right: The scores\\nare accumulated with Equation 6 into\\nimage-sentence score matrix Skl.\\n\\nthen interprets the inner product vT\\ni sj between an image fragment vi and a sentence fragment sj as\\na similarity score, and computes the image-sentence similarity as a \\ufb01xed function of the scores of\\ntheir respective fragments.\\nWe are motivated by two criteria in designing the objective function. First, the image-sentence\\nsimilarities should be consistent with the ground truth correspondences. That is, corresponding\\nimage-sentence pairs should have a higher score than all other image-sentence pairs. This will\\nbe enforced by the Global Ranking Objective. Second, we introduce a Fragment Alignment\\nObjective that explicitly learns the appearance of sentence fragments (such as \\u201cblack dog\\u201d) in the\\nvisual domain. Our full objective is the sum of these two objectives and a regularization term:\\n\\nC(\\u03b8) = CF (\\u03b8) + \\u03b2CG(\\u03b8) + \\u03b1||\\u03b8||2\\n2,\\n\\n(3)\\nwhere \\u03b8 is a shorthand for parameters of our neural network (\\u03b8 = {We, WR, bR, Wm, bm, \\u03b8c}) and\\n\\u03b1 and \\u03b2 are hyperparameters that we cross-validate. We now describe both objectives in more detail.\\n3.3.1 Fragment Alignment Objective\\nThe Fragment Alignment Objective encodes the intuition that if a sentence contains a fragment\\n(e.g.\\u201cblue ball\\u201d, Figure 3), at least one of the boxes in the corresponding image should have a high\\nscore with this fragment, while all the other boxes in all the other images that have no mention of\\n\\u201cblue ball\\u201d should have a low score. This assumption can be violated in multiple ways: a triplet\\nmay not refer to anything visually identi\\ufb01able in the image. The box that the triplet refers to may\\nnot be detected by the RCNN. Lastly, other images may contain the described visual concept but\\nits mention may omitted in the associated sentence descriptions. Nonetheless, the assumption is\\nstill satis\\ufb01ed in many cases and can be used to formulate a cost function. Consider an (incomplete)\\nFragment Alignment Objective that assumes a dense alignment between every corresponding image\\nand sentence fragments:\\n\\n(cid:88)\\n\\n(cid:88)\\n\\nC0(\\u03b8) =\\n\\nmax(0, 1 \\u2212 yijvT\\n\\ni sj).\\n\\n(4)\\n\\ni\\n\\nj\\n\\nHere, the sum is over all pairs of image and sentence fragments in the training set. The quantity vT\\ni sj\\ncan be interpreted as the alignment score of visual fragment vi and sentence fragment sj. In this\\nincomplete objective, we de\\ufb01ne yij as +1 if fragments vi and sj occur together in a corresponding\\nimage-sentence pair, and \\u22121 otherwise. Intuitively, C0(\\u03b8) encourages scores in red regions of Figure\\n3 to be less than -1 and scores along the block diagonal (green and yellow) to be more than +1.\\nMultiple Instance Learning extension. The problem with the objective C0(\\u03b8) is that it assumes\\ndense alignment between all pairs of fragments in every corresponding image-sentence pair. How-\\never, this is hardly ever the case. For example, in Figure 3, the \\u201cboy playing\\u201d triplet refers to only\\none of the three detections. We now describe a Multiple Instance Learning (MIL) [39] extension\\nof the objective C0 that attempts to infer the latent alignment between fragments in corresponding\\nimage-sentence pairs. Concretely, for every triplet we put image fragments in the associated im-\\nage into a positive bag, while image fragments in every other image become negative examples.\\nOur precise formulation is inspired by the mi-SVM [40], which is a simple and natural extension\\nof a Support Vector Machine to a Multiple Instance Learning setting. Instead of treating the yij as\\nconstants, we minimize over them by wrapping Equation 4 as follows:\\n\\n4\\n\\n\\x0cs.t. (cid:88)\\n\\nCF (\\u03b8) = min\\nyij\\nyij + 1\\n\\nC0(\\u03b8)\\n\\u2265 1 \\u2200j\\n\\n2\\n\\ni\\u2208pj\\nyij = \\u22121 \\u2200i, j s.t. mv(i) (cid:54)= ms(j) and yij \\u2208 {\\u22121, 1}\\n\\n(5)\\n\\nHere, we de\\ufb01ne pj to be the set of image fragments in the positive bag for sentence fragment j.\\nmv(i) and ms(j) return the index of the image and sentence (an element of {1, . . . , N}) that the\\nfragments vi and sj belong to. Note that the inequality simply states that at least one of the yij\\nshould be positive for every sentence fragment j (i.e. at least one green box in every column in\\nFigure 3). This objective cannot be solved ef\\ufb01ciently [40] but a commonly used heuristic is to set\\ni sj). If the constraint is not satis\\ufb01ed for any positive bag (i.e. all scores were below\\nyij = sign(vT\\nzero), the highest-scoring item in the positive bag is set to have a positive label.\\n3.3.2 Global Ranking Objective\\nRecall that the Global Ranking Objective ensures that the computed image-sentence similarities are\\nconsistent with the ground truth annotation. First, we de\\ufb01ne the image-sentence alignment score to\\nbe the average thresholded score of their pairwise fragment scores:\\n\\nSkl =\\n\\n1\\n\\n|gk|(|gl| + n)\\n\\n(cid:88)\\n\\n(cid:88)\\n\\ni\\u2208gk\\n\\nj\\u2208gl\\n\\nmax(0, vT\\n\\ni sj).\\n\\n(6)\\n\\n(cid:105)\\n\\n.\\n\\n(cid:125)\\n\\nHere, gk is the set of image fragments in image k and gl is the set of sentence fragments in sentence\\nl. Both k, l range from 1, . . . , N. We truncate scores at zero because in the mi-SVM objective, scores\\ngreater than 0 are considered correct alignments and scores less than 0 are considered to be incorrect\\nalignments (i.e. false members of a positive bag). In practice, we found that it was helpful to add\\na smoothing term n, since short sentences can otherwise have an advantage (we found that n = 5\\nworks well and that this setting is not very sensitive). The Global Ranking Objective then becomes:\\n\\nCG(\\u03b8) =\\n\\nmax(0, Skl \\u2212 Skk + \\u2206)\\n\\n+\\n\\nmax(0, Slk \\u2212 Skk + \\u2206)\\n\\n(7)\\n\\n(cid:88)\\n(cid:124)\\n\\nl\\n\\n(cid:125)\\n\\n(cid:123)(cid:122)\\n\\n(cid:88)\\n\\nk\\n\\n(cid:104)(cid:88)\\n(cid:124)\\n\\nl\\n\\n(cid:123)(cid:122)\\n\\nrank images\\n\\nrank sentences\\n\\nHere, \\u2206 is a hyperparameter that we cross-validate. The objective stipulates that the score for true\\nimage-sentence pairs Skk should be higher than Skl or Slk for any l (cid:54)= k by at least a margin of \\u2206.\\n3.4 Optimization\\nWe use Stochastic Gradient Descent (SGD) with mini-batches of 100, momentum of 0.9 and make\\n20 epochs through the training data. The learning rate is cross-validated and annealed by a fraction\\nof \\xd70.1 for the last two epochs. Since both Multiple Instance Learning and CNN \\ufb01netuning bene\\ufb01t\\nfrom a good initialization, we run the \\ufb01rst 10 epochs with the fragment alignment objective C0\\nand CNN weights \\u03b8c \\ufb01xed. After 10 epochs, we switch to the full MIL objective CF and begin\\n\\ufb01netuning the CNN. The word embedding matrix We is kept \\ufb01xed due to over\\ufb01tting concerns. Our\\nimplementation runs at approximately 1 second per batch on a standard CPU workstation.\\n4 Experiments\\nDatasets. We evaluate our image-sentence retrieval performance on Pascal1K [2], Flickr8K [3] and\\nFlickr30K [4] datasets. The datasets contain 1,000, 8,000 and 30,000 images respectively and each\\nimage is annotated using Amazon Mechanical Turk with 5 independent sentences.\\nSentence Data Preprocessing. We did not explicitly \\ufb01lter, spellcheck or normalize any of the\\nsentences for simplicity. We use the Stanford CoreNLP parser to compute the dependency trees\\nfor every sentence. Since there are many possible relation types (as many as hundreds), due to\\nover\\ufb01tting concerns and practical considerations we remove all relation types that occur less than\\n1% of the time in each dataset. In practice, this reduces the number of relations from 136 to 16 in\\nPascal1K, 170 to 17 in Flickr8K, and from 212 to 21 in Flickr30K. Additionally, all words that are\\nnot found in our dictionary of 400,000 words [34] are discarded.\\nImage Data Preprocessing. We use the Caffe [41] implementation of the ImageNet Detection\\nRCNN model [27] to detect objects in all images. On our machine with a Tesla K40 GPU,\\nthe RCNN processes one image in approximately 25 seconds. We discard the predictions for\\n200 ImageNet detection classes and only keep the 4096-D activations of the fully connect layer\\nimmediately before the classi\\ufb01er at all of the top 19 detected locations and from the entire image.\\n\\n5\\n\\n\\x0cPascal1K\\n\\nImage Annotation\\n\\nImage Search\\n\\nModel\\nRandom Ranking\\nSocher et al. [22]\\nkCCA. [22]\\nDeViSE [21]\\nSDT-RNN [22]\\nOur model\\n\\nR@1 R@5 R@10 Mean r R@1 R@5 R@10 Mean r\\n4.0\\n23.0\\n21.0\\n17.0\\n25.0\\n39.0\\n\\n5.2\\n46.6\\n41.4\\n54.6\\n65.2\\n65.2\\n\\n12.0\\n63.0\\n61.0\\n68.0\\n70.0\\n79.0\\n\\n71.0\\n16.9\\n18.0\\n11.9\\n13.4\\n10.5\\n\\n9.0\\n45.0\\n47.0\\n57.0\\n56.0\\n68.0\\n\\n10.6\\n65.6\\n58.0\\n72.4\\n84.4\\n79.8\\n\\n50.0\\n12.5\\n15.9\\n9.5\\n7.0\\n7.6\\n\\n1.6\\n16.4\\n16.4\\n21.6\\n25.4\\n23.6\\n\\nTable 1: Pascal1K ranking experiments. R@K is Recall@K (high is good). Mean r is the mean rank (low is\\ngood). Note that the test set only consists of 100 images.\\n\\nFlickr8K\\n\\nImage Annotation\\n\\nImage Search\\n\\nModel\\nRandom Ranking\\nSocher et al. [22]\\nDeViSE [21]\\nSDT-RNN [22]\\nFragment Alignment Objective\\nGlobal Ranking Objective\\n(\\u2020) Fragment + Global\\n\\u2020 \\u2192 Images: Fullframe Only\\n\\u2020 \\u2192 Sentences: BOW\\n\\u2020 \\u2192 Sentences: Bigrams\\nOur model (\\u2020 + MIL)\\n* Hodosh et al. [3]\\n* Our model (\\u2020 + MIL)\\nTable 2: Flickr8K experiments. R@K is Recall@K (high is good). Med r is the median rank (low is good).\\nThe starred evaluation criterion (*) in [3] is slightly different since it discards 4,000 out of 5,000 test sentences.\\n\\nR@1 R@5 R@10 Med r R@1 R@5 R@10 Med r\\n500\\n0.1\\n29\\n4.5\\n29\\n4.8\\n25\\n6.0\\n26\\n7.2\\n5.8\\n21\\n17\\n12.5\\n32\\n5.9\\n23\\n9.1\\n20\\n8.7\\n12.6\\n15\\n38\\n8.3\\n9.3\\n17\\n\\n1.1\\n28.6\\n27.3\\n34.0\\n31.8\\n34.8\\n43.8\\n27.3\\n40.7\\n41.0\\n44.0\\n30.3\\n37.4\\n\\n0.6\\n18.0\\n16.5\\n22.7\\n21.9\\n21.8\\n29.4\\n19.2\\n25.9\\n28.5\\n32.9\\n21.6\\n24.9\\n\\n0.5\\n18.5\\n20.1\\n21.6\\n20.0\\n23.4\\n26.7\\n17.6\\n22.4\\n25.2\\n29.6\\n20.7\\n27.9\\n\\n1.0\\n29.0\\n29.6\\n31.7\\n30.3\\n35.0\\n38.7\\n26.5\\n34.0\\n37.0\\n42.5\\n30.1\\n41.3\\n\\n631\\n32\\n28\\n23\\n25\\n20\\n14\\n34\\n17\\n16\\n14\\n34\\n21\\n\\n0.1\\n6.1\\n5.9\\n6.6\\n5.9\\n7.5\\n8.6\\n5.2\\n6.9\\n8.5\\n9.7\\n7.6\\n8.8\\n\\nEvaluation Protocol for Bidirectional Retrieval. For Pascal1K we follow Socher et al. [22] and\\nuse 800 images for training, 100 for validation and 100 for testing. For Flickr datasets we use\\n1,000 images for validation, 1,000 for testing and the rest for training (consistent with [3]). We\\ncompute the dense image-sentence similarity Skl between every image-sentence pair in the test set\\nand rank images and sentences in order of decreasing score. For both Image Annotation and Image\\nSearch, we report the median rank of the closest ground truth result in the list, as well as Recall@K\\nwhich computes the fraction of times the correct result was found among the top K items. When\\ncomparing to Hodosh et al. [3] we closely follow their evaluation protocol and only keep a subset\\nof N sentences out of total 5N (we use the \\ufb01rst sentence out of every group of 5).\\n\\n4.1 Comparison Methods\\nSDT-RNN. Socher et al. [22] embed a fullframe CNN representation with the sentence representa-\\ntion from a Semantic Dependency Tree Recursive Neural Network (SDT-RNN). Their loss matches\\nour global ranking objective. We requested the source code of Socher et al. [22] and substituted the\\nFlickr8K and Flick30K datasets. To better understand the bene\\ufb01ts of using our detection CNN and\\nfor a more fair comparison we also train their method with our CNN features. Since we have multi-\\nple objects per image, we average representations from all objects with detection con\\ufb01dence above a\\n(cross-validated) threshold. We refer to the exact method of Socher et al. [22] with a single fullframe\\nCNN as \\u201cSocher et al\\u201d, and to their method with our combined CNN features as \\u201cSDT-RNN\\u201d.\\nDeViSE. The DeViSE [21] source code is not publicly available but their approach is a special case\\nof our method with the following modi\\ufb01cations: we use the average (L2-normalized) word vectors\\nas a sentence fragment, the average CNN activation of all objects above a detection threshold (as\\ndiscussed in case of SDT-RNN) as an image fragment and only use the global ranking objective.\\n\\n4.2 Quantitative Evaluation\\nOur model outperforms previous methods. Our full method consistently outperforms previous\\nmethods on Flickr8K (Table 2) and Flickr30K (Table 3) datasets. On Pascal1K (Table 1) the\\nSDT-RNN appears to be competitive on Image Search.\\nFragment and Global Objectives are complementary. As seen in Tables 2 and 3, both objectives\\nperform well independently, but bene\\ufb01t from the combination. Note that the Global Objective\\nperforms consistently better, possibly because it directly minimizes the evaluation criterion (ranking\\n\\n6\\n\\n\\x0cFlickr30K\\n\\nImage Annotation\\n\\nImage Search\\n\\nModel\\nRandom Ranking\\nDeViSE [21]\\nSDT-RNN [22]\\nFragment Alignment Objective\\nGlobal Ranking Objective\\n(\\u2020) Fragment + Global\\nOur model (\\u2020 + MIL)\\nOur model + Finetune CNN\\n\\nR@1 R@5 R@10 Med r R@1 R@5 R@10 Med r\\n500\\n0.1\\n25\\n4.5\\n9.6\\n16\\n22\\n11\\n17\\n11.5\\n14\\n12.0\\n14\\n14.2\\n16.4\\n13\\n\\n1.1\\n29.2\\n41.1\\n39.3\\n44.9\\n50.0\\n51.3\\n54.7\\n\\n0.6\\n18.1\\n29.8\\n28.7\\n33.2\\n37.1\\n37.7\\n40.2\\n\\n1.0\\n32.7\\n41.1\\n34.5\\n38.4\\n43.2\\n44.2\\n44.5\\n\\n631\\n26\\n16\\n18\\n14\\n10\\n10\\n8\\n\\n0.1\\n6.7\\n8.9\\n7.6\\n8.8\\n9.9\\n10.2\\n10.3\\n\\n0.5\\n21.9\\n29.8\\n23.8\\n27.6\\n30.5\\n30.8\\n31.4\\n\\nTable 3: Flickr30K experiments. R@K is Recall@K (high is good). Med r is the median rank (low is good).\\n\\nFigure 4: Qualitative Image Annotation results. Below each image we show the top 5 sentences (among a set\\nof 5,000 test sentences) in descending con\\ufb01dence. We also show the triplets for the top sentence and connect\\neach to the detections with the highest compatibility score (indicated by lines). The numbers next to each triplet\\nindicate the matching fragment score. We color a sentence green if it correct and red otherwise.\\n\\ncost), while the Fragment Alignment Objective only does so indirectly.\\nExtracting object representations is important. Using only the global scene-level CNN repre-\\nsentation as a single fragment for every image leads to a consistent drop in performance, suggesting\\nthat a single fullframe CNN alone is inadequate in effectively representing the images. (Table 2)\\nDependency tree relations outperform BoW/bigram representations. We compare to a simpler\\nBag of Words (BoW) baseline to understand the contribution of dependency relations.\\nIn BoW\\nbaseline we iterate over words instead of dependency triplets when creating bags of sentence\\nfragments (set w1 = w2 in Equation1). As can be seen in the Table 2, this leads to a consistent drop\\nin performance. This drop could be attributed to a difference between using one word or two words\\nat a time, so we also compare to a bigram baseline where the words w1, w2 in Equation 1 refer to\\nconsecutive words in a sentence, not nodes that share an edge in the dependency tree. Again, we\\nobserve a consistent performance drop, which suggests that the dependency relations provide useful\\nstructure that the neural network takes advantage of.\\nFinetuning the CNN helps on Flickr30K. Our end-to-end Neural Network approach allows us to\\nbackpropagate gradients all the way down to raw data (pixels or 1-of-k word encodings). In particu-\\nlar, we observed additional improvements on the Flickr30K dataset (Table 3) when we \\ufb01netune the\\nCNN. Training the CNN improves the validation error for a while but the model eventually starts to\\nover\\ufb01t. Thus, we found it critical to keep track of the validation error and freeze the model before it\\nover\\ufb01ts. We were not able to improve the validation performance on Pascal1K and Flickr8K datasets\\nand suspect that there is an insuf\\ufb01cient amount of training data.\\n4.3 Qualitative Experiments\\nInterpretable Predictions. We show some example sentence retrieval results in Figure 4. The\\nalignment in our model is explicitly inferred on the fragment level, which allows us to interpret the\\nscores between images and sentences. For instance, in the last image it is apparent that the model\\nretrieved the top sentence because it erroneously associated a mention of a blue person to the blue\\n\\ufb02ag on the bottom right of the image.\\nFragment Alignment Objective trains attribute detectors.\\nThe detection CNN is trained to\\npredict one of 200 ImageNet Detection classes, so it is not clear if the representation is powerful\\nenough to support learning of more complex attributes of the objects or generalize to novel classes.\\nTo see whether our model successfully learns to predict sentence triplets, we \\ufb01x a triplet vector and\\n\\n7\\n\\n\\x0cFigure 5: We \\ufb01x a triplet and retrieve the highest scoring image fragments in the test set. Note that ball, person\\nand dog are ImageNet Detection classes but their visual properties (e.g. soccer, standing, snowboarding, black)\\nare not. Jackets and rocky scenes are not ImageNet Detection classes. Find more in supplementary material.\\nsearch for the highest scoring boxes in the test set. Qualitative results shown in Figure 5 suggest\\nthat the model is indeed capable of generalizing to more \\ufb01ne-grained subcategories (such as \\u201cblack\\ndog\\u201d, \\u201csoccer ball\\u201d) and to out of sample classes such as \\u201crocky terrain\\u201d and \\u201cjacket\\u201d.\\nLimitations. Our model is subject to multiple limitations. From a modeling perspective, the use of\\nedges from a dependency tree is simple, but not always appropriate. First, a single complex phrase\\nthat describes a single visual entity can be split across multiple sentence fragments. For example,\\n\\u201cblack and white dog\\u201d is parsed as two relations (CONJ, black, white) and (AMOD, white, dog).\\nConversely, there are many dependency relations that don\\u2019t have a clear grounding in the image (for\\nexample \\u201ceach other\\u201d). Furthermore, phrases such as \\u201cthree children playing\\u201d that describe some\\nparticular number of visual entiries are not modeled. While we have shown that the relations give\\nrise to more powerful representations than words or bigrams, a more careful treatment of sentence\\nfragments will likely lead to further improvements. On the image side, the non-maximum suppres-\\nsion in the RCNN can sometimes detect, for example, multiple people inside one person. Since the\\nmodel does not take into account any spatial information associated with the detections, it is hard\\nfor it to disambiguate between two distinct people or spurious detections of one person.\\n5 Conclusions\\nWe addressed the problem of bidirectional retrieval of images and sentences. Our neural network\\nlearns a multi-modal embedding space for fragments of images and sentences and reasons about\\ntheir latent, inter-modal alignment. We have shown that our model signi\\ufb01cantly improves the re-\\ntrieval performance on image sentence retrieval tasks compared to previous work. Our model also\\nproduces interpretable predictions.\\nIn future work we hope to develop better sentence fragment\\nrepresentations, incorporate spatial reasoning, and move beyond bags of fragments.\\nAcknowledgments. We thank Justin Johnson and Jon Krause for helpful comments and discussions.\\nWe gratefully acknowledge the support of NVIDIA Corporation with the donation of the GPUs used\\nfor this research. This research is supported by an ONR MURI grant, and NSF ISS-1115313.\\nReferences\\n[1] De Marneffe, M.C., MacCartney, B., Manning, C.D., et al.: Generating typed dependency parses from\\n\\nphrase structure parses. In: Proceedings of LREC. Volume 6. (2006) 449\\u2013454\\n\\n[2] Rashtchian, C., Young, P., Hodosh, M., Hockenmaier, J.: Collecting image annotations using amazon\\u2019s\\nmechanical turk. In: Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language\\nData with Amazon\\u2019s Mechanical Turk, Association for Computational Linguistics (2010) 139\\u2013147\\n\\n[3] Hodosh, M., Young, P., Hockenmaier, J.: Framing image description as a ranking task: data, models and\\n\\nevaluation metrics. Journal of Arti\\ufb01cial Intelligence Research (2013)\\n\\n[4] Young, P., Lai, A., Hodosh, M., Hockenmaier, J.: From image descriptions to visual denotations: New\\n\\nsimilarity metrics for semantic inference over event descriptions. TACL (2014)\\n\\n[5] Farhadi, A., Hejrati, M., Sadeghi, M.A., Young, P., Rashtchian, C., Hockenmaier, J., Forsyth, D.: Every\\n\\npicture tells a story: Generating sentences from images. In: ECCV. (2010)\\n\\n[6] Ordonez, V., Kulkarni, G., Berg, T.L.: Im2text: Describing images using 1 million captioned photographs.\\n\\nIn: NIPS. (2011)\\n\\n[7] Kulkarni, G., Premraj, V., Dhar, S., Li, S., Choi, Y., Berg, A.C., Berg, T.L.: Baby talk: Understanding\\n\\nand generating simple image descriptions. In: CVPR. (2011)\\n\\n8\\n\\n\\x0c[8] Yao, B.Z., Yang, X., Lin, L., Lee, M.W., Zhu, S.C.: I2t: Image parsing to text description. Proceedings\\n\\n[9] Yang, Y., Teo, C.L., Daum\\xb4e III, H., Aloimonos, Y.: Corpus-guided sentence generation of natural images.\\n\\nof the IEEE 98(8) (2010) 1485\\u20131508\\n\\nIn: EMNLP. (2011)\\n\\nweb-scale n-grams. In: CoNLL. (2011)\\n\\n[10] Li, S., Kulkarni, G., Berg, T.L., Berg, A.C., Choi, Y.: Composing simple image descriptions using\\n\\n[11] Mitchell, M., Han, X., Dodge, J., Mensch, A., Goyal, A., Berg, A., Yamaguchi, K., Berg, T., Stratos,\\nK., Daum\\xb4e, III, H.: Midge: Generating image descriptions from computer vision detections. In: EACL.\\n(2012)\\n\\n[12] Kuznetsova, P., Ordonez, V., Berg, A.C., Berg, T.L., Choi, Y.: Collective generation of natural image\\n\\ndescriptions. In: ACL. (2012)\\n\\n[13] Socher, R., Fei-Fei, L.: Connecting modalities: Semi-supervised segmentation and annotation of images\\n\\nusing unaligned text corpora. In: CVPR. (2010)\\n\\n[14] Zitnick, C.L., Parikh, D., Vanderwende, L.: Learning the visual interpretation of sentences. ICCV (2013)\\n[15] Matuszek*, C., FitzGerald*, N., Zettlemoyer, L., Bo, L., Fox, D.: A Joint Model of Language and\\nPerception for Grounded Attribute Learning. In: Proc. of the 2012 International Conference on Machine\\nLearning, Edinburgh, Scotland (June 2012)\\n\\n[16] Srivastava, N., Salakhutdinov, R.: Multimodal learning with deep boltzmann machines. In: NIPS. (2012)\\n[17] Kiros, R., Zemel, R.S., Salakhutdinov, R.: Multimodal neural language models. ICML (2014)\\n[18] Jia, Y., Salzmann, M., Darrell, T.: Learning cross-modality similarity for multinomial data. In: ICCV.\\n\\n[19] Barnard, K., Duygulu, P., Forsyth, D., De Freitas, N., Blei, D.M., Jordan, M.I.: Matching words and\\n\\n(2011)\\n\\npictures. JMLR (2003)\\n\\n[20] Ngiam, J., Khosla, A., Kim, M., Nam, J., Lee, H., Ng, A.Y.: Multimodal deep learning. In: ICML. (2011)\\n[21] Frome, A., Corrado, G.S., Shlens, J., Bengio, S., Dean, J., Mikolov, T., et al.: Devise: A deep visual-\\n\\nsemantic embedding model. In: NIPS. (2013)\\n\\n[22] Socher, R., Karpathy, A., Le, Q.V., Manning, C.D., Ng, A.Y.: Grounded compositional semantics for\\n\\n\\ufb01nding and describing images with sentences. TACL (2014)\\n\\n[23] LeCun, Y., Bottou, L., Bengio, Y., Haffner, P.: Gradient-based learning applied to document recognition.\\n\\nProceedings of the IEEE 86(11) (1998) 2278\\u20132324\\n\\n[24] Le, Q.V.: Building high-level features using large scale unsupervised learning. In: Acoustics, Speech and\\n\\nSignal Processing (ICASSP), 2013 IEEE International Conference on, IEEE (2013) 8595\\u20138598\\n\\n[25] Krizhevsky, A., Sutskever, I., Hinton, G.E.: Imagenet classi\\ufb01cation with deep convolutional neural net-\\n\\n[26] Zeiler, M.D., Fergus, R.: Visualizing and understanding convolutional neural networks. arXiv preprint\\n\\nworks. In: NIPS. (2012)\\n\\narXiv:1311.2901 (2013)\\n\\n[27] Girshick, R., Donahue, J., Darrell, T., Malik, J.: Rich feature hierarchies for accurate object detection and\\n\\nsemantic segmentation. In: CVPR. (2014)\\n\\n[28] Sermanet, P., Eigen, D., Zhang, X., Mathieu, M., Fergus, R., LeCun, Y.: Overfeat: Integrated recognition,\\n\\nlocalization and detection using convolutional networks. In: ICLR. (2014)\\n\\n[29] Bengio, Y., Schwenk, H., Sen\\xb4ecal, J.S., Morin, F., Gauvain, J.L.: Neural probabilistic language models.\\n\\nIn: Innovations in Machine Learning. Springer (2006)\\n\\n[30] Mnih, A., Hinton, G.: Three new graphical models for statistical language modelling. In: ICML. (2007)\\n[31] Mikolov, T., Sutskever, I., Chen, K., Corrado, G.S., Dean, J.: Distributed representations of words and\\n\\nphrases and their compositionality. In: NIPS. (2013)\\n\\n[32] Turian, J., Ratinov, L., Bengio, Y.: Word representations: a simple and general method for semi-\\n\\n[33] Collobert, R., Weston, J.: A uni\\ufb01ed architecture for natural language processing: Deep neural networks\\n\\nsupervised learning. In: ACL. (2010)\\n\\nwith multitask learning. In: ICML. (2008)\\n\\n[34] Huang, E.H., Socher, R., Manning, C.D., Ng, A.Y.: Improving word representations via global context\\n\\nand multiple word prototypes. In: ACL. (2012)\\n\\n[35] Socher, R., Lin, C.C., Manning, C., Ng, A.Y.: Parsing natural scenes and natural language with recursive\\n\\nneural networks. In: ICML. (2011)\\n\\n[36] Le, Q.V., Mikolov, T.: Distributed representations of sentences and documents. ICML (2014)\\n[37] Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: Imagenet: A large-scale hierarchical image\\n\\ndatabase. In: CVPR. (2009)\\n\\n[38] Russakovsky, O., Deng, J., Krause, J., Berg, A., Fei-Fei, L.: Large scale visual recognition challenge\\n\\n2013. http://image-net.org/challenges/LSVRC/2013/ (2013)\\n\\n[39] Chen, Y., Bi, J., Wang, J.Z.: Miles: Multiple-instance learning via embedded instance selection. CVPR\\n\\n[40] Andrews, S., Hofmann, T., Tsochantaridis, I.: Multiple instance learning with generalized support vector\\n\\n28(12) (2006)\\n\\nmachines. In: AAAI/IAAI. (2002) 943\\u2013944\\n\\nhttp://caffe.berkeleyvision.org/ (2013)\\n\\n[41] Jia, Y.:\\n\\nCaffe: An open source convolutional architecture for\\n\\nfast\\n\\nfeature embedding.\\n\\n9\\n\\n\\x0c', u'Effective Multi(cid:173)Modal Retrieval based on Stacked\\n\\nAuto(cid:173)Encoders\\n\\nWei Wang\\u2020, Beng Chin Ooi\\u2020, Xiaoyan Yang\\u2021, Dongxiang Zhang\\u2020, Yueting Zhuang\\xa7\\n\\n\\u2020School of Computing, National University of Singapore, Singapore\\n\\n\\u2021Advanced Digital Sciences Center, Illinois at Singapore Pte, Singapore\\n\\n\\xa7College of Computer Science, Zhejiang University, China\\n\\n\\u2020{wangwei, ooibc, zhangdo}@comp.nus.edu.sg, \\u2021xiaoyan.yang@adsc.com.sg, \\xa7yzhuang@zju.edu.cn\\n\\nABSTRACT\\n\\nMulti-modal retrieval is emerging as a new search paradigm that\\nenables seamless information retrieval from various types of me-\\ndia. For example, users can simply snap a movie poster to search\\nrelevant reviews and trailers. To solve the problem, a set of map-\\nping functions are learned to project high-dimensional features ex-\\ntracted from data of different media types into a common low-\\ndimensional space so that metric distance measures can be applied.\\nIn this paper, we propose an effective mapping mechanism based\\non deep learning (i.e., stacked auto-encoders) for multi-modal re-\\ntrieval. Mapping functions are learned by optimizing a new ob-\\njective function, which captures both intra-modal and inter-modal\\nsemantic relationships of data from heterogeneous sources effec-\\ntively. Compared with previous works which require a substan-\\ntial amount of prior knowledge such as similarity matrices of intra-\\nmodal data and ranking examples, our method requires little prior\\nknowledge. Given a large training dataset, we split it into mini-\\nbatches and continually adjust the mapping functions for each batch\\nof input. Hence, our method is memory ef\\ufb01cient with respect to the\\ndata volume. Experiments on three real datasets illustrate that our\\nproposed method achieves signi\\ufb01cant improvement in search accu-\\nracy over the state-of-the-art methods.\\n\\n1.\\n\\nINTRODUCTION\\n\\nThe prevalence of social networking has signi\\ufb01cantly increased\\nthe volume and velocity of information shared on the Internet. A\\ntremendous amount of data in various media types is being gener-\\nated every day in the social networking systems, and images and\\nvideo contribute the main bulk of the data. For instance, Twitter\\nrecently reported that over 340 million tweets were sent each day1,\\nwhile Facebook reported around 300 million photos were created\\neach day2. These data, together with other domain speci\\ufb01c data,\\n\\n1https://blog.twitter.com/2012/twitter-turns-six\\n2http://techcrunch.com/2012/08/22/how-big-is-facebooks-data-2-\\n5-billion-pieces-of-content-and-500-terabytes-ingested-every-day/\\n\\nlicensed under\\n\\nThis work is\\nthe Creative Commons Attribution(cid:173)\\nNonCommercial(cid:173)NoDerivs 3.0 Unported License. To view a copy of this li(cid:173)\\ncense, visit http://creativecommons.org/licenses/by(cid:173)nc(cid:173)nd/3.0/. Obtain per(cid:173)\\nmission prior to any use beyond those covered by the license. Contact\\ncopyright holder by emailing info@vldb.org. Articles from this volume\\nwere invited to present their results at the 40th International Conference on\\nVery Large Data Bases, September 1st (cid:173) 5th 2014, Hangzhou, China.\\nProceedings of the VLDB Endowment, Vol. 7, No. 8\\nCopyright 2014 VLDB Endowment 2150(cid:173)8097/14/04.\\n\\nsuch as medical data, surveillance and sensory data, are Big Da-\\nta that can be exploited for insights and contextual observations.\\nHowever, effective retrieval of such huge amounts of media data\\nfrom heterogeneous sources remains a big challenge.\\n\\nIn this paper, we study the problem of large-scale information\\nretrieval from multiple modalities. Each modality represents one\\ntype of multimedia, such as text, image or video, and depending\\non the heterogeneity of data sources, we can have two following\\nsearches:\\n\\n1. Intra-modal search has been extensively studied and wide-\\nly used in commercial systems. Examples include web docu-\\nment retrieval via keyword queries and content-based image\\nretrieval.\\n\\n2. Cross-modal search enables users to explore more relevan-\\nt resources from different modalities. For example, a user\\ncan use a tweet to retrieve relevant photos and videos from\\nother heterogeneous data sources, or search relevant textual\\ndescriptions or videos by submitting an interesting image as\\na query.\\n\\nThere has been a long stream of research on multi-modal re-\\ntrieval [27, 1, 16, 9, 25, 12, 26, 20]. These works share a sim-\\nilar query processing strategy which consists of two major steps.\\nFirst, they learn a set of mapping functions to project the high-\\ndimensional features from different modalities into a common low-\\ndimensional latent space. Second, a multi-dimensional index for\\neach modality in the metric space is built for ef\\ufb01cient similarity\\nretrieval. Since the second step is a classic kNN problem and has\\nbeen extensively studied [6, 23], we shall focus on the optimiza-\\ntion of the \\ufb01rst step and propose a novel learning algorithm to \\ufb01nd\\neffective mapping functions.\\n\\nWe observe that most existing works, such as CVH [9], IMH [20],\\nMLBE [25], CMSSH [1], and LSCMR [12], require a substantial\\namount of prior knowledge about the training data to learn effective\\nmapping functions. Preparing prior knowledge in terms of large\\ntraining dataset is labor-intensive, and due to manual intervention,\\nthe prepared knowledge may not be comprehensive in capturing\\nthe regularities (e.g., distribution or similarity relationships) of da-\\nta. For example, MLBE and IMH require the similarity matrix of\\nintra-modal data (one for each modality).\\nIt is usually collected\\nby retrieving k (set by humans) nearest neighbors of every object\\nin the original high-dimensional space and setting the correspond-\\ning entries in the similarity matrix as 1 (other entries as 0). Each\\ntraining input of LSCMR is a ranking example, i.e., a list of objects\\nranked based on their relevance to the \\ufb01rst one. Ranking examples\\nare generated based on the labels of objects, which have to be cre-\\nated manually. CMSSH requires irrelevant inter-modal data pairs\\n\\n649\\n\\n\\x0cthat are also not available directly in most cases. In addition, many\\nexisting works (e.g., CVH, CMSSH, LCMH, IMH) have to load the\\nwhole training dataset into memory which becomes the bottleneck\\nof training when the training dataset is too large to \\ufb01t in memory.\\n\\nTo tackle the above issues, we propose a new mapping mech-\\nanism for multi-modal retrieval. Our mapping mechanism, called\\nMulti-modal Stacked Auto-Encoders (MSAE), builds one Stacked\\nAuto-Encoders (SAE) [22] for each modality and projects features\\nfrom different media types into a common latent space. The SAE\\nis a form of deep learning and has been successfully used in many\\nunsupervised feature learning and classi\\ufb01cation tasks [17, 22, 4,\\n19]. The proposed MSAE has several advantages over existing\\napproaches. Firstly, the stacked structure of MSAE renders our\\nmapping method (which is non-linear) more expressive than lin-\\near projections used in previous studies, such as CVH and LCMH.\\nSecondly, compared with existing solutions, our method requires\\nminimum amount of prior knowledge of the training data. Simple\\nrelevant pairs, e.g., images and their associated tags, are used as\\nlearning inputs. Thirdly, the memory usage of our learning algo-\\nrithm is independent of the training dataset size, and can be set to a\\nconstant.\\n\\nFor effective multi-modal retrieval, we need to learn a set of pa-\\nrameters in the MSAE such that the mapped latent features capture\\nboth inter-modal semantics and intra-modal semantics well. We\\ndesign an objective function which takes the two requirements into\\nconsideration: the inter-modal semantics is preserved by minimiz-\\ning the distance of semantic relevant inter-modal pairs in the latent\\nspace; the intra-modal semantics is captured by minimizing the re-\\nconstruction error (which will be discussed in detail in Section 2.2)\\nof the SAE for each modality. By minimizing the reconstruction\\nerror, SAEs preserve the regularities (e.g., distribution) of the orig-\\ninal features so that if the original features capture the semantics of\\ndata well, so do the latent features. When the original features of\\none modality are of low quality and cannot capture the intra-modal\\nsemantics, we decrease its weight of reconstruction error in the ob-\\njective function. The intuition is that their intra-modal semantics\\ncan be preserved or even enhanced through inter-modal relation-\\nships with other modalities whose features are of high quality. Ex-\\nperimental results show that the latent features generated from our\\nmapping mechanism are more effective than those generated by ex-\\nisting methods for multi-modal retrieval.\\n\\nThe main contributions of this paper are:\\n\\n\\u2022 We propose a novel mapping mechanism based on stacked\\nauto-encoders to project high-dimensional feature vectors for\\ndata from different modalities into a common low-dimensional\\nlatent space, which enables effective and ef\\ufb01cient multi-modal\\nretrieval.\\n\\n\\u2022 We design a new learning objective function which takes\\nboth intra-modal and inter-modal semantics into considera-\\ntion, and thus generates effective mapping functions.\\n\\n\\u2022 We conduct extensive experiments on three real datasets to\\nevaluate our proposed mapping mechanism. Experimental\\nresults show that the performance of our method is superior\\nto state-of-art methods.\\n\\nThe remainder of the paper is organized as follows. Problem\\nstatements and some backgrounds are provided in Section 2. After\\nthat, we describe the algorithm for training our mapping method\\nin Section 3. Query processing is described in Section 4. Related\\nworks are discussed in Section 5. Finally we report our experiments\\nin Section 6.\\n\\nLatent Layer\\n\\n(cid:1)1 \\n\\n. . .\\n\\nEncoder\\n\\nDecoder\\n\\n(cid:5)1, (cid:7)1\\n\\n(cid:5)2, (cid:7)2\\n\\n(cid:1)0 \\n\\n. . .\\n\\n. . .\\n\\n(cid:1)2 \\n\\nInput Layer\\n\\nReconstruction Layer\\n\\nFigure 1: Auto-Encoder\\n\\n2. PRELIMINARY\\n\\n2.1 Problem Statements\\n\\nIn our data model, the database D consists of objects from mul-\\ntiple modalities. For ease of presentation, we use images and text\\nas two modalities to explain our idea, i.e., we assume that D =\\nDI S DT , even though our proposal is general enough to support\\n\\nany number of domains. Formally, an image is said to be relevant\\nto a text document (e.g., a set of tags) if their semantic distance is\\nsmall. Since there is no widely accepted measure to calculate the\\ndistance between an image and text, a common approach is to map\\nimages and text into the same latent space in which the two types\\nof objects are comparable.\\n\\nDEFINITION 1. Common Latent Space Mapping\\n\\nGiven an image feature vector x \\u2208 DI and a text feature vector y \\u2208\\nDT , \\ufb01nd two mapping functions fI : DI \\u2192 Z and fT : DT \\u2192 Z\\nsuch that if x and y are semantically relevant, the distance between\\nfI (x) and fT (y), denoted by dist(fI (x), fT (y)), is small in the\\ncommon latent space Z.\\n\\nThe common latent space mapping provides a uni\\ufb01ed approach\\nto measuring distance of objects from different modalities. As long\\nas all objects can be mapped into the same latent space, they be-\\ncome comparable. If the mapping functions fI and fT have been\\ndetermined, the multi-modal search can then be transformed into\\nthe classic kNN problem, de\\ufb01ned as following:\\n\\nDEFINITION 2. Multi-Modal Search\\n\\nGiven a query object Q \\u2208 Dq (q \\u2208 {I, T }) and a target do-\\nmain Dt \\u2282 D (t \\u2208 {I, T }), \\ufb01nd a set O \\u2282 Dt with k ob-\\njects such that \\u2200o \\u2208 O and o\\u2032 \\u2208 Dt/O, dist(fq(Q), ft(o\\u2032)) \\u2265\\ndist(fq(Q), ft(o)).\\n\\nSince both q and t have two choices, four types of queries can\\nbe derived, namely Qq\\u2192t and q, t \\u2208 {I, T }. For instance, QI\\u2192T\\nsearches relevant text (e.g., tags) in DT given an image from DI .\\nBy mapping objects from high-dimensional feature space into low-\\ndimensional latent space, queries can be ef\\ufb01ciently processed using\\nexisting multi-dimensional indexes [6, 23]. Our goal is then to learn\\na set of effective mapping functions which preserve well both intra-\\nmodal semantics (i.e., semantic relationships within each modali-\\nty) and inter-modal semantics (i.e., semantic relationships across\\nmodalities) in the latent space. The effectiveness is measured by\\nthe accuracy of multi-modal retrieval using latent features.\\n\\n2.2 Auto(cid:173)encoder\\n\\nAuto-encoder and its variants have been widely used in unsu-\\npervised feature learning and classi\\ufb01cation tasks [17, 22, 4, 19]. It\\ncan be seen as a special neural network with three layers \\u2013 the input\\nlayer, the latent layer, and the reconstruction layer (as shown in Fig-\\nure 1). An auto-encoder contains two parts: (1) The encoder maps\\nan input x0 \\u2208 Rd0 to the latent representation (feature) x1 \\u2208 Rd1\\nvia a deterministic mapping fe:\\n\\n650\\n\\n\\x0cTest Images\\n\\nImage Query\\n\\nx1 = fe(x0) = se(W T\\n\\n1 x0 + b1)\\n\\n(1)\\n\\nwhere se is the activation function of the encoder, whose input is\\ncalled the activation of the latent layer, and {W1, b1} is the pa-\\nrameter set with a weight matrix W1 \\u2208 Rd0\\xd7d1 and a bias vector\\nb1 \\u2208 Rd1 . (2) The decoder maps the latent representation x1 back\\nto a reconstruction x2 \\u2208 Rd0 via another mapping function fd:\\n\\nMSAE\\n\\nTraining\\n\\nTraining data\\n\\nImage SAE\\n\\nQI->T\\n\\nQT->I\\n\\nQI->I\\n\\nQT->T\\n\\nText SAE\\n\\nx2 = fd(x1) = sd(W T\\n\\n2 x1 + b2)\\n\\n(2)\\n\\nTest Text\\n\\nText Query\\n\\nIndexed Latent \\nfeature vectors\\n\\nSimilarly, sd is the activation function of the decoder with pa-\\nrameters {W2, b2}, W2 \\u2208 Rd1\\xd7d0 , b2 \\u2208 Rd0 . The input of\\nsd is called the activation of the reconstruction layer. Parameters\\nare learned through back-propagation [10] by minimizing the loss\\nfunction L(x0, x2) in Equation 3,\\n\\nL(x0, x2) = Lr(x0, x2) + 0.5\\u03be(||W1||2\\n\\n2 + ||W2||2\\n2)\\n\\n(3)\\n\\nwhich consists of the reconstruction error Lr(x0, x2) and the L2\\nregularization of W1 and W2. By minimizing the reconstruction er-\\nror, we require the latent features should be able to reconstruct the\\noriginal input as much as possible. In this way, the latent features\\npreserve regularities of the original data. The squared Euclidean\\ndistance is often used for Lr(x0, x2). Other loss functions such\\nas negative log likelihood and cross-entropy are also used. The L2\\nregularization term is a weight-decay which is added to the objec-\\ntive function to penalize large weights and reduce over-\\ufb01tting [5] 3.\\n\\u03be is the weight decay cost, which is usually a small number (e.g.,\\n10\\u22124 in our experiments).\\n\\n2.3 Stacked Auto(cid:173)encoders (SAE)\\n\\nThe stacked auto-encoders (SAE) is a neural network with multi-\\nple layers of auto-encoders. It has been widely used as a deep learn-\\ning method for dimensionality reduction [18] and feature learning\\n[17, 22, 4, 19].\\n\\nh-th (top) \\nauto-encoder\\n\\n.\\n \\n.\\n \\n.\\n\\n2nd auto-encoder\\n\\n1st (bottom) \\nauto-encoder\\n\\n.\\n \\n.\\n \\n.\\n\\n. . .\\n\\n. . .\\n\\n. . .\\n\\n. . .\\n\\n. . .\\n\\n. . .\\n\\n. . .\\n\\nFigure 2: Stacked Auto-Encoders\\n\\nAs illustrated in Figure 2, there are h auto-encoders which are\\ntrained in a bottom-up and layer-wise manner. The input vectors\\n(blue color in the \\ufb01gure) are fed to the bottom auto-encoder. After\\n\\ufb01nishing training the bottom auto-encoder, the output latent repre-\\nsentations are propagated to the higher layer. The sigmoid function\\nor tanh function is typically used for the activation functions se and\\nsd. The same procedure is repeated until all the auto-encoders are\\ntrained. After such a pre-training stage, the whole neural network is\\n\\ufb01ne-tuned based on a pre-de\\ufb01ned objective. The latent layer of the\\n\\n3For ease of presentation, we time all squared terms in this paper\\nwith a coef\\ufb01cient 0.5 to dismiss coef\\ufb01cients of the derivatives. E.g,\\nthe derivative of \\u22020.5|W1|2\\n\\n= W1\\n\\n\\u2202W1\\n\\nFigure 3: Process Flow of Learning and Query Processing.\\n\\ntop auto-encoder is the output of the stacked auto-encoders, which\\ncan be further fed into other applications, such as SVM for classi-\\n\\ufb01cation. The unsupervised pre-training can automatically exploit\\nlarge amounts of unlabeled data to obtain a good weight initializa-\\ntion for the neural network than traditional random initialization.\\n\\n2.4 Overview\\n\\nTaking the image and text modalities as an example, we pro-\\nvide the process \\ufb02ow of the learning and query processing in Fig-\\nure 3. Our learning algorithm MSAE is trained with simple image-\\ntext pairs. After training, we obtain one SAE for each modality.\\nTest (database) images (resp.\\ntext), are mapped into the latent s-\\npace through the image SAE (resp. text SAE). To support ef\\ufb01cient\\nquery processing, we create an index for the latent feature vectors\\nof each modality. All the steps are conducted of\\ufb02ine. For real-time\\nquery processing, we map the query into the latent space through\\nits modal-speci\\ufb01c SAE, and return the kNNs using an appropriate\\nindex based on the query type. For example, the image index is\\nused for queries against the image database, i.e., QI\\u2192I and QT \\u2192I .\\n\\n3. TRAINING\\n\\nIn this section, we design a two-stage training algorithm to learn\\nthe mapping functions of MSAE. A complete training process with\\nimage and text as two example modalities is illustrated in Figure 4.\\nIn stage I, one SAE is trained for each modality. This train-\\ning stage serves as the pre-training of stage II. As shown in step\\n1 and step 2 in the \\ufb01gure, we train each SAE independently with\\nthe objective to map similar features close to each other in the la-\\ntent space. This pre-training stage provides a good initialization to\\nthe parameters in MSAE and improves the training performance.\\nIn stage II, we iterate over all SAEs, adjusting the parameters in\\none SAE at a time with the goal to capture both intra-modal se-\\nmantics and inter-modal semantics. The learned MSAE projects\\nsemantically relevant objects close to each other in the latent space\\nas shown by step 3 and 4 in Figure 4.\\n\\n3.1 Single(cid:173)Modal Training\\n\\nSince we use image and text as two exemplar modalities through-\\nout this paper, we describe the details of single-modal training for\\nthese two modalities in this section. Besides image and text, the\\nSAEs have been applied for other modalities such as audio [13].\\nOur method can be applied for all modalities supported by SAEs.\\n\\nImage Input We represent an image by a high dimensional real-\\nvalued vector.\\nIn the encoder, each input image is mapped to a\\nlatent vector using sigmoid function as the activation function se\\n(Equation 1). However, in the decoder, the sigmoid activation func-\\ntion, whose range is [0,1], performs poorly on reconstruction be-\\ncause the input unit (referring to a dimension of the input) is not\\nnecessarily within [0,1]. To solve the issue, we follow Hinton [5]\\n\\n651\\n\\n\\x0c1\\n\\n4\\n\\n.\\n.\\n.\\n\\n.\\n.\\n.\\n\\n.\\n.\\n.\\n\\nImage SAE\\n\\n(Adjust) Text SAE\\n\\n(Fix) Image SAE\\n\\nTrain MSAE\\n\\n2\\n\\n3\\n\\n.\\n.\\n.\\n\\n.\\n.\\n.\\n\\n.\\n.\\n.\\n\\n(a)Input\\n\\n(b) Training Stage I (Step 1&2)\\n\\n(c) Training Stage II (Step 3&4)\\n\\nText SAE\\n\\n(Fix) Text SAE\\n\\n(Adjust) Image SAE\\n\\nFigure 4: Flowchart of Training. Relevant images (or text) are associated with the same shape (e.g., (cid:4)).\\n\\n2.0\\n\\n1.5\\n\\n1.0\\n\\n0.5\\n\\n140\\n\\n120\\n\\n100\\n\\n80\\n\\n60\\n\\n40\\n\\n20\\n\\n0.0\\n\\n0.0\\n\\n0.5\\n\\n1.0\\n\\n1.5\\n\\n2.0\\n\\n0\\n0.0\\n\\naverage unit value\\n\\n0.5\\n\\n1.0\\n\\n1.5\\n\\n2.0\\n\\n2.5\\n\\n3.0\\n\\n3.5\\n\\n4.0\\n\\naverage unit value\\n\\n\\xd710\\u22122\\n\\nwhere P ois(n, \\u03bb) = e\\u2212\\u03bb\\u03bbn/n!. Based on Equation 5, we de\\ufb01ne\\nthe reconstruction error as the negative log likelihood:\\n\\nr (x0, x2) = \\u2212log Y\\nLT\\n\\ni\\n\\np(x2i = x0i)\\n\\n(6)\\n\\nGiven a set of input feature vectors X 0 (each row is an image or\\ntext), the SAE consisting of h auto-encoders is trained by minimiz-\\ning the following objective function:\\n\\n(a)\\n\\n(b)\\n\\nL(X 0) = Lr(X 0, X 2h) + 0.5\\u03be\\n\\nh\\n\\nX\\n\\ni=1\\n\\n(||W i\\n\\n1||2\\n\\n2 + ||W i\\n\\n2||2\\n2)\\n\\n(7)\\n\\nFigure 5: Distribution of image (5a) and text (5b) features extracted\\nfrom NUS-WIDE training dataset (See Section 6). Each \\ufb01gure is\\ngenerated by averaging the units for each feature vector, and then\\nplot the histogram for all data.\\n\\nand model the input unit as a linear unit with independent Gaus-\\nsian noise. The reason is that the unit of image feature typically\\nfollows Gaussian distribution as shown in Figure 5a. Furthermore,\\nthe Gaussian noise term can be omitted if the input data is nor-\\nmalized with zero mean and unit variance. Consequently, we can\\nuse an identity function, denoted as sI\\nd, for the activation function\\nsd in the decoder. We employ Euclidean distance to measure the\\nreconstruction error for images, denoted as LI\\nr :\\nr(x0, x2) = 0.5||x0 \\u2212 x2||2\\n2\\n\\nLI\\n\\n(4)\\n\\nText Input The text inputs are represented by word count vectors\\nor tag occurrence vectors 4. We adopt the Rate Adapting Poisson\\nmodel [18] for the reconstruction because the histogram for text\\ninput unit generally follows Poisson distribution. Figure 5b shows\\nthe distribution of tags associated with the training images from\\nNUS-WIDE training dataset. The activation function in the decoder\\nis x2 = sT\\nd (z2) = l \\xd7 ez2 /Pj ez2j , where l = Pj x0j is the\\nnumber of words in input text, x1 is computed as in Equation 1 and\\nz2 = W T\\n2 x1 + b2. The probability of a reconstruction unit x2i\\nbeing the same as the input unit x0i is calculated as following:\\n\\np(x2i = x0i) = P ois(x0i, x2i)\\n\\n(5)\\n\\n4The value for each dimension indicates the corresponding tag ap-\\npears or not.\\n\\n652\\n\\nwhere X 2h is the reconstruction of X 0 and Lr(X 0, X 2h) is the\\naverage reconstruction error over examples in X 0 calculated using\\neither Equation 4 or 6. The second term is the L2 regularization of\\nall parameter matrices. The objective function can be considered\\nas an extension of Equation 3 to the stacked scenario. Generally\\nspeaking, if the reconstruction error is small, then the latent feature\\nfrom the top auto-encoder would be able to reconstruct the original\\ninput well, and consequently, capture the regularities of the input\\ndata well. Therefore, if the feature vectors of two objects are simi-\\nlar in the original space, then they would be close in the latent space\\nas shown by step 1 and step 2 in Figure 4.\\n\\nThe detailed procedure of training a single-modal SAE is shown\\nin Algorithm 1. It consists of two major components: a layer-wise\\ntraining stage (lines 1-3) which trains the auto-encoders from bot-\\ntom to top with the objective function as Equation 3, and a \\ufb01ne-\\ntuning stage (line 4) which adjusts all auto-encoders together to\\nminimize Equation 7. The layer-wise training learns one single\\nauto-encoder at a time, which can be seen as a special SAE with\\nonly one auto-encoder, i.e., h = 1. Its objective function is similar\\nto that in the \\ufb01ne-tuning of SAE. Thus, they both call trainNN to\\nperform the training task.\\n\\ntrainNN is an adaptation of the back-propagation algorithm [10]\\nfor training neural networks. By unfolding the SAE in to a neural\\nnetwork as shown in Figure 6, we can call trainNN to train it. As\\nin [5], we divide the training dataset X 0 into mini-batches of the\\nsame size (line 2). For example, given a dataset with 60, 000 im-\\nages, we can divide them into 600 batches, each with 100 images.\\nFor each mini-batch, we forward propagate the input to compute\\nthe value of each layer (fProp in line 3). Speci\\ufb01cally, the latent\\n\\n\\x0cAlgorithm 1 trainSAE(h, X 0, d)\\n\\nEncoders\\n\\nDecoders\\n\\nInput: h, height of SAE\\nInput: X 0, training data, one example per row\\nInput: d, a sequence of dimensions for each layer\\nOutput: \\u03b8 = {\\u03b8i}h\\n1. for i = 1 to h do\\n2.\\n3.\\n4. \\u03b8 \\u2190trainNN(h, X 0, \\u03b8)\\n\\nrandom init \\u03b8i \\u2190 di\\u22121, di\\n(\\u03b8i, X i)=trainNN(1, X i\\u22121, \\u03b8i)\\n\\ni=1, parameters of SAE\\n\\ntrainNN(h, X, \\u03b8)\\n\\nfor batch B0 in X do\\n\\nZ, B=fProp(2h, B0, \\u03b8)\\n\\u03b42h = \\u2202L(B0)\\n\\u2202Z 2h\\nbProp(2h, \\u03b42h, B, Z, \\u03b8) //(see Appendix)\\n\\n1. repeat\\n2.\\n3.\\n\\n4.\\n\\n5.\\n6. until converge\\n7. return fProp(h, X, \\u03b8)\\n\\nd (resp. sT\\n\\nlayers (in Figure 6) are calculated according to Equation 1. The\\nreconstruction layers are calculated according to Equation 2 except\\nthat of the bottom auto-encoder (i.e., the right most layer in Fig-\\nure 6), which is modal dependent. For image (resp. text) SAE the\\nsd of the bottom auto-encoder (i.e., the right most sd in Figure 6)\\nis sI\\nd ). In line 4, we calculate the partial derivative of\\nthe objective function L w.r.t. the activation of the last layer. After\\nthat, we back-propagate the derivative to get gradients of param-\\neters in each layer (bProp in line 5) and update them according\\nto Stochastic Gradient Descent. The details of parameter updating\\nare described in the Appendix section. In line 7, the latent features\\nfrom the top auto-encoder are returned as the input for training up-\\nper auto-encoders.\\n\\n3.2 Multi(cid:173)Modal Training\\n\\nSingle modal training initializes one SAE for each modality with\\nthe objective to minimize the reconstruction error. The generated\\nlatent features thus preserve the regularities of the original features\\nwell. But it does not necessarily capture the semantics of the intra-\\nmodal data. If the original features do not capture the intra-modal\\nsemantics well, the latent features would fail to capture the intra-\\nmodal semantics. Moreover, inter-modal semantics is not involved\\nin the single modal training, thus cannot be captured by the latent\\nfeatures. To preserve both intra-modal semantics and inter-modal\\nsemantics in the latent features, we combine all modalities together\\nto learn an effective mapping mechanism in this section.\\n\\nThe intuition of multi-modal training is as follows. On the one\\nhand, in the learning objective function, we minimize the distance\\nof latent features of semantic relevant inter-modal pairs. The learned\\nmapping functions would then try to map semantic relevant inter-\\nmodal pairs into similar latent features. On the other hand, we\\nminimize the reconstruction error for the modalities whose original\\nfeatures are of high quality in capturing intra-modal semantics. In\\nthis way, the latent features preserve the regularities of the original\\nfeatures well and thus captures semantics well. For modalities with\\nlow quality features, we assign small weights for their reconstruc-\\ntion error in the objective function. In this manner, the restriction\\nof minimizing the reconstruction error is relaxed, while the restric-\\ntion on minimizing the inter-modal distance is enhanced relatively.\\nConsequently, the intra-modal semantics of low quality modalities\\ncan be preserved or even enhanced through their inter-modal rela-\\ntionships with the modalities of high quality features. For exam-\\n\\n(cid:1)1\\n\\n1\\n1, (cid:5)1\\n(cid:6)(cid:8)  \\n\\nX0\\n\\n(cid:1)1\\n\\n...\\n\\n\\u210e\\n\\n\\u210e , (cid:5)1\\n(cid:6)(cid:8)  \\n\\nXh\\n\\n(cid:1)2\\n\\n\\u210e , (cid:5)2\\n\\u210e\\n...\\n(cid:6)(cid:7)  \\n\\n(cid:1)2\\n\\n1, (cid:5)2\\n1\\n(cid:6)(cid:7)  \\n\\nX2h\\n\\n(cid:9)1 \\n\\n(cid:9)2 \\n\\n(cid:9) 2\\u210e \\u22121\\n\\n(cid:9)2\\u210e  \\n\\n...\\n\\n...\\n\\nInput layer\\n\\nLatent layers\\n\\nReconstruction layers\\n\\nfProp :\\nbProp:\\n\\nFigure 6: Unfolded Stacked Auto-Encoders\\n\\nple, let x0, y0 be two semantic relevant objects from two different\\nmodalities, namely, x and y, where x\\u2019s feature is of low quality in\\ncapturing semantics while y\\u2019s feature is of high quality. If x1 and\\ny1 are their latent features generated by minimizing the reconstruc-\\ntion error, then y1 can preserve the semantics well, but x1 is not as\\nmeaningful due to the low quality of x0. To solve this problem, we\\ncombine the inter-modal distance between x1 and y1 in the learning\\nobjective function and assign smaller weight for the reconstruction\\nerror of x1. The effect is the same as increasing the weight for min-\\nimizing the inter-modal distance with y1. As a result, the objective\\nfunction will adjust the parameters to make the distance between x1\\nand y1 become smaller. In this way, the semantics of low quality\\nx1 is indirectly enhanced by the high quality feature y1.\\n\\nWith the above learning intuition, we de\\ufb01ne our objective func-\\n\\ntion for multi-modal training as follows,\\n\\nL(X 0, Y 0) = \\u03b1LI\\n\\nr(X 0, X 2h) + \\u03b2LT\\n+Ld(X h, Y h) + \\u03be(\\u03b8)\\n\\nr (Y 0, Y 2h)\\n\\n(8)\\n\\nwhere X 0 (resp. Y 0) is a matrix for the image (resp. text) training\\nfeature vectors; each row of X 0 and Y 0 makes a semantic rele-\\nvant inter-modal pair, e.g., an image and its associated tags; X 2h\\n(resp. Y 2h) is the corresponding reconstruction matrix, which is\\ncalculated by forwarding the input through the unfolded image (re-\\ntext) SAE as shown in Figure 7. X h and Y h are the latent\\nsp.\\nfeature matrices, which are the output of MSAE (see Figure 7). LI\\nr\\n(see Equation 4) is the reconstruction error from image SAE, and\\nLT\\nr (see Equation 6) is the reconstruction error from text SAE. Ld\\nis the distance function for latent features, which is the Euclidean\\ndistance (see Equation 4) in our implementation; similar to Equa-\\ntion 7, the last term \\u03be(\\u03b8) is the L2 regularization of the parameter\\nmatrices involved in all SAEs.\\n\\n\\u03b1 and \\u03b2 are the weights for the reconstruction error of image and\\ntext SAEs respectively, which are set according to the quality of the\\noriginal features in capturing intra-modal semantics. After single\\nmodal training, we can test the quality of the original features for\\neach modality on a validation dataset by performing intra-modal\\nsearch against the latent features. The weight, e.g., \\u03b1, is assigned\\nwith smaller value if the performance is worse, i.e., the original\\nfeatures are not good at capturing semantics of the data. In this\\nmanner, the weight of Ld would increase relatively in terms of the\\nimage modality, which moves images close to their semantic rel-\\nevant text in the latent space. Consequently, the latent features of\\nimages would gain more semantics from the text latent features. S-\\nince the dimensions of the latent space and the original space are\\nusually of different orders of magnitude, the scale of LI\\nr and\\nLd are different. \\u03b1 and \\u03b2 need to be scaled to make them compa-\\nrable, i.e., within an order of magnitude.\\n\\nr, LT\\n\\n653\\n\\n\\x0cEncoders\\n\\nDecoders\\n\\nIndexed Image Latent Feature Vectors\\n\\nY0\\n\\n(cid:1)(cid:2)  \\n\\n...\\n\\n(cid:1)(cid:2)  \\n\\nYh\\n\\nfProp :\\nbProp:\\n\\nX0\\n\\n(cid:1)(cid:2)  \\n\\n(cid:4) 1 \\n\\n...\\n\\n(cid:1)(cid:2)  \\n\\nXh\\n\\n...\\n\\n(cid:1)(cid:3)  \\n\\n...\\n\\n...\\n\\n(cid:4) 2 \\n\\n(cid:4) 2\\u210e \\u22121\\n\\nX2h\\n\\n(cid:1)(cid:3)  \\n\\n(cid:4) 2\\u210e  \\n\\nLatent layers\\n\\nReconstruction layers\\n\\nFigure 7: Unfolded Multi-modal Stacked Auto-Encoders\\n\\nAlgorithm 2 trainMSAE(h, X 0, Y 0, \\u03b8)\\n\\nInput: h, height of MSAE\\nInput: X 0, Y 0, image and text input data\\nInput: \\u03b8=(\\u03b8X , \\u03b8Y ), parameters of MSAE, initialized by trainSAE\\nOutput: \\u03b8, updated parameters\\n\\n1. repeat\\n2.\\n3.\\n4. until converge\\n\\ntrainMNN(h, X 0, Y 0, \\u03b8X , \\u03b8Y )//train image SAE\\ntrainMNN(h, Y 0, X 0, \\u03b8Y , \\u03b8X )//train text SAE\\n\\ntrainMNN(h, X, Y, \\u03b8X , \\u03b8Y )\\nInput: X, input data for the modality whose SAE is to be updated\\nInput: Y , input data for the modality whose SAE is \\ufb01xed\\nInput: \\u03b8X , \\u03b8Y , parameters for the two SAEs.\\n\\n1. repeat\\n2.\\n3.\\n4.\\n\\n5.\\n\\n6.\\n\\n7.\\n\\nfor batch (B0\\n\\nX , B0\\n\\nY ) in (X, Y ) do\\n\\nX , \\u03b8X )\\n\\nY , \\u03b8Y )\\n\\nY )\\n\\nX ,B0\\n\\nBX , ZX =fProp(2h, B0\\nBY , ZY =fProp(h, B0\\n\\u03b42h = \\u2202L(B0\\n\\u03b4h=bProp(h, \\u03b42h, {Bi\\n\\u03b4h+ = \\u2202Ld(Bh\\n\\u2202Zh\\nX\\nbProp(h, \\u03b4h, {Bi\\n\\nX ,Bh\\nY )\\n\\n\\u2202Z 2h\\nX\\n\\nX }h\\n\\nX }2h\\n\\ni=h, {Z i\\n\\nX }2h\\n\\ni=h, {\\u03b8i\\n\\nX }2h\\n\\ni=h)\\n\\n8.\\n9. until converge\\n\\ni=0, {Z i\\n\\nX }h\\n\\ni=1, {\\u03b8i\\n\\nX }h\\n\\ni=1)\\n\\nAlgorithm 2 shows the procedure of training Multi-modal SAE\\n(MSAE). Instead of adjusting both SAEs simultaneously, we iterate\\nover SAEs, adjusting one SAE at a time with the other one \\ufb01xed,\\nas shown in lines 2-3. This is because the training of the neural\\nnetworks is prone to local optimum. Tuning multiple SAEs simul-\\ntaneously makes the training algorithm more dif\\ufb01cult to converge.\\nBy adjusting only one SAE with the other one \\ufb01xed, the training of\\nMSAE turns to be much easier. Experiments show that one to two\\niterations is enough for converging. The convergence is monitored\\nby performing multi-modal retrieval on a validation dataset.\\n\\ntrainMNN in Algorithm 2 adjusts the SAE for modality X with\\nthe SAE of modality Y \\ufb01xed. It is an extension of trainNN, and\\nthus adjusts parameters based on Stochastic Gradient Descent as\\nwell. We illustrate the training algorithm taking X being the image\\nmodality as an example. The unfolded image SAE is shown in Fig-\\n\\n654\\n\\nImage DB\\n\\n.\\n.\\n.\\n\\nQI->I\\n\\n[0.2,0.4,\\u2026,0.1]\\n\\nImage SAE\\n\\n.\\n.\\n.\\n\\n[0.1,0.9,\\u2026,0.5]\\n\\nQT->T\\n\\nQT->I\\n\\nQI->T\\n\\nImage Query\\n\\nText Query\\n\\nText DB\\n\\nOffline\\nIndexing\\n\\nOnline \\nQuerying\\n\\nOffline\\nIndexing\\n\\nText SAE\\n\\nIndexed Text Latent Feature Vectors\\n\\nFigure 8: Illustration of Query Processing\\n\\nure 7, which is exactly the same as that in Figure 6. Since the text\\nr (Y 0, Y 2h) is a constant in Equation 8. The recon-\\nSAE is \\ufb01xed, LT\\nstruction layers of the text SAE are then not involved in learning\\nparameters of the image SAE. Hence, we do not show them in Fig-\\nure 7. In line 3, we forward propagate the input through all layers\\nof the image SAE including latent layers and reconstruction layers.\\nIn line 4, we calculate the latent layers of text SAE. To update the\\nparameters of the decoders, we \\ufb01rstly calculate the partial deriva-\\ntive of the objective function, i.e., Equation 8, w.r.t. the activation\\nof the last reconstruction layer. Next, we back-propagate the partial\\nderivative to the top latent layer in line 6 calculating the gradients\\nof parameters in each decoder and updating them. In line 7, the\\npartial derivative of Ld is combined. In line 8, bProp updates the\\nparameters of each encoder according to their gradients.\\n\\nNote that Algorithm 2 can be easily extended to support more\\nthan two modalities. For example, we can add one line after line 3\\nfor training audio SAE. In this case, trainMNN may \\ufb01x other two\\nmodalities, and adjusts only one modality by considering its intra-\\nmodal semantics and its inter-modal semantic relationships with\\nother two modalities.\\n\\n4. QUERY PROCESSING\\n\\nAfter training MSAE, each modality is associated with its own\\nSAE whose parameters are already well learned. Given a set of het-\\nerogeneous data sources, high-dimensional features are extracted\\nfrom each source and mapped into a low-dimensional latent space\\nusing the trained SAE. For example, if we have one million images,\\nwe \\ufb01rst convert them into bag-of-words representations. Speci\\ufb01-\\ncally, SIFT features are extracted from images and clustered into\\nN bags. Each bag is considered a visual word and each image is\\nrepresented by an N -dimensional vector. Our goal is to map the N -\\ndimensional feature into a common latent space with m dimensions\\n(m is normally set small, e.g., 16, 24 or 32). The mapping proce-\\ndure is illustrated in Algorithm 3. Image input (resp.\\ntext input)\\nis forwarded through encoders of the image SAE (resp. text SAE)\\nto the top latent layer. Line 1 extracts parameters of the modal-\\nspeci\\ufb01c encoders (see Figure 6). The actual mapping is conducted\\nby fProp (see Algorithm 1) at line 2.\\n\\nAfter the mapping, we create VA-File [23] to index the latent fea-\\ntures (one index per modality). Given a query input, we check its\\nmedia type and map it into the low-dimensional space through its\\nmodal-speci\\ufb01c SAE as shown by Algorithm 3. Next, intra-modal\\nand inter-modal searches are conducted against the corresponding\\nindex shown in Figure 8. For example, the task of searching rele-\\nvant tags of one image, i.e., QI\\u2192T , is processed by the index for\\nthe text latent vectors.\\n\\n\\x0cAlgorithm 3 Inference(D)\\n\\nTable 1: The Statistics of Datasets\\n\\nInput: D, high-dimensional (image/text) query feature vectors\\n1. \\u03b8 \\u2192 {(W 1\\n\\n1), \\xb7 \\xb7 \\xb7 , (W h\\n\\n1 )} //parameters of\\n\\n1 , bh\\n\\n1 , b1\\n\\n1), (W 2\\n\\n1 , b2\\n\\nencoders\\n\\n2. return fProp(h, D, \\u03b8)\\n\\nTo further improve the search ef\\ufb01ciency, we convert the real-\\nvalued latent features into binary features, and search based on\\nHamming distance. The conversion is conducted using existing\\nhash methods that preserve the neighborhood relationship based on\\nEuclidean distance. For example, in our experiment, we choose\\nSpectral Hashing [24], which converts real-valued vectors (data\\npoints) into binary codes with the objective to minimize the Ham-\\nming distance of data points who are close in the original Euclidean\\nspace.\\n\\nHowever, the conversion from real-valued features to binary fea-\\ntures trades off effectiveness for ef\\ufb01ciency. Since there is informa-\\ntion loss when real-valued data is converted to binaries, it affects\\nthe retrieval performance. We study the trade-off between ef\\ufb01cien-\\ncy and effectiveness on binary features and real-valued features in\\nthe experiment section.\\n\\n5. RELATED WORK\\n\\nThe key problem of multi-modal retrieval is to \\ufb01nd an effective\\nmapping mechanism, which maps data from different modalities\\ninto a common latent space. An effective mapping mechanism\\nwould preserve both intra-modal semantics and inter-modal seman-\\ntics well in the latent space, and thus generates good retrieval per-\\nformance.\\n\\nLinear projection has been studied to solve this problem [9, 20,\\n26]. Generally they try to \\ufb01nd a linear projection matrix for each\\nmodality which maps semantic relevant data into similar latent vec-\\ntors. However, if the distribution of the original data is non-linear,\\nit would be hard to \\ufb01nd a set of good projection matrices to make\\nthe latent vectors of relevant data close. CVH [9] extends the Spec-\\ntral Hashing [24] to multi-modal data by \\ufb01nding a linear projection\\nfor each modality that minimizes the Euclidean distance of relevan-\\nt data in the latent space. Similarity matrices for both inter-modal\\ndata and intra-modal data are required to learn a set of good map-\\nping functions. IMH [20] learns the latent features of all training\\ndata \\ufb01rstly, which costs expensively. LCMH [26] exploits the intra-\\nmodal correlations by representing data from each modality using\\nits distance to cluster centroids of the training data. Projection ma-\\ntrices are then learned to minimize the distance of relevant data\\n(e.g., image and tags) from different modalities.\\n\\nOther recent works include CMSSH [1], MLBE [25] and LSCM-\\nR [12]. CMSSH uses a boosting method to learn the projection\\nfunction for each dimension of the latent space. However, it re-\\nquires prior knowledge such as semantic relevant and irrelevant\\npairs. MLBE learns the latent features of all training data using a\\nprobabilistic graphic model \\ufb01rstly. Then it learns the latent features\\nof queries based on their correlation with the training data. It does\\nnot consider the original features (e.g., image visual feature or text\\nfeature). Instead, only the correlation of data (both inter-similarity\\nand intra-similarity matrices) are involved in the probabilistic mod-\\nel. However, the label of a query is usually not available in practice,\\nwhich makes it impossible to obtain its correlation with the training\\ndata. LSCMR [12] learns the mapping functions with the objective\\nto optimize the ranking criteria (e.g., MAP) directly. Ranking ex-\\namples (a ranking example is a query and its ranking list) are need-\\ned for training. In our algorithm, we use simple relevant pairs (e.g.,\\n\\nDataset\\n\\nTotal size\\n\\nTraining set\\n\\nValidation set\\n\\nTest set\\n\\nAverage Text Length\\n\\n6\\n\\nNUS-WIDE Wiki\\n\\nFlickr1M\\n\\n190,421\\n\\n2,866\\n\\n1,000,000\\n\\n60,000\\n\\n10,000\\n\\n120,421\\n\\n2,000\\n\\n975,000\\n\\n366\\n\\n500\\n\\n131\\n\\n6,000\\n\\n6,000\\n\\n5\\n\\nimage and its tags) as training input, thus no prior knowledge such\\nas irrelevant pairs, similarity matrix, ranking examples and labels\\nof queries, is needed.\\n\\nMulti-modal deep learning [15, 21] extends Deep Learning to\\nmulti-modal scenario.\\n[21] combines two Deep Boltzman Ma-\\nchines (DBM) (one for image, one for text) with a common latent\\nlayer to construct a Multi-modal DBM. [15] constructs a Bimodal\\ndeep auto-encoder with two deep auto-encoders (one for audio, one\\nfor video). Both two models aim to improve the classi\\ufb01cation ac-\\ncuracy of objects with features from multiple modalities. Thus\\nthey combine different features to learn a good (high dimensional)\\nlatent feature.\\nIn this paper, we aim to represent data with low-\\ndimensional latent features to enable effective and ef\\ufb01cient multi-\\nmodal retrieval, where both the query and database objects may\\nhave features from only one modality.\\n\\n6. EXPERIMENTAL STUDY\\n\\nThis section provides an extensive performance study of our so-\\nlution in comparison with the state-of-the-art methods: CVH [9],\\nCMSSH [1] and LCMH [26] 5. We examine both ef\\ufb01ciency and ef-\\nfectiveness of our method including training overhead, query pro-\\ncessing time and accuracy. All the experiments are conducted on\\nCentOS 6.4 using CUDA 5.0 with NVIDIA GPU (GeForce GTX\\nTITAN). The size of main memory is 64GB and GPU memory is\\n4GB. The code and hyper-parameter setting is available online 6.\\n\\n6.1 Datasets\\n\\nWe evaluate the performance on three benchmark datasets\\u2014NUS-\\n\\nWIDE [2], Wiki [16] and Flickr1M [7].\\n\\nNUS-WIDE The dataset contains 269,648 images from Flickr,\\neach associated with 6 tags in average. We refer to the image and\\nits tags as an image-text pair. There are 81 ground truth concepts\\nmanually annotated for evaluation. Following previous works [11,\\n26], we extract 190,421 image-text pairs annotated with the most\\nfrequent 21 concepts and split them into three subsets for training,\\nvalidation and test respectively. The size of each subset is shown\\nin Table 1. For validation (resp.\\ntest), 100 (resp. 1000) queries\\nare randomly selected from the validation (resp. test) dataset. Im-\\nage and text features have been provided in the dataset [2]. For\\nimages, SIFT features are extracted and clustered into 500 visu-\\nal words. Hence, an image is represented by a 500 dimensional\\nbag-of-visual-words vector. Its associated tags are represented by a\\n1, 000 dimensional tag occurrence vector.\\n\\nWiki This dataset contains 2,866 image-text pairs from Wikipedi-\\na\\u2019s featured articles. An article in Wikipedia contains multiple sec-\\ntions. The text information and its associated image in one sec-\\ntion is considered as an image-text pair. Every image-text pair has\\n\\n5The code and parameter con\\ufb01gurations for CVH and CMSSH are\\navailable online at http://www.cse.ust.hk/\\u02dcdyyeung/\\ncode/mlbe.zip; The code for LCMH is provided by the au-\\nthors. Parameters are set according to the suggestions provided in\\nthe paper.\\n6http://www.comp.nus.edu.sg/\\u02dcwangwei/code\\n\\n655\\n\\n\\x0ca concept inherited from the article\\u2019s category (there are 10 cate-\\ngories in total). We randomly split the dataset into three subsets\\nas shown in Table 1. For validation (resp.\\ntest), we randomly s-\\nelect 50 (resp. 100) pairs from the validation (resp.\\ntest) set as\\nthe query set. Images are represented by 128 dimensional bag-of-\\nvisual-words vectors based on SIFT feature. For text, we construct\\na vocabulary with the most frequent 1,000 words excluding stop\\nwords, and then represent one text section by 1,000 dimensional\\nword count vector like [12]. The average number of words in one\\nsection is 131 which is much higher than that in NUS-WIDE. To\\navoid over\\ufb02ow in Equation 5 and smooth the text input, we normal-\\nize each unit x as log(x + 1) [18].\\n\\nFlickr1M This dataset contains 1 million images associated with\\ntags from Flickr. 25,000 of them are annotated with concepts (there\\nare 38 concepts in total). The image feature is a 3,857 dimensional\\nvector concatenated by SIFT feature, color histogram, and etc [21].\\nLike NUS-WIDE, the text feature is represented by a tag occur-\\nrence vector with 2,000 dimensions. All the image-text pairs with-\\nout annotations are used for training. For validation and test, we\\nrandomly select 6,000 pairs with annotations respectively, among\\nwhich 1,000 pairs are used as queries.\\n\\nBefore training, we use ZCA whitening [8] to normalize each\\ndimension of image feature to have zero mean and unit variance.\\nGiven a query, the ground truth is de\\ufb01ned as:\\nif a result shares\\nat least one common concept with the query, it is considered as a\\nrelevant result; otherwise it is irrelevant.\\n\\n6.2 Evaluation Metrics\\n\\nFirstly, we study the effectiveness of the mapping mechanism.\\nIt is re\\ufb02ected by the effectiveness of the multi-modal search, i.e.,\\nQq\\u2192t(q, t \\u2208 {T, I}), using the mapped latent features 7. Hence,\\nwe use the Mean Average Precision (MAP) [14], one of the stan-\\ndard information retrieval metrics, as the major effectiveness eval-\\nuation metric. Given a set of queries, we \\ufb01rst calculate the Average\\n\\nPrecision (AP) for each query, AP (q) = PR\\n\\nk=1 P (k)\\u03b4(k)/PR\\n\\nj=1 \\u03b4(j),\\n\\nwhere R is the size of the test dataset; \\u03b4(k) = 1 if the k-th result\\nis relevant, otherwise \\u03b4(k) = 0; P (k) is the precision of the result\\nranked at position k, which is the fraction of true relevant docu-\\nments in the top k results. By averaging AP for all queries, we get\\nthe MAP score. The larger the MAP score, the better the search\\nperformance. In addition to MAP, we also measure the precision\\nand recall of search tasks.\\n\\nBesides effectiveness, we also evaluate the training overhead in\\nterms of time cost and memory consumption. In addition, we report\\nthe evaluation on query processing time at last.\\n\\n6.3 Visualization of Training Process\\n\\nIn this section we visualize the training process of MSAE using\\nthe NUS-WIDE dataset as an example to help understand the intu-\\nition of the training algorithm and the setting of the weight param-\\neters, i.e., \\u03b1 and \\u03b2. Our goal is to learn a set of mapping functions\\nsuch that the mapped latent features capture both intra-modal se-\\nmantics and inter-modal semantics well. Generally, the inter-modal\\nsemantics is preserved by minimizing the distance of the latent fea-\\ntures of relevant inter-modal pairs. The intra-modal semantics is\\npreserved by minimizing the reconstruction error of each SAE and\\nthrough inter-modal semantics (see Section 3 for details).\\n\\nFirstly, following the training procedure in Section 3, we train\\na 4-layer image SAE with the dimension of each layer as 500 \\u2192\\n128 \\u2192 16 \\u2192 2 using Algorithm 1. Similarly, a 4-layer text SAE\\n(the structure is 1000 \\u2192 128 \\u2192 16 \\u2192 2) is trained. There is\\n\\n7Without speci\\ufb01cations, searches are conducted against real-valued\\nlatent features using Euclidean distance.\\n\\n656\\n\\nno standard guidlines for setting the number of latent layers and\\nunits in each latent layer for deep learning. In all our experiments,\\nwe simply set the units to be the power of two. The number of\\nlatent layers (normally 2 or 3) is set to be large when the differ-\\nence of dimensionality between the input and latent space is large\\nto avoid great changes of units in connected layers. Latent features\\nof sampled images and text pairs from the validation set are plot-\\nted in Figure 9a. The pre-training stage initializes SAEs to capture\\nregularities of the original features of each modality in the laten-\\nt features. On the one hand, the original features may be of low\\nquality to capture intra-modal semantics. In such a case, the la-\\ntent features would also fail to capture the intra-modal semantics.\\nIn fact, we evaluate the quality of the mapped latent features from\\neach SAE by intra-modal search on the validation dataset. The\\nMAP of the image intra-modal search is about 0.37, while that of\\nthe text intra-modal search is around 0.51. On the other hand, the\\nSAEs are trained separately. Therefore, inter-modal semantics are\\nnot considered. We randomly pick 25 relevant image-text pairs and\\nconnect them with red lines in Figure 9b. We can see the latent fea-\\ntures of most pairs are far away from each other, which indicates\\nthat the inter-modal semantics are not captured by these latent fea-\\ntures. To solve the above problems, we resort to the multi-modal\\ntraining by Algorithm 2. In the following \\ufb01gures, we only plot the\\ndistribution of these 25 pairs for ease of illustration.\\n\\n(a) 300 random pairs\\n\\n(b) 25 pairs connected using red lines\\n\\nFigure 9: Latent Features (Blue circles are image latent features;\\nWhite circles are text latent features)\\n\\nSecondly, we adjust the image SAE with the text SAE \\ufb01xed as\\nline 2 of Algorithm 2 from epoch 1 to epoch 30. One epoch means\\none pass of the whole training dataset. Since the MAP of the image\\nintra-modal search is worse than that of the text intra-modal search,\\naccording to the analysis in Section 3.2, we should use small \\u03b1 to\\ndecrease the weight of image reconstruction error LI\\nr in the ob-\\njective function, i.e., Equation 8. To verify the correctness of the\\nanalysis, we compare the performance of two choices of \\u03b1, namely\\n\\u03b1 = 0 and \\u03b1 = 0.01. The \\ufb01rst two columns of Figure 10 show the\\nlatent features generated by the image SAE after epoch 1 and epoch\\n30. Comparing Figure 10b and 10e (pair by pair), we can see that\\nwith smaller \\u03b1, the image latent features are moved closer to their\\nrelevant text latent features. This is in accordance with Equation 8,\\nwhere smaller \\u03b1 relaxes the restriction on the image reconstruction\\nerror, and in turn increases the weight for Ld. By moving close\\nto relevant text latent features, the image latent features gain more\\nsemantics. As shown in Figure 10c, the MAP curves keep increas-\\ning with the training going on and converge when inter-modal pairs\\nare close enough. QT \\u2192T does not change because the text SAE is\\n\\ufb01xed. Because image latent features are hardly moved close to the\\nrelevant text latent features when \\u03b1 = 0.01 as shown in Figure 10d\\nand 10e, the MAP curves do not increase in Figure 10f. We use\\nthe results with \\u03b1 = 0 to continue the training procedure in the\\nfollowing section.\\n\\n\\x0cTable 2: Mean Average Precision on NUS-WIDE dataset\\n\\nTask\\n\\nQI\\u2192I\\n\\nQT \\u2192T\\n\\nQI\\u2192T\\n\\nQT \\u2192I\\n\\nAlgorithm\\n\\nLCMH CMSSH CVH MSAE LCMH CMSSH CVH MSAE LCMH CMSSH CVH MSAE LCMH CMSSH CVH MSAE\\n\\nDimension of\\n\\nLatent Space\\n\\nL\\n\\n16\\n\\n24\\n\\n32\\n\\n0.353\\n\\n0.343\\n\\n0.343\\n\\n0.355\\n\\n0.356\\n\\n0.357\\n\\n0.365\\n\\n0.358\\n\\n0.354\\n\\n0.417\\n\\n0.412\\n\\n0.413\\n\\n0.373\\n\\n0.373\\n\\n0.374\\n\\n0.400\\n\\n0.402\\n\\n0.403\\n\\n0.374\\n\\n0.364\\n\\n0.357\\n\\n0.498\\n\\n0.480\\n\\n0.470\\n\\n0.328\\n\\n0.333\\n\\n0.333\\n\\n0.391\\n\\n0.388\\n\\n0.382\\n\\n0.359\\n\\n0.351\\n\\n0.345\\n\\n0.447\\n\\n0.444\\n\\n0.402\\n\\n0.331\\n\\n0.323\\n\\n0.324\\n\\n0.337\\n\\n0.336\\n\\n0.335\\n\\n0.368\\n\\n0.360\\n\\n0.355\\n\\n0.432\\n\\n0.427\\n\\n0.435\\n\\n(a) \\u03b1 = 0,epoch 1\\n\\n(b) \\u03b1 = 0,epoch 30\\n\\n(c) \\u03b1 = 0\\n\\nture the regularities (semantics) of the original features. Therefore,\\nboth QT \\u2192T and QI\\u2192T grows gradually. Comparing Figure 11a\\nand 11d, we can see the distance of relevant latent features in Fig-\\nure 11d is larger than that in Figure 11a. The reason is that when \\u03b2\\nis larger, the objective function, i.e., Equation 8, pays more effort\\nto minimize the reconstruction error LT\\nr . Consequently, less effort\\nis paid to minimize the inter-modal distance Ld. Hence, relevant\\ninter-modal pairs cannot be moved closer. This effect is re\\ufb02ect-\\ned as minor changes at epoch 31 in Figure 11f. Similarly, small\\nchanges happen between Figure 11d and 11e, which leads to minor\\nchanges from epoch 32 to 60 in terms of MAP in Figure 11f.\\n\\n(d) \\u03b1 = 0.01,epoch 1 (e) \\u03b1 = 0.01,epoch 30\\n\\n(f) \\u03b1 = 0.01\\n\\n6.4 Evaluation of Model Effectiveness\\n\\nFigure 10: Adjust Image SAE with Different \\u03b1 (best view in color)\\n\\n6.4.1 NUS(cid:173)WIDE dataset\\n\\nWe \\ufb01rst examine the mean average precision (MAP) of our method\\n\\ncompared using Euclidean distance against the real-valued features.\\nLet L be the dimension of the latent space. Our MSAE is con-\\n\\ufb01gured with 3 layers, where the image features are mapped from\\n500 dimensions to 128, and \\ufb01nally to L. Similarly, the dimension\\nof text features are reduced from 1000 \\u2192 128 \\u2192 L by the text\\nSAE. \\u03b1 and \\u03b2 are set to 0 and 0.01 respectively according to Sec-\\ntion 6.3. We test L with values 16, 24 and 32. The results compared\\nwith other methods are reported in Table 2. Our MSAE achieves\\nthe best performance for all the four search tasks. It demonstrates\\nan average improvement of 17%, 27%, 21%, and 26% for QI\\u2192I ,\\nQT \\u2192T ,QI\\u2192T , and QT \\u2192I respectively. CVH and CMSSH prefer\\nsmaller L in queries QI\\u2192T and QT \\u2192I . The reason is that it needs\\nto train far more parameters in higher dimensions and the learned\\nmodels will be farther from the optimal solutions. Our method is\\nless sensitive to the value of L. This is probably because with multi-\\nple layers, MSAE has stronger representation power and can better\\navoid local optimality by a good initialization from unsupervised\\npre-training, and thus is more robust under different L.\\n\\nFigure 12 shows the precision-recall curves, and the recall-candidates\\n\\nratio curves (used by [25, 26]) which show the change of recall\\nwhen inspecting more results on the returned rank list. Due to the\\nspace limitation, we only show the results of QT \\u2192I and QI\\u2192T . We\\nachieve similar trends on results of QT \\u2192T and QI\\u2192I . Our method\\nshows the best accuracy except when recall is 0 8, whose precision\\np implies that the nearest neighbor of the query appears in the 1\\np -th\\nreturned result. This indicates that our method performs the best\\nfor general top-k similarity retrieval except k=1. For the measure\\nof recall-candidates ratio, the curve of our method is always above\\nthose of other methods. It means that we get better recall when in-\\nspecting the same number of objects. In other words, our method\\nranks more relevant objects at the higher (front) positions. Hence,\\nour method is superiror to other methods.\\n\\nBesides real-valued features, we also conduct an experiment a-\\ngainst binary latent features for which Hamming distance is used\\nas the distance function. In our implementation, we choose Spec-\\ntral Hashing [24] to convert real-valued latent feature vectors into\\n\\n8Here, recall r =\\n\\n#all relevant results \\u2248 0.\\n\\n1\\n\\n(a) \\u03b2 = 0.01,epoch 31 (b) \\u03b2 = 0.01,epoch 60\\n\\n(c) \\u03b2 = 0.01\\n\\n(d) \\u03b2 = 0.1,epoch 31\\n\\n(e) \\u03b2 = 0.1,epoch 60\\n\\n(f) \\u03b2 = 0.1\\n\\nFigure 11: Adjust Text SAE with Different \\u03b2 (best view in color)\\n\\nThirdly, according to line 3 of Algorithm 2, we adjust the text\\nSAE with the image SAE \\ufb01xed from epoch 31 to epoch 60. We\\nalso compare two choices of \\u03b2, namely 0.01 and 0.1. Figure 11\\nshows the snapshots of latent features and the MAP curves of each\\nsetting. From Figure 10b to 11a, which are two consecutive snap-\\nshots taken from epoch 30 and 31 respectively, we can see that the\\ntext latent features are moved much close to the relevant image la-\\ntent features. It leads to the big changes at epoch 31 in Figure 11c.\\nFor example, QT \\u2192T drops a lot. This is because the sudden move\\nchanges the intra-modal relationships of text latent features. An-\\nother big change happens on QI\\u2192T , which increases dramatically.\\nThe reason is that when we \\ufb01x the text features from epoch 1 to\\n30, an image feature I is pulled to be close to (or nearest neighbor\\nof) its relevant text feature T . However, T may not be the reverse\\nnearest neighbor of I. In epoch 31, we actually move T such that\\nT is more likely to be the reverse nearest neighbor of I. Hence, the\\nMAP of query QI\\u2192T is greatly improved. On the opposite, QT \\u2192I\\ndecreases. From epoch 32 to epoch 60, the text latent features on\\nthe one hand move close to relevant image latent features slowly,\\non the other hand rebuild their intra-modal relationships. The latter\\nis achieved by minimizing the reconstruction error, i.e., LT\\nr , to cap-\\n\\n657\\n\\n\\x0cTable 3: Mean Average Precision on NUS-WIDE dataset (using Binary Latent Features)\\n\\nTask\\n\\nQI\\u2192I\\n\\nQT \\u2192T\\n\\nQI\\u2192T\\n\\nQT \\u2192I\\n\\nAlgorithm\\n\\nLCMH CMSSH CVH MSAE LCMH CMSSH CVH MSAE LCMH CMSSH CVH MSAE LCMH CMSSH CVH MSAE\\n\\nDimension of\\n\\nLatent Space\\n\\nL\\n\\n16\\n\\n24\\n\\n32\\n\\n0.353\\n\\n0.347\\n\\n0.345\\n\\n0.357\\n\\n0.358\\n\\n0.358\\n\\n0.352\\n\\n0.346\\n\\n0.343\\n\\n0.376\\n\\n0.368\\n\\n0.359\\n\\n0.387\\n\\n0.392\\n\\n0.395\\n\\n0.391\\n\\n0.396\\n\\n0.397\\n\\n0.379\\n\\n0.372\\n\\n0.365\\n\\n0.397\\n\\n0.412\\n\\n0.434\\n\\n0.328\\n\\n0.333\\n\\n0.320\\n\\n0.339\\n\\n0.346\\n\\n0.340\\n\\n0.359\\n\\n0.353\\n\\n0.348\\n\\n0.364\\n\\n0.371\\n\\n0.373\\n\\n0.325\\n\\n0.324\\n\\n0.318\\n\\n0.346\\n\\n0.352\\n\\n0.347\\n\\n0.359\\n\\n0.353\\n\\n0.348\\n\\n0.392\\n\\n0.380\\n\\n0.372\\n\\n(a) QI\\u2192T , L = 16\\n\\n(b) QT \\u2192I , L = 16\\n\\n(c) QI\\u2192T , L = 16\\n\\n(d) QT \\u2192I , L = 16\\n\\n(e) QI\\u2192T , L = 24\\n\\n(f) QT \\u2192I , L = 24\\n\\n(g) QI\\u2192T , L = 24\\n\\n(h) QT \\u2192I , L = 24\\n\\n(i) QI\\u2192T , L = 32\\n\\n(j) QT \\u2192I , L = 32\\n\\n(k) QI\\u2192T , L = 32\\n\\n(l) QT \\u2192I , L = 32\\n\\nFigure 12: Precision-Recall and Recall-Candidates Ratio on NUS-WIDE dataset\\n\\nbinary codes. Other comparison algorithms use their own conver-\\nsion mechanisms. The MAP scores are reported in Table 3. We\\ncan see that 1) MSAE still performs better than other methods. 2)\\nThe MAP scores using Hamming distance is not as good as Eu-\\nclidean distance. This is caused by the information loss resulted\\nfrom converting real-valued features into binary features.\\n\\n6.4.2 Wiki Dataset\\n\\nWe conduct similar evaluations on Wiki dataset as on NUS-\\nWIDE. For MSAE with latent feature of dimension L, the structure\\nof its image SAE is 128 \\u2192 128 \\u2192 L, and the structure of its text\\nSAE is 1000 \\u2192 128 \\u2192 L. Similar to the setting on NUS-WIDE,\\n\\u03b1 is set to 0 due to the low quality of image features, and \\u03b2 is set\\nto 0.01 to make LT\\n\\nr and Ld within the same scale.\\n\\nThe performance is report in Table 4. The MAPs on Wiki dataset\\nare much smaller than those on NUS-WIDE except for QT \\u2192T .\\nThis is because the images of Wiki are of much lower quality.\\nIt contains only 2, 000 images that are highly diversi\\ufb01ed, mak-\\ning it dif\\ufb01cult to capture the semantic relationships between im-\\nages and text. Query task QT \\u2192T is not affected as Wkipedia\\u2019s\\nfeatured articles are well edited and rich in text information.\\nIn\\ngeneral, our method achieves an average improvement of 8.1%,\\n\\n30.4%,32.8%,26.8% for QI\\u2192I , QT \\u2192T ,QI\\u2192T , and QT \\u2192I respec-\\ntively. We do not plot the precision-recall curves and recall-candidates\\nratio curves due to space limitation. Generally, these curves show\\nsimilar trends to those of NUS-WIDE.\\n\\n6.4.3 Flickr1M Dataset\\n\\nWe con\\ufb01gure a 4-layer image SAE for this dataset as 3857 \\u2192\\n1000 \\u2192 128 \\u2192 L, and the text SAE is con\\ufb01gured as 2000 \\u2192\\n1000 \\u2192 128 \\u2192 L. Because the image feature of this dataset\\nconsists of both local and global feature, its quality is better. In\\nfact, the image latent feature performs equally well for intra-modal\\nsearch as the text latent feature. Hence, we set both \\u03b1 and \\u03b2 to\\n0.01.\\n\\nWe compare the MAP of MSAE and CVH in Table 5. MSAE\\noutperforms CVH in most of the search tasks. The results of LCMH\\nand CMSSH cannot be reported as both methods run out of memory\\nin the training stage.\\n\\n6.5 Evaluation of Training Overhead\\n\\nWe use Flickr1M to evaluate the training time and memory con-\\nsumption and report the results in Figure 13. The training cost of\\n\\n658\\n\\n\\x0cTable 4: Mean Average Precision on Wiki dataset\\n\\nTask\\n\\nQI\\u2192I\\n\\nQT \\u2192T\\n\\nQI\\u2192T\\n\\nQT \\u2192I\\n\\nAlgorithm\\n\\nLCMH CMSSH CVH MSAE LCMH CMSSH CVH MSAE LCMH CMSSH CVH MSAE LCMH CMSSH CVH MSAE\\n\\nDimension of\\n\\nLatent Space\\n\\nL\\n\\n16\\n\\n24\\n\\n32\\n\\n0.146\\n\\n0.149\\n\\n0.147\\n\\n0.148\\n\\n0.151\\n\\n0.149\\n\\n0.147\\n\\n0.150\\n\\n0.148\\n\\n0.162\\n\\n0.161\\n\\n0.162\\n\\n0.359\\n\\n0.345\\n\\n0.333\\n\\n0.318\\n\\n0.320\\n\\n0.312\\n\\n0.153\\n\\n0.151\\n\\n0.152\\n\\n0.462\\n\\n0.437\\n\\n0.453\\n\\n0.133\\n\\n0.129\\n\\n0.137\\n\\n0.138\\n\\n0.135\\n\\n0.133\\n\\n0.126\\n\\n0.123\\n\\n0.128\\n\\n0.182\\n\\n0.176\\n\\n0.187\\n\\n0.117\\n\\n0.124\\n\\n0.119\\n\\n0.140\\n\\n0.138\\n\\n0.137\\n\\n0.122\\n\\n0.123\\n\\n0.123\\n\\n0.179\\n\\n0.168\\n\\n0.179\\n\\nTable 5: Mean Average Precision on Flickr1M dataset\\n\\nTask\\n\\nQI\\u2192I\\n\\nQT \\u2192T\\n\\nQI\\u2192T\\n\\nQT \\u2192I\\n\\nAlgorithm\\n\\nCVH MSAE CVH MSAE CVH MSAE CVH MSAE\\n\\nDimension of\\n\\nLatent Space\\n\\nL\\n\\n16\\n\\n24\\n\\n32\\n\\n0.622\\n\\n0.616\\n\\n0.603\\n\\n0.621\\n\\n0.619\\n\\n0.622\\n\\n0.610\\n\\n0.604\\n\\n0.587\\n\\n0.624\\n\\n0.629\\n\\n0.630\\n\\n0.610\\n\\n0.605\\n\\n0.588\\n\\n0.632\\n\\n0.628\\n\\n0.632\\n\\n0.616\\n\\n0.612\\n\\n0.598\\n\\n0.608\\n\\n0.612\\n\\n0.614\\n\\nFigure 14: Querying Time Comparison Using Real-valued and Bi-\\nnary Latent Features\\n\\ntures (based on Hamming distance) respectively. We can see that\\nthe querying time increases linearly with respect to the dataset size\\nfor both binary and real-valued latent features. But, the searching\\nagainst binary latent features is 10\\xd7 faster than that against real-\\nvalued latent features. This is because the computation of Ham-\\nming distance is more ef\\ufb01cient than that of Euclidean distance. By\\ntaking into account the results from effectiveness evaluations, we\\ncan see that there is a trade-off between ef\\ufb01ciency and effectiveness\\nin feature representation. The binary encoding greatly improves the\\nef\\ufb01ciency in the expense of accuracy degradation.\\n\\n7. CONCLUSION\\n\\nIn this paper, we have proposed a new mapping mechanism for\\nmulti-modal retrieval based on the stacked auto-encoders (SAE).\\nOur mapping mechanism, called multi-modal stacked auto-encoders\\n(MSAE), learns a set of SAEs (one for each modality) to map the\\nhigh-dimensional features of different media types (i.e., modalities)\\ninto a common low-dimensional latent space so that metric dis-\\ntance measures can be applied ef\\ufb01ciently. By considering both the\\nintra-modal semantics and the inter-modal semantics in the learn-\\ning objective function, we learn a set of effective SAEs for feature\\nmapping. Compared to existing methods which usually require a\\nsubstantial amount of prior knowledge about the training data, our\\nmethod requires little prior knowledge. Experiment results con-\\n\\ufb01rmed the improvements of our method over previous works in\\nsearch accuracy.\\n\\n8. ACKNOWLEDGMENTS\\n\\nThis work was supported by A*STAR project 1321202073. We\\n\\nwould also like to thank Shenghua Gao for valuable discussions.\\n\\n9. REFERENCES\\n[1] M. M. Bronstein, A. M. Bronstein, F. Michel, and\\n\\nN. Paragios. Data fusion through cross-modality metric\\nlearning using similarity-sensitive hashing. In CVPR, pages\\n3594\\u20133601, 2010.\\n\\n[2] T.-S. Chua, J. Tang, R. Hong, H. Li, Z. Luo, and Y.-T. Zheng.\\n\\nNus-wide: A real-world web image database from national\\nuniversity of singapore. In Proc. of ACM Conf. on Image and\\nVideo Retrieval (CIVR\\u201909), Santorini, Greece., July 8-10,\\n2009.\\n\\n(a)\\n\\n(b)\\n\\nFigure 13: Training Time and Memory Consumption\\n\\nLCMH and CMSSH are not reported because they run out of mem-\\nory on this dataset. We can see that the training time of MSAE\\nand CVH increases linearly with respect to the size of the train-\\ning dataset. Due to the stacked structure and multiple iterations of\\npassing the dataset, MSAE is not as ef\\ufb01cient as CVH. Roughly, the\\noverhead is the number of training iterations times the height of\\nMSAE. Possible solutions for accelerating the MSAE training in-\\nclude adopting Distributed deep learning [3]. We leave this as our\\nfuture work.\\n\\nFigure 13b shows the memory usage of the training process. Giv-\\nen a training dataset, MSAE splits them into mini-batches and con-\\nducts the training batch by batch (see Algorithm 2). It stores the\\nmodel parameters and one mini-batch in memory, both of which\\nare independent of the training dataset size. Hence, the memory\\nusage stays constant when the size of the training dataset increas-\\nes. In fact, the minimum memory usage for MSAE is smaller than\\n10GB. We allocate more space to load multiple mini-batches into\\nmemory to save disk reading cost. For CVH, it has to load all train-\\ning data into memory for matrix operations. Therefore, the memory\\nusage increases with respect to the size of the training dataset.\\n\\n6.6 Evaluation of Query Processing Ef\\ufb01ciency\\nFinally, we compare the ef\\ufb01ciency of query processing using bi-\\nnary latent features and real-valued latent features. Notice that all\\nmethods (i.e., MSAE, CVH, CMSSH and LCMH) perform simi-\\nlarly in query processing after mapping the original data into la-\\ntent features of same dimensions. Data from the Flickr1M train-\\ning dataset is mapped into a 32 dimensional latent space to form\\na large dataset for searching. To speed up the query processing of\\nreal-valued latent features, we create an index (i.e., VA-File [23])\\nfor each modality. For binary latent features, we do not create any\\nindexes, because linear scan is fast enough as shown in Figure 14.\\nIt shows the time (averaged over 100 random queries) of search-\\ning 50 nearest neighbors against datasets represented using binary\\nlatent features (based on Euclidean distance) and real-valued fea-\\n\\n659\\n\\n\\x0c[3] J. Dean, G. Corrado, R. Monga, K. Chen, M. Devin, Q. V.\\n\\nLe, M. Z. Mao, M. Ranzato, A. W. Senior, P. A. Tucker,\\nK. Yang, and A. Y. Ng. Large scale distributed deep\\nnetworks. In NIPS, pages 1232\\u20131240, 2012.\\n\\n[23] R. Weber, H.-J. Schek, and S. Blott. A quantitative analysis\\n\\nand performance study for similarity-search methods in\\nhigh-dimensional spaces. In VLDB, pages 194\\u2013205, 1998.\\n[24] Y. Weiss, A. Torralba, and R. Fergus. Spectral hashing. In\\n\\n[4] R. Goroshin and Y. LeCun. Saturating auto-encoder. CoRR,\\n\\nNIPS, pages 1753\\u20131760, 2008.\\n\\nabs/1301.3577, 2013.\\n\\n[5] G. Hinton. A Practical Guide to Training Restricted\\n\\nBoltzmann Machines. Technical report, 2010.\\n\\n[6] G. R. Hjaltason and H. Samet. Index-driven similarity search\\n\\nin metric spaces. ACM Trans. Database Syst.,\\n28(4):517\\u2013580, 2003.\\n\\n[7] M. J. Huiskes and M. S. Lew. The mir \\ufb02ickr retrieval\\n\\nevaluation. In Multimedia Information Retrieval, pages\\n39\\u201343, 2008.\\n\\n[8] A. Krizhevsky. Learning multiple layers of features from tiny\\n\\nimages. Technical report, 2009.\\n\\n[9] S. Kumar and R. Udupa. Learning hash functions for\\n\\ncross-view similarity search. In IJCAI, pages 1360\\u20131365,\\n2011.\\n\\n[10] Y. LeCun, L. Bottou, G. Orr, and K. M\\xa8uller. Ef\\ufb01cient\\n\\nBackProp. In G. Orr and K.-R. M\\xa8uller, editors, Neural\\nNetworks: Tricks of the Trade, volume 1524 of Lecture Notes\\nin Computer Science, chapter 2, pages 9\\u201350. Springer Berlin\\nHeidelberg, Berlin, Heidelberg, Mar. 1998.\\n\\n[11] W. Liu, J. Wang, S. Kumar, and S.-F. Chang. Hashing with\\n\\ngraphs. In ICML, pages 1\\u20138, 2011.\\n\\n[12] X. Lu, F. Wu, S. Tang, Z. Zhang, X. He, and Y. Zhuang. A\\n\\nlow rank structural large margin method for cross-modal\\nranking. In SIGIR, pages 433\\u2013442, 2013.\\n\\n[13] A. L. Maas, Q. V. Le, T. M. O\\u2019Neil, O. Vinyals, P. Nguyen,\\nand A. Y. Ng. Recurrent neural networks for noise reduction\\nin robust asr. In INTERSPEECH, 2012.\\n\\n[14] C. D. Manning, P. Raghavan, and H. Sch\\xa8utze. Introduction to\\ninformation retrieval, pages 151\\u2013175. Cambridge University\\nPress, 2008.\\n\\n[15] J. Ngiam, A. Khosla, M. Kim, J. Nam, H. Lee, and A. Y. Ng.\\n\\nMultimodal deep learning. In ICML, pages 689\\u2013696, 2011.\\n[16] N. Rasiwasia, J. C. Pereira, E. Coviello, G. Doyle, G. R. G.\\nLanckriet, R. Levy, and N. Vasconcelos. A new approach to\\ncross-modal multimedia retrieval. In ACM Multimedia, pages\\n251\\u2013260, 2010.\\n\\n[17] S. Rifai, P. Vincent, X. Muller, X. Glorot, and Y. Bengio.\\n\\nContractive auto-encoders: Explicit invariance during feature\\nextraction. In ICML, pages 833\\u2013840, 2011.\\n\\n[18] R. Salakhutdinov and G. E. Hinton. Semantic hashing. Int. J.\\n\\nApprox. Reasoning, 50(7):969\\u2013978, 2009.\\n\\n[25] Y. Zhen and D.-Y. Yeung. A probabilistic model for\\n\\nmultimodal hash function learning. In KDD, pages 940\\u2013948,\\n2012.\\n\\n[26] X. Zhu, Z. Huang, H. T. Shen, and X. Zhao. Linear\\n\\ncross-modal hashing for ef\\ufb01cient multimodal search. MM,\\n2013.\\n\\n[27] Y. Zhuang, Y. Yang, and F. Wu. Mining semantic correlation\\nof heterogeneous multimedia data for cross-media retrieval.\\nIEEE Transactions on Multimedia, 10(2):221\\u2013229, 2008.\\n\\nAPPENDIX\\n\\nSince the parameter updating procedures are similar for Algorith-\\nm 1 and 2, we describe only the procedure (bProp) of Algorithm 1\\nin detail. All following equations are in matrix form, and can be\\nveri\\ufb01ed element-wisely. Parameters \\u03b8 are updated according to the\\nStochastic Gradient Descent, i.e., \\u03b8 = \\u03b8 \\u2212 \\u03b3 \\u2217 \\u2202L\\n\\u2202\\u03b8 , where \\u03b3 is\\na hyper-parameter, called learning rate. Speci\\ufb01cally, to calculate\\nthe partial derivative of the objective function L w.r.t. the weight\\nmatrix W and bias b, the partial derivative w.r.t.\\nthe activation\\nZ of each layer is calculated \\ufb01rstly. For layer-wise training, i.e.,\\ntrainNN with h = 1,\\n\\n\\u2202L(B0)\\n\\u2202Z2h\\n\\nEq 7\\n=\\n\\n\\u2202Lr(B0, B2h)\\n\\n\\u2202Z2h\\n\\n(Lr \\u2208 {LI\\n\\nr, LT\\n\\nr })\\n\\n= \\uf8f1\\uf8f2\\n\\uf8f3\\n\\n(B2h \\u2212 B0) \\u2217\\n\\n\\u2202s(Z2h)\\n\\n\\u2202Z2h\\n\\n, otherwise\\n\\n(9a)\\n\\nB2h \\u2212 B0\\n\\n, bottom auto-encoder\\n\\n(9b)\\n\\nEquation 9a is for auto-encoders in the upper layers as shown in\\nFigure 2. We use the Sigmoid function for se() and sd(), uniform-\\nly denoted as s(). The partial derivative of the Sigmoid function\\nis s(Z) \\u2217 (1 \\u2212 s(Z)), where \\u2217 stands for element-wise multiplica-\\ntion. For the bottom auto-encoder, it has modal speci\\ufb01c activation\\nfunction for the reconstruction layer and error function, thus has\\ndifferent partial derivatives, as shown by Equation 9b. The bottom\\nauto-encoders for the image modality and the text modality share\\nthe same partial derivative by coincidence. For the \\ufb01ne-tuning of\\nthe SAE, the activation function of the last reconstruction layer and\\nerror function are the same to those of the bottom auto-encoder\\nin layer-wise training respectively. Hence it has the same partial\\nderivative, i.e., Equation 9b.\\n\\nWith the above partial derivative, denoted as \\u03b42h, we calculate\\n\\n[19] R. Socher, J. Pennington, E. H. Huang, A. Y. Ng, and C. D.\\n\\nthe partial derivative for W and b in the i-th layer of Figure 7 as,\\n\\nManning. Semi-supervised recursive autoencoders for\\npredicting sentiment distributions. In EMNLP, pages\\n151\\u2013161, 2011.\\n\\n[20] J. Song, Y. Yang, Y. Yang, Z. Huang, and H. T. Shen.\\n\\nInter-media hashing for large-scale retrieval from\\nheterogeneous data sources. In SIGMOD Conference, pages\\n785\\u2013796, 2013.\\n\\n[21] N. Srivastava and R. Salakhutdinov. Multimodal learning\\n\\nwith deep boltzmann machines. In NIPS, pages 2231\\u20132239,\\n2012.\\n\\n[22] P. Vincent, H. Larochelle, Y. Bengio, and P.-A. Manzagol.\\nExtracting and composing robust features with denoising\\nautoencoders. In ICML, pages 1096\\u20131103, 2008.\\n\\n\\u2202L(B0)\\n\\n\\u2202W i\\n\\nEq 7\\n=\\n\\n\\u2202Lr(B0, B2h)\\n\\n\\u2202W i\\n\\n+ W i \\u2217 \\u03be\\n\\n= Bi\\u22121T\\n\\n\\u03b4i + W i \\u2217 \\u03be\\n\\n\\u2202L(B0)\\n\\n\\u2202bi\\n\\nEq 7\\n=\\n\\n\\u2202Lr(B0, B2h)\\n\\n\\u2202bi\\n\\n\\u03b4i\\nj\\n\\n= X\\n\\nj\\n\\nTo update parameters in the (i \\u2212 1)-th layer, we have to calculate\\n\\nthe \\u03b4i\\u22121 \\ufb01rstly,\\n\\n\\u03b4i\\u22121 =\\n\\n\\u2202L(B0)\\n\\u2202Z i\\u22121 = \\u03b4iW iT\\n\\n\\u2217\\n\\n\\u2202s(Z i\\u22121)\\n\\n\\u2202Z i\\u22121\\n\\n660\\n\\n\\x0c', u'Deep Learning for Detecting Robotic Grasps\\n\\nIan Lenz,\\u2020 Honglak Lee,\\u2217 and Ashutosh Saxena\\u2020\\n\\n\\u2020 Department of Computer Science, Cornell University.\\n\\n\\u2217 Department of EECS, University of Michigan, Ann Arbor.\\n\\nEmail: ianlenz@cs.cornell.edu, honglak@eecs.umich.edu, asaxena@cs.cornell.edu\\n\\n4\\n1\\n0\\n2\\n\\n \\n\\ng\\nu\\nA\\n1\\n2\\n\\n \\n\\n \\n \\n]\\n\\nG\\nL\\n.\\ns\\nc\\n[\\n \\n \\n\\n6\\nv\\n2\\n9\\n5\\n3\\n\\n.\\n\\n1\\n0\\n3\\n1\\n:\\nv\\ni\\nX\\nr\\na\\n\\nAbstract\\u2014We consider the problem of detecting robotic grasps\\nin an RGB-D view of a scene containing objects. In this work,\\nwe apply a deep learning approach to solve this problem, which\\navoids time-consuming hand-design of features. This presents two\\nmain challenges. First, we need to evaluate a huge number of\\ncandidate grasps. In order to make detection fast and robust,\\nwe present a two-step cascaded system with two deep networks,\\nwhere the top detections from the \\ufb01rst are re-evaluated by\\nthe second. The \\ufb01rst network has fewer features, is faster to\\nrun, and can effectively prune out unlikely candidate grasps.\\nThe second, with more features,\\nis slower but has to run\\nonly on the top few detections. Second, we need to handle\\nmultimodal inputs effectively, for which we present a method\\nthat applies structured regularization on the weights based on\\nmultimodal group regularization. We show that our method\\nimproves performance on an RGBD robotic grasping dataset,\\nand can be used to successfully execute grasps on two different\\nrobotic platforms. 1\\n\\nKeywords: Robotic Grasping, deep learning, RGB-D multi-\\nmodal data, Baxter, PR2, 3D feature learning.\\n\\nI. INTRODUCTION\\n\\nRobotic grasping is a challenging problem involving percep-\\ntion, planning, and control. Some recent works [54, 56, 28, 67]\\naddress the perception aspect of this problem by converting it\\ninto a detection problem in which, given a noisy, partial view\\nof the object from a camera, the goal is to infer the top loca-\\ntions where a robotic gripper could be placed (see Figure 1).\\nUnlike generic vision problems based on static images, such\\nrobotic perception problems are often used in closed loop with\\ncontrollers, so there are stringent requirements on performance\\nand computational speed. In the past, hand-designing features\\nhas been the most popular method for several robotic tasks\\n[40, 32]. However, this is cumbersome and time-consuming,\\nespecially when we must incorporate new input modalities\\nsuch as RGB-D cameras.\\n\\nRecent methods based on deep learning [1] have demon-\\nstrated state-of-the-art performance in a wide variety of tasks,\\nincluding visual recognition [35, 60], audio recognition [39,\\n41], and natural language processing [12]. These techniques\\nare especially powerful because they are capable of learning\\nuseful features directly from both unlabeled and labeled data,\\navoiding the need for hand-engineering.\\n\\n1Parts of this work were presented at ICLR 2013 as a workshop paper,\\nand at RSS 2013 as a conference paper. This version includes signi\\ufb01cantly\\nextended related work, algorithmic descriptions, and extensive robotic exper-\\niments which were not present in previous versions.\\n\\nHowever, most work in deep learning has been applied in\\nthe context of recognition. Grasping is inherently a detection\\nproblem, and previous applications of deep learning to detec-\\ntion have typically focused on speci\\ufb01c vision applications such\\nas face detection [45] and pedestrian detection [57]. Our goal\\nis not only to infer a viable grasp, but to infer the optimal grasp\\nfor a given object that maximizes the chance of successfully\\ngrasping it, which differs signi\\ufb01cantly from the problem of\\nobject detection. Thus, the \\ufb01rst major contribution of our work\\nis to apply deep learning to the problem of robotic grasping, in\\na fashion which could generalize to similar detection problems.\\nThe second major contribution of our work is to propose\\na new method for handling multimodal data in the context\\nof feature learning. The use of RGB-D data, as opposed\\nto simple 2D image data, has been shown to signi\\ufb01cantly\\nimprove grasp detection results [28, 14, 56]. In this work, we\\npresent a multimodal feature learning algorithm which adds a\\nstructured regularization penalty to the objective function to\\nbe optimized during learning. As opposed to previous works\\nin deep learning, which either ignore modality information at\\nthe \\ufb01rst layer (i.e., encourage all features to use all modalities)\\n[59] or train separate \\ufb01rst-layer features for each modality\\n[43, 61], our approach allows for a middle-ground in which\\neach feature is encouraged to use only a subset of the input\\nmodalities, but is not forced to use only particular ones.\\n\\nIn summary, the contributions of this paper are:\\n\\u2022 We present a deep learning algorithm for detecting\\n\\nWe also propose a two-stage cascaded detection system\\nbased on deep learning. Here, we use fewer features for the\\n\\ufb01rst pass, providing faster, but only approximately accurate\\ndetections. The second pass uses more features, giving more\\naccurate detections. In our experiments, we found that the\\n\\ufb01rst deep network, with fewer features, was better at avoiding\\nover\\ufb01tting but less accurate. We feed the top-ranked rectangles\\nfrom the \\ufb01rst layer into the second layer, leading to robust\\nearly rejection of false positives. Unlike manually designed\\ntwo-step features as in [28], our method uses deep learning,\\nwhich allows us to learn detectors that not only give higher\\nperformance, but are also computationally ef\\ufb01cient.\\n\\nWe test our approach on a challenging dataset, where\\nwe show that our algorithm improves both recognition and\\ndetection performance for grasping rectangle data. We also\\nshow that our two-stage approach is not only able to match\\nthe performance of a single-stage system, but, in fact, improves\\nresults while signi\\ufb01cantly reducing the computational time\\nneeded for detection.\\n\\n\\x0cFig. 1: Detecting robotic grasps: Left: A cluttered lab scene labeled with rectangles corresponding to robotic grasps for objects in the\\nscene. Green lines correspond to robotic gripper plates. We use a two-stage system based on deep learning to learn features and perform\\ndetection for robotic grasping. Center: Our Baxter robot \\u201cYogi\\u201d successfully executing a grasp detected by our algorithm. Right: The grasp\\ndetected for this case, in the RGB (top) and depth (bottom) images obtained from Kinect.\\n\\nrobotic grasps. To the best of our knowledge, this is the\\n\\ufb01rst work to do so.\\n\\n\\u2022 In order to handle multimodal inputs, we present a new\\nway to apply structured regularization to the weights to\\nthese inputs based on multimodal group regularization.\\n\\u2022 We present a multi-step cascaded system for detection,\\n\\nsigni\\ufb01cantly reducing its computational cost.\\n\\n\\u2022 Our method outperforms the state-of-the-art for rectangle-\\nbased grasp detection, as well as previous deep learning\\nalgorithms.\\n\\n\\u2022 We implement our algorithm on both a Baxter and a\\nPR2 robot, and show success rates of 84% and 89%,\\nrespectively, for executing grasps on a highly varied set\\nof objects.\\n\\nThe rest of the paper is organized as follows: We discuss\\nrelated work in Section II. We present our two-step cascaded\\ndetection system in Section III, and some additional details\\nin Section IV. We then describe our feature learning algo-\\nrithm and structured regularization method in Section V. We\\npresent our experiments in Section VI, and discuss results in\\nSection VII. We then present experiments on both Baxter and\\nPR2 robots in Section VIII. We present several interesting\\ndirections for future work in Section IX, then conclude in\\nSection X.\\n\\nII. RELATED WORK\\n\\nA. Robotic Grasping\\n\\nIn this section, we will focus on perception- and learning-\\nbased approaches for robotic grasping. For a more complete\\nreview of the \\ufb01eld, we refer the reader to review papers by\\nBohg et al. [4], Sahbani et al. [53], Bicchi and Kumar [2] and\\nShimoga [58].\\n\\nMost works de\\ufb01ne a \\u201cgrasp\\u201d as an end-effector con\\ufb01g-\\nuration which achieves partial or complete form- or force-\\nclosure of a given object. This is a challenging problem\\nbecause it depends on the pose and con\\ufb01guration of the robotic\\ngripper as well as the shape and physical properties of the\\nobject to be grasped, and typically requires a search over a\\nlarge number of possible gripper con\\ufb01gurations. Early works\\n\\n[34, 44, 49] focused on testing for form- and force-closure,\\nand synthesizing grasps ful\\ufb01lling these properties according to\\nsome hand-designed \\u201cquality score\\u201d [17]. More recent works\\nhave re\\ufb01ned these de\\ufb01nitions [50]. These works assumed full\\nknowledge of object shape and physical properties.\\nGrasping Given 3D Model: Fast synthesis of grasps for\\nknown 3D models remains an active research topic [14, 20,\\n65], with recent methods using advanced physical simulation\\nto \\ufb01nd optimal grasps. Gallegos et al. [18] performed opti-\\nmization of grasps given both a 3D model of the object to be\\ngrasped and the desired contact points for the robotic gripper.\\nPokorny et al. [48] de\\ufb01ne spaces of graspable objects, then\\nmap new objects to these spaces to discover grasps. However,\\nthese works are only applicable when the full 3D model of\\nthe object is exactly known, which may not be the case when\\na robot is interacting with a new environment. We note that\\nsome of these physics-based approaches might be combined\\nwith our approach in a multi-pass system, discussed further in\\nSec. IX.\\nSensing for Grasping: In a real-world robotic setting, a robot\\nwill not have full knowledge of the 3D model and pose of an\\nobject to be grasped, but rather only incomplete information\\nfrom some set of sensors such as color or depth cameras,\\ntactile sensors, etc. This makes the problem of grasping\\nsigni\\ufb01cantly more challenging [4], as the algorithm must use\\nmore limited and potentially noisier information to detect a\\ngood grasp. While some works [10, 46] simply attempt to\\nestimate the poses of known objects and then apply full-model\\ngrasping algorithms based on these results, others avoid this\\nassumption, functioning on novel objects which the algorithm\\nhas not seen before.\\n\\nSuch works often made use of other simplifying assump-\\ntions, such as assuming that objects belong to one of a\\nset of primitive shapes [47, 6], or are planar [42]. Other\\nworks produced impressive results for speci\\ufb01c cases, such as\\ngrasping the corners of towels [40]. While such works escape\\nthe assumption of a fully-known object model, hand-coded\\ngrasping rules have a hard time dealing with the wide range\\nof objects seen in real-world human environments, and are\\n\\n\\x0cdif\\ufb01cult and time-consuming to create.\\nLearning for Grasping: Machine learning methods have\\nproven effective for a wide range of perception problems\\n[64, 22, 38, 59, 3], allowing a perception system to learn\\na mapping from some feature set to various visual proper-\\nties. Early work by Kamon et al. [31] showed that learning\\napproaches could also be applied to the problem of grasping\\nfrom vision, introducing a learning component to grasp quality\\nscores.\\n\\nRecent works have employed richer features and learning\\nmethods, allowing robots to grasp known objects which might\\nbe partially occluded [27] or in an unknown pose [13] as\\nwell as fully novel objects which the system has not seen\\nbefore [54]. Here, we will address the latter case. Earlier\\nwork focused on detecting only a single grasping point from\\n2D partial-view data, using heuristic methods to determine\\na gripper pose based on this point. [55]. The use of 3D\\ndata was shown to signi\\ufb01cantly improve these results [56]\\nthanks to giving direct physical information about the object\\nin question. With the advent of low-cost RGB-D sensors such\\nas the Kinect, the use of depth data for robotic grasping has\\nbecome ubiquitous.\\n\\nSeveral other works attempted to use the learning algorithm\\nto more fully constrain the detected grasps. Ekvall and Kragic\\n[15] and Huebner and Kragic [23] used shape-based approxi-\\nmations as bases for learning algorithms which directly gave\\nan approach vector. Le et al. [36] treated grasp detection as a\\nranking problem over sets of contact points in image space.\\nJiang et al. [28] represented a grasp as a 2D oriented rectangle\\nin image space, with two edges corresponding to the gripper\\nplates, using surface normals to determine the grasp approach\\nvector. These approaches allow the detection algorithm to\\ndetect more exactly the gripper pose which should be used\\nfor grasping. In this work, we will follow the rectangle-based\\nmethod.\\n\\nLearning-based approaches have shown impressive results\\nin grasping novel objects, showing that learning some param-\\neters of the detection system can outperform human tuning.\\nHowever, these approaches still require a signi\\ufb01cant degree of\\nhand-engineering in the form of designing good input features.\\nOther Applications with RGBD Data. Due to the avail-\\nability of inexpensive depth sensors, RGB-D data has been a\\nsigni\\ufb01cant research focus in recent years for various robotics\\napplications. For example, Jiang et al. [30] consider robotic\\nplacement of objects, while Teuliere and Marchand [63] used\\nRGB-D data for visual servoing. Several works,\\nincluding\\nthose of Endres et al. [16] and Whelan et al. [66] have ex-\\ntended and improved Simultaneous Localization and Mapping\\n(SLAM) for RGB-D data. Object detection and recognition\\nhas been a major focus in research on RGB-D data [11, 33, 7].\\nMost such works use hand-engineered features such as [52].\\nThe few works that perform feature learning for RGB-D data\\n[59, 3] largely ignore the multimodal nature of the data, not\\ndistinguishing the color and depth channels. Here, we present\\na structured regularization approach which allows us to learn\\n\\nmore robust features for RGB-D and other multimodal data.\\n\\nB. Deep Learning\\n\\nDeep learning approaches have demonstrated the ability to\\nlearn useful features directly from data for a wide variety of\\ntasks. Early work by Hinton and Salakhutdinov [22] showed\\nthat a deep network trained on images of hand-written digits\\nwill learn features corresponding to pen-strokes. Later work\\nusing localized convolutional features [38] showed that these\\nnetworks learn features corresponding to object parts when\\ntrained on natural images. This demonstrates that even the\\nbasic features learned by these systems will adapt to the data\\ngiven. In fact, these approaches are not restricted to the visual\\ndomain, but rather have been shown to learn useful features for\\na wide range of domains, such as audio [39, 41] and natural\\nlanguage data [12].\\nDeep Learning for Detection: However, the vast majority\\nof work in deep learning focuses on classi\\ufb01cation problems.\\nOnly a handful of previous works have applied these methods\\nto detection problems [45, 37, 9]. For example, Osadchy et al.\\n[45] and LeCun et al. [37] applied a deep energy-based model\\nto the problem of face detection, Sermanet et al. [57] applied\\na convolutional neural network for pedestrian detection, and\\nCoates et al. [9] used a deep learning approach to detect text in\\nimages. Girshick et al. [19] used learned convolutional features\\nover image regions for object detection, while Szegedy et al.\\n[62] used a multi-scale approach based on deep networks for\\nthe same task.\\n\\nin which the goal\\n\\nAll these approaches focused on object detection and similar\\nproblems,\\nis to \\ufb01nd a bounding box\\nwhich tightly contains the item to be detected, and for each\\nitem, all valid bounding boxes will be similar. However, in\\nrobotic grasp detection, there may be several valid grasps for\\nan object in different regions, making it more important to\\nselect the one with the highest chance success. In addition,\\norientation matters much more to robotic grasp detection, as\\nmost grasps will only be viable for a small subset of the\\npossible gripper orientations. Our approach to grasp detection\\nwill also generalize across object classes, and even to classes\\nnever seen before by the system, as opposed to the class-\\nspeci\\ufb01c nature of object detection.\\nMultimodal Deep Learning: Recent works in deep learning\\nhave extended these methods to handle multiple modalities\\nof input data, such as audio and video [43], text and image\\ndata [61], and even RGB-D data [59, 3]. However, all of these\\napproaches have fallen into two camps - either learning com-\\npletely separate low-level features for each modality [43, 61],\\nor simply concatenating the modalities [59, 3]. The former\\napproaches have proven effective for data where the basic\\nmodalities differ signi\\ufb01cantly, such as the aforementioned case\\nof text and images, while the latter is more effective in cases\\nwhere the modalities are more similar, such as RGB-D data.\\nFor some new combinations of modalities and tasks, it\\nmay not be clear which of these approaches will give better\\nperformance. In fact, in the ideal feature set, different features\\n\\n\\x0cFig. 2: Detecting and executing grasps: From left to right: Our system obtains an RGB-D image from a Kinect mounted on the robot,\\nand searches over a large space of possible grasps, for which some candidates are shown. For each of these, it extracts a set of raw features\\ncorresponding to the color and depth images and surface normals, then uses these as inputs to a deep network which scores each rectangle.\\nFinally, the top-ranked rectangle is selected and the corresponding grasp is executed using the parameters of the detected rectangle and the\\nsurface normal at its center. Red and green lines correspond to gripper plates, blue in RGB-D features indicates masked-out pixels.\\n\\nmay use different subsets of the modalities. In this work, we\\nwill give a structured regularization method which guides the\\nlearning algorithm to select such subsets, without imposing\\nhard constraints on network structure.\\nStructured Learning and Structured Regularization: Sev-\\neral approaches have been proposed which attempt to use a\\nspecially-designed regularization function to impose structure\\non a set of learned parameters without directly enforcing it.\\nJalali et al. [26] used a group regularization function in the\\nmultitask learning setting, where one set of features is used for\\nmultiple tasks. This function applies high-order regularization\\nseparately to particular groups of parameters. Their function\\nregularized the number of features used for each task in a set of\\nmulti-class classi\\ufb01cation tasks solved by softmax regression.\\nIntuitively, this encodes the belief that only some subset of\\nthe input features will be useful for each task, but this set of\\nuseful features might vary between tasks.\\n\\nA few works have also explored the use of structured\\nregularization in deep learning. The Topographic ICA algo-\\nrithm [24] is a feature-learning approach that applies a similar\\npenalty term to feature activations, but not to the weights\\nthemselves. Coates and Ng [8] investigate the problem of\\nselecting receptive \\ufb01elds, i.e., subsets of the input features\\nto be used together in a higher-level feature. The structure\\nof the network is learned \\ufb01rst, then \\ufb01xed before learning the\\nparameters of the network.\\n\\nIII. DEEP LEARNING FOR GRASP DETECTION:\\n\\nSYSTEM AND MODEL\\n\\nIn this work, we will present an algorithm for robotic grasp\\ndetection from a single RGB-D view. Our approach will be\\nbased on machine learning, but distinguish itself from previous\\napproaches by learning not only the weights used to rank\\nprospective grasps, but also the features used to rank them,\\nwhich were previously hand-engineered.\\n\\nWe will do this using deep learning methods, learning a\\nset of RGB-D features which will be extracted from each\\n\\ncandidate grasp, then used to score that grasp. Our approach\\nwill include a structured multimodal regularization method\\nwhich improves the quality of the features learned from\\nRGB-D data without constraining network structure.\\n\\nIn our system for robotic grasping, as shown in Fig. 2, the\\nrobot \\ufb01rst obtains an RGB-D image of the scene containing\\nobjects to be grasped. A small deep network is used to score\\npotential grasps in this image, and a small candidate set of the\\ntop-ranked grasps is provided to a larger deep network, which\\nyields a single best-ranked grasp.\\n\\nIn this work, we will represent potential grasps using\\noriented rectangles in the image plane as seen on the left in\\nFig. 2, with one pair of parallel edges corresponding to the\\nrobotic gripper [28]. Each rectangle is thus parameterized by\\nthe X and Y coordinates of its upper-left corner, its width,\\nheight, and orientation in the image plane, giving a \\ufb01ve-\\ndimensional search space for potential grasps. Grasps will be\\nranked based on features extracted from the RGB-D image\\nregion contained inside their corresponding rectangle, aligned\\nto the gripper plates, as seen in the center of Fig. 2.\\n\\nTo translate a rectangle such as that shown on the right in\\nFig. 2 into a gripper pose for grasping we \\ufb01nd the point with\\nthe minimum depth inside the central third (horizontally) of\\nthe rectangle. We then use the averaged surface normal around\\nthis point to determine the approach vector for the gripper.\\nThe orientation of the detected rectangle is translated to a\\nrotation around this vector to orient the gripper. We use the\\nX-Y coordinates of the rectangle center along with the depth\\nof the closest point to determine a grasping point in the robot\\u2019s\\ncoordinate frame. We compute a pre-grasp position by shifting\\n10 cm back from the grasping point along this approach vector\\nand position the gripper at this point. We then approach the\\nobject along the approach vector and grasp it.\\n\\nUsing a standard feature learning approach such as sparse\\nauto-encoder [21], a deep network can be trained for the\\nproblem of grasping rectangle recognition (i.e., does a given\\nrectangle in image space correspond to a valid robotic grasp?).\\n\\n\\x0cFig. 3: Illustration of our two-stage detection process: Given an image of an object to grasp, a small deep network is used to exhaustively\\nsearch potential rectangles, producing a small set of top-ranked rectangles. A larger deep network is then used to \\ufb01nd the top-ranked rectangle\\nfrom these candidates, producing a single optimal grasp for the given object.\\n\\ndetections for the second stage. Using deep learning allows us\\nto circumvent the costly manual design of features by simply\\ntraining networks of two different sizes, using the smaller for\\nthe exhaustive \\ufb01rst pass, and the larger to re-rank the candidate\\ndetection results.\\nModel: To detect robotic grasps from the rectangle repre-\\nsentation, we model the probability of a rectangle G(t), with\\nfeatures x(t) \\u2208 RN being graspable, using a random variable\\n\\u02c6y(t) \\u2208 {0, 1} which indicates whether or not we predict G(t) to\\nbe graspable. We use a deep network, as shown in Fig. 4-left,\\nwith two layers of sigmoidal hidden units h[1] and h[2], with\\nK1 and K2 units per layer, respectively. A logistic classi\\ufb01er\\nover the outputs of the second-layer hidden units then predicts\\nP (\\u02c6y(t)|x(t); \\u0398), so chosen because ground-truth graspability is\\nrepresented as binary. Each layer (cid:96) will have a set of weights\\nW [(cid:96)] mapping from its inputs to its hidden units, so the param-\\neters of our model are \\u0398 = {W [1], W [2], W [3]}. Each hidden\\nunit forms output by a sigmoid \\u03c3(a) = 1/(1 + exp(\\u2212a)) over\\nits weighted input:\\n\\n(cid:32) N(cid:88)\\n(cid:32) K1(cid:88)\\n(cid:32) K2(cid:88)\\n\\ni=1\\n\\ni=1\\n\\nx(t)\\ni W [1]\\n\\ni,j\\n\\nh[1](t)\\ni W [2]\\n\\ni,j\\n\\n(cid:33)\\n\\n(cid:33)\\n(cid:33)\\n\\nh[1](t)\\nj\\n\\n= \\u03c3\\n\\nh[2](t)\\nj\\n\\n= \\u03c3\\n\\nP (\\u02c6y(t) = 1|x(t); \\u0398) = \\u03c3\\n\\nh[2](t)\\ni W [3]\\n\\ni\\n\\n(1)\\n\\ni=1\\n\\nA. Inference and Learning\\n\\nDuring inference, our goal is to \\ufb01nd the single grasping\\nrectangle with the maximum probability of being graspable for\\nsome new object. With G representing a particular grasping\\nrectangle position, orientation, and size, we \\ufb01nd this best\\nrectangle as:\\n\\nG\\u2217 = arg max\\n\\nP (\\u02c6y(t) = 1|\\u03c6(G); \\u0398)\\n\\n(2)\\n\\nG\\n\\nHere, the function \\u03c6 extracts the appropriate input representa-\\ntion for rectangle G.\\n\\nDuring learning, our goal is to learn the parameters \\u0398 that\\noptimize the recognition accuracy of our system. Here, input\\ndata is given as a set of pairs of features x(t) \\u2208 RN and\\n\\nFig. 4: Deep network and auto-encoder: Left: A deep network\\nwith two hidden layers, which transform the input representation,\\nand a logistic classi\\ufb01er at the top layer, which uses the features\\nfrom the second hidden layer to predict the probability of a grasp\\nbeing feasible. Right: An auto-encoder, used for pretraining. A set of\\nweights projects input features to a hidden layer. The same weights\\nare then used to project these hidden unit outputs to a reconstruction\\nof the inputs. In the sparse auto-encoder (SAE) algorithm, the hidden\\nunit activations are also penalized.\\n\\nHowever, in a real-world robotic setting, our system needs to\\nperform detection (i.e., given an image containing an object,\\nhow should the robot grasp it?). This task is signi\\ufb01cantly more\\nchallenging than simple recognition.\\nTwo-stage Cascaded Detection:\\nIn order to perform detec-\\ntion, one naive approach could be to consider each possible\\noriented rectangle in the image (perhaps discretized to some\\nlevel), and evaluate each rectangle with a deep network\\ntrained for recognition. However, such near-exhaustive search\\nof possible rectangles (based on positions, sizes, and orienta-\\ntions) can be quite expensive in practice for real-time robotic\\ngrasping.\\n\\nMotivated by multi-step cascaded approaches in previous\\nwork [28, 64], we instead take a two-stage approach to\\ndetection: First, we use a reduced feature set to determine\\na set of top candidates. Then, we use a larger, more robust\\nfeature set to rank these candidates.\\n\\nHowever, these approaches require the design of two sepa-\\nrate sets of features. In particular, it can be dif\\ufb01cult to manually\\ndesign a small set of \\ufb01rst-stage features which is both quick to\\ncompute and robust enough to produce a good set of candidate\\n\\n\\x0cground-truth labels y(t) \\u2208 {0, 1} for t = 1, . . . , M. As in most\\ndeep learning works, we use a two-phase learning approach.\\nIn the \\ufb01rst phase, we will use unsupervised feature learning\\nto initialize the hidden-layer weights W [1] and W [2]. Pre-\\ntraining weights this way is critical to avoid over\\ufb01tting. We\\nwill use a variant of a sparse auto-encoder (SAE) [21], as\\nillustrated in Fig. 4-right. We de\\ufb01ne g(h) as a sparsity penalty\\nfunction over hidden unit activations, with \\u03bb controlling its\\nweight. With f (W ) as a regularization function, weighted\\nby \\u03b2, and \\u02c6x(t) as the reconstruction of x(t), SAE solves the\\nfollowing to initialize hidden-layer weights:\\n\\nM(cid:88)\\n\\nK(cid:88)\\n\\nW \\u2217 = arg min\\n\\n(||\\u02c6x(t) \\u2212 x(t)||2\\n\\n2 + \\u03bb\\n\\nt=1\\n\\nj=1\\n\\nx(t)\\ni Wi,j)\\n\\nh(t)\\nj = \\u03c3(\\n\\nW\\n\\nN(cid:88)\\nK(cid:88)\\n\\ni=1\\n\\nj=1\\n\\n\\u02c6x(t)\\ni =\\n\\nh(t)\\nj Wi,j\\n\\nFig. 5: Preserving aspect ratio: Left: a pair of sunglasses with\\na potential grasping rectangle. Red edges indicate gripper plates.\\nCenter: image taken from the rectangle and rescaled to \\ufb01t a square\\naspect ratio. Right: same image, padded and centered in the receptive\\n\\ufb01eld. Blue areas indicate masked-out padding. When rescaled, the\\nrectangle incorrectly appears graspable. Preserving aspect ratio and\\npadding allows the rectangle to correctly appear non-graspable.\\n\\ng(h(t)\\n\\nj )) + \\u03b2f (W )\\n\\n(3)\\n\\nFig. 6: Improvement from mask-based scaling: Left: Result with-\\nout mask-based scaling. Right: Result with mask-based scaling.\\n\\nM(cid:88)\\n\\nt=1\\n\\nWe \\ufb01rst use this algorithm to initialize W [1] to reconstruct x.\\nWe then \\ufb01x W [1] and learn W [2] to reconstruct h[1].\\n\\nDuring the supervised phase of the learning algorithm, we\\nthen jointly learn classi\\ufb01er weights W [3] and \\ufb01ne-tune hidden\\nlayer weights W [1] and W [2] for recognition. We maximize the\\nlog-likelihood of the data along with regularization penalties\\non hidden layer weights:\\n\\n\\u0398\\u2217 = arg max\\n\\n\\u0398\\n\\nlog P (\\u02c6y(t) = y(t)|x(t); \\u0398)\\n\\u2212 \\u03b21f (W [1]) \\u2212 \\u03b22f (W [2])\\n\\n(4)\\n\\nTwo-stage Detection Model: During inference for two-stage\\ndetection, we will \\ufb01rst use a smaller network to produce a set\\nof the top T rectangles with the highest probability of being\\ngraspable according to network parameters \\u03981. We will then\\nuse a larger network with a separate set of parameters \\u03982 to\\nre-rank these T rectangles and obtain a single best one. The\\nonly change to learning for the two-stage model is that these\\ntwo sets of parameters are learned separately, using the same\\napproach.\\n\\nIV. SYSTEM DETAILS\\n\\nIn this section, we will de\\ufb01ne the set of raw features which\\nour system will use, forming x in the equations above, and how\\nthey are extracted from an RGB-D image. Some examples of\\nthese features are shown in Fig 2.\\n\\nOur algorithm uses only local information - speci\\ufb01cally, we\\nextract the RGB-D sub-image contained within each rectangle,\\nand use this to generate features for that rectangle. This image\\nis rotated so that its left and right edges correspond to the\\ngripper plates, and then re-scaled to \\ufb01t inside the network\\u2019s\\nreceptive \\ufb01eld.\\n\\nFrom this 24x24 pixel image, seven channels\\u2019 worth of\\nfeatures are extracted, giving 24x24x7 = 4032 input features.\\n\\nThe \\ufb01rst three channels are the image in YUV color space,\\nused because it represents image intensity and color separately.\\nThe next is simply the depth channel of the image. The last\\nthree are the X, Y, and Z components of surface normals\\ncomputed based on the depth channel. These are computed\\nafter the image is aligned to the gripper so that they are always\\nrelative to the gripper plates.\\n\\nA. Data Pre-Processing\\n\\nWhitening data is critical for deep learning approaches to\\nwork well, especially in cases such as multimodal data where\\nthe statistics of the input data may vary greatly. While PCA-\\nbased approaches have been shown to be effective [25], they\\nare dif\\ufb01cult to apply in cases such as ours where large portions\\nof the data may be masked out.\\n\\nDepth data, in particular, can be dif\\ufb01cult to whiten because\\nthe range of values may be very different for different patches\\nin the image. Thus, we \\ufb01rst whiten each depth patch indi-\\nvidually, subtracting the patch-wise mean and dividing by the\\npatch-wise standard deviation, down to some minimum.\\n\\nFor multimodal data, the statistics of the data for each\\nmodality should match as closely as possible, to avoid learning\\nfeatures which are biased towards or away from using partic-\\nular modes. This is particularly important when regularizing\\neach modality separately, as in our approach. Thus, we drop\\nmean values for each feature separately, but scale the data for\\neach channel by dividing by the standard deviation of all its\\nfeatures combined.\\n\\nB. Preserving Aspect Ratio.\\n\\nIt is important for to preserve aspect ratio when feeding\\nfeatures into the network. This is because distorting image fea-\\ntures may cause non-graspable rectangles to appear graspable,\\nas shown in Fig. 5. However, padding with zeros can cause\\nrectangles with less padding to receive higher graspability\\n\\n\\x0cFig. 7: Three possible models for multimodal deep learning: Left: fully dense model\\u2014all visible features are concatenated and modality\\ninformation is ignored. Middle: modality-speci\\ufb01c sparse model - separate \\ufb01rst layer features are trained for each modality. Right: group-sparse\\nmodel\\u2014a structured regularization term encourages features to use only a subset of the input modes.\\n\\nscores, as the network will have more nonzero inputs. It is\\nimportant to account for this because in many cases the ideal\\ngrasp for an object might be represented by a thin rectangle\\nwhich would thus contain many zero values in its receptive\\n\\ufb01eld from padding.\\n\\nTo address this problem, we scale up the magnitude of the\\navailable input for each rectangle based on the fraction of\\nthe rectangle which is masked out. In particular, we de\\ufb01ne a\\nmultiplicative scaling factor for the inputs from each modality,\\nbased on the fraction of each mode which is masked out, since\\neach mode may have a different mask.\\n\\n(cid:17)\\n\\ni=1 Sr,i/\\n\\nis 1 if x(t)\\ni\\n\\n(cid:16)(cid:80)N\\n\\nr = (cid:80)N\\ni =(cid:80)R\\n\\nIn the multimodal setting, we assume that the input data x is\\nknown to come from R distinct modalities, for example audio\\nand video data, or depth and RGB data. We de\\ufb01ne the modality\\nmatrix S as an RxN binary matrix, where each element Sr,i\\nindicates membership of visible unit xi in a particular modality\\nr, such as depth or image intensity. The scaling factor for mode\\nr is then de\\ufb01ned as: \\u03a8(t)\\n,\\nwhere \\xb5(t)\\nis masked in, 0 otherwise. The scaling\\ni\\nfactor for case i is: \\u03c8(t)\\nWe could simply scale up each value of x by its correspond-\\ning scale factor when training our model, as x(cid:48)(t)\\ni x(t)\\n.\\nHowever, since our sparse autoencoder penalizes squared error,\\nscaling x linearly will scale the error for the corresponding\\ncases quadratically, causing the learning algorithm to lend\\nincreased signi\\ufb01cance to cases where more data is masked out.\\nInstead, we can use the scaled x(cid:48) as input to the network, but\\npenalize reconstruction based on the original x, only scaling\\nafter the squared error has been computed:\\n\\nr=1 Sr,i\\u03a8(t)\\nr .\\n\\ni=1 Sr,i\\xb5(t)\\n\\ni\\n\\ni = \\u03c8(t)\\n\\ni\\n\\nM(cid:88)\\n\\n\\uf8eb\\uf8ed N(cid:88)\\n\\nK(cid:88)\\n\\n\\uf8f6\\uf8f8\\n\\n(6)\\n\\nW \\u2217 = arg min\\n\\nW\\n\\n\\u03c8(t)\\ni (\\u02c6x(t)\\n\\ni \\u2212 x(t)\\n\\ni )2 + \\u03bb\\n\\ng(h(t)\\nj )\\n\\nt=1\\n\\ni=1\\n\\nj=1\\n\\n(5)\\nWe rede\\ufb01ne the hidden units to use the scaled visible input:\\n\\n(cid:32) N(cid:88)\\n\\n(cid:33)\\n\\nh(t)\\nj = \\u03c3\\n\\nx(cid:48)(t)\\n\\ni Wi,j\\n\\nThis approach is equivalent to adding additional, potentially\\nfractional, \\u2018virtual\\u2019 visible units to the model based on the\\n\\ni=1\\n\\nr , c).\\n\\nscaling factor for each mode. In practice, we found it necessary\\nto limit the scaling factor to a maximum of some value c, as\\n\\u03a8(cid:48)(t)\\n\\nr = min(\\u03a8(t)\\nAs shown in Table III our mask-based scaling technique at\\nthe visible layer improves grasping results by over 25% for\\nboth metrics. As seen in Figure 6, it removes the network\\u2019s\\ninherent bias towards square rectangles, exhibiting a much\\nwider range of aspect ratios that more closely matches that\\nof the ground-truth data.\\n\\nV. STRUCTURED REGULARIZATION FOR\\n\\nFEATURE LEARNING\\n\\nA naive way of applying feature learning to multimodal\\ndata is to simply take x (as a concatenated vector) as input\\nto the model described above,\\nignoring information about\\nspeci\\ufb01c modalities, as seen on the lefthand side of Figure 7.\\nThis approach may either 1) prematurely learn features which\\ninclude all modalities, which can lead to over\\ufb01tting, or 2) fail\\nto learn associations between modalities with very different\\nunderlying statistics.\\n\\nInstead of concatenating multimodal\\n\\ninput as a vector,\\nNgiam et al. [43] proposed training a \\ufb01rst layer representation\\nfor each modality separately, as shown in Figure 7-middle.\\nThis approach makes the assumption that the ideal low-level\\nfeatures for each modality are purely unimodal, while higher-\\nlayer features are purely multimodal. This approach may work\\nbetter for some problems where the modalities have very\\ndifferent basic representations, such as the video and audio\\ndata (as used in [43]), so that separate \\ufb01rst layer features\\nmay give better performance. However, for modalities such\\nas RGB-D data, where the input modes represent different\\nchannels of an image, learning low-level correlations can lead\\nto more robust features \\u2013 our experiments in Section VI show\\nthat simply concatenating the input modalities signi\\ufb01cantly\\noutperforms training separate \\ufb01rst-layer features for robotic\\ngrasp detection from RGB-D data.\\n\\nFor many problems, it may be dif\\ufb01cult to tell which of these\\napproaches will perform better, and time-consuming to tune\\nand comparatively evaluate multiple algorithms. In addition,\\nthe ideal feature set for some problems may contain features\\n\\n\\x0c(a) Features corresponding to positive grasps.\\n\\n(b) Features corresponding to negative grasps.\\n\\nFig. 8: Features learned from grasping data: Each feature contains seven channels - from left to right, depth, Y, U, and V image channels,\\nand X, Y, and Z surface normal components. Vertical edges correspond to gripper plates. Left: eight features with the strong positive\\ncorrelations to rectangle graspability. Right: similar, but negative correlations. Group regularization eliminates many modalities from many\\nof these features, making them more robust.\\n\\nwhich use some, but not all, of the input modalities, a case\\nwhich neither of these approaches are designed to handle.\\n\\nTo solve these problems, we propose a new algorithm for\\nfeature learning for multimodal data. Our approach incorpo-\\nrates a structured penalty term into the optimization problem\\nto be solved during learning. This technique allows the model\\nto learn correlated features between multiple input modalities,\\nbut regularizes the number of modalities used per feature\\n(hidden unit), discouraging the model from learning weak\\ncorrelations between modalities. With this regularization term,\\nthe algorithm can specify how mode-sparse or mode-dense the\\nfeatures should be, representing a continuum between the two\\nextremes outlined above.\\nRegularization in Deep Learning:\\nIn a typical deep learn-\\ning model, L2 regularization (i.e., f (W ) = ||W||2\\n2) or L1\\nregularization (i.e., f (W ) = ||W||1) are commonly used in\\ntraining (e.g., as speci\\ufb01ed in Equations (3) and (4)). These are\\noften called a \\u201cweight cost\\u201d (or \\u201cweight decay\\u201d), and are left\\nimplicit in many works.\\n\\nApplying regularization is well known to improve the gen-\\neralization performance of feature learning algorithms. One\\nmight expect that a simple L1 penalty would eliminate weak\\ncorrelations in multimodal features, leading to features which\\nuse only a subset of the modes each. However, we found that in\\npractice, a value of \\u03b2 large enough to cause this also degraded\\nthe quality of features for the remaining modes and lead to\\ndecreased task performance.\\nMultimodal Regularization: Structured regularization, such\\nas in [26], takes a set of groups of weights, and applies\\nsome regularization function (typically high-order) separately\\nto each group. In our structured multimodal regularization\\nalgorithm, each modality will be used as a regularization group\\nseparately for each hidden unit. For example, a group-wise p-\\nnorm would be applied as:\\n\\nK(cid:88)\\n\\nR(cid:88)\\n\\n(cid:32) N(cid:88)\\n\\n(cid:33)1/p\\ni,j|\\nSr,i|W p\\n\\nf (W ) =\\n\\nlower-valued ones. This also means that forming a high-valued\\nweight in a group with other high-valued weights will accrue\\na lower additional penalty than doing so for a group with\\nonly low-valued weights. At the limit (p \\u2192 \\u221e), this group\\nregularization becomes equivalent\\nto the in\\ufb01nity (or max)\\nnorm:\\n\\nf (W ) =\\n\\nSr,i|Wi,j|\\n\\nmax\\n\\ni\\n\\n(8)\\n\\nK(cid:88)\\n\\nR(cid:88)\\n\\nj=1\\n\\nr=1\\n\\nwhich penalizes only the maximum weight from each mode to\\neach feature. In practice, the in\\ufb01nity norm is not differentiable\\nand therefore is dif\\ufb01cult to apply gradient-based optimization\\nmethods; in this paper, we use the log-sum-exponential as a\\ndifferentiable approximation to the max norm.\\n\\nIn experiments, this regularization function produces \\ufb01rst-\\nlayer weights concentrated in fewer modes per feature. How-\\never, we found that at values of \\u03b2 suf\\ufb01cient to induce the\\ndesired mode-wise sparsity patterns, penalizing the maximum\\nalso had the undesirable side-effect of causing many of the\\nweights for other modes to saturate at their mode\\u2019s maximum,\\nsuggesting that the features were overly constrained. In some\\ncases, constraining the weights in this manner also caused\\nthe algorithm to learn duplicate (or redundant) features, in\\neffect scaling up the feature\\u2019s contribution to reconstruction to\\ncompensate for its constrained maximum. This is obviously an\\nundesirable effect, as it reduces the effective size (or diversity)\\nof the learned feature set.\\n\\nThis suggests that\\n\\nthe max-norm may be overly con-\\nstraining. A more desirable sparsity function would penalize\\nnonzero weight maxima for each mode for each feature\\nwithout additional penalty for larger values of these maxima.\\nWe can achieve this effect by applying the L0 norm, which\\ntakes a value of 0 for an input of 0, and 1 otherwise, on top\\nof the max-norm from above:\\n\\n(7)\\n\\nf (W ) =\\n\\nI{(max\\n\\ni\\n\\nSr,i|Wi,j|) > 0}\\n\\n(9)\\n\\nK(cid:88)\\n\\nR(cid:88)\\n\\nj=1\\n\\nr=1\\n\\nj=1\\n\\nr=1\\n\\ni=1\\n\\nwhere Sr,i is 1 if feature i belongs to group r and 0 otherwise.\\nUsing a high value of p allows us to penalize higher-valued\\nweights from each mode to each feature more strongly than\\n\\nwhere I is the indicator function, which takes a value of 1\\nif its argument is true, 0 otherwise. Again, for a gradient-\\nbased method, we used an approximation to the L0 norm,\\nsuch as log(1 + x2). This regularization function now encodes\\n\\n\\x0cTABLE I: Recognition results for Cornell grasping dataset.\\n\\nAlgorithm\\nChance\\nJiang et al. [28]\\nJiang et al. [28] + FPFH\\nSparse AE, separate layer-1 feat.\\nSparse AE\\nSparse AE, group reg.\\n\\nAccuracy (%)\\n50\\n84.7\\n89.6\\n92.8\\n93.7\\n93.7\\n\\nexhaustive search using this network, then used the 200-unit\\nnetwork to re-rank the 100 highest-ranked rectangles found by\\nthe 50-unit network.\\n\\nB. Baselines\\n\\nWe compare our recognition results in the Cornell grasping\\ndataset with the features from [28], as well as the combination\\nof these features and Fast Point Feature Histogram (FPFH)\\nfeatures [51]. We used a linear SVM for classi\\ufb01cation, which\\ngave the best results among all other kernels. We also report\\nchance performance, obtained by randomly selecting a label\\nin the recognition case, and randomly assigning scores to\\nrectangles in the detection case.\\n\\nWe also compare our algorithm to other deep learning\\napproaches. We compare to a network trained only with\\nstandard L1 regularization, and a network trained in a manner\\nsimilar to [43], where three separate sets of \\ufb01rst layer features\\nare learned for the depth channel, the combination of the Y,\\nU, and V channels, and the combination of the X, Y, and Z\\nsurface normal components.\\n\\nC. Metrics for Detection\\n\\nFor detection, we compare the top-ranked rectangle for\\neach method with the set of ground-truth rectangles for each\\nimage. We present results using two metrics, the \\u201cpoint\\u201d and\\n\\u201crectangle\\u201d metric.\\n\\nFor the point metric, similar to Saxena et al. [55], we com-\\npute the center point of the predicted rectangle, and consider\\nthe grasp a success if it is within some distance from at least\\none ground-truth rectangle center. We note that this metric\\nignores grasp orientation, and therefore might overestimate the\\nperformance of an algorithm for robotic applications.\\n\\nFor the rectangle metric, similar to Jiang et al. [28], let G be\\nthe top-ranked grasping rectangle predicted by the algorithm,\\nand G\\u2217 be a ground-truth rectangle. Any rectangles with\\nan orientation error of more than 30o from G are rejected.\\nFrom the remaining set, we use the common bounding box\\nevaluation metric of intersection divided by union - i.e.\\nArea(G\\u2229 G\\u2217)/Area(G\\u222a G\\u2217). Since a ground-truth rectangle\\ncan de\\ufb01ne a large space of graspable rectangles (e.g., covering\\nthe entire length of a pen), we consider a prediction to be\\ncorrect if it scores at least 25% by this metric.\\n\\nVII. RESULTS AND DISCUSSION\\nA. Deep Learning for Robotic Grasp Detection\\n\\nFigure 8 shows the features learned by the unsupervised\\nphase of our algorithm which have a high correlation to\\n\\nFig. 9: Example objects from the Cornell grasping dataset: [28].\\nThis dataset contains objects from a large variety of categories.\\n\\na direct penalty on the number of modes used for each\\nweight, without further constraining the weights of modes with\\nnonzero maxima.\\n\\nFigure 8 shows features learned from the unsupervised stage\\nof our group-regularized deep learning algorithm. We discuss\\nthese features, and their implications for robotic grasping, in\\nSection VII.\\n\\nVI. EXPERIMENTS\\n\\nA. Dataset\\n\\nWe used the extended version of the Cornell grasping\\ndataset for our experiments. This dataset, along with code for\\nthis paper, is available at http://pr.cs.cornell.edu/\\ndeepgrasping. We note that this is an updated version\\nof the dataset used in [28], containing several more complex\\nobjects, and thus results for their algorithms will be different\\nfrom those in [28]. This dataset contains 1035 images of\\n280 graspable objects, several of which are shown in Fig. 9.\\nEach image is annotated with several ground-truth positive\\nand negative grasping rectangles. While the vast majority of\\npossible rectangles for most objects will be non-graspable, the\\ndataset contains roughly equal numbers of graspable and non-\\ngraspable rectangles. We will show that this is useful for an\\nunsupervised learning algorithm, as it allows learning a good\\nrepresentation for graspable rectangles even from unlabeled\\ndata.\\n\\nWe performed \\ufb01ve-fold cross-validation, and present results\\nfor splits on per image (i.e., the training set and the validation\\nset do not share the same image) and per object (i.e., the\\ntraining set and the validation set do not share any images\\nfrom the same object) basis. Hyper-parameters were selected\\nby validating performance on a separate set of 300 grasps not\\nused in any of the cross-validation splits.\\n\\nWe take seven 24x24 pixel channels as described in Sec-\\ntion IV as input, giving 4032 input features to each network.\\nWe trained a deep network with 200 hidden units each at\\nthe \\ufb01rst and second layers using our learning algorithm as\\ndescribed in Sections III and V. Training this network took\\nroughly 30 minutes. For trials involving our two-pass system,\\nwe trained a second network with 50 hidden units at each\\nlayer in the same manner. During inference we performed an\\n\\n\\x0ce\\nv\\ni\\nt\\ni\\ns\\no\\nP\\n\\ne\\nv\\ni\\nt\\na\\ng\\ne\\nN\\n\\nFig. 10: Learned 3D depth features: 3D meshes for depth channels of the four features with strongest positive (top) and negative(bottom)\\ncorrelations to rectangle graspability. Here X and Y coordinates corresponds to positions in the deep network\\u2019s receptive \\ufb01eld, and Z\\ncoordinates corresponds to weight values to the depth channel for each location. Feature shapes clearly correspond to graspable and non-\\ngraspable structures, respectively.\\n\\npositive and negative grasping cases. Many of these features\\nshow non-zero weights to the depth channel, indicating that\\nit learns the correlation of depths to graspability. We can see\\nthat weights to many of the modalities for these features have\\nbeen eliminated by our structured regularization approach. In\\nparticular, many of these features lack weights to the U and V\\n(3rd and 4th) channels, which correspond to color, allowing\\nthe system to be more robust to different-colored objects.\\n\\nFigure 10 shows 3D meshes for the depth channels of\\nthe four features with the strongest positive and negative\\ncorrelations to valid grasps. Even without any supervised infor-\\nmation, our algorithm was able to learn several features which\\ncorrelate strongly to graspable cases and non-graspable cases.\\nThe \\ufb01rst two positive-correlated features represent handles,\\nor other cases with a raised region in the center, while the\\nsecond two represent circular rims or handles. The negatively-\\ncorrelated features represent obviously non-graspable cases,\\nsuch as ridges perpendicular to the gripper plane and \\u201cvalleys\\u201d\\nbetween the gripper plates. From these features, we can see\\nthat even during unsupervised feature learning, our approach\\nis able to learn a representation useful for the task at hand,\\nthanks purely to the fact that the data used is composed of\\nhalf graspable and half non-graspable cases.\\n\\nFrom Table I, we see that the recognition performance is\\nsigni\\ufb01cantly improved with deep learning methods, improving\\n9% over the features from [28] and 4.1% over those features\\ncombined with FPFH features. Both L1 and group regulariza-\\ntion performed similarly for recognition, but training separate\\n\\ufb01rst layer features decreased performance slightly. This shows\\nthat learned features, in addition to avoiding hand-design, are\\nable to improve performance signi\\ufb01cantly over the state of\\nthe art. It demonstrates that a deep network is able to learn\\nthe concept of \\u201cgraspability\\u201d in a way that generalizes to new\\nobjects it hasn\\u2019t seen before.\\n\\nTable II shows that even using any one of the three input\\nmodalities (RGB, depth, or surface normals), our algorithm\\nis able to learn features which outperform hand-engineered\\nones for recognition. Depth gives the highest performance\\nof any single-mode network. Combining depth and normal\\n\\nTABLE II: Recognition results for different modalities, for a deep\\nnetwork pre-trained using SAE.\\n\\nModes\\nChance\\nRGB\\nDepth\\nSurf. Normals\\nDepth + Surf. Normals\\nRGB + Depth + Surf. Normals\\n\\nAccuracy (%)\\n50\\n90.3\\n92.4\\n90.3\\n92.8\\n93.7\\n\\ninformation improves results over either alone, indicating that\\nthey give non-redundant information.\\n\\nThe highest accuracy is still obtained by using all\\n\\nthe\\ninput modalities. This shows that combining depth and color\\ninformation leads to a system which is more robust\\nthan\\neither modality alone. This is due to the fact\\nthat some\\ngraspable cases (rims of monochromatic objects, etc.) can only\\nbe detected using depth information, while in others, the depth\\nchannel may be extremely noisy, requiring the use of color\\ninformation. From this, we can see that integrating multimodal\\ninformation, a major focus of this work,\\nin\\nrecognizing good robotic grasps.\\n\\nis important\\n\\nTable III shows that the performance gains from deep learn-\\ning for recognition carry over to detection, as well. Once mask-\\nbased scaling has been applied, all deep learning approaches\\nexcept for training separate \\ufb01rst-layer features outperform the\\nhand-engineered features from [28] by up to 13% for the point\\nmetric and 17% for the rectangle metric, while also avoiding\\nthe need to design task-speci\\ufb01c features. Without mask-based\\nscaling, the system performs poorly, due to the bias illustrated\\nin Fig. 6. Separate \\ufb01rst-layer features also give weak detection\\nperformance, indicating that the relative scores assigned by\\nthis form of network are less robust than those learned using\\nour structured regularization approach.\\n\\nUsing structured multimodal regularization also improves\\nresults over standard L1regularization by up to 1.8%, showing\\nthat our method also learns more robust features than standard\\napproaches which ignore modality information. Even though\\nusing the \\ufb01rst-pass network alone underperforms the second-\\n\\n\\x0cTABLE III: Detection results for point and rectangle metrics, for\\nvarious learning algorithms.\\n\\nAlgorithm\\nChance\\nJiang et al. [28]\\nSAE, no mask-based scaling\\nSAE, separate layer-1 feat.\\nSAE, L1 reg.\\nSAE, struct. reg., 1st pass only\\nSAE, struct. reg., 2nd pass only\\nSAE, struct. reg. two-stage\\n\\nImage-wise split\\nPoint\\n35.9\\n75.3\\n62.1\\n70.3\\n87.2\\n86.4\\n87.5\\n88.4\\n\\nRect\\n6.7\\n60.5\\n39.9\\n43.3\\n72.9\\n70.6\\n73.8\\n73.9\\n\\nObject-wise split\\nPoint\\n35.9\\n74.9\\n56.2\\n70.7\\n88.7\\n85.2\\n87.6\\n88.1\\n\\nRect\\n6.7\\n58.3\\n35.4\\n40.0\\n71.4\\n64.9\\n73.2\\n75.6\\n\\nr\\ne\\np\\np\\ni\\nr\\ng\\n\\ne\\nd\\ni\\nW\\n\\nr\\ne\\np\\np\\ni\\nr\\ng\\n\\nn\\ni\\nh\\nT\\n\\n0 degrees\\n\\n45 degrees\\n\\n90 degrees\\n\\nFig. 11: Visualization of grasping scores for different grippers:\\nRed indicates maximum score for a grasp with left gripper plane\\ncentered at each point, blue is similar for the right plate. Best-scoring\\nrectangle shown in green/yellow.\\n\\npass network alone by up to 8.3%, integrating both in our two-\\npass system outperforms the solo second-pass network by up\\nto 2.4%. This shows that the two-pass system improves not\\nonly ef\\ufb01ciency, but accuracy as well. The performance gains\\nfrom multimodal regularization and the two-pass system are\\ndiscussed in detail below.\\n\\nOur system outperforms all baseline approaches by all\\nmetrics except for the point metric in the object-wise split\\ncase. However, we can see that the chance performance is\\nmuch higher for the point metric than for the rectangle metric.\\nThis shows that the point metric can overstate performance,\\nand the rectangle metric is a better indicator of the accuracy\\nof a grasp detection system.\\nAdaptability: One important advantage of our detection\\nsystem is that we can \\ufb02exibly specify the constraints of the\\ngripper in our detection system. This is particularly important\\nfor a robot like Baxter, where different objects might require\\ndifferent gripper settings to grasp. We can constrain the\\ndetectors to handle this. Figure 11 shows detection scores\\nfor systems constrained based on two different settings of\\nBaxter\\u2019s gripper, one wide and one thin. The implications of\\nthese results for other types of grippers will be discussed in\\nSection IX.\\n\\nB. Multimodal Group Regularization\\n\\nOur group regularization term improves detection accuracy\\nover simple L1 regularization. The improvement\\nis more\\nsigni\\ufb01cant for the object-wise split than for the image-wise\\nsplit because the group regularization helps the network to\\navoid over\\ufb01tting, which will tend to occur more when the\\nlearning algorithm is evaluated on unseen objects.\\n\\nFig. 12: Improvements from group regularization: Cases where\\nour group regularization approach produces a viable grasp (shown\\nin green and yellow), while a network trained only with simple L1\\nregularization does not (shown in blue and red). Top: RGB image,\\nbottom: depth channel. Green and blue edges correspond to gripper.\\n\\nFig. 13: Improvements from two-stage system: Example cases\\nwhere the two-stage system produces a viable grasp (shown in green\\nand yellow), while the single-stage system does not (shown in blue\\nand red). Top: RGB image, bottom: depth channel. Green and blue\\nedges correspond to gripper.\\n\\nFigure 12 shows typical cases where a network trained using\\nour group regularization \\ufb01nds a valid grasp, but a network\\ntrained with L1 regularization does not. In these cases, the\\ngrasp chosen by the L1-regularized network appears valid for\\nsome modalities \\u2013 the depth channel for the sunglasses and nail\\npolish bottle, and the RGB channels for the scissors. However,\\nwhen all modalities are considered, the grasp is clearly invalid.\\nThe group-regularized network does a better job of combining\\ninformation from all modalities and is more robust to noise and\\nmissing data in the depth channel, as seen in these cases.\\n\\nC. Two-stage Detection System\\n\\nUsing our two-pass system enhanced both computational\\nperformance and accuracy. The number of rectangles the full-\\nsize network needed to evaluate was reduced by roughly a\\nfactor of 1000. Meanwhile, detection performance increased\\nby up to 2.4% as compared to a single pass with the large-\\nsize network, even though using the small network alone\\nsigni\\ufb01cantly underperforms the larger network. In most cases,\\nthe top 100 rectangles from the \\ufb01rst pass contained the top-\\nranked rectangle from an exhaustive search using the second-\\nstage network, and thus results were unaffected.\\n\\nFigure 13 shows some cases where the \\ufb01rst-stage network\\npruned away rectangles corresponding to weak grasps which\\nmight otherwise be chosen by the second-stage network. In\\n\\n\\x0cPR2: Our second platform was our PR2 robot, \\u201cKodiak.\\u201d\\nSimilar to Baxter, PR2 has two 7-DoF arms with approx-\\nimately 1 m reach, and we used only the left for these\\nexperiments. PR2\\u2019s grippers open to a width of 8 cm, and\\nare capable of closing completely from that span, so we did\\nnot need to use two settings as with Baxter. We augmented\\nPR2\\u2019s gripper friction with gaffer tape on the \\ufb01ngertips.\\n\\nFor the experiments on PR2, we used the Kinect already\\nmounted to Kodiak\\u2019s head, and used ROS\\u2019s built-in function-\\nality to obtain 3D locations from that Kinect and transform\\nthese to Kodiak\\u2019s body frame for manipulation. Control was\\nperformed using the ee cart stiffness controller [5] with tra-\\njectories provided by our own custom MATLAB code.\\nExperimental Setup: For each experiment, we placed a\\nsingle object within a 25 cm x 25 cm square on the ta-\\nble, approximately 1.2 m below the mounting point of the\\nKinect. This square was chosen to be well-contained within\\neach robot\\u2019s workspace, allowing objects to be reached from\\nmost approach vectors. Object positions and orientations were\\nvaried between trials, although objects were always placed in\\ncon\\ufb01gurations in which at least one viable grasp was visible\\nand accessible to the robot.\\n\\nWhen using Baxter, due to the limited stroke (span from\\nopen to closed) of its gripper, we pre-selected one of the\\ntwo gripper settings discussed above for each object. We\\nconstrained the search space as illustrated in Fig. 11 to \\ufb01nd\\ngrasps for that particular setting.\\n\\nTo detect grasps, we \\ufb01rst took an RGB-D image from the\\nKinect with no objects in the scene as a background image.\\nThe depth channel of this image was used to segment objects\\nfrom the scene, and to correct for the slant of the Kinect. Once\\nan object was segmented, we used our algorithm, as described\\nabove, to obtain a single best-ranked grasping rectangle.\\n\\nThe search space for the \\ufb01rst-pass network progressed in 15-\\ndegree increments from 15 to 180 degrees (angles larger than\\n180 being mirror-images of grasps already tested), searching\\nover 10-pixel increments across the image for the X and Y\\ncoordinates of the upper-left corner of the rectangle. For the\\nthin gripper setting, rectangle widths and heights from 10 to\\n40 pixels in 10-pixel increments were searched, while for the\\nthick setting these ranged from 40 pixels to 100 pixels in\\n20-pixel increments. In both cases, rectangles taller than they\\nwere wide were ignored. Once a single best-scoring grasp was\\ndetected, we translated it to a robotic grasp consisting of a\\ngrasping point and an approach vector using the rectangle\\u2019s\\nparameters and the surface normal at the rectangle\\u2019s center as\\ndescribed above.\\n\\nTo execute the grasp, we \\ufb01rst positioned the gripper at\\na location 10 cm back from the grasping point along the\\napproach vector. The gripper was oriented to the approach\\nvector, and rotated around it based on the orientation of the\\ndetected grasping rectangle.\\n\\nSince Baxter\\u2019s arms are highly compliant, slight impreci-\\nsions in end-effector positioning are to be expected \\u2013 we found\\nthat errors of up to 2 cm were typical. Thus, we implemented a\\nvisual servoing system using its hand camera, which provides\\n\\nFig. 14: Robotic experiment objects: Several of the objects used\\nin experiments, including challenging cases such as an oddly-shaped\\nRC car controller, a cloth towel, plush cat, and white ice cube tray.\\n\\nthese cases, the grasp chosen by the single-stage system might\\nbe feasible for a robotic gripper, but the rectangle chosen by\\nthe two-stage system represents a grasp which would clearly\\nbe successful.\\n\\nThe two-stage system also signi\\ufb01cantly increases the com-\\nputational ef\\ufb01ciency of our detection system. Average infer-\\nence time for a MATLAB implementation of the deep network\\nwas reduced from 24.6s/image for an exhaustive search using\\nthe larger network to 13.5s/image using the two-stage system.\\n\\nVIII. ROBOTIC EXPERIMENTS\\n\\nIn order to evaluate the performance of our algorithms in the\\nreal world, we ran an extensive series of robotic experiments.\\nTo explore the generalizability and effect of the robot on\\nthe success rate of our algorithms, we performed experiments\\non two different robotic platforms, a Baxter Research Robot\\n(\\u201cYogi\\u201d) and a PR2 (\\u201cKodiak\\u201d).\\nBaxter: The \\ufb01rst platform used is our Baxter Research Robot,\\nwhich we call \\u201cYogi.\\u201d Baxter has two arms with seven degrees\\nof freedom each and a maximum reach of 104 cm, although we\\nused only the left arm for these experiments. The end-effector\\nfor this arm is a two-\\ufb01nger parallel gripper. We augmented the\\ngripper tips using rubber bands for additional friction. Baxter\\u2019s\\ngrippers are interchangable, and we used two settings for these\\nexperiments - a \\u201cwide\\u201d setting with an open width of 8 cm\\nand closed width of 4 cm, and a \\u201cthin\\u201d setting with an open\\nwidth of 4 cm and a closed width of 0 cm (completely closed,\\ngripper tips touching).\\n\\nTo detect grasps, we mounted a Kinect sensor to Yogi\\u2019s\\nhead, approximately 1.75 m above the ground. angled down-\\nwards at roughly a 75o angle towards a table in front of it.\\nThe Kinect gives RGB-D images at a resolution of 640x480\\npixels. We calibrated the transformation between the Kinect\\u2019s\\nand Yogi\\u2019s coordinate frames by marking four points corre-\\nsponding to a set of 3D axes, and obtaining the coordinates\\nof these points in both Kinect\\u2019s and Yogi\\u2019s frames.\\n\\nAll control for Baxter was done by specifying an end-\\neffector position and orientation, and using the inverse kine-\\nmatics provided with Baxter to determine a set of joint angles\\nfor this pose. Baxter\\u2019s built-in control systems were used to\\ndrive the arm to these new joint angles.\\n\\n\\x0cTABLE IV: Results for robotic experiments for Baxter, sorted by object category, for a total of 100 trials.\\n\\nTr. indicates number of trials, Acc. indicates accuracy (in terms of success percentage.)\\n\\nKitchen tools\\n\\nLab tools\\n\\nContainers\\n\\nObject\\nCan opener\\nKnife\\nBrush\\nTongs\\nTowel\\nGrater\\nAverage\\nOverall\\n\\n3\\n3\\n3\\n3\\n3\\n3\\n\\nObject\\nKinect\\n\\nTr. Acc.\\n100\\n100 Wire bundle\\n100 Mouse\\n100\\n100\\n100\\n100\\n84\\n\\nHot glue gun\\nQuad-rotor\\nDuct tape roll\\nAverage\\n\\nObject\\nColored cereal box\\n\\nTr. Acc.\\n100\\n100 White cereal box\\nCap-shaped bowl\\n100\\n67\\nCoffee mug\\n75\\nIce cube tray\\n100 Martini glass\\n90\\n\\nAverage\\n\\n5\\n3\\n3\\n3\\n4\\n4\\n\\nTr. Acc.\\n100\\n50\\n100\\n100\\n100\\n0\\n75\\n\\n3\\n4\\n3\\n3\\n3\\n3\\n\\nToys\\n\\nObject\\nPlastic whale\\nPlastic elephant\\nPlush cat\\nRC controller\\nXBox controller\\nPlastic frog\\nAverage\\n\\nOthers\\n\\nObject\\nTr. Acc.\\nElectric shaver\\n75\\nUmbrella\\n100\\nDesk lamp\\n75\\nRemote control\\n67\\n50 Metal bookend\\n67\\n72\\n\\nGlove\\nAverage\\n\\n4\\n4\\n4\\n3\\n4\\n3\\n\\nTABLE V: Results for robotic experiments for PR2, sorted by object category, for a total of 100 trials.\\n\\nTr. indicates number of trials, Acc. indicates accuracy (in terms of success percentage.)\\n\\nKitchen tools\\n\\nLab tools\\n\\nContainers\\n\\nObject\\nCan opener\\nKnife\\nBrush\\nTongs\\nTowel\\nGrater\\nAverage\\nOverall\\n\\n3\\n3\\n3\\n3\\n3\\n3\\n\\nObject\\nKinect\\n\\nTr. Acc.\\n100\\n100 Wire bundle\\n100 Mouse\\n100\\n100\\n100\\n100\\n89\\n\\nHot glue gun\\nQuad-rotor\\nDuct tape roll\\nAverage\\n\\nObject\\nColored cereal box\\n\\nTr. Acc.\\n100\\n100 White cereal box\\nCap-shaped bowl\\n100\\n67\\nCoffee mug\\n100\\nIce cube tray\\n100 Martini glass\\n95\\n\\nAverage\\n\\n5\\n3\\n3\\n3\\n4\\n4\\n\\nTr. Acc.\\n100\\n100\\n100\\n100\\n100\\n0\\n83\\n\\n3\\n4\\n3\\n3\\n3\\n3\\n\\nToys\\n\\nObject\\nPlastic whale\\nPlastic elephant\\nPlush cat\\nRC controller\\nXBox controller\\nPlastic frog\\nAverage\\n\\nOthers\\n\\nObject\\nTr. Acc.\\nElectric shaver\\n75\\nUmbrella\\n100\\nDesk lamp\\n100\\nRemote control\\n67\\n25 Metal bookend\\n67\\n72\\n\\nGlove\\nAverage\\n\\n4\\n4\\n4\\n3\\n4\\n3\\n\\nTr. Acc.\\n100\\n75\\n100\\n100\\n33\\n100\\n85\\n\\n3\\n4\\n3\\n5\\n3\\n3\\n\\nTr. Acc.\\n100\\n100\\n100\\n100\\n67\\n100\\n95\\n\\n3\\n4\\n3\\n5\\n3\\n3\\n\\nFig. 15: Robots executing grasps: Our robots grasping several objects from the experimental dataset. Top row: Baxter grasping a quad-rotor\\ncasing, coffee mug, ice cube tray, knife, and electric shaver. Middle row: Baxter grasping a desk lamp, cheese grater, umbrella, cloth towel,\\nand hot glue gun. Bottom row: PR2 grasping a plush cat, RC car controller, cereal box, toy elephant, and glove.\\n\\n\\x0cRGB images at a resolution of 320x200 pixels. We used color\\nsegmentation to separate the object from the background, and\\nused its lateral position in image space to drive Yogi\\u2019s end-\\neffector to center the object. We did not implement visual\\nservoing for PR2 because its gripper positioning was found to\\nbe precise to within 0.5 cm.\\n\\nAfter visual servoing was completed, we drove the gripper\\n14 cm forwards from its current position along the approach\\nvector, so that the grasping point was well-contained within\\nit. We then closed the gripper, grasping the object, and moved\\nit 30 cm upwards. A grasp was determined to be successful if\\nit was suf\\ufb01cient to lift the object and hold it for one second.\\nObjects to be Grasped: For our robotic experiments, we\\ncollected a diverse set of 35 objects within a size of .3 m x .3\\nm x .3 m and weighing at most 2.5 kg (although most were\\nless than 1 kg) from our of\\ufb01ces, homes, and lab. Many of them\\nare shown in Fig. 14. Most of these objects were not present\\nin the training dataset, and thus were completely new to the\\ngrasp detection algorithm.\\n\\nDue to the physical\\n\\nlimitations of the robots\\u2019 grippers,\\nwe found that \\ufb01ve of these objects were not graspable even\\nwhen given a hand-chosen grasp. The small pair of pliers\\nwas too low to the table to grip properly. The spray paint\\ncan was too smooth for the gripper to get enough friction\\nto lift it. The weight of the hammer was too imbalanced,\\ncausing the hammer to rotate and slip out of the gripper when\\ngrasped. Similar problems were encountered with the bicycle\\nU-lock. The bevel spatula\\u2019s handle was too close to the thin-\\nset size of Baxter\\u2019s gripper, so that we could not position\\nit precisely enough to grasp it reliably. We did not consider\\nthese objects for purposes of our experimental results, since\\nour focus was on evaluating the performance of our grasp\\ndetection algorithm.\\nResults: Table IV shows the results of our robotic experiments\\non Baxter for the remaining 30 objects, a total of 100 trials.\\nUsing our algorithm, Yogi was able to successfully execute a\\ngrasp in 84% of the trials. Figure 15 shows Yogi executing\\nseveral of these grasps. In 8% of the trials, our algorithm\\ndetected a valid grasp which was not executed correctly by\\nYogi. Thus, we were able to successfully detect a good grasp\\nin 92% of the trials. Video of some of these trials is available\\nat http://pr.cs.cornell.edu/deepgrasping.\\n\\nPR2 yielded a higher success rate as seen in Table V,\\nsucceeding in 89% of trials. This is largely due to the much\\nwider span of PR2\\u2019s gripper from open to closed and its ability\\nto fully close from its widest position, as well as PR2\\u2019s ability\\nto apply a larger gripping force. Some speci\\ufb01c instances where\\nPR2 and Baxter\\u2019s performance differed are discussed below.\\nFor comparison purposes, we ran a small set of control\\nexperiments for 16 of the objects in the dataset. The control\\nalgorithm simply returned a \\ufb01xed-size rectangle centered at the\\nobject\\u2019s center of mass, as determined by depth segmentation\\nfrom the background. The rectangle was aligned so that the\\ngripper plates ran parallel to the object\\u2019s principal axis. This\\nalgorithm was only successful in 31% of cases, signi\\ufb01cantly\\n\\nunderperforming our system.\\n\\nOn Baxter, our algorithm sometimes detected a grasp which\\nwas not realizable by the current setting of its gripper, but\\nmight be executable by others. For example, our algorithm\\ndetected grasps across the leg of the plush cat, and the region\\nbetween the handle and body of the umbrella, both too thin\\nfor the wide setting of Baxter\\u2019s gripper to grasp since it has\\na minimum span of 4 cm. Since PR2\\u2019s gripper can close\\ncompletely from any position, it did not encounter these issues\\nand thus achieved a 100% success rate for both these objects.\\nThe XBox controller proved to be a very dif\\ufb01cult object for\\neither robot to grasp. From a top-down angle, there is only a\\nsmall space of viable grasps with a span of less than 8 cm, but\\nmany which have either a slightly larger span (making them\\nnon-realizable by either gripper), or are subtly non-viable (e.g.\\ngrasps across the two \\u201chandles,\\u201d which tend to slip off.) All\\nviable grasps are very near to the 8 cm span of both grippers,\\nmeaning that even slight imprecision in positioning can lead to\\nfailure. Due to this, Baxter achieved a higher success rate for\\nthe XBox controller thanks to visual servoing, succeeding in\\n50% of cases as compared to the 25% success rate for PR2.\\nOur algorithm was able to consistently detect and execute\\nvalid grasps for a red cereal box, but had some failures on a\\nwhite and yellow one. This is because the background for all\\nobjects in the dataset is white, leading the algorithm to learn\\nfeatures relating white areas at the edges of the gripper region\\nto graspable cases. However, it was able to detect and execute\\ncorrect grasps for an all-white ice cube tray, and so does not\\nfail for all white objects. This could be remedied by extending\\nthe dataset to include cases with different background colors.\\nInterestingly, even though the parameters of grasps detected\\nfor the white box were similar for PR2 and Baxter, PR2 was\\nable to succeed in every case while Baxter succeeded only\\nhalf the time. This is because PR2\\u2019s increased gripper strength\\nallowed it to execute grasps across corners of the box, crushing\\nit slightly in the process.\\n\\nOther failures were due to the limitations of the Kinect\\nsensor. We were never able to properly grasp the martini glass\\nbecause its glossy \\ufb01nish prevented Kinect from returning any\\ndepth estimates for it. Even if a valid grasp were detected\\nusing color information only, there was no way to infer a\\nproper grasping position without depth information. Grasps\\nfor the metal bookend failed for similar reasons, but it was\\nnot as glossy as the martini glass, and gave enough returns\\nfor some to succeed.\\n\\nHowever, our algorithm also had many noteworthy suc-\\ncesses. It was able to consistently detect and execute grasps for\\na crumpled cloth towel, a complex and irregular case which\\nbore little resemblance to any object in the dataset. It was also\\nable to \\ufb01nd and grasp the rims of objects such as the plastic\\nbaseball cap and coffee mug, cases where there is little visual\\ndistinction between the rim and body of the object. These\\nobjects underscore the importance of the depth channel for\\nrobotic grasping, as none of these grasps would be detectable\\nwithout depth information.\\n\\nOur algorithm was also able to successfully detect and\\n\\n\\x0cexecute many grasps for which the approach vector was non-\\nvertical. The grasps shown for the coffee mug, desk lamp,\\ncereal box, RC car controller, and toy elephant shown in\\nFig. 15 were all executed by aligning the gripper to such an\\napproach vector. Indeed, many of these grasps may have failed\\nhad the gripper been aligned vertically. This shows that our\\nalgorithm is not restricted to detecting top-down grasps, but\\nrather encodes a more general notion of graspability which\\ncan be applied to grasps from many angles, albeit within the\\nconstraints of visibility from a single-view perspective.\\n\\nWhile a few failures occurred, our algorithm still achieved\\na high rate of accuracy for other oddly-shaped objects such\\nas the quad-rotor casing, RC car controller, and glue gun.\\nFor objects with clearly de\\ufb01ned handles, such as the cheese\\ngrater, kitchen tongs, can opener, and knife, our algorithm was\\nable to detect and execute successful grasps in every trial,\\nshowing that there is a wide range of objects which it can\\ngrasp extremely consistently.\\n\\nIX. DISCUSSION AND FUTURE WORK\\n\\nOur algorithm focuses on the problem of grasp detection for\\na two-\\ufb01ngered parallel-plate style gripper. It would be directly\\napplicable to other grippers with \\ufb01xed con\\ufb01gurations, simply\\nrequiring new training data labeled with grasps for the gripper\\nin question. Our system would allow even the basic features\\nused for grasp detection to adapt to the gripper. This might be\\nuseful in cases such as jamming grippers [29], or two-\\ufb01ngered\\ngrippers with differently-shaped contact surfaces, which might\\nrequire different features to determine a graspable area.\\n\\nOur detection algorithm does not directly address the prob-\\nlem of 3D orientation of the gripper \\u2013 this orientation is\\ndetermined only after an optimal rectangle has been detected,\\norienting the grasp based on the object\\u2019s surface normals.\\nHowever, just as our approach here considers aligns a 2D\\nfeature window to the gripper, an extension of this work might\\nalign a 3D window \\u2013 using voxels, rather than pixels, as its\\nbasic unit of representation for input features to the network.\\nThis would allow the system to search across the full 6-DoF\\n3D pose of the gripper, while still leveraging the power of\\nfeature learning.\\n\\nOur system gives only a gripper pose as output, but multi-\\n\\ufb01ngered recon\\ufb01gurable hands also require a con\\ufb01guration of\\nthe \\ufb01ngers in order to grasp an object. In this case, our\\nalgorithm could be used as a heuristic to \\ufb01nd one or more\\nlocations likely to be graspable (similar to the \\ufb01rst pass in our\\ntwo-pass system), greatly reducing the search space needed to\\n\\ufb01nd an optimal gripper con\\ufb01guration.\\n\\nOur algorithm also depends only on local features to de-\\ntermine grasping locations. However, many household objects\\nmay have some areas which are strongly preferable to grasp\\nover others - for example, a knife might be graspable by\\nthe blade, or a hot glue gun by the barrel, but both should\\nactually be grasped by their respective handles. Since these\\nregions are more likely to be labeled as graspable in the\\ndata, our system already weakly encodes this, but some may\\nnot be readily distinguishable using only local information.\\n\\nAdding a term modeling the probability of each region of\\nthe image being a semantically-appropriate area to grasp the\\nobject would allow us to incorporate this information. This\\nterm could be computed once for the entire image, then added\\nto each local detection score, keeping detection ef\\ufb01cient.\\n\\nIn this work, our visual-servoing algorithm was purely\\nheuristic, simply attempting to center the segmented object\\nunderneath the hand camera. However, in future work, a simi-\\nlar feature-learning approach might be applied to hand camera\\nimages of graspable and non-graspable regions, improving the\\nvisual servoing system\\u2019s ability to \\ufb01ne-tune gripper position\\nto ensure a good grasp.\\n\\nMany robotics problems require the use of perceptual infor-\\nmation, but can be dif\\ufb01cult and time-consuming to engineer\\ngood features for, particularly when using RGB-D data. In\\nfuture work, our approach could be extended to a wide range\\nof such problems. Our system could easily be applied to\\nother detection problems such as object detection or obstacle\\ndetection. However, it could also be adapted to other similar\\nproblems, such as object tracking and visual servoing.\\n\\nMultimodal data has become extremely important\\n\\nfor\\nrobotics, due both to the advent of new sensors such as the\\nKinect and the application of robots to more challenging tasks\\nwhich require multiple modalities of information to perform\\nwell. However, it can be very dif\\ufb01cult to design features which\\ndo a good job of integrating many modalities. While our\\nwork focuses on color, depth, and surface normals as input\\nmodes, our structured multimodal regularization algorithm\\nmight also be applied to others. This approach could improve\\nperformance while allowing roboticists to focus on other\\nengineering challenges.\\n\\nX. CONCLUSIONS\\n\\nWe presented a system for detecting robotic grasps from\\nRGB-D data using a deep learning approach. Our method\\nhas several advantages over current state-of-the-art methods.\\nFirst, using deep learning allows us to avoid hand-engineering\\nfeatures, learning them instead. Second, our results show that\\ndeep learning methods signi\\ufb01cantly outperform even well-\\ndesigned hand-engineered features from previous work.\\n\\nWe also presented a novel feature learning algorithm for\\nmultimodal data based on group regularization. In extensive\\nexperiments, we demonstrated that this algorithm produces\\nbetter features for robotic grasp detection than existing deep\\nlearning approaches to multimodal data. Our experiments and\\nresults, both of\\ufb02ine and on real robotic platforms, show that\\nour two-stage deep learning system with group regularization\\nis capable of robustly detecting grasps for a wide range of\\nobjects, even those previously unseen by the system.\\n\\nACKNOWLEDGEMENTS\\n\\nWe would like to thank Yun Jiang and Marcus Lim for\\nuseful discussions and help with baseline experiments. This\\nresearch was funded in part by ARO award W911NF-12-1-\\n0267, Microsoft Faculty Fellowship and NSF CAREER Award\\n(Saxena), and Google Faculty Research Award (Lee).\\n\\n\\x0cREFERENCES\\n\\n[1] Y. Bengio. Learning deep architectures for AI. FTML,\\n\\n[2] A. Bicchi and V. Kumar. Robotic grasping and contact:\\n\\n2(1):1\\u2013127, 2009.\\n\\na review. In ICRA, 2000.\\n\\n[3] L. Bo, X. Ren, and D. Fox. Unsupervised Feature\\nLearning for RGB-D Based Object Recognition. In ISER,\\n2012.\\n\\n[4] J. Bohg, A. Morales, T. Asfour, and D. Kragic. Data-\\n\\ndriven grasp synthesis - a survey. accepted.\\n\\n[5] M. Bollini, J. Barry, and D. Rus. Bakebot: Baking\\n\\ncookies with the pr2. In IROS PR2 Workshop, 2011.\\n\\n[6] D. Bowers and R. Lumia. Manipulation of unmodeled\\nobjects using intelligent grasping schemes. IEEE Trans\\nFuzzy Sys, 11(3), 2003.\\n\\n[7] C. Cadena and J. Kosecka. Semantic parsing for priming\\nobject detection in rgb-d scenes. In ICRA Workshop on\\nSemantic Perception, Mapping and Exploration, 2013.\\n\\n[8] A. Coates and A. Y. Ng. Selecting receptive \\ufb01elds in\\n\\ndeep networks. In NIPS, 2011.\\n\\n[9] A. Coates, B. Carpenter, C. Case, S. Satheesh, B. Suresh,\\nT. Wang, D. J. Wu, and A. Y. Ng. Text detection and\\ncharacter recognition in scene images with unsupervised\\nfeature learning. In ICDAR, 2011.\\n\\n[10] A. Collet Romea, D. Berenson, S. Srinivasa, and D. Fer-\\nguson . Object recognition and full pose registration from\\na single image for robotic manipulation. In ICRA, 2009.\\n[11] A. Collet Romea, M. Martinez Torres, and S. Srinivasa.\\nThe moped framework: Object recognition and pose\\nestimation for manipulation. IJRR, 30(10):1284 \\u2013 1306,\\n2011.\\n\\n[12] R. Collobert,\\n\\nJ. Weston, L. Bottou, M. Karlen,\\nK. Kavukcuoglu, and P. Kuksa. Natural language pro-\\nJMLR, 12:2493\\u20132537,\\ncessing (almost) from scratch.\\n2011.\\n\\n[13] R. Detry, E. Baseski, M. Popovic, Y. Touati, N. Kruger,\\nO. Kroemer, J. Peters, and J. Piater. Learning object-\\nspeci\\ufb01c grasp affordance densities. In ICDL, 2009.\\n\\n[14] M. Dogar, K. Hsiao, M. Ciocarlie, and S. Srinivasa.\\nIn RSS,\\n\\nPhysics-based grasp planning through clutter.\\n2012.\\n\\n[15] S. Ekvall and D. Kragic. Learning and evaluation of\\nthe approach vector for automatic grasp generation and\\nplanning. In ICRA, 2007.\\n\\n[16] F. Endres, J. Hess, J. Sturm, D. Cremers, and W. Burgard.\\nInternational\\n\\n3d mapping with an RGB-D camera.\\nJournal of Robotics Research (IJRR), 2013.\\n\\n[17] C. Ferrari and J. Canny. Planning optimal grasps. ICRA,\\n\\n1992.\\n\\n[18] C. R. Gallegos, J. Porta, and L. Ros. Global optimization\\n\\nof robotic grasps. In RSS, 2011.\\n\\n[19] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich\\nfeature hierarchies for accurate object detection and\\nsemantic segmentation. In CVPR, 2014.\\n\\n[20] C. Goldfeder, M. Ciocarlie, H. Dang, and P. K. Allen.\\n\\nThe Columbia grasp database. In ICRA, 2009.\\n\\n[21] I. Goodfellow, Q. Le, A. Saxe, H. Lee, and A. Y. Ng.\\nMeasuring invariances in deep networks. In NIPS, 2009.\\n[22] G. Hinton and R. Salakhutdinov. Reducing the dimen-\\nsionality of data with neural networks. Science, 313\\n(5786):504\\u2013507, 2006.\\n\\n[23] K. Huebner and D. Kragic. Selection of Robot Pre-\\nGrasps using Box-Based Shape Approximation. In IROS,\\n2008.\\n\\n[24] A. Hyv\\xa8arinen, P. O. Hoyer, and M. Inki. Topographic\\nindependent component analysis. Neural computation,\\n13(7):1527\\u20131558, 2001.\\n\\n[25] A. Hyv\\xa8arinen, J. Karhunen, and E. Oja.\\n\\nPrincipal\\nComponent Analysis and Whitening, chapter 6, pages\\n125\\u2013144. John Wiley & Sons, Inc., 2002.\\n\\n[26] A. Jalali, P. Ravikumar, S. Sanghavi, and C. Ruan. A\\n\\ndirty model for multi-task learning. In NIPS, 2010.\\n\\n[27] N. R. Jared Glover, Daniela Rus. Probabilistic models\\n\\nof object geometry for grasp planning. In RSS, 2008.\\n\\n[28] Y. Jiang, S. Moseson, and A. Saxena. Ef\\ufb01cient grasping\\nfrom RGBD images: Learning using a new rectangle\\nrepresentation. In ICRA, 2011.\\n\\n[29] Y. Jiang, J. R. Amend, H. Lipson, and A. Saxena. Learn-\\ning hardware agnostic grasps for a universal jamming\\ngripper. In ICRA, 2012.\\n\\n[30] Y. Jiang, M. Lim, C. Zheng, and A. Saxena. Learning to\\n\\nplace new objects in a scene. IJRR, 31(9), 2012.\\n\\n[31] I. Kamon, T. Flash, and S. Edelman. Learning to grasp\\n\\nusing visual information. In ICRA, 1996.\\n\\n[32] D. Kragic and H. I. Christensen. Robust visual servoing.\\n\\nIJRR, 2003.\\n\\n[33] K. Lai, L. Bo, X. Ren, and D. Fox. A large-scale\\nIn ICRA,\\n\\nhierarchical multi-view rgb-d object dataset.\\n2011.\\n\\n[34] K. Lakshminarayana. Mechanics of form closure. ASME,\\n\\n1978.\\n\\n[35] Q. Le, M. Ranzato, R. Monga, M. Devin, K. Chen,\\nG. Corrado, J. Dean, and A. Ng. Building high-level\\nfeatures using large scale unsupervised learning.\\nIn\\nICML, 2012.\\n\\n[36] Q. V. Le, D. Kamm, A. F. Kara, and A. Y. Ng. Learning\\nto grasp objects with multiple contact points. In ICRA,\\n2010.\\n\\n[37] Y. LeCun, F. Huang, and L. Bottou. Learning methods\\nfor generic object recognition with invariance to pose and\\nlighting. In CVPR, 2004.\\n\\n[38] H. Lee, R. Grosse, R. Ranganath, and A. Y. Ng. Convo-\\nlutional deep belief networks for scalable unsupervised\\nlearning of hierarchical representations. In ICML, 2009.\\n[39] H. Lee, Y. Largman, P. Pham, and A. Y. Ng. Unsu-\\npervised feature learning for audio classi\\ufb01cation using\\nconvolutional deep belief networks. In NIPS, 2009.\\n\\n[40] J. Maitin-shepard, M. Cusumano-towner, J. Lei, and\\nP. Abbeel. Cloth grasp point detection based on multiple-\\nview geometric cues with application to robotic towel\\nfolding. In ICRA, 2010.\\n\\n\\x0c[63] C. Teuliere and E. Marchand. Direct 3d servoing using\\n\\ndense depth maps. In IROS, 2012.\\n\\n[64] P. Viola and M. Jones. Rapid object detection using a\\n\\nboosted cascade of simple features. In CVPR, 2001.\\n\\n[65] J. Weisz and P. K. Allen. Pose error robust grasping from\\n\\ncontact wrench space metrics. In ICRA, 2012.\\n\\n[66] T. Whelan, H. Johannsson, M. Kaess, J. Leonard, and\\nJ. McDonald. Robust real-time visual odometry for dense\\nRGB-D mapping. In ICRA, 2013.\\n\\n[67] L. Zhang, M. Ciocarlie, and K. Hsiao. Grasp evaluation\\nIn RSS Workshop on\\n\\nwith graspable feature matching.\\nMobile Manipulation, 2011.\\n\\n[41] A.-R. Mohamed, G. Dahl, and G. E. Hinton. Acoustic\\nmodeling using deep belief networks. IEEE Trans Audio,\\nSpeech, and Language Processing, 20(1):14\\u201322, 2012.\\n[42] A. Morales, P. J. Sanz, and `Angel P. del Pobil. Vision-\\nbased computation of three-\\ufb01nger grasps on unknown\\nplanar objects. In IROS, 2002.\\n\\n[43] J. Ngiam, A. Khosla, M. Kim, J. Nam, H. Lee, and A. Y.\\n\\nNg. Multimodal deep learning. In ICML, 2011.\\n\\n[44] V. Nguyen. Constructing stable force-closure grasps. In\\n\\nACM Fall joint computer conf, 1986.\\n\\n[45] M. Osadchy, Y. LeCun, and M. Miller. Synergistic face\\ndetection and pose estimation with energy-based models.\\nJMLR, 8:1197\\u20131215, 2007.\\n\\n[46] C. Papazov, S. Haddadin, S. Parusel, K. Krieger, and\\nD. Burschka. Rigid 3d geometry matching for grasping\\nof known objects in cluttered scenes. IJRR, 31(4):538\\u2013\\n553, Apr. 2012.\\n\\n[47] J. H. Piater. Learning visual features to predict hand\\n\\norientations. In ICML, 2002.\\n\\n[48] F. T. Pokorny, K. Hang, and D. Kragic. Grasp moduli\\n\\nspaces. In RSS, 2013.\\n\\n[49] J. Ponce, D. Stam, and B. Faverjon. On computing two-\\n\\ufb01nger force-closure grasps of curved 2D objects. IJRR,\\n12(3):263, 1993.\\n\\n[50] A. Rodriguez, M. Mason, and S. Ferry. From caging to\\n\\ngrasping. In RSS, 2011.\\n\\n[51] R. B. Rusu, N. Blodow, and M. Beetz. Fast point feature\\nhistograms (FPFH) for 3D registration. In ICRA, 2009.\\n[52] R. B. Rusu, G. Bradski, R. Thibaux, and J. Hsu. Fast\\n3D recognition and pose using the viewpoint feature\\nhistogram. In IROS, 2010.\\n\\n[53] A. Sahbani, S. El-Khoury, and P. Bidaud. An overview\\nof 3d object grasp synthesis algorithms. Robot. Auton.\\nSyst., 60(3):326\\u2013336, Mar. 2012.\\n\\n[54] A. Saxena, J. Driemeyer, J. Kearns, and A. Ng. Robotic\\n\\ngrasping of novel objects. In NIPS, 2006.\\n\\n[55] A. Saxena, J. Driemeyer, and A. Y. Ng. Robotic grasping\\nIJRR, 27(2):157\\u2013173,\\n\\nof novel objects using vision.\\n2008.\\n\\n[56] A. Saxena, L. L. S. Wong, and A. Y. Ng. Learning grasp\\nstrategies with partial shape information. In AAAI, 2008.\\n[57] P. Sermanet, K. Kavukcuoglu, S. Chintala, and Y. Le-\\nCun. Pedestrian detection with unsupervised multi-stage\\nfeature learning. In CVPR. 2013.\\n\\n[58] K. B. Shimoga. Robot grasp synthesis algorithms: A\\n\\nsurvey. IJRR, 15(3):230\\u2013266, June 1996.\\n\\n[59] R. Socher, B. Huval, B. Bhat, C. D. Manning, and A. Y.\\nNg. Convolutional-recursive deep learning for 3D object\\nclassi\\ufb01cation. In NIPS, 2012.\\n\\n[60] K. Sohn, D. Y. Jung, H. Lee, and A. Hero III. Ef\\ufb01cient\\nlearning of sparse, distributed, convolutional feature rep-\\nresentations for object recognition. In ICCV, 2011.\\n\\n[61] N. Srivastava and R. Salakhutdinov. Multimodal learning\\n\\nwith deep Boltzmann machines. In NIPS, 2012.\\n\\n[62] C. Szegedy, A. Toshev, and D. Erhan. Deep neural\\n\\nnetworks for object detection. In NIPS. 2013.\\n\\n\\x0c', u'Simultaneous Deep Transfer Across Domains and Tasks\\n\\nEric Tzeng\\u2217, Judy Hoffman\\u2217, Trevor Darrell\\n\\nUC Berkeley, EECS & ICSI\\n\\nKate Saenko\\n\\nUMass Lowell, CS\\n\\n{etzeng,jhoffman,trevor}@eecs.berkeley.edu\\n\\nsaenko@cs.uml.edu\\n\\nAbstract\\n\\nRecent reports suggest that a generic supervised deep\\nCNN model trained on a large-scale dataset reduces, but\\ndoes not remove, dataset bias. Fine-tuning deep models in\\na new domain can require a signi\\ufb01cant amount of labeled\\ndata, which for many applications is simply not available.\\nWe propose a new CNN architecture to exploit unlabeled and\\nsparsely labeled target domain data. Our approach simulta-\\nneously optimizes for domain invariance to facilitate domain\\ntransfer and uses a soft label distribution matching loss to\\ntransfer information between tasks. Our proposed adapta-\\ntion method offers empirical performance which exceeds\\npreviously published results on two standard benchmark vi-\\nsual domain adaptation tasks, evaluated across supervised\\nand semi-supervised adaptation settings.\\n\\n1. Introduction\\n\\nConsider a group of robots trained by the manufacturer\\nto recognize thousands of common objects using standard\\nimage databases, then shipped to households around the\\ncountry. As each robot starts to operate in its own unique\\nenvironment, it is likely to have degraded performance due\\nto the shift in domain. It is clear that, given enough ex-\\ntra supervised data from the new environment, the original\\nperformance could be recovered. However, state-of-the-art\\nrecognition algorithms rely on high capacity convolutional\\nneural network (CNN) models that require millions of su-\\npervised images for initial training. Even the traditional\\napproach for adapting deep models, \\ufb01ne-tuning [14, 29],\\nmay require hundreds or thousands of labeled examples for\\neach object category that needs to be adapted.\\n\\nIt is reasonable to assume that the robot\\u2019s new owner\\nwill label a handful of examples for a few types of objects,\\nbut completely unrealistic to presume full supervision in\\nthe new environment. Therefore, we propose an algorithm\\nthat effectively adapts between the training (source) and test\\n(target) environments by utilizing both generic statistics from\\n\\n\\u2217 Authors contributed equally.\\n\\nSource domain!\\n\\nTarget domain!\\n\\n!\\n\\n!\\n\\n!\\n\\n!\\n\\n!\\n\\n!\\n\\n1. Maximize domain confusion!\\n\\n!\\n\\n!\\n\\n!\\n!\\n!\\n!\\n\\n(cid:1)!\\n(cid:1)!\\n(cid:1)!\\n(cid:1)!\\n\\nt l e M u g\\n\\nB o t\\n\\nC h a i r\\n\\nL a p t o p\\n\\nK e y b o a r d\\n\\nt l e M u g\\n\\nB o t\\n\\nC h a i r\\n\\nL a p t o p\\n\\nK e y b o a r d\\n\\n2. Transfer task correlation!\\n\\nFigure 1. We transfer discriminative category information from\\na source domain to a target domain via two methods. First, we\\nmaximize domain confusion by making the marginal distributions\\nof the two domains as similar as possible. Second, we transfer cor-\\nrelations between classes learned on the source examples directly to\\nthe target examples, thereby preserving the relationships between\\nclasses.\\n\\nunlabeled data collected in the new environment as well as a\\nfew human labeled examples from a subset of the categories\\nof interest. Our approach performs transfer learning both\\nacross domains and across tasks (see Figure 1). Intuitively,\\ndomain transfer is accomplished by making the marginal\\nfeature distributions of source and target as similar to each\\nother as possible. Task transfer is enabled by transferring\\nempirical category correlations learned on the source to the\\ntarget domain. This helps to preserve relationships between\\ncategories, e.g., bottle is similar to mug but different from\\nkeyboard. Previous work proposed techniques for domain\\ntransfer with CNN models [12, 24] but did not utilize the\\nlearned source semantic structure for task transfer.\\n\\nTo enable domain transfer, we use the unlabeled target\\ndata to compute an estimated marginal distribution over the\\nnew environment and explicitly optimize a feature repre-\\n\\n14068\\n\\n\\x0csentation that minimizes the distance between the source\\nand target domain distributions. Dataset bias was classi-\\ncally illustrated in computer vision by the \\u201cname the dataset\\u201d\\ngame of Torralba and Efros [31], which trained a classi\\ufb01er\\nto predict which dataset an image originates from, thereby\\nshowing that visual datasets are biased samples of the visual\\nworld. Indeed, this turns out to be formally connected to\\nmeasures of domain discrepancy [21, 5]. Optimizing for\\ndomain invariance, therefore, can be considered equivalent\\nto the task of learning to predict the class labels while simul-\\ntaneously \\ufb01nding a representation that makes the domains\\nappear as similar as possible. This principle forms the do-\\nmain transfer component of our proposed approach. We\\nlearn deep representations by optimizing over a loss which\\nincludes both classi\\ufb01cation error on the labeled data as well\\nas a domain confusion loss which seeks to make the domains\\nindistinguishable.\\n\\nHowever, while maximizing domain confusion pulls the\\nmarginal distributions of the domains together, it does not\\nnecessarily align the classes in the target with those in the\\nsource. Thus, we also explicitly transfer the similarity struc-\\nture amongst categories from the source to the target and\\nfurther optimize our representation to produce the same struc-\\nture in the target domain using the few target labeled exam-\\nples as reference points. We are inspired by prior work on\\ndistilling deep models [3, 16] and extend the ideas presented\\nin these works to a domain adaptation setting. We \\ufb01rst com-\\npute the average output probability distribution, or \\u201csoft\\nlabel,\\u201d over the source training examples in each category.\\nThen, for each target labeled example, we directly optimize\\nour model to match the distribution over classes to the soft\\nlabel. In this way we are able to perform task adaptation by\\ntransferring information to categories with no explicit labels\\nin the target domain.\\n\\nWe solve the two problems jointly using a new CNN ar-\\nchitecture, outlined in Figure 2. We combine a domain con-\\nfusion and softmax cross-entropy losses to train the network\\nwith the target data. Our architecture can be used to solve su-\\npervised adaptation, when a small amount of target labeled\\ndata is available from each category, and semi-supervised\\nadaptation, when a small amount of target labeled data is\\navailable from a subset of the categories. We provide a com-\\nprehensive evaluation on the popular Of\\ufb01ce benchmark [28]\\nand the recently introduced cross-dataset collection [30] for\\nclassi\\ufb01cation across visually distinct domains. We demon-\\nstrate that by jointly optimizing for domain confusion and\\nmatching soft labels, we are able to outperform the current\\nstate-of-the-art visual domain adaptation results.\\n\\n2. Related work\\n\\nThere have been many approaches proposed in recent\\nyears to solve the visual domain adaptation problem, which\\nis also commonly framed as the visual dataset bias prob-\\n\\nlem [31]. All recognize that there is a shift in the distri-\\nbution of the source and target data representations.\\nIn\\nfact, the size of a domain shift is often measured by the\\ndistance between the source and target subspace representa-\\ntions [5, 11, 21, 25, 27]. A large number of methods have\\nsought to overcome this difference by learning a feature\\nspace transformation to align the source and target repre-\\nsentations [28, 23, 11, 15]. For the supervised adaptation\\nscenario, when a limited amount of labeled data is available\\nin the target domain, some approaches have been proposed\\nto learn a target classi\\ufb01er regularized against the source clas-\\nsi\\ufb01er [32, 2, 1]. Others have sought to both learn a feature\\ntransformation and regularize a target classi\\ufb01er simultane-\\nously [18, 10].\\n\\nRecently, supervised CNN based feature representations\\nhave been shown to be extremely effective for a variety of\\nvisual recognition tasks [22, 9, 14, 29]. In particular, using\\ndeep representations dramatically reduces the effect of reso-\\nlution and lighting on domain shifts [9, 19]. Parallel CNN\\narchitectures such as Siamese networks have been shown\\nto be effective for learning invariant representations [6, 8].\\nHowever, training these networks requires labels for each\\ntraining instance, so it is unclear how to extend these meth-\\nods to unsupervised or semi-supervised settings. Multimodal\\ndeep learning architectures have also been explored to learn\\nrepresentations that are invariant to different input modal-\\nities [26]. However, this method operated primarily in a\\ngenerative context and therefore did not leverage the full\\nrepresentational power of supervised CNN representations.\\n\\nTraining a joint source and target CNN architecture was\\nproposed by [7], but was limited to two layers and so was\\nsigni\\ufb01cantly outperformed by the methods which used a\\ndeeper architecture [22], pre-trained on a large auxiliary\\ndata source (ex: ImageNet [4]). [13] proposed pre-training\\nwith a denoising auto-encoder, then training a two-layer net-\\nwork simultaneously with the MMD domain confusion loss.\\nThis effectively learns a domain invariant representation, but\\nagain, because the learned network is relatively shallow, it\\nlacks the strong semantic representation that is learned by di-\\nrectly optimizing a classi\\ufb01cation objective with a supervised\\ndeep CNN.\\n\\nUsing classi\\ufb01er output distributions instead of category\\nlabels during training has been explored in the context of\\nmodel compression or distillation [3, 16]. However, we are\\nthe \\ufb01rst to apply this technique in a domain adaptation setting\\nin order to transfer class correlations between domains.\\n\\nOther works have cotemporaneously explored the idea\\nof directly optimizing a representation for domain invari-\\nance [12, 24]. However, they either use weaker measures\\nof domain invariance or make use of optimization methods\\nthat are less robust than our proposed method, and they do\\nnot attempt to solve the task transfer problem in the semi-\\nsupervised setting.\\n\\n4069\\n\\n\\x0c3. Joint CNN architecture for domain and task\\n\\ntransfer\\n\\nWe \\ufb01rst give an overview of our convolutional network\\n(CNN) architecture, depicted in Figure 2, that learns a rep-\\nresentation which both aligns visual domains and transfers\\nthe semantic structure from a well labeled source domain to\\nthe sparsely labeled target domain. We assume access to a\\nlimited amount of labeled target data, potentially from only\\na subset of the categories of interest. With limited labels on\\na subset of the categories, the traditional domain transfer ap-\\nproach of \\ufb01ne-tuning on the available target data [14, 29, 17]\\nis not effective. Instead, since the source labeled data shares\\nthe label space of our target domain, we use the source data\\nto guide training of the corresponding classi\\ufb01ers.\\n\\nOur method takes as input the labeled source data\\n{xS, yS} (blue box Figure 2) and the target data {xT , yT }\\n(green box Figure 2), where the labels yT are only provided\\nfor a subset of the target examples. Our goal is to produce\\na category classi\\ufb01er \\u03b8C that operates on an image feature\\nrepresentation f (x; \\u03b8repr) parameterized by representation\\nparameters \\u03b8repr and can correctly classify target examples\\nat test time.\\n\\nFor a setting with K categories, let our desired classi\\ufb01ca-\\n\\ntion objective be de\\ufb01ned as the standard softmax loss\\n\\nLC(x, y; \\u03b8repr, \\u03b8C) = \\u2212 X\\n\\n\\u2736[y = k] log pk\\n\\n(1)\\n\\nk\\n\\nwhere p is the softmax of the classi\\ufb01er activations,\\np = softmax(\\u03b8T\\n\\nCf (x; \\u03b8repr)).\\n\\nWe could use the available source labeled data to train\\nour representation and classi\\ufb01er parameters according to\\nEquation (1), but this often leads to over\\ufb01tting to the source\\ndistribution, causing reduced performance at test time when\\nrecognizing in the target domain. However, we note that\\nif the source and target domains are very similar then the\\nclassi\\ufb01er trained on the source will perform well on the\\ntarget. In fact, it is suf\\ufb01cient for the source and target data to\\nbe similar under the learned representation, \\u03b8repr.\\n\\nInspired by the \\u201cname the dataset\\u201d game of Torralba\\nand Efros [31], we can directly train a domain classi\\ufb01er\\n\\u03b8D to identify whether a training example originates from\\nthe source or target domain given its feature representation.\\nIntuitively, if our choice of representation suffers from do-\\nmain shift, then they will lie in distinct parts of the feature\\nspace, and a classi\\ufb01er will be able to easily separate the\\ndomains. We use this notion to add a new domain confusion\\nloss Lconf(xS, xT , \\u03b8D; \\u03b8repr) to our objective and directly op-\\ntimize our representation so as to minimize the discrepancy\\nbetween the source and target distributions. This loss is\\ndescribed in more detail in Section 3.1.\\n\\nDomain confusion can be applied to learn a representation\\nthat aligns source and target data without any target labeled\\n\\ndata. However, we also presume a handful of sparse labels\\nin the target domain, yT . In this setting, a simple approach is\\nto incorporate the target labeled data along with the source\\nlabeled data into the classi\\ufb01cation objective of Equation (1)1.\\nHowever, \\ufb01ne-tuning with hard category labels limits the\\nimpact of a single training example, making it hard for the\\nnetwork to learn to generalize from the limited labeled data.\\nAdditionally, \\ufb01ne-tuning with hard labels is ineffective when\\nlabeled data is available for only a subset of the categories.\\nFor our approach, we draw inspiration from recent net-\\nwork distillation works [3, 16], which demonstrate that a\\nlarge network can be \\u201cdistilled\\u201d into a simpler model by re-\\nplacing the hard labels with the softmax activations from the\\noriginal large model. This modi\\ufb01cation proves to be critical,\\nas the distribution holds key information about the relation-\\nships between categories and imposes additional structure\\nduring the training process. In essence, because each train-\\ning example is paired with an output distribution, it provides\\nvaluable information about not only the category it belongs\\nto, but also each other category the classi\\ufb01er is trained to\\nrecognize.\\n\\nThus, we propose using the labeled target data to op-\\ntimize the network parameters through a soft label loss,\\nLsoft(xT , yT ; \\u03b8repr, \\u03b8C). This loss will train the network pa-\\nrameters to produce a \\u201csoft label\\u201d activation that matches\\nthe average output distribution of source examples on a net-\\nwork trained to classify source data. This loss is described in\\nmore detail in Section 3.2. By training the network to match\\nthe expected source output distributions on target data, we\\ntransfer the learned inter-class correlations from the source\\ndomain to examples in the target domain. This directly trans-\\nfers useful information from source to target, such as the fact\\nthat bookshelves appear more similar to \\ufb01ling cabinets than\\nto bicycles.\\n\\nOur full method then minimizes the joint loss function\\n\\nL(xS, yS, xT , yT , \\u03b8D;\\u03b8repr, \\u03b8C) =\\n\\nLC(xS, yS, xT , yT ; \\u03b8repr, \\u03b8C)\\n+ \\u03bbLconf(xS, xT , \\u03b8D; \\u03b8repr)\\n+ \\u03bdLsoft(xT , yT ; \\u03b8repr, \\u03b8C).\\n\\n(2)\\n\\nwhere the hyperparameters \\u03bb and \\u03bd determine how strongly\\ndomain confusion and soft labels in\\ufb02uence the optimization.\\nOur ideas of domain confusion and soft label loss for task\\ntransfer are generic and can be applied to any CNN classi\\ufb01-\\ncation architecture. For our experiments and for the detailed\\ndiscussion in this paper we modify the standard Krizhevsky\\narchitecture [22], which has \\ufb01ve convolutional layers (conv1\\u2013\\nconv5) and three fully connected layers (fc6\\u2013fc8). The rep-\\nresentation parameter \\u03b8repr corresponds to layers 1\\u20137 of the\\nnetwork, and the classi\\ufb01cation parameter \\u03b8C corresponds to\\nlayer 8. For the remainder of this section, we provide further\\n\\n1We present this approach as one of our baselines.\\n\\n4070\\n\\n\\x0cbackpack\\n\\nchair\\n\\nbike\\n\\nSource Data\\n\\nbackpack\\n\\nTarget Data\\n\\n?\\n\\nconv1\\n\\nconv5\\n\\nfc6\\n\\nfc7\\n\\nsource data\\n\\nfc8\\n\\nd\\ne\\nr\\na\\nh\\ns\\n\\nd\\ne\\nr\\na\\nh\\ns\\n\\nd\\ne\\nr\\na\\nh\\ns\\n\\nd\\ne\\nr\\na\\nh\\ns\\n\\nconv1\\n\\nconv5\\n\\nfc6\\n\\nfc7\\n\\ns\\n\\no\\n\\nu\\n\\nr\\n\\nc\\n\\ne\\n\\n \\n\\nd\\n\\na\\n\\nt\\n\\na\\n\\nfcD\\n\\nata\\net d\\nall targ\\n\\nd\\ne\\nr\\na\\nh\\ns\\n\\ndomain \\n\\nconfusion \\n\\nloss\\n\\ndomain \\nclassi\\ufb01er \\n\\nloss\\n\\nclassi\\ufb01cation \\n\\nloss\\n\\nfc8\\n\\nsoftmax \\nhigh temp\\n\\nsoftlabel \\n\\nloss\\n\\nlabeled target data\\n\\nSource softlabels\\n\\nFigure 2. Our overall CNN architecture for domain and task transfer. We use a domain confusion loss over all source and target (both labeled\\nand unlabeled) data to learn a domain invariant representation. We simultaneously transfer the learned source semantic structure to the target\\ndomain by optimizing the network to produce activation distributions that match those learned for source data in the source only CNN. Best\\nviewed in color.\\n\\ndetails on our novel loss de\\ufb01nitions and the implementation\\nof our model.\\n\\n3.1. Aligning domains via domain confusion\\n\\nIn this section we describe in detail our proposed domain\\nconfusion loss objective. Recall that we introduce the domain\\nconfusion loss as a means to learn a representation that is\\ndomain invariant, and thus will allow us to better utilize a\\nclassi\\ufb01er trained using the labeled source data. We consider\\na representation to be domain invariant if a classi\\ufb01er trained\\nusing that representation can not distinguish examples from\\nthe two domains.\\n\\nTo this end, we add an additional domain classi\\ufb01cation\\nlayer, denoted as fcD in Figure 2, with parameters \\u03b8D. This\\nlayer simply performs binary classi\\ufb01cation using the domain\\ncorresponding to an image as its label. For a particular fea-\\nture representation, \\u03b8repr, we evaluate its domain invariance\\nby learning the best domain classi\\ufb01er on the representation.\\nThis can be learned by optimizing the following objective,\\nwhere yD denotes the domain that the example is drawn\\nfrom:\\n\\nLD(xS, xT , \\u03b8repr; \\u03b8D) = \\u2212 X\\n\\n\\u2736[yD = d] log qd\\n\\n(3)\\n\\nd\\n\\nwith q corresponding to the softmax of the domain classi\\ufb01er\\nactivation: q = softmax(\\u03b8T\\n\\nDf (x; \\u03b8repr)).\\n\\nFor a particular domain classi\\ufb01er, \\u03b8D, we can now in-\\ntroduce our loss which seeks to \\u201cmaximally confuse\\u201d the\\ntwo domains by computing the cross entropy between the\\noutput predicted domain labels and a uniform distribution\\nover domain labels:\\n\\nLconf(xS, xT , \\u03b8D; \\u03b8repr) = \\u2212 X\\n\\nd\\n\\n1\\nD\\n\\nlog qd.\\n\\n(4)\\n\\nThis domain confusion loss seeks to learn domain invari-\\nance by \\ufb01nding a representation in which the best domain\\nclassi\\ufb01er performs poorly.\\n\\nIdeally, we want to simultaneously minimize Equa-\\ntions (3) and (4) for the representation and the domain clas-\\nsi\\ufb01er parameters. However, the two losses stand in direct\\nopposition to one another: learning a fully domain invariant\\nrepresentation means the domain classi\\ufb01er must do poorly,\\nand learning an effective domain classi\\ufb01er means that the\\nrepresentation is not domain invariant. Rather than globally\\noptimizing \\u03b8D and \\u03b8repr, we instead perform iterative updates\\nfor the following two objectives given the \\ufb01xed parameters\\nfrom the previous iteration:\\n\\nmin\\n\\u03b8D\\n\\nmin\\n\\u03b8repr\\n\\nLD(xS, xT , \\u03b8repr; \\u03b8D)\\n\\nLconf(xS, xT , \\u03b8D; \\u03b8repr).\\n\\n(5)\\n\\n(6)\\n\\nThese losses are readily implemented in standard deep\\nlearning frameworks, and after setting learning rates properly\\nso that Equation (5) only updates \\u03b8D and Equation (6) only\\nupdates \\u03b8repr, the updates can be performed via standard\\nbackpropagation. Together, these updates ensure that we\\nlearn a representation that is domain invariant.\\n\\n3.2. Aligning source and target classes via soft labels\\n\\nWhile training the network to confuse the domains acts\\nto align their marginal distributions, there are no guarantees\\nabout the alignment of classes between each domain. To\\nensure that the relationships between classes are preserved\\nacross source and target, we \\ufb01ne-tune the network against\\n\\u201csoft labels\\u201d rather than the image category hard label.\\n\\nWe de\\ufb01ne a soft label for category k as the average over\\nthe softmax of all activations of source examples in category\\n\\n4071\\n\\n\\x0cSource \\n\\nCNN\\n\\nsoftmax \\n\\nhigh \\ntemp\\n\\nSource \\n\\nCNN\\n\\nsoftmax \\n\\nhigh \\ntemp\\n\\nt l e M u g\\n\\nB o t\\n\\nC h a i r\\n\\nL a p t o p\\n\\nK e y b o a r d\\n\\nt l e M u g\\n\\nB o t\\n\\nC h a i r\\n\\nL a p t o p\\n\\nK e y b o a r d\\n\\n+\\n\\nSource \\n\\nCNN\\n\\nsoftmax \\n\\nhigh \\ntemp\\n\\nt l e M u g\\n\\nB o t\\n\\nC h a i r\\n\\nL a p t o p\\n\\nK e y b o a r d\\n\\nt l e M u g\\n\\nB o t\\n\\nC h a i r\\n\\nL a p t o p\\n\\nK e y b o a r d\\n\\nFigure 3. Soft label distributions are learned by averaging the per-\\ncategory activations of source training examples using the source\\nmodel. An example, with 5 categories, depicted here to demonstrate\\nthe \\ufb01nal soft activation for the bottle category will be primarily\\ndominated by bottle and mug with very little mass on chair, laptop,\\nand keyboard.\\n\\nvisually similar. Thus, soft label training with this particular\\nsoft label directly enforces the relationship that bottles and\\nmugs should be closer in feature space than bottles and\\nkeyboards.\\n\\nOne important bene\\ufb01t of using this soft label loss is that\\nwe ensure that the parameters for categories without any\\nlabeled target data are still updated to output non-zero proba-\\nbilities. We explore this bene\\ufb01t in Section 4, where we train\\na network using labels from a subset of the target categories\\nand \\ufb01nd signi\\ufb01cant performance improvement even when\\nevaluating only on the unlabeled categories.\\n\\n4. Evaluation\\n\\nTo analyze the effectiveness of our method, we evaluate it\\non the Of\\ufb01ce dataset, a standard benchmark dataset for visual\\ndomain adaptation, and on a new large-scale cross-dataset\\ndomain adaptation challenge.\\n\\nAdapt CNN\\n\\nbackprop\\n\\n4.1. Adaptation on the Of\\ufb01ce dataset\\n\\nsoftmax \\n\\nhigh \\ntemp\\n\\n\\u201cBottle\\u201d\\n\\nt l e M u g\\n\\nB o t\\n\\nC h a i r\\n\\nL a p t o p\\n\\nK e y b o a r d\\n\\nCross Entropy Loss\\n\\nSource Activations \\n\\nPer Class\\n\\nt l e M u g\\n\\nB o t\\n\\nC h a i r\\n\\nL a p t o p\\n\\nK e y b o a r d\\n\\nFigure 4. Depiction of the use of source per-category soft activa-\\ntions with the cross entropy loss function over the current target\\nactivations.\\n\\nk, depicted graphically in Figure 3, and denote this aver-\\nage as l(k). Note that, since the source network was trained\\npurely to optimize a classi\\ufb01cation objective, a simple soft-\\nmax over each zi\\nS will hide much of the useful information\\nby producing a very peaked distribution. Instead, we use a\\nsoftmax with a high temperature \\u03c4 so that the related classes\\nhave enough probability mass to have an effect during \\ufb01ne-\\ntuning. With our computed per-category soft labels we can\\nnow de\\ufb01ne our soft label loss:\\n\\nLsoft(xT , yT ; \\u03b8repr, \\u03b8C) = \\u2212 X\\n\\nl(yT )\\n\\ni\\n\\nlog pi\\n\\n(7)\\n\\ni\\n\\nwhere p denotes the soft activation of the target image,\\np = softmax(\\u03b8T\\nCf (xT ; \\u03b8repr)/\\u03c4 ). The loss above corre-\\nsponds to the cross-entropy loss between the soft activation\\nof a particular target image and the soft label corresponding\\nto the category of that image, as shown in Figure 4.\\n\\nTo see why this will help, consider the soft label for a\\nparticular category, such as bottle. The soft label l(bottle) is\\na K-dimensional vector, where each dimension indicates\\nthe similarity of bottles to each of the K categories. In this\\nexample, the bottle soft label will have a higher weight on\\nmug than on keyboard, since bottles and mugs are more\\n\\nThe Of\\ufb01ce dataset is a collection of images from three\\ndistinct domains, Amazon, DSLR, and Webcam, the largest\\nof which has 2817 labeled images [28]. The 31 categories\\nin the dataset consist of objects commonly encountered in\\nof\\ufb01ce settings, such as keyboards, \\ufb01le cabinets, and laptops.\\n\\nWe evaluate our method in two different settings:\\n\\n\\u2022 Supervised adaptation Labeled training data for all\\ncategories is available in source and sparsely in target.\\n\\u2022 Semi-supervised adaptation (task adaptation) La-\\nbeled training data is available in source and sparsely\\nfor a subset of the target categories.\\n\\nFor all experiments we initialize the parameters of conv1\\u2013\\nfc7 using the released CaffeNet [20] weights. We then fur-\\nther \\ufb01ne-tune the network using the source labeled data in or-\\nder to produce the soft label distributions and use the learned\\nsource CNN weights as the initial parameters for training\\nour method. All implementations are produced using the\\nopen source Caffe [20] framework, and the network de\\ufb01ni-\\ntion \\ufb01les and cross entropy loss layer needed for training\\nwill be released upon acceptance. We optimize the network\\nusing a learning rate of 0.001 and set the hyper-parameters\\nto \\u03bb = 0.01 (confusion) and \\u03bd = 0.1 (soft).\\n\\nFor each of the six domain shifts, we evaluate across \\ufb01ve\\ntrain/test splits, which are generated by sampling examples\\nfrom the full set of images per domain. In the source domain,\\nwe follow the standard protocol for this dataset and generate\\nsplits by sampling 20 examples per category for the Amazon\\ndomain, and 8 examples per category for the DSLR and\\nWebcam domains.\\n\\nWe \\ufb01rst present results for the supervised setting, where\\n3 labeled examples are provided for each category in the\\ntarget domain. We report accuracies on the remaining un-\\nlabeled images, following the standard protocol introduced\\n\\n4072\\n\\n\\x0cA \\u2192 W\\n\\nA \\u2192 D W \\u2192 A W \\u2192 D\\n\\nD \\u2192 A\\n\\nD \\u2192 W Average\\n\\nDLID [7]\\nDeCAF6 S+T [9]\\nDaNN [13]\\nSource CNN\\nTarget CNN\\nSource+Target CNN\\n\\n\\u2013\\n\\u2013\\n\\u2013\\n\\n\\u2013\\n\\u2013\\n\\u2013\\n\\n\\u2013\\n\\u2013\\n\\u2013\\n\\n51.9\\n\\n89.9\\n\\n78.2\\n\\n94.8 \\xb1 1.2\\n80.7 \\xb1 2.3\\n53.6 \\xb1 0.2\\n71.2 \\xb1 0.0\\n56.5 \\xb1 0.3 64.6 \\xb1 0.4 42.7 \\xb1 0.1 93.6 \\xb1 0.2 47.6 \\xb1 0.1 92.4 \\xb1 0.3\\n80.5 \\xb1 0.5 81.8 \\xb1 1.0 59.9 \\xb1 0.3 81.8 \\xb1 1.0 59.9 \\xb1 0.3 80.5 \\xb1 0.5\\n82.5 \\xb1 0.9 85.2 \\xb1 1.1 65.2 \\xb1 0.7 96.3 \\xb1 0.5 65.8 \\xb1 0.5 93.9 \\xb1 0.5\\n\\n83.5 \\xb1 0.0\\n\\n\\u2013\\n\\n82.8 \\xb1 0.9 85.9 \\xb1 1.1 64.9 \\xb1 0.5 97.5 \\xb1 0.2 66.2 \\xb1 0.4 95.6 \\xb1 0.4\\nOurs: dom confusion only\\n82.7 \\xb1 0.7 84.9 \\xb1 1.2 65.2 \\xb1 0.6 98.3 \\xb1 0.3 66.0 \\xb1 0.5 95.9 \\xb1 0.6\\nOurs: soft labels only\\nOurs: dom confusion+soft labels 82.7 \\xb1 0.8 86.1 \\xb1 1.2 65.0 \\xb1 0.5 97.6 \\xb1 0.2 66.2 \\xb1 0.3 95.7 \\xb1 0.5\\n\\n\\u2013\\n\\u2013\\n\\u2013\\n\\n66.22\\n74.05\\n81.50\\n\\n82.13\\n82.17\\n82.22\\n\\nTable 1. Multi-class accuracy evaluation on the standard supervised adaptation setting with the Of\\ufb01ce dataset. We evaluate on all 31 categories\\nusing the standard experimental protocol from [28]. Here, we compare against three state-of-the-art domain adaptation methods as well as a\\nCNN trained using only source data, only target data, or both source and target data together.\\n\\nA \\u2192 W\\n\\nA \\u2192 D W \\u2192 A W \\u2192 D\\n\\nD \\u2192 A\\n\\nD \\u2192 W Average\\n\\nMMDT [18]\\nSource CNN\\n\\n\\u2013\\n\\n44.6 \\xb1 0.3\\n\\n\\u2013\\n\\n58.3 \\xb1 0.5\\n\\n\\u2013\\n\\n\\u2013\\n\\n54.2 \\xb1 0.6 63.2 \\xb1 0.4 34.7 \\xb1 0.1 94.5 \\xb1 0.2 36.4 \\xb1 0.1 89.3 \\xb1 0.5\\n\\n55.2 \\xb1 0.6 63.7 \\xb1 0.9 41.1 \\xb1 0.0 96.5 \\xb1 0.1 41.2 \\xb1 0.1 91.3 \\xb1 0.4\\nOurs: dom confusion only\\n56.8 \\xb1 0.4 65.2 \\xb1 0.9 38.8 \\xb1 0.4 96.5 \\xb1 0.2 41.7 \\xb1 0.3 89.6 \\xb1 0.1\\nOurs: soft labels only\\nOurs: dom confusion+soft labels 59.3 \\xb1 0.6 68.0 \\xb1 0.5 40.5 \\xb1 0.2 97.5 \\xb1 0.1 43.1 \\xb1 0.2 90.0 \\xb1 0.2\\n\\n\\u2013\\n\\n62.0\\n\\n64.8\\n64.8\\n66.4\\n\\nTable 2. Multi-class accuracy evaluation on the standard semi-supervised adaptation setting with the Of\\ufb01ce dataset. We evaluate on 16\\nheld-out categories for which we have no access to target labeled data. We show results on these unsupervised categories for the source only\\nmodel, our model trained using only soft labels for the 15 auxiliary categories, and \\ufb01nally using domain confusion together with soft labels\\non the 15 auxiliary categories.\\n\\nwith the dataset [28]. In addition to a variety of baselines, we\\nreport numbers for both soft label \\ufb01ne-tuning alone as well\\nas soft labels with domain confusion in Table 1. Because the\\nOf\\ufb01ce dataset is imbalanced, we report multi-class accura-\\ncies, which are obtained by computing per-class accuracies\\nindependently, then averaging over all 31 categories.\\n\\nWe see that \\ufb01ne-tuning with soft labels or domain con-\\nfusion provides a consistent improvement over hard label\\ntraining in 5 of 6 shifts. Combining soft labels with do-\\nmain confusion produces marginally higher performance on\\naverage. This result follows the intuitive notion that when\\nenough target labeled examples are present, directly opti-\\nmizing for the joint source and target classi\\ufb01cation objective\\n(Source+Target CNN) is a strong baseline and so using ei-\\nther of our new losses adds enough regularization to improve\\nperformance.\\n\\nNext, we experiment with the semi-supervised adaptation\\nsetting. We consider the case in which training data and\\nlabels are available for some, but not all of the categories in\\nthe target domain. We are interested in seeing whether we\\ncan transfer information learned from the labeled classes to\\nthe unlabeled classes.\\n\\nTo do this, we consider having 10 target labeled exam-\\nples per category from only 15 of the 31 total categories,\\nfollowing the standard protocol introduced with the Of\\ufb01ce\\ndataset [28]. We then evaluate our classi\\ufb01cation performance\\n\\non the remaining 16 categories for which no data was avail-\\nable at training time.\\n\\nIn Table 2 we present multi-class accuracies over the 16\\nheld-out categories and compare our method to a previous\\ndomain adaptation method [18] as well as a source-only\\ntrained CNN. Note that, since the performance here is com-\\nputed over only a subset of the categories in the dataset, the\\nnumbers in this table should not be directly compared to the\\nsupervised setting in Table 1.\\n\\nWe \\ufb01nd that all variations of our method (only soft label\\nloss, only domain confusion, and both together) outperform\\nthe baselines. Contrary to the fully supervised case, here we\\nnote that both domain confusion and soft labels contribute\\nsigni\\ufb01cantly to the overall performance improvement of our\\nmethod. This stems from the fact that we are now evaluat-\\ning on categories which lack labeled target data, and thus\\nthe network can not implicitly enforce domain invariance\\nthrough the classi\\ufb01cation objective alone. Separately, the\\nfact that we get improvement from the soft label training on\\nrelated tasks indicates that information is being effectively\\ntransferred between tasks.\\n5, we\\n\\nthe\\nAmazon\\u2192Webcam shift where our method correctly\\nclassi\\ufb01es images from held out object categories and the\\nbaseline does not. We \\ufb01nd that our method is able to\\nconsistently overcome error cases, such as the notebooks\\n\\nshow examples\\n\\nFigure\\n\\nfor\\n\\nIn\\n\\n4073\\n\\n\\x0cring binder\\n\\nlaptop computer\\n\\nspeaker\\n\\nmonitor\\n\\nmonitor\\n\\nmonitor\\n\\nscissors\\n\\nmug\\n\\nmouse\\n\\nmug\\n\\nmouse\\n\\nmug\\n\\nlaptop computer\\n\\npaper notebook\\n\\nletter tray\\n\\nletter tray\\n\\nletter tray\\n\\nletter tray\\n\\nlaptop computer\\n\\ncalculator\\n\\ncalculator\\n\\npaper notebook\\n\\npaper notebook\\n\\npaper notebook\\n\\npaper notebook\\n\\npaper notebook\\n\\nphone\\n\\nphone\\n\\nfile cabinet\\n\\nfile cabinet\\n\\nfile cabinet\\n\\nlaptop computer\\n\\nlaptop computer\\n\\nfile cabinet\\n\\nphone\\n\\nprinter\\n\\nprinter\\n\\nprinter\\n\\nprojector\\n\\nprojector\\n\\nprojector\\n\\nprojector\\n\\nkeyboard\\n\\nprojector\\n\\ntape dispenser\\n\\nlaptop computer\\n\\nkeyboard\\n\\nkeyboard\\n\\nletter tray\\n\\nlaptop computer\\n\\npunchers\\n\\nring binder\\n\\nring binder\\n\\nring binder\\n\\nring binder\\n\\nring binder\\n\\nFigure 5. Examples from the Amazon\\u2192Webcam shift in the\\nsemi-supervised adaptation setting, where our method (the bot-\\ntom turquoise label) correctly classi\\ufb01es images while the baseline\\n(the top purple label) does not.\\n\\nthat were previously confused with letter trays, or the black\\nmugs that were confused with black computer mice.\\n\\n4.2. Adaptation between diverse domains\\n\\nFor an evaluation with larger, more distinct domains, we\\ntest on the recent testbed for cross-dataset analysis [30],\\nwhich collects images from classes shared in common among\\ncomputer vision datasets. We use the dense version of this\\ntestbed, which consists of 40 categories shared between\\nthe ImageNet, Caltech-256, SUN, and Bing datasets, and\\nevaluate speci\\ufb01cally with ImageNet as source and Caltech-\\n256 as target.\\n\\nWe follow the protocol outlined in [30] and generate 5\\nsplits by selecting 5534 images from ImageNet and 4366\\nimages from Caltech-256 across the 40 shared categories.\\nEach split is then equally divided into a train and test set.\\nHowever, since we are most interested in evaluating in the\\nsetting with limited target data, we further subsample the\\ntarget training set into smaller sets with only 1, 3, and 5\\nlabeled examples per category.\\n\\nResults from this evaluation are shown in Figure 6. We\\ncompare our method to both CNNs \\ufb01ne-tuned using only\\nsource data using source and target labeled data. Contrary to\\nthe previous supervised adaptation experiment, our method\\nsigni\\ufb01cantly outperforms both baselines. We see that our\\nfull architecture, combining domain confusion with the soft\\nlabel loss, performs the best overall and is able to operate\\nin the regime of no labeled examples in the target (corre-\\nsponding to the red line at point 0 on the x-axis). We \\ufb01nd\\nthat the most bene\\ufb01t of our method arises when there are\\nfew labeled training examples per category in the target do-\\nmain. As we increase the number of labeled examples in\\nthe target, the standard \\ufb01ne-tuning strategy begins to ap-\\n\\ny\\nc\\na\\nr\\nu\\nc\\nc\\nA\\n \\ns\\ns\\na\\nc\\n-\\ni\\nt\\nl\\nu\\nM\\n\\nl\\n\\n78\\n\\n77\\n\\n76\\n\\n75\\n\\n74\\n\\n73\\n\\n72\\n\\nSource CNN\\nSource+Target CNN\\nOurs: softlabels only\\nOurs: dom confusion+softlabels\\n\\n0\\n\\n1\\n\\n3\\n\\n5\\n\\nNumber Labeled Target Examples per Category\\n\\nFigure 6. ImageNet\\u2192Caltech supervised adaptation from the Cross-\\ndataset [30] testbed with varying numbers of labeled target exam-\\nples per category. We \\ufb01nd that our method using soft label loss\\n(with and without domain confusion) outperforms the baselines\\nof training on source data alone or using a standard \\ufb01ne-tuning\\nstrategy to train with the source and target data. Best viewed in\\ncolor.\\n\\nproach the performance of the adaptation approach. This\\nindicates that direct joint source and target \\ufb01ne-tuning is\\na viable adaptation approach when you have a reasonable\\nnumber of training examples per category. In comparison,\\n\\ufb01ne-tuning on the target examples alone yields accuracies\\nof 36.6 \\xb1 0.6, 60.9 \\xb1 0.5, and 67.7 \\xb1 0.5 for the cases of 1,\\n3, and 5 labeled examples per category, respectively. All of\\nthese numbers underperform the source only model, indicat-\\ning that adaptation is crucial in the setting of limited training\\ndata.\\n\\nFinally, we note that our results are signi\\ufb01cantly higher\\nthan the 24.8% result reported in [30], despite the use of\\nmuch less training data. This difference is explained by their\\nuse of SURF BoW features, indicating that CNN features\\nare a much stronger feature for use in adaptation tasks.\\n\\n5. Analysis\\n\\nOur experimental results demonstrate that our method\\nimproves classi\\ufb01cation performance in a variety of domain\\nadaptation settings. We now perform additional analysis on\\nour method by con\\ufb01rming our claims that it exhibits domain\\ninvariance and transfers information across tasks.\\n\\n5.1. Domain confusion enforces domain invariance\\n\\nWe begin by evaluating the effectiveness of domain con-\\nfusion at learning a domain invariant representation. As\\npreviously explained, we consider a representation to be\\ndomain invariant if an optimal classi\\ufb01er has dif\\ufb01culty pre-\\ndicting which domain an image originates from. Thus, for\\nour representation learned with a domain confusion loss, we\\nexpect a trained domain classi\\ufb01er to perform poorly.\\n\\nWe train two support vector machines (SVMs) to clas-\\nsify images into domains: one using the baseline CaffeNet\\n\\n4074\\n\\n\\x0cTarget test image\\n\\nSource soft labels\\n\\nring binder\\n\\nmonitor\\n\\nback pack\\n\\nbike\\n\\nbike helmet\\n\\nbookcase\\n\\nbottle\\n\\ncalculator\\n\\ndesk chair\\n\\ndesk lamp\\n\\ndesktop computer\\n\\nfile cabinet\\n\\nheadphones\\n\\nkeyboard\\n\\nlaptop computer\\n\\nletter tray\\n\\nmobile phone\\n\\n0.1\\n\\n0.09\\n\\n0.08\\n\\n0.07\\n\\n0.06\\n\\n0.05\\n\\n0.04\\n\\n0.03\\n\\n0.02\\n\\n0.01\\n\\n0\\n\\nk  p\\n\\nc\\n\\na\\nb ik\\n\\nc\\n\\na\\n\\nb\\n\\ne t\\nc\\nk\\n\\na\\n\\nk\\nb ik\\ne   h\\n\\ne\\ne l m\\no\\no\\nb\\n\\ne\\n\\ns\\n\\nb\\nc\\n\\nBaseline soft label\\n\\nBaseline soft activation\\n\\nOurs soft label\\n\\nOur soft activation\\n\\n0.1\\n\\n0.09\\n\\n0.08\\n\\n0.07\\n\\n0.06\\n\\n0.05\\n\\n0.04\\n\\n0.03\\n\\n0.02\\n\\n0.01\\n\\na ir\\nh\\nk la\\ns\\no\\np  c\\n\\np\\nu t e r\\nm\\nb in\\np\\na\\nm\\nfile  c\\np\\nd\\na\\ne\\nh\\n\\nla\\n\\ns\\n\\no\\no\\n\\ne\\nn\\nb\\ny\\np  c\\n\\ne t\\no\\ne\\n\\nh\\nk\\np t o\\n\\nu t e r\\na r d\\nle tt e r tr a\\np\\nm\\nb ile   p\\n\\no\\n\\nm\\n\\ny\\nh\\n\\nn\\n\\ne\\n\\no\\n\\nn it o r\\no\\nm\\n\\no\\n\\nm\\n\\np\\n\\na\\n\\np\\n\\nu\\nm\\no t e\\n\\ne\\n\\ns\\nu\\ne r  n\\n\\nk\\n\\no\\n\\ng\\nb\\n\\no\\n\\nn\\nh\\n\\no\\n\\np\\n\\ne\\np\\n\\nn\\n\\ne\\np rin t e r\\np r o je\\np\\n\\nu\\n\\nct o r\\ne r s\\nh\\ng   b in\\nc\\nn\\nrin\\n\\nd\\n\\ne r\\nr u le r\\ncis\\ns\\n\\ns\\ns\\n\\no r s\\ne\\np\\n\\np\\n\\nt a\\n\\nn\\n\\na\\n\\ne r\\nh  c\\n\\ns\\ns\\n\\np le r\\nn\\ne\\np\\ntr a\\n\\na\\n\\ne r\\nk\\nst a\\ne   d is\\n\\n0\\n\\nk  p\\n\\nc\\n\\na\\nb ik\\n\\nc\\n\\na\\n\\nb\\n\\ne t\\nc\\nk\\n\\na\\n\\nk\\nb ik\\ne   h\\n\\ne\\ne l m\\no\\no\\nb\\n\\ne\\n\\ns\\n\\nb\\nc\\n\\nu la t o r\\no ttle\\nk c\\na lc\\ne\\ne\\nd\\nd\\nkt o\\n\\ns\\n\\ns\\n\\ne\\n\\nd\\n\\nu la t o r\\no ttle\\nk c\\na lc\\ne\\ne\\nd\\nd\\nkt o\\n\\ns\\n\\ns\\n\\ne\\n\\nd\\n\\na ir\\nh\\nk la\\ns\\no\\np  c\\n\\np\\nu t e r\\nm\\nb in\\np\\na\\nm\\nfile  c\\np\\nd\\na\\ne\\nh\\n\\nla\\n\\ns\\n\\no\\no\\n\\ne\\nn\\nb\\ny\\np  c\\n\\ne t\\no\\ne\\n\\nh\\nk\\np t o\\n\\nu t e r\\na r d\\nle tt e r tr a\\np\\nm\\nb ile   p\\n\\no\\n\\nm\\n\\ny\\nh\\n\\nn\\n\\ne\\n\\no\\n\\nn it o r\\no\\nm\\n\\no\\n\\nm\\n\\np\\n\\na\\n\\np\\n\\nu\\nm\\no t e\\n\\ne\\n\\ns\\nu\\ne r  n\\n\\nk\\n\\no\\n\\ng\\nb\\n\\no\\n\\nn\\nh\\n\\no\\n\\np\\n\\ne\\np\\n\\nn\\n\\ne\\np rin t e r\\np r o je\\np\\n\\nu\\n\\nct o r\\ne r s\\nh\\ng   b in\\nc\\nn\\nrin\\n\\nd\\n\\ne r\\nr u le r\\ncis\\ns\\n\\ns\\ns\\n\\no r s\\ne\\np\\n\\np\\n\\nt a\\n\\nn\\n\\na\\n\\ne r\\nh  c\\n\\ns\\ns\\n\\np le r\\nn\\ne\\np\\ntr a\\n\\na\\n\\ne r\\nk\\nst a\\ne   d is\\n\\nFigure 8. Our method (bottom turquoise label) correctly predicts\\nthe category of this image, whereas the baseline (top purple label)\\ndoes not. The source per-category soft labels for the 15 categories\\nwith labeled target data are shown in the upper right corner, where\\nthe x-axis of the plot represents the 31 categories and the y-axis is\\nthe output probability. We highlight the index corresponding to the\\nmonitor category in red. As no labeled target data is available for\\nthe correct category, monitor, we \\ufb01nd that in our method the related\\ncategory of laptop computer (outlined with yellow box) transfers\\ninformation to the monitor category. As a result, after training, our\\nmethod places the highest weight on the correct category. Probabil-\\nity score per category for the baseline and our method are shown\\nin the bottom left and right, respectively, training categories are\\nopaque and correct test category is shown in red.\\n\\narchitecture which simultaneously optimizes for domain in-\\nvariance, to facilitate domain transfer, while transferring\\ntask information between domains in the form of a cross\\nentropy soft label loss. We demonstrate the ability of our\\narchitecture to improve adaptation performance in the super-\\nvised and semi-supervised settings by experimenting with\\ntwo standard domain adaptation benchmark datasets. In the\\nsemi-supervised adaptation setting, we see an average rela-\\ntive improvement of 13% over the baselines on the four most\\nchallenging shifts in the Of\\ufb01ce dataset. Overall, our method\\ncan be easily implemented as an alternative \\ufb01ne-tuning strat-\\negy when limited or no labeled data is available per category\\nin the target domain.\\n\\nFigure 7. We compare the baseline CaffeNet representation to\\nour representation learned with domain confusion by training a\\nsupport vector machine to predict the domains of Amazon and\\nWebcam images. For each representation, we plot a histogram of\\nthe classi\\ufb01er decision scores of the test images. In the baseline\\nrepresentation, the classi\\ufb01er is able to separate the two domains\\nwith 99% accuracy. In contrast, the representation learned with\\ndomain confusion is domain invariant, and the classi\\ufb01er can do no\\nbetter than 56%.\\n\\nfc7 representation, and the other using our fc7 learned with\\ndomain confusion. These SVMs are trained using 160 im-\\nages, 80 from Amazon and 80 from Webcam, then tested\\non the remaining images from those domains. We plot the\\nclassi\\ufb01er scores for each test image in Figure 7. It is obvious\\nthat the domain confusion representation is domain invariant,\\nmaking it much harder to separate the two domains\\u2014the\\ntest accuracy on the domain confusion representation is only\\n56%, not much better than random. In contrast, on the base-\\nline CaffeNet representation, the domain classi\\ufb01er achieves\\n99% test accuracy.\\n\\n5.2. Soft labels for task transfer\\n\\nWe now examine the effect of soft labels in transfer-\\nring information between categories. We consider the\\nAmazon\\u2192Webcam shift from the semi-supervised adapta-\\ntion experiment in the previous section. Recall that in this\\nsetting, we have access to target labeled data for only half\\nof our categories. We use soft label information from the\\nsource domain to provide information about the held-out\\ncategories which lack labeled target examples. Figure 8\\nexamines one target example from the held-out category\\nmonitor. No labeled target monitors were available during\\ntraining; however, as shown in the upper right corner of Fig-\\nure 8, the soft labels for laptop computer was present during\\ntraining and assigns a relatively high weight to the monitor\\nclass. Soft label \\ufb01ne-tuning thus allows us to exploit the fact\\nthat these categories are similar. We see that the baseline\\nmodel misclassi\\ufb01es this image as a ring binder, while our\\nsoft label model correctly assigns the monitor label.\\n\\n6. Conclusion\\n\\nWe have presented a CNN architecture that effectively\\nadapts to a new domain with limited or no labeled data per\\ntarget category. We accomplish this through a novel CNN\\n\\nAcknowledgements This work was supported by DARPA;\\nAFRL; DoD MURI award N000141110688; NSF awards\\n113629, IIS-1427425, and IIS-1212798; and the Berkeley\\nVision and Learning Center.\\n\\n4075\\n\\n\\x0cReferences\\n\\n[1] L. T. Alessandro Bergamo. Exploiting weakly-labeled web\\nimages to improve object classi\\ufb01cation: a domain adaptation\\napproach. In Neural Information Processing Systems (NIPS),\\nDec. 2010. 2\\n\\n[2] Y. Aytar and A. Zisserman. Tabula rasa: Model transfer for\\n\\nobject category detection. In Proc. ICCV, 2011. 2\\n\\n[3] J. Ba and R. Caruana. Do deep nets really need to be deep?\\nIn Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and\\nK. Weinberger, editors, Advances in Neural Information Pro-\\ncessing Systems 27, pages 2654\\u20132662. Curran Associates,\\nInc., 2014. 2, 3\\n\\n[4] A. Berg, J. Deng, and L. Fei-Fei.\\n\\nImageNet Large Scale\\n\\nVisual Recognition Challenge 2012. 2012. 2\\n\\n[5] K. M. Borgwardt, A. Gretton, M. J. Rasch, H.-P. Kriegel,\\nB. Sch\\xa8olkopf, and A. J. Smola. Integrating structured biologi-\\ncal data by kernel maximum mean discrepancy. In Bioinfor-\\nmatics, 2006. 2\\n\\n[6] J. Bromley, J. W. Bentz, L. Bottou, I. Guyon, Y. LeCun,\\nC. Moore, E. S\\xa8ackinger, and R. Shah. Signature veri\\ufb01cation\\nusing a siamese time delay neural network. International\\nJournal of Pattern Recognition and Arti\\ufb01cial Intelligence,\\n7(04):669\\u2013688, 1993. 2\\n\\n[7] S. Chopra, S. Balakrishnan, and R. Gopalan. DLID: Deep\\nlearning for domain adaptation by interpolating between do-\\nmains. In ICML Workshop on Challenges in Representation\\nLearning, 2013. 2, 6\\n\\n[8] S. Chopra, R. Hadsell, and Y. LeCun. Learning a similar-\\nity metric discriminatively, with application to face veri\\ufb01-\\ncation. In Computer Vision and Pattern Recognition, 2005.\\nCVPR 2005. IEEE Computer Society Conference on, vol-\\nume 1, pages 539\\u2013546. IEEE, 2005. 2\\n\\n[9] J. Donahue, Y. Jia, O. Vinyals, J. Hoffman, N. Zhang,\\nE. Tzeng, and T. Darrell. DeCAF: A Deep Convolutional\\nActivation Feature for Generic Visual Recognition. In Proc.\\nICML, 2014. 2, 6\\n\\n[10] L. Duan, D. Xu, and I. W. Tsang. Learning with augmented\\nfeatures for heterogeneous domain adaptation. In Proc. ICML,\\n2012. 2\\n\\n[11] B. Fernando, A. Habrard, M. Sebban, and T. Tuytelaars. Unsu-\\npervised visual domain adaptation using subspace alignment.\\nIn Proc. ICCV, 2013. 2\\n\\n[12] Y. Ganin and V. Lempitsky. Unsupervised Domain Adap-\\ntation by Backpropagation. ArXiv e-prints, Sept. 2014. 1,\\n2\\n\\n[13] M. Ghifary, W. B. Kleijn, and M. Zhang. Domain adaptive\\nneural networks for object recognition. CoRR, abs/1409.6041,\\n2014. 2, 6\\n\\n[14] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich\\nfeature hierarchies for accurate object detection and semantic\\nsegmentation. arXiv e-prints, 2013. 1, 2, 3\\n\\n[15] B. Gong, Y. Shi, F. Sha, and K. Grauman. Geodesic \\ufb02ow\\nkernel for unsupervised domain adaptation. In Proc. CVPR,\\n2012. 2\\n\\n[16] G. Hinton, O. Vinyals, and J. Dean. Distilling the knowledge\\nin a neural network. In NIPS Deep Learning and Representa-\\ntion Learning Workshop, 2014. 2, 3\\n\\n[17] J. Hoffman, S. Guadarrama, E. Tzeng, R. Hu, J. Donahue,\\nR. Girshick, T. Darrell, and K. Saenko. LSDA: Large scale de-\\ntection through adaptation. In Neural Information Processing\\nSystems (NIPS), 2014. 3\\n\\n[18] J. Hoffman, E. Rodner, J. Donahue, K. Saenko, and T. Darrell.\\nEf\\ufb01cient learning of domain-invariant image representations.\\nIn Proc. ICLR, 2013. 2, 6\\n\\n[19] J. Hoffman, E. Tzeng, J. Donahue, , Y. Jia, K. Saenko, and\\nT. Darrell. One-shot learning of supervised deep convolutional\\nmodels. In arXiv 1312.6204; presented at ICLR Workshop,\\n2014. 2\\n\\n[20] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Gir-\\nshick, S. Guadarrama, and T. Darrell. Caffe: Convolu-\\ntional architecture for fast feature embedding. arXiv preprint\\narXiv:1408.5093, 2014. 5\\n\\n[21] D. Kifer, S. Ben-David, and J. Gehrke. Detecting change in\\n\\ndata streams. In Proc. VLDB, 2004. 2\\n\\n[22] A. Krizhevsky, I. Sutskever, and G. E. Hinton. ImageNet\\nclassi\\ufb01cation with deep convolutional neural networks. In\\nProc. NIPS, 2012. 2, 3\\n\\n[23] B. Kulis, K. Saenko, and T. Darrell. What you saw is not\\nwhat you get: Domain adaptation using asymmetric kernel\\ntransforms. In Proc. CVPR, 2011. 2\\n\\n[24] M. Long and J. Wang. Learning transferable features with\\ndeep adaptation networks. CoRR, abs/1502.02791, 2015. 1, 2\\n[25] Y. Mansour, M. Mohri, and A. Rostamizadeh. Domain adap-\\nIn COLT, 2009.\\n\\ntation: Learning bounds and algorithms.\\n2\\n\\n[26] J. Ngiam, A. Khosla, M. Kim, J. Nam, H. Lee, and A. Y.\\nNg. Multimodal deep learning. In Proceedings of the 28th\\nInternational Conference on Machine Learning (ICML-11),\\npages 689\\u2013696, 2011. 2\\n\\n[27] S. J. Pan, I. W. Tsang, J. T. Kwok, and Q. Yang. Domain\\nadaptation via transfer component analysis. In IJCA, 2009. 2\\n[28] K. Saenko, B. Kulis, M. Fritz, and T. Darrell. Adapting visual\\ncategory models to new domains. In Proc. ECCV, 2010. 2, 5,\\n6\\n\\n[29] P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus,\\nand Y. LeCun. Overfeat: Integrated recognition, localiza-\\ntion and detection using convolutional networks. CoRR,\\nabs/1312.6229, 2013. 1, 2, 3\\n\\n[30] T. Tommasi, T. Tuytelaars, and B. Caputo. A testbed for\\ncross-dataset analysis. In TASK-CV Workshop, ECCV, 2014.\\n2, 7\\n\\n[31] A. Torralba and A. Efros. Unbiased look at dataset bias. In\\n\\nProc. CVPR, 2011. 2, 3\\n\\n[32] J. Yang, R. Yan, and A. Hauptmann. Adapting SVM classi-\\n\\ufb01ers to data with shifted distributions. In ICDM Workshops,\\n2007. 2\\n\\n4076\\n\\n\\x0c', u'Cross-modal Retrieval with Correspondence Autoencoder\\n\\nFangxiang Feng\\n\\nTelecommunications\\n\\nBeijing, China\\n\\nXiaojie Wang\\n\\nTelecommunications\\n\\nBeijing, China\\n\\nRuifan Li\\n\\nTelecommunications\\n\\nBeijing, China\\n\\nr\\ufb02i@bupt.edu.cn\\n\\nBeijing University of Posts and\\n\\nBeijing University of Posts and\\n\\nBeijing University of Posts and\\n\\nf.fangxiang@gmail.com\\n\\nxjwang@bupt.edu.cn\\n\\nABSTRACT\\nThe problem of cross-modal retrieval, e.g., using a text query\\nto search for images and vice-versa, is considered in this pa-\\nper. A novel model involving correspondence autoencoder\\n(Corr-AE) is proposed here for solving this problem. The\\nmodel is constructed by correlating hidden representation-\\ns of two uni-modal autoencoders. A novel optimal objec-\\ntive, which minimizes a linear combination of representation\\nlearning errors for each modality and correlation learning er-\\nror between hidden representations of two modalities, is used\\nto train the model as a whole. Minimization of correlation\\nlearning error forces the model to learn hidden representa-\\ntions with only common information in di\\ufb00erent modalities,\\nwhile minimization of representation learning error makes\\nhidden representations are good enough to reconstruct in-\\nput of each modality. A parameter \\u03b1 is used to balance\\nthe representation learning error and the correlation learn-\\ning error. Based on two di\\ufb00erent multi-modal autoencoders,\\nCorr-AE is extended to other two correspondence models,\\nhere we called Corr-Cross-AE and Corr-Full-AE. The pro-\\nposed models are evaluated on three publicly available data\\nsets from real scenes. We demonstrate that the three cor-\\nrespondence autoencoders perform signi\\ufb01cantly better than\\nthree canonical correlation analysis based models and two\\npopular multi-modal deep models on cross-modal retrieval\\ntasks.\\n\\nCategories and Subject Descriptors\\nH.3.3 [Information Search and Retrieval]: Retrieval\\nModels; I.2.6 [Arti\\ufb01cial Intelligence]: Learning\\n\\nGeneral Terms\\nAlgorithms; Design\\n\\nKeywords\\nCross-modal; retrieval; image and text; deep learning; au-\\ntoencoder\\n\\nPermission to make digital or hard copies of all or part of this work for personal or\\nclassroom use is granted without fee provided that copies are not made or distributed\\nfor pro\\ufb01t or commercial advantage and that copies bear this notice and the full cita-\\ntion on the\\ufb01rst page. Copyrights for components of this work owned by others than\\nACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-\\npublish, to post on servers or to redistribute to lists, requires prior speci\\ufb01c permission\\nand/or a fee. Request permissions from Permissions@acm.org.\\nMM\\u201914, November 03\\u201307, 2014, Orlando, FL, USA.\\nCopyright 2014 ACM 978-1-4503-3063-3/14/11 ...$15.00.\\nhttp://dx.doi.org/10.1145/2647868.2654902.\\n\\n1.\\n\\nINTRODUCTION\\n\\nThe inclusion of multi-modal data in webpages has be-\\ncome a widespread trend. For example, a webpage telling\\na story often contains some illustrations and other images\\naccompanying the text. A travel photo shared on the web is\\nusually tagged with words. The presence of massive multi-\\nmodal data on the Internet brings huge cross-modal retrieval\\nrequirements, such as using an image query to search for\\ntexts and using a text query to search for images. Unlike\\ntraditional information retrieval tasks in a single modality,\\nsuch as using a text query to search for text, cross-modal re-\\ntrieval focuses on mining the correlations between data from\\ndi\\ufb00erent modalities.\\n\\n1.1 Previous Work\\n\\nMany approaches have been proposed to develop solutions\\nto these challenging tasks. There are two main strategies for\\nmodeling cross-modal correlations among previous work.\\n\\nOne strategy involves modeling the correlation between\\n\\ndi\\ufb00erent modalities with a shared layer.\\n\\nA portion of them use topic models to achieve it. Cor-\\nrespondence LDA (Corr-LDA) [4] extended LDA [5] to \\ufb01nd\\nthe topic-level relationships between images and text anno-\\ntations, where topics\\u2019 distributions act as middle layer for\\nboth images and texts. For images with loosely-coupled text,\\na mixture of directed and undirected probabilistic graphical\\nmodel, called MDRF, was proposed in [14]. This model was\\nbuilt using a Markov random \\ufb01eld over LDA. These LDA-\\nbased models essentially can be understood as a two-layer\\narchitecture with only one hidden layer.\\n\\nRecently, there has been a trend of developing deep ar-\\nchitecture for tackling complex AI problems [2], which is\\ninspired by the architectural depth of the brain. Some rep-\\nresentative models, such as deep autoencoders (DAE) [11],\\ndeep belief networks (DBN) [13], and deep Boltzmann Ma-\\nchine (DBM) [21], and their corresponding learning algo-\\nrithms have been proposed. These models also have been\\nextended to model multi-modal data. Ngiam et al. [17] used\\nmultimodal DAE to learn a shared representation for both\\nspeech and visual inputs. Srivastava and Salakhutdinov [25]\\nused multimodal DBM and DBN to learn a uni\\ufb01ed represen-\\ntation for both images and texts. We also noticed several\\ndeep learning methods [29, 9, 23] for learning a joint em-\\nbedding space of image and text very recently. Those mod-\\nels have shown signi\\ufb01cant advantages at image annotation,\\nobjective classi\\ufb01cation and zero-shot learning tasks. But,\\nto our best knowledge, neither of them has been used for\\ncross-modal retrieval up to now.\\n\\n7\\x0charmful to cross-modal retrieval. A shared representation\\nlearns both common and modality-speci\\ufb01c information. Al-\\nthough it showed its strength on learning complementarity\\nof data from di\\ufb00erent modalities, it is therefore not a ex-\\nactly well-\\ufb01tting representation for cross-modal retrieval. A\\nrepresentation which can learn only common information for\\ndi\\ufb00erent modalities is a better choice. This is the \\ufb01rst mo-\\ntivation to build our model in this paper.\\n\\nThe second strategy separates correlation learning from\\nrepresentation learning. This strategy is too weak to exploit\\nthe complex correlations of representations from di\\ufb00erent\\nmodalities, especially when autoencoders are used to learn\\nrepresentations. Autoencoders can be used learn di\\ufb00erent\\nlevel representation for inputs with di\\ufb00erent reconstruction\\nerrors, one problem for building correlations between data\\nfrom two di\\ufb00erent modalities is that it is di\\ufb03cult to de-\\ncide which level is best for building correlations. In other\\nwords, two di\\ufb00erent modalities may be correlated at di\\ufb00er-\\nent abstract levels of representation. Therefore, correlation\\nlearning need be considered with representation learning as\\na whole. This is the second motivation to build our model\\nin this paper.\\n1.3 Contribution\\n\\nIn this paper, we propose correspondence autoencoder\\n(Corr-AE) based on two basic uni-modal autoencoders. The\\ndi\\ufb00erence between two-stage methods and our Corr-AE is\\nillustrated in Figure 2. The two-stage methods ignore the\\ncorrelation between di\\ufb00erent modalities when perform rep-\\nresentation learning. Corr-AE incorporates representation\\nlearning and correlation learning into a single process. A\\nnovel loss function is here designed.\\nIt includes not only\\nthe loss of di\\ufb00erent autoencoders for all modalities, but al-\\nso the loss of correlation between di\\ufb00erent modalities. This\\nmodel is evaluated using three publicly available data sets.\\nCompared against several multi-modal deep learning model-\\ns, our Corr-AE demonstrate its e\\ufb00ectiveness. Besides, based\\non two other multi-modal autoencoders, we extend Corr-AE\\nto two correspondence models, Corr-Cross-AE and Corr-\\nFull-AE. Experimental results show that based on di\\ufb00eren-\\nt autoencoders, the combination of representation learning\\nand correlation learning is more e\\ufb00ective than the two-stage\\nmethods.\\n\\nThe remainder of this paper is organized as follows. In the\\nnext section, the details of the basic Corr-AE are described.\\nThe Corr-AE are then extended to Corr-Cross-AE and a\\ncombinational model. Section 3 describes the experimental\\nresults and compares the performance of di\\ufb00erent models.\\nConclusions are presented in Section 4.\\n\\n2. LEARNING ARCHITECTURE\\n\\nIn this section, the details of the architecture of the basic\\nCorr-AE are described. Then a loss function suitable for\\ntraining Corr-AE for learning similar representations of dif-\\nferent modalities is proposed. Next, two extensions of the\\nCorr-AE are introduced. Lastly, the deep architecture with\\nthe training algorithms is described.\\n2.1 Correspondence Autoencoder\\n\\nAs illustrated in Figure 3, the Corr-AE architecture con-\\nsists of two subnetworks, each a basic autoencoder. These\\ntwo networks are connected by a prede\\ufb01ned similarity mea-\\nsure on the code layer. Each subnetwork in the Corr-AE is\\n\\n(cid:2)(cid:5)(cid:16)(cid:14)(cid:19)(cid:1)(cid:10)(cid:6)(cid:7)(cid:11)(cid:10)(cid:18)(cid:1)(cid:13)(cid:7)(cid:17)(cid:18)(cid:1)(cid:4)(cid:8)(cid:15)(cid:5)(cid:18)(cid:1)(cid:3)(cid:15)(cid:14)(cid:15)(cid:9)(cid:10)(cid:18)(cid:1)(cid:10)(cid:6)(cid:7)(cid:7)(cid:11)(cid:12)\\n\\n\\u201csky\\u201d\\n\\nFigure 1: An image and its tags.\\nand\\n\\u201cblue\\u201d are common information in both image and\\ntext modalities.\\n\\u201cnikon\\u201d and \\u201cnikkor\\u201d are tex-\\nt modality-speci\\ufb01c information while \\u201c\\ufb02owers\\u201d and\\n\\u201cclouds\\u201d are image modality-speci\\ufb01c information.\\nCommon information are the key to cross-modal re-\\ntrieval tasks.\\n\\nThe other strategy involves a two-stage framework. It \\ufb01rst\\nlearns or extracts features for each modality separately then\\nuses canonical correlation analysis (CCA) [10] to build a\\nlower-dimensional common representation space. This choice\\nis more straightforward for cross-modal retrieval task. CCA\\nis a method of data analysis used to discover a subspace\\nof multiple data spaces. Given the training pairs p and q,\\nCCA \\ufb01nds matrices U and V such that U p and V q have\\nmaximum correlations. The \\ufb01rst d canonical components of\\nU and V could be used for projecting new input pairs in-\\nto a d-dimension space where cross-modal retrieval could be\\nconducted by calculating a simple similarity measurement,\\nsuch as Euclidean or cosine distance. Rasiwasia et al.\\n[19]\\nproposed correlation matching mapping the features of im-\\nages and texts extracted by LDA into same representation\\nspace using CCA. Ngiam et al. [17] used DAE to extract\\nfeatures and suggested using CCA to form a shared repre-\\nsentation of audio and video data. As in Ngiam et al.\\u2019s work,\\nKim et al. [15] learned the shared semantic space of di\\ufb00erent\\nlanguage with DAE and CCA.\\n\\n1.2 Motivation\\n\\nIn the \\ufb01rst strategy, the shared layer was built by jointly\\nlearning from di\\ufb00erent modalities. We think the shared lay-\\ner learnt in this way might no \\ufb01t the need of cross-modal\\nretrieval. Perceived data from di\\ufb00erent modalities for a\\nsame object normally comprise of common information in\\nall modalities and modality-speci\\ufb01c information. Figure 1\\ngives an example. There is a picture accompanying with\\nseveral text tags in \\ufb01gure 1, where \\u201csky\\u201d and \\u201cblue\\u201d are\\ncommon information in both image and text modalities.\\n\\u201cnikon\\u201d and \\u201cnikkor\\u201d are text modality-speci\\ufb01c information\\nwhich can hardly be acquired from the image. While \\u201c\\ufb02ow-\\ners\\u201d and \\u201cclouds\\u201d are only image modality-speci\\ufb01c informa-\\ntion which can not be captured in text tags.\\nIntuitive-\\nly, common information like \\u201csky\\u201d and \\u201cblue\\u201d are key to\\ncross-modal retrieval. Both text modality-speci\\ufb01c and im-\\nage modality-speci\\ufb01c information are not necessary and even\\n\\n8\\x0cImage and text \\nautoencoder\\n\\nCCA\\n\\nFirst stage\\n\\nSecond stage\\n\\nflower pink  \\nexcellence \\nblossoms\\n\\nnature \\nflowers  pink \\ncactus\\n\\nguns \\nweapons\\n\\nmilitary \\nsoldier gun\\n\\nfood rice\\n\\nfood fish rice\\n\\nCorrespondence autoencoder\\n\\nFigure 2: Di\\ufb00erence between two-stage methods and our Corr-AE: Corr-AE incorporates representation\\nlearning and correlation learning into a single process while two-stage methods separate the two processes.\\n\\nImage Reconstruction\\n\\nText Reconstruction\\n\\nCode layer\\n\\nImage Representation\\n\\nText Representation\\n\\nFigure 3: Correspondence autoencoder\\n\\nresponsible for each modality. In this way, the inputs to each\\nsubnetwork are features from one modality. During learning,\\nthe two subnetworks are coupled at their code layer using a\\nsimilarity measure. After learning, the two subnetworks in\\nthe Corr-AE exhibit di\\ufb00erent parameters even if they have\\nthe same architecture. As a result, the code for new inputs\\ncan be obtained using the learned network parameters.\\n\\nFormally, the mapping from the inputs of these two sub-\\n\\nnetworks to the code layers is denoted as f (p; Wf ) and g(q; Wg),\\nin which, f is image modality and g is text modality; W de-\\nnotes the weight parameters in these two subnetworks. The\\nsubscripts denote the corresponding modalities. f and g are\\nlogistic activation function. The similarity measure between\\nith pair of image representation p(i) and the given text rep-\\nresentation q(i) are here de\\ufb01ned as follows:\\n; Wf ) \\u2212 g(q\\n\\n(cid:2)(cid:2)(cid:2)f (p\\n\\n(cid:2)(cid:2)(cid:2)2\\n\\n; Wg)\\n\\n(1)\\n\\n(i)\\n\\n(i)\\n\\n(i)\\n\\n(i)\\n\\n2\\n\\n, q\\n\\nC(p\\n\\n; Wf , Wg) =\\nwhere (cid:2)\\xb7(cid:2)2 is the L2 norm.\\n\\nTo learn the similar representations of these two modali-\\nties for one object, a loss function given input image repre-\\nsentation p(i) and its text representation q(i) are established.\\nTo simplify the notation, the network parameters Wf , Wg\\nare grouped as \\u0398. The loss function on any pair of inputs\\ncan then be de\\ufb01ned as follows:\\n\\n(cid:4)\\n\\n(cid:3)\\n\\n; \\u0398) =(1 \\u2212 \\u03b1)\\n\\n(i)\\n\\nLI (p\\n\\n; \\u0398) + LT (q\\n\\n(i)\\n\\n; \\u0398)\\n\\nL(p\\n\\n(i)\\n\\n(i)\\n\\n, q\\n\\nwhere\\n\\n+ \\u03b1LC (p\\n\\n(i)\\n\\n(i)\\n\\n, q\\n\\n; \\u0398)\\n\\nLI (p\\n\\n(i)\\n\\n(i)\\n\\n, q\\n\\n; \\u0398) =\\n\\nLT (p\\n\\n(i)\\n\\n(i)\\n\\n, q\\n\\n; \\u0398) =\\n\\n(cid:2)(cid:2)(cid:2)p\\n(i) \\u2212 \\u02c6p\\n(cid:2)(cid:2)(cid:2)q\\n(i) \\u2212 \\u02c6q\\n\\n(i)\\nI\\n\\n(i)\\nT\\n\\n(cid:2)(cid:2)(cid:2)2\\n(cid:2)(cid:2)(cid:2)2\\n\\n2\\n\\n2\\n\\nLC (p\\n\\n(i)\\n\\n(i)\\n\\n, q\\n\\n; \\u0398) = C(p\\n\\n(i)\\n\\n(i)\\n\\n, q\\n\\n; \\u0398)\\n\\n(2)\\n\\n(3a)\\n\\n(3b)\\n\\n(3c)\\n\\nHere, (cid:2)\\xb7(cid:2)2 is the L2 norm. LI and LT are the losses caused\\n\\n(i)\\nand \\u02c6q\\nI\\n\\nby data reconstruction errors for the given inputs (an image\\nand its text) of two subnetworks, speci\\ufb01cally image and text\\n(i)\\nmodalities. \\u02c6p\\nare the reconstruction data from\\nI\\np(i) and q(i) respectively; LC is the correlation loss; \\u03b1(0 <\\n\\u03b1 < 1) in the total loss function (2) is a parameter used\\nto trade o\\ufb00 between two groups of objectives: correlation\\nlosses and reconstruction losses. An appropriate value for \\u03b1\\nis crucial. If \\u03b1 = 0, the loss function degenerates to the loss\\nfunction of the autoencoders. It cannot then capture any\\ncorrelations between inputs from di\\ufb00erent modalities. At\\nthe other extreme, considering only correlation loss, \\u03b1 is set\\nto 1. This assumes that any pair of inputs has correlations,\\nregardless of whether the image and text inputs match or\\nnot. An intuitive interpretation for this is that the cost\\nfunction only focuses on the constraints of correlations and\\nignores the characteristics of the data completely.\\n\\n9\\x0cText Reconstruction\\n\\nImage Reconstruction\\n\\nCode layer\\n\\nCode layer\\n\\nThird Component\\n\\nImage Representation\\n\\nText Representation\\n\\nSecond Component\\n\\nRBM\\n\\n RBM\\n\\nFigure 4: Correspondence cross-modal autoencoder\\n\\nImage Reconstruction\\n\\nText Reconstruction\\n\\nImage Reconstruction\\n\\nText Reconstruction\\n\\nGaussian RBM\\n\\nReplicated Softmax RBM\\n\\nFirst Component\\n\\nCode layer\\n\\nImage Input\\n\\nText Input\\n\\nFigure 6: Deep architecture\\n\\nImage Representation\\n\\nText Representation\\n\\nFigure 5: Correspondence full-modal autoencoder\\n\\nIn summary, minimizing the loss function de\\ufb01ned in E-\\nq. (2) enables our Corr-AE to learn similar representations\\nfrom bimodal feature representations.\\n2.2 Correspondence Cross-modal Autoencoder\\nAs illustrated in Figure 4, we propose the Corr-Cross-AE,\\nwhich replace the basic autoencoders to cross-modal autoen-\\ncoders. Unlike the basic autoencoders, which reconstruct\\nthe input itself, cross-modal autoencoders reconstruct input\\nfrom di\\ufb00erent modalities. The loss function on any pair of\\ninputs of the Corr-Cross-AE is de\\ufb01ned as follows:\\n\\nL(p\\n\\n(i)\\n\\n(i)\\n\\n, q\\n\\n; \\u0398) =(1 \\u2212 \\u03b1)\\n\\nLI (p\\n\\n(i)\\n\\n(i)\\n\\n, q\\n\\n; \\u0398) + LT (p\\n\\n(i)\\n\\n(i)\\n\\n, q\\n\\n; \\u0398)\\n\\n(cid:3)\\n\\n(cid:4)\\n\\n+ \\u03b1LC (p\\n\\n(i)\\n\\n(i)\\n\\n, q\\n\\n; \\u0398)\\n\\nLC (p\\n\\n(i)\\n\\n(i)\\n\\n, q\\n\\n; \\u0398) = C(p\\n\\n(i)\\n\\n(i)\\n\\n, q\\n\\n; \\u0398)\\n\\nwhere\\n\\nLI (p\\n\\n(i)\\n\\n(i)\\n\\n, q\\n\\n; \\u0398) =\\n\\nLT (p\\n\\n(i)\\n\\n(i)\\n\\n, q\\n\\n; \\u0398) =\\n\\n(cid:2)(cid:2)(cid:2)q\\n(i) \\u2212 \\u02c6q\\n(cid:2)(cid:2)(cid:2)p\\n(i) \\u2212 \\u02c6p\\n\\n(i)\\nI\\n\\n(i)\\nT\\n\\n(cid:2)(cid:2)(cid:2)2\\n(cid:2)(cid:2)(cid:2)2\\n\\n2\\n\\n2\\n\\nLC (p\\n\\n(i)\\n\\n(i)\\n\\n, q\\n\\n; \\u0398) = C(p\\n\\n(i)\\n\\n(i)\\n\\n, q\\n\\n; \\u0398)\\n\\n(4)\\n\\n(5a)\\n\\n(5b)\\n\\n(5c)\\n\\nand \\u02c6p\\n\\n(i)\\nHere, \\u02c6q\\nI\\n\\n(i)\\nT are the reconstruction data from im-\\nage and text subnet, respectively. The meanings of other\\nsymbols are the same as in Eq. (2).\\n\\nThe representation learning of image modality in cross-\\nmodal autoencoder considers the information from the text\\nmodality and vice-versa. This causes some correlations to\\nbe captured in the reconstruction loss.\\n2.3 Correspondence Full-modal Autoencoder\\nThe full-modal autoencoder can be viewed as a combina-\\ntion of a basic autoencoder and cross-modal autoencoder.\\nThis autoencoder is proposed to model the audio and video\\n\\n(i)\\nand \\u02c6q\\nI\\n\\n(i)\\nHere, \\u02c6p\\nI\\n\\nare the reconstruction data from p(i)\\n(i)\\nand q(i) in the image subnet; \\u02c6p\\nT are the recon-\\nstruction data from p(i) and q(i) in the text subnet. The\\nmeanings of other symbols are the same as in Eq. (2).\\n2.4 Deep Architecture\\n\\n(i)\\nT and \\u02c6q\\n\\nData from di\\ufb00erent modalities may have very di\\ufb00erent\\nstatistical properties. This makes it di\\ufb03cult to capture cor-\\nrelations across modalities directly. To address this, a deep\\narchitecture is here proposed. This \\ufb01rst involves using some\\nstacked modality-friendly models to learn higher-level rep-\\nresentations that remove such modality-speci\\ufb01c properties.\\nThen the Corr-AE are used to learn similar representations\\nat a higher level.\\n\\nAs illustrated in Figure 6, the deep architecture has three\\nstacked components. The \\ufb01rst two components are all re-\\nstricted Boltzmann machines (RBMs). There are two ex-\\ntended RBMs for the \\ufb01rst component and two basic RBMs\\nfor the second component. To be brief, RBM[22] is an undi-\\nrected graphical model with stochastic binary units in a vis-\\n\\ndata in [17]. The basic Corr-AE are also easy to extend\\nto Corr-Full-AE based on full-modal autoencoder. As illus-\\ntrated in Figure 5, the autoencoder reconstruct not only the\\ninput itself but also input from di\\ufb00erent modalities. This\\n\\u201cfull\\u201d representation space contains information from both\\nmodalities. The loss function of any pair of inputs of the\\nCorr-Full-AE is de\\ufb01ned as follows:\\n\\nL(p\\n\\n(i)\\n\\n(i)\\n\\n, q\\n\\n; \\u0398) =(1 \\u2212 \\u03b1)\\n\\nLI (p\\n\\n(i)\\n\\n(i)\\n\\n, q\\n\\n; \\u0398) + LT (p\\n\\n(i)\\n\\n(i)\\n\\n, q\\n\\n; \\u0398)\\n\\n(cid:3)\\n\\n+ \\u03b1LC (p\\n\\n(i)\\n\\n(i)\\n\\n, q\\n\\n; \\u0398)\\n\\nwhere\\n\\nLI (p\\n\\n(i)\\n\\n(i)\\n\\n, q\\n\\n; \\u0398) =\\n\\nLT (p\\n\\n(i)\\n\\n(i)\\n\\n, q\\n\\n; \\u0398) =\\n\\n(cid:2)(cid:2)(cid:2)p\\n(cid:2)(cid:2)(cid:2)p\\n\\n(i) \\u2212 \\u02c6p\\n(i) \\u2212 \\u02c6p\\n\\n(i)\\nI\\n\\n(i)\\nT\\n\\n(cid:2)(cid:2)(cid:2)q\\n(cid:2)(cid:2)(cid:2)q\\n\\n+\\n\\n+\\n\\n(cid:2)(cid:2)(cid:2)2\\n(cid:2)(cid:2)(cid:2)2\\n\\n2\\n\\n2\\n\\n(i) \\u2212 \\u02c6q\\n(i) \\u2212 \\u02c6q\\n\\n(i)\\nI\\n\\n(i)\\nT\\n\\n(cid:4)\\n\\n(cid:2)(cid:2)(cid:2)2\\n(cid:2)(cid:2)(cid:2)2\\n\\n2\\n\\n2\\n\\n(6)\\n\\n(7a)\\n\\n(7b)\\n\\n(7c)\\n\\n10\\x0cible layer and hidden layer but without connections between\\nthe units within these two layers. Given that each unit is\\ndistributed by Bernoulli distribution with logistic activation\\nfunction, a joint probabilistic distribution of visible units\\nand hidden units can be de\\ufb01ned. The basic RBM can be\\nextended to exponential family. All the models can be e\\ufb03-\\nciently learned by using the contrastive divergence approx-\\nimation (CD) [12]. For the \\ufb01rst layer, Gaussian RBM [28]\\nand replicated softmax RBM [20] can be used to model the\\nreal-valued feature vectors for image and the discrete sparse\\nword count vectors for text, respectively. After learning the\\nRBMs, the hidden layer can then be used as the input for\\nthe second component. The second component involves two\\nbasic RBMs, which are used to learn higher-level features\\nfor image and text. The third component can involve any\\none of the three correspondence autoencoders given above.\\nThe learning for Corr-AE can be performed using standard\\nback-propagation algorithm.\\n\\nBecause di\\ufb00erent modalities are mapped into the same\\nrepresentation space by the deep architecture, it is straight-\\nforward to use the model for cross-modal retrieval tasks. For\\nexample, given a text query, we expect the relevant images\\ncould be returned1. After learning this deep architecture,\\nall test images are mapped into the representation space\\nfrom the three-layer image subnetwork. A new text query\\nis mapped into the same space from the three-layer text\\nsubnetwork. The similarity between the text query and all\\nthe candidate images can be calculated by a simple distance\\nmetric in the representation space. In this way, in search-\\ning for image using text, image ranked list will return by\\nincreasing distance for any text query.\\n\\n3. EXPERIMENTS\\n\\nWe evaluate our models on three public available real-\\nworld data sets. In this section, we \\ufb01rst give a detailed de-\\nscription of these data sets. Then, the evaluation protocol\\nused in the experiments is introduced. Next, the perfor-\\nmance of several models is reported. Finally, an analysis of\\nthe impact of \\u03b1 on the models is given.\\n3.1 Data sets and Feature Extraction\\n\\nWikipedia. The data set [19] was collected from \\u201dWikipedi-\\n\\na featured articles\\u201d. It contains 2,866 image/text pairs be-\\nlonging to 10 semantic categories. The data set was split\\ninto three subsets: 2,173 cases as training set, 231 cases as\\nvalidation set and 462 cases as testing set. For image repre-\\nsentation, we extract the following three types of features:\\n\\u2022 Pyramid Histogram of Words (PHOW) [6]. For PHOW\\nfeatures, dense SIFT descriptors is \\ufb01rst extracted by\\nVLfeat [27] from per training image and then a 1000-\\ndimensional visual word codebook learned with K-means\\nclustering.\\n\\n\\u2022 Gist [18]. The gist descriptor is extracted by the pub-\\nlic available package2 with default parameters. This\\nresults in a 512-dimensional feature vector.\\n\\n\\u2022 MPEG-7 descriptors [16]. We use the public available\\nsoftware [1] to extract four di\\ufb00erent visual descriptors\\n1Text retrieval using an image query can be done in a similar\\nway.\\n2\\n\\nhttp://people.csail.mit.edu/torralba/code/\\n\\nspatialenvelope/\\n\\n(CSD, SCD, CLD, EHD) de\\ufb01ned in MPEG-7 for image\\nrepresentations. The dimension of obtained MPEG-7\\nfeature vector is 784.\\n\\nThus, each image is represented by a 2296-dimensional fea-\\nture vector. For text representation, we use bag of words\\nmodel. A dictionary of 3000 high-frequency words is built\\nfrom all training texts. We use Python Natural Language\\nToolkit [3] to stem the text. This data set is available at\\nhttp://www.svcl.ucsd.edu/projects/crossmodal/.\\n\\nPascal. The data set [8] contains 1,000 image/text pairs\\nfrom 20 categories, 50 cases per categories. The images\\nare randomly selected from 2008 PASCAL development kit.\\nEach image is labeled with 5 sentences. We split the data\\ninto three subsets, 800 for training (40 cases per category),\\n100 for validation (5 cases per category) and 100 for testing\\n(5 cases per category). The feature extraction for images\\nand texts is the same as for the Wikipedia data set except\\nthat the dimension of text is 1,000. This data set is available\\nat http://vision.cs.uiuc.edu/pascal-sentences/.\\n\\nNUS-WIDE-10k. This data set is a subset of NUS-\\nWIDE [7], which contains about 270k images with tag an-\\nnotations from 81 categories. We only choose 10 categories\\nwith the largest quantity and 1,000 image/text pairs per\\ncategory from NUS-WIDE. Each pair of our NUS-WIDE-\\n10k only belongs to a single category. The 10 categories are\\nanimal, clouds, \\ufb02owers, food, grass, person, sky, toy, water\\nand window. We randomly split the data set into three sub-\\nsets: 8,000 cases for training (800 cases per category), 1,000\\nfor validation (100 cases per category) and 1,000 for test-\\ning (100 cases per category). Each image is represented by\\nsix descriptors, including 64-D color histogram, 144-D color\\ncorrelogram, 73-D edge direction histogram, 128-D wavelet\\ntexture, 225-D block-wise color moments and 500-D bag of\\nwords based on SIFT descriptions. Each text is represented\\nby 1000-dimensional bag of words. This data set with ex-\\ntracted features is available at http://lms.comp.nus.edu.\\nsg/research/NUS-WIDE.htm.\\n\\nAs we can see, these data sets have very distinct proper-\\nties. For example, the text modality of the three data sets\\nWikipedia, Pascal and NUS-WIDE-10k is quite di\\ufb00erent,\\nwhich are article, sentences and tags, respectively. Besides,\\nthe sizes of these data sets range from 1k to 10k and the\\nnumber of the categories range from 10 to 20.\\n3.2 Evaluation metric\\n\\nWe consider two cross-modal retrieval tasks: text retrieval\\nfrom an image query, and image retrieval from a text query.\\nFollowing [30], retrieval performance is evaluated using t-\\nwo metrics, mean average precision (mAP ) and top 20%\\npercentage. The \\ufb01rst one represents the ability of learning\\ndiscriminative cross-modal mapping functions while the lat-\\ner one reveals the ability of learning corresponding latent\\nconcepts.\\n\\nmAP Given one query and \\ufb01rst R top-ranked retrieved\\n\\ndata, the average precision is de\\ufb01ned as\\np(r) \\xb7 rel(r)\\n\\nR(cid:5)\\n\\n1\\nM\\n\\nr=1\\n\\n(8)\\n\\nwhere M is the number of relevant data in the retrieved\\nresult, p(r) is precision of at r, and rel(r) presents the rel-\\nevance of a given rank (one if relevant and zero otherwise).\\nThe retrieved data is considered as relevant if it has the same\\n\\n11\\x0csemantic label as the query. mAP is obtained by averaging\\nAP of all the queries. We report mAP @50 (R = 50) in all\\nexperiments. Semantic labels are only used for evaluation.\\nTraining of our models does not need any semantic label.\\n\\nTop 20% Jia et al. [14] proposed this evaluation metric\\nfor data sets without semantic label. Because there is on-\\nly one ground-truth for each image-text pair in this case,\\nthe position of the ground-truth image/text in the ranked\\nlist is used to evaluate performance. Speci\\ufb01cally, top 20%\\npercentage is the relative number of images/texts correctly\\nretrieved in the \\ufb01rst 20% of the ranked list.\\n3.3 Baseline\\n\\nFor a fair comparison, the inputs to our models and base-\\nline methods3 are all features learned by \\ufb01rst two compo-\\nnents of deep architecture described in section 2.4. That\\nmeans that the only di\\ufb00erence is the third component of\\nthe deep architecture. Besides, cosine distance is used to\\nmeasure the similarity in all experiments. We compare our\\nmodels with three CCA based models and two multi-modal\\nmodels:\\n\\n\\u2022 CCA-AE[15]. We \\ufb01rst use two uni-modal autoencoders\\nto learn higher level image feature and text feature\\nrespectively, and then use CCA4 to learn a common\\nrepresentation space on the learned features.\\n\\n\\u2022 CCA-Cross-AE. Instead of using the uni-modal au-\\ntoencoders with CCA, this method combines imme-\\ndiately cross-modal autoencoders and CCA.\\n\\n\\u2022 CCA-Full-AE. Literally, this method combines full-\\n\\nmodal autoencoders and CCA.\\n\\n\\u2022 Bimodal AE[17]. We train a bimodal autoencoder to\\nperform shared representation learning. Follow the\\ntraining algorithm described in [17], we add training\\nexamples that have zero values for one of the input\\nmodalities (e.g., image) and original values for the oth-\\ner input modality (e.g., text), but still require the net-\\nwork to reconstruct both modalities (image and text).\\nThus, one-third of the training data has only image\\nfor input, while another one-third of the data has only\\ntext, and the last one-third of the data has both image\\nand text. After learning the bimodal autoencoder, a\\nsingle modality input(image or text) can be mapped\\ninto the shared representation space.\\n\\n\\u2022 Bimodal DBN[17, 24]. A bimodal DBN is obtained by\\nconnecting image and text features with a joint layer.\\nThe model does not give a direct matching function\\nof image and text input. However, it can generate the\\nunknown modality conditioned on a given modality. In\\nother words, queries can be mapped into the feature\\nspace of the other modality and a suitable similarity\\nmeasure can be used to retrieve results that are close\\nto the query.\\n\\n3.4 Model Architecture\\n\\nWe perform grid search for the number of hidden units of\\neach layer with the setting 32, 64, 128, 256, 512, 1024. In\\n3The code for all baseline models are available online at\\nhttps://github.com/nitishsrivastava/deepnet.\\n4Matlab code of the CCA can be downloaded at http://\\nwww.davidroihardoon.com/Professional/Code.html.\\n\\nall models, to reduce the search space, the number of units\\nfor all hidden layers in the deep architecture is restricted\\nto the same. The validation sets are used to determine the\\nbest number of hidden units. The dimensionality of the\\nCCA latent space and the iterations of Bimodal AEs and\\nour three correspondence models are also determined by the\\nvalidation sets. For our three correspondence models, we\\nneed to choose an appropriate value for the parameter \\u03b1.\\nThe parameter \\u03b1 is not sensitive to data sets. So, in all\\ndata sets, the value of \\u03b1 for Corr-AE and Corr-Full-AE is\\nset to 0.8. For Corr-Cross-AE, \\u03b1 is set to 0.2. A detailed\\nanalysis of \\u03b1 will be given at the end of this section.\\n\\nFor the models involving CCA, to achieve a better per-\\nformance, CCA is applied to learn the correlation between\\nthe image and text hidden layers with di\\ufb00erent number of\\nunits. Three copies of all training data are used in Bimodal\\nAE training. The \\ufb01rst copy remains data from both modal-\\nities (original data). The second is only image data with\\ntext data setting to zeros. And the third is only text data\\nwith image data setting to all zeros. We do not weight the\\nreconstruction errors from di\\ufb00erent modalities. So do our\\ncorrespondence models. For the Bimodal DBN, variational\\nmean \\ufb01eld with ten steps is used to generate the unknown\\nmodality conditioned on a given modality. Gibbs sampling\\ndoes not show improvement in our experiments. The code\\nwith parameter speci\\ufb01cations of our three models and the\\nbaseline methods are available online5.\\n3.5 Results\\n\\nTable 1 summarizes the mAP scores and top 20% of the\\ntwo cross-modal retrieval tasks for Wikipedia, Pascal and\\nNUS-WIDE-10k data sets respectively. On all data sets,\\nour three correspondence autoencoders signi\\ufb01cantly outper-\\nform other models on both tasks of text and image retrieval.\\nTaking Corr-Full-AE as an example, we compare it with the\\nbest results on each task achieved by the \\ufb01ve baseline mod-\\nels. It improves mAP scores 12.3% and 16.6% respectively\\non searching texts by images and searching images by texts\\non Wikipedia data set, improves 12.4% and 2.2% respec-\\ntively on the two tasks on Pascal data set, improves 32.4%\\nand 10.2% respectively on the two tasks on NUS-WIDE-10k\\ndata set.\\n\\nCompared with CCA-AE, our Corr-AE improves average\\nmAP value of two tasks 53.6%, 81.5%, 48.3% on Wikipedia,\\nPascal, NUS-WIDE-10k data sets respectively. Compared\\nwith CCA-Cross-AE, our Corr-Cross-AE improves 57.9%,\\n73.6%, 28.3% on the three data sets. Compared with CCA-\\nFull-AE, our Corr-Full-AE improves 12.8%, 71.2%, 46.7%\\non the three data sets. Based on three di\\ufb00erent multi-modal\\nautoencoders, our correspondence models signi\\ufb01cantly out-\\nperform the two-stage methods. The advantage of our corre-\\nspondence models over two-stage models is that correspon-\\ndence models combined representation learning and correla-\\ntion learning into a whole. In other words, compared with\\nour correspondence models, the two-stage methods are sub-\\noptimal. It is also noticeable that the di\\ufb00erence within the\\nthree correspondence autoencoders are found to be smaller\\nthan the di\\ufb00erence among the three CCA-AEs. This also\\ndemonstrates the e\\ufb00ectiveness of the combination of repre-\\nsentation learning and correlation learning processes.\\n\\nCompared with Bimodal AE, our Corr-AE improves mAP\\nscores 15.6% and 10.4% respectively on searching texts by\\n5https://github.com/fangxiangfeng/deepnet\\n\\n12\\x0cTable 1: Results of two retrieval protocols: mAP scores and top 20% on three data sets.\\n\\nModel\\n\\nCCA-AE[15]\\n\\nCCA-Cross-AE\\nCCA-Full-AE\\n\\nBimodal AE[17]\\n\\nBimodal DBN[17, 24]\\n\\nCorr-AE\\n\\nCorr-Cross-AE\\nCorr-Full-AE\\n\\nModel\\n\\nCCA-AE[15]\\n\\nCCA-Cross-AE\\nCCA-Full-AE\\n\\nBimodal AE[17]\\n\\nBimodal DBN[17, 24]\\n\\nCorr-AE\\n\\nCorr-Cross-AE\\nCorr-Full-AE\\n\\nModel\\n\\nCCA-AE[15]\\n\\nCCA-Cross-AE\\nCCA-Full-AE\\n\\nBimodal AE[17]\\n\\nBimodal DBN[17, 24]\\n\\nCorr-AE\\n\\nCorr-Cross-AE\\nCorr-Full-AE\\n\\n(a) Wikipedia\\n\\nmAP\\n\\nTop 20%\\n\\nImage Query Text Query Average\\n\\nImage Query Text Query Average\\n\\n0.213\\n0.197\\n0.293\\n0.282\\n0.189\\n0.326\\n0.336\\n0.335\\n\\n0.235\\n0.230\\n0.331\\n0.327\\n0.222\\n0.361\\n0.341\\n0.368\\n\\n0.224\\n0.214\\n0.312\\n0.305\\n0.206\\n0.344\\n0.338\\n0.352\\n\\n(b) Pascal\\n\\nmAP\\n\\n28.35\\n25.54\\n51.08\\n44.16\\n26.19\\n56.06\\n55.41\\n57.36\\n\\n23.59\\n28.14\\n49.57\\n42.42\\n31.6\\n55.19\\n58.66\\n57.79\\n\\nTop 20%\\n\\n25.97\\n26.84\\n50.33\\n43.29\\n28.90\\n55.63\\n57.04\\n57.58\\n\\nImage Query Text Query Average\\n\\nImage Query Text Query Average\\n\\n0.161\\n0.137\\n0.148\\n0.250\\n0.219\\n0.290\\n0.271\\n0.281\\n\\n0.153\\n0.182\\n0.177\\n0.270\\n0.219\\n0.279\\n0.280\\n0.276\\n\\n0.157\\n0.159\\n0.163\\n0.260\\n0.219\\n0.285\\n0.276\\n0.279\\n\\n36\\n18\\n32\\n68\\n55\\n72\\n78\\n74\\n\\n30\\n16\\n35\\n68\\n61\\n67\\n75\\n73\\n\\n33\\n17\\n33.5\\n68\\n58\\n69.5\\n76.5\\n73.5\\n\\n(c) NUS-WIDE-10k\\n\\nmAP\\n\\nTop 20%\\n\\nImage Query Text Query Average\\n\\nImage Query Text Query Average\\n\\n0.199\\n0.199\\n0.241\\n0.250\\n0.173\\n0.319\\n0.349\\n0.331\\n\\n0.268\\n0.344\\n0.242\\n0.297\\n0.203\\n0.375\\n0.348\\n0.379\\n\\n0.234\\n0.272\\n0.242\\n0.274\\n0.188\\n0.347\\n0.349\\n0.355\\n\\n37\\n29\\n37.1\\n30.2\\n25.3\\n47.1\\n53.1\\n49.6\\n\\n33.8\\n47.7\\n38.2\\n35.4\\n27\\n53.5\\n59.7\\n56.5\\n\\n35.4\\n38.35\\n37.65\\n32.8\\n26.15\\n50.3\\n56.4\\n53.05\\n\\nimages and searching images by texts on Wikipedia data set,\\nimproves 16.0% and 3.3% respectively on the two tasks on\\nPascal data set, improves 27.6% and 26.3% respectively on\\nthe two tasks on NUS-WIDE-10k data set. Both Bimodal\\nAE and Bimodal DBN model the multi-modal inputs with a\\nshared hidden layer. Representation learned in shared hid-\\nden layer should cover di\\ufb00erence between two modalities for\\nachieving good performance in both autoencoders or RBM-\\ns.\\nIt therefore focuses more on learning complementarity\\ninstead of correlation across data from di\\ufb00erent modalities.\\nFigure 7 shows three examples of text-based cross-modal\\nusing our Corr-Full-AE and the best baseline method. In\\nthese examples, the top four retrieved images by Corr-Full-\\nAE are all relevant to correspondence text query. Figure 8\\nshows several failure cases of Corr-Full-AE on the NUS-\\nWIDE-10k test data set.\\n\\nAs for the top 20% percentage metric, our three correspon-\\ndence autoencoders also signi\\ufb01cantly outperform the other\\nbaseline models. There is only one ground-truth for each\\nimage-text pair under this evaluation metric. Figure 9 shows\\nseveral top-1-hit retrieval examples by our Corr-Full-AE. In\\n\\nthese cases, the \\ufb01rst returned results by the query are al-\\nl ground truth on both image query text and text query\\nimage tasks.\\n\\n3.6 Analysis of \\u03b1\\n\\nHere, mAP values of three correspondence autoencoders\\nwith di\\ufb00erent values of \\u03b1 in all data sets are given in Fig-\\nure 10. Both too small values and too large values of \\u03b1 show\\npoor performance in the three data sets. This is consisten-\\nt with the impact of \\u03b1 in the present models. Too small\\nvalues of \\u03b1 overemphasize the \\u201cindividuality\\u201d of data and\\nignore the correlations. Too large values overemphasize the\\ncorrelations and ignore the \\u201cindividuality\\u201d of the data.\\n\\nTo validate the hypothesis of the e\\ufb00ect of \\u03b1 to the \\u201cindivid-\\nuality\\u201d of data, we use tSNE [26] to visualize the image and\\ntext representation learned by our Corr-Full-AE. As shown\\nin Figure 11, when \\u03b1 = 0.01, image and text representa-\\ntion space are almost disjoint. In this case, image and text\\nhave strong \\u201cindividuality\\u201d so that the learned representa-\\ntion of image and text have no correlations. To the other\\nextreme of the \\u03b1, when \\u03b1 = 0.99, \\u201cindividuality\\u201d of im-\\n\\n13\\x0cText Query\\nwater street \\n\\nreflections windows \\nmen wedding glasses \\n\\nyork post ties\\n\\nperson\\n\\nblue red travel face \\namerica sculpture \\nchile memorial tourist\\n\\nsky\\n\\nTop five retrieved images\\n\\nImage query\\n\\ngreen trees shade\\n\\ngreen grass shoes\\n\\nTop five retrieved texts\\n\\nfield washington rain \\n\\nunited\\n\\ngreen coast path\\n\\nphotography grass \\ngarden eos home \\n\\ngolf\\n\\nwindow\\n\\nwindow\\n\\nwindow\\n\\nsky\\n\\nanimal\\n\\nwater\\n\\ngrass\\n\\ngrass\\n\\nwindow\\n\\ngrass\\n\\ngrass\\n\\nwindow\\n\\nwater\\n\\nwindow\\n\\nclouds\\n\\nperson\\n\\nwarter\\n\\nflowers\\n\\nflowers\\n\\nflowers\\n\\nflowers\\n\\nflowers\\n\\nmacro flower \\n\\nflowers pink closeup \\nrain excellence rose \\nflora turkey blooms\\n\\nnature blue flowers \\nfab fruit blossoms \\n\\nblooms\\n\\nnature bravo flower \\n\\npink excellence \\n\\nblooms\\n\\nbravo color colors \\nsummer excellence \\n\\nrainbow petals\\n\\nflower pink \\nwashington \\n\\nexcellence blossoms\\n\\n(a) Failure cases: text query image\\n\\n(b) Failure cases: image query text\\n\\nFigure 8: Several failure cases of Corr-Full-AE on the NUS-WIDE-10k test data set. The words under the\\ntexts/images are the correspondence semantic labels.\\nIn the cases of text query images, the images are\\nrelated to the subset of the text query. For example, in the \\ufb01rst case, the \\ufb01rst retrieved image is related\\nto the \\u201cwindows\\u201d and \\u201cglasses\\u201d. In the cases of image query texts, the image queries are too di\\ufb03cult to be\\nrecognized. For example, in the second case, the image query is easy to be recognized as the \\ufb02owers falsely.\\n\\n0.40\\n\\n0.35\\n\\nP\\nA\\nm\\n\\n0.30\\n\\n0.25\\n\\n0.20\\n\\n0.15\\n\\n0.0\\n\\nP\\nA\\nm\\n\\n0.30\\n\\n0.28\\n\\n0.26\\n\\n0.24\\n\\n0.22\\n\\n0.20\\n\\n0.18\\n\\n0.16\\n\\n0.14\\n\\n0.12\\n\\n0.0\\n\\n0.40\\n\\n0.35\\n\\nP\\nA\\nm\\n\\n0.30\\n\\n0.25\\n\\n0.20\\n\\n0.15\\n\\n0.0\\n\\nCorr_AEs\\nCross_Corr_AEs\\nFull_Corr_AEs\\nBest baseline\\n\\n0.2\\n\\n0.4\\n\\n0.6\\n\\n0.8\\n\\n1.0\\n\\n\\xae\\n\\n(c) NUS-WIDE-10k\\n\\nCorr_AEs\\nCross_Corr_AEs\\nFull_Corr_AEs\\nBest baseline\\n\\n0.2\\n\\n0.4\\n\\n0.6\\n\\n0.8\\n\\n1.0\\n\\n\\xae\\n\\n(b) Pascal\\n\\nCorr_AEs\\nCross_Corr_AEs\\nFull_Corr_AEs\\nBest baseline\\n\\n0.2\\n\\n0.4\\n\\n0.6\\n\\n0.8\\n\\n1.0\\n\\n\\xae\\n\\n(a) Wikipedia\\n\\nFigure 10: mAP values of three correspondence autoencoders with di\\ufb00erent values of \\u03b1 in all data sets. The\\nX-axis gives values for \\u03b1. The Y-axis gives the mAP scores. The yellow line denotes the mAP score of the\\nbest baseline model, which does not change with \\u03b1.\\n\\n(a) \\u03b1 = 0.01\\n\\n(b) \\u03b1 = 0.8\\n\\n(c) \\u03b1 = 0.99\\n\\nFigure 11: Image and text representation visualization of di\\ufb00erent value for \\u03b1 on the NUS-WIDE-10k test\\ndata set. Di\\ufb00erent shapes denote di\\ufb00erent modalities: \\u201csquare\\u201d for image, \\u201cplus\\u201d for text. Di\\ufb00erent colors\\ndenote di\\ufb00erent semantic categories.\\n\\nage and text representation is missing due to the confusion\\nrepresentation space. In this case, any image-text pair has\\ncorrelations, regardless of whether the image and text inputs\\nmatch or not.When \\u03b1 is set to 0.8, the representation space\\nis quite e\\ufb00ective to the cross-modal retrieval task, since a\\nlarge number of image-text pairs with same semantic labels\\nare clustered.\\n\\nAs shown in Figure 10, the yellow line denotes the mAP\\nscores of the best baseline model. On all data sets, our three\\ncorrespondence autoencoders outperform the best baseline\\nwhen \\u03b1 is in a quite large range.\\n\\n4. CONCLUSION\\n\\nIn this work, a cross-modal learning model, Corr-AE, is\\npresented. This model incorporates representation learn-\\ning and correlation learning into a single process, so com-\\nbining autoencoder cost with correlation cost. Corr-AE is\\nhere extended to Corr-Cross-AE by replacing the basic au-\\ntoencoder with a cross-modal autoencoder. Finally, Corr-\\nFull-AE is built by combining Corr-AE and Corr-Cross-AE.\\nThese three models are compared to CCA-based and multi-\\nmodal deep learning models and found to be e\\ufb00ective in\\n\\n14\\x0ccross-modal retrieval of information from three publicly avail-\\nable data sets.\\n\\n5. ACKNOWLEDGEMENTS\\n\\nThis work was partially supported by National Natural\\nScience Foundation of China(No. 61273365), National High\\nTechnology Research and Development Program of China(No.\\n2012AA011103), discipline building plan in 111 base(No.\\nB08004), the Fundamental Research Funds for the Central\\nUniversities (No. 2013RC0304) and Engineering Research\\nCenter of Information Networks, Ministry of Education.\\n\\n6. REFERENCES\\n\\n[1] M. Bastan, H. Cam, U. G\\xb4l\\xb4zd\\xb4l\\xb4zkbay, and z. Ulusoy.\\n\\nBilvideo-7: an mpeg-7- compatible video indexing and\\nretrieval system. IEEE MultiMedia, 17(3):62\\u201373, 2010.\\n\\n[2] Y. Bengio. Learning deep architectures for AI.\\nFoundations and Trends in Machine Learning,\\n2(1):1\\u2013127, 2009.\\n\\n[3] E. L. Bird, Steven and E. Klein. In Natural Language\\n\\nProcessing with Python. O\\u2019Reilly Media Inc., 2009.\\n[4] D. M. Blei and M. I. Jordan. Modeling annotated\\n\\ndata. ACM SIGIR, pages 127\\u2013134, 2003.\\n\\n[5] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent\\n\\ndirichlet allocation. JMLR, 3:993\\u20131022, 2003.\\n\\n[6] A. Bosch, A. Zisserman, and X. Mu\\u02dcnoz. Image\\n\\nclassi\\ufb01cation using random forests and ferns. In ICCV,\\npages 1\\u20138. IEEE, 2007.\\n\\n[15] J. Kim, J. Nam, and I. Gurevych. Learning semantics\\n\\nwith deep belief network for cross-language\\ninformation retrieval. COLING, pages 579\\u2013588, 2012.\\n\\n[16] B. S. Manjunath, J. R. Ohm, V. V. Vinod, , and\\nA. Yamada. Color and texture descriptors. IEEE\\nTrans. Circuits and Systems for Video Technology,\\nSpecial Issue on MPEG-7, 11(6):703\\u2013715, June 2001.\\n\\n[17] J. Ngiam, A. Khosla, M. Kim, J. Nam, H. Lee, and\\nA. Y. Ng. Multimodal deep learning. ICML, pages\\n689\\u2013696, 2011.\\n\\n[18] A. Oliva and A. Torralba. Modeling the shape of the\\n\\nscene: A holistic representation of the spatial\\nenvelope. IJCV, 42(3):145\\u2013175, 2001.\\n\\n[19] N. Rasiwasia, J. C. Pereira, E. Coviello, G. Doyle,\\n\\nG. R. G. Lanckriet, R. Levy, and N. Vasconcelos. A\\nnew approach to cross-modal multimedia retrieval.\\nACM MM, pages 251\\u2013260, 2010.\\n\\n[20] R. Salakhutdinov and G. Hinton. Replicated softmax:\\n\\nan undirected topic model. NIPS, pages 1607\\u20131614,\\n2009.\\n\\n[21] R. R. Salakhutdinov and G. G. Hinton. An e\\ufb03cient\\n\\nlearning procedure for deep Boltzmann machines.\\nNeural computation, 24(8):1967\\u20132006, 2012.\\n\\n[22] P. Smolensky. Parallel distributed processing:\\n\\nexplorations in the microstructure of cognition, vol. 1.\\nchapter Information processing in dynamical systems:\\nfoundations of harmony theory, pages 194\\u2013281. MIT\\nPress, Cambridge, MA, USA, 1986.\\n\\n[23] R. Socher, M. Ganjoo, C. D. Manning, and A. Ng.\\nZero-shot learning through cross-modal transfer. In\\nNIPS, pages 935\\u2013943, 2013.\\n\\n[7] T.-S. Chua, J. Tang, R. Hong, H. Li, Z. Luo, and\\n\\n[24] N. Srivastava and R. Salakhutdinov. Learning\\n\\nrepresentations for multimodal data with deep belief\\nnets. ICML Representation Learning Workshop, 2012.\\n\\n[25] N. Srivastava and R. Salakhutdinov. Multimodal\\n\\nlearning with deep Boltzmann machines. NIPS, pages\\n2231\\u20132239, 2012.\\n\\n[26] L. van der Maaten and G. Hinton. Visualizing\\n\\nhigh-dimensional data using t-sne. JMLR, 2008.\\n\\n[27] A. Vedaldi and B. Fulkerson. Vlfeat: an open and\\nportable library of computer vision algorithms. In\\nA. D. Bimbo, S.-F. Chang, and A. W. M. Smeulders,\\neditors, ACM MM, pages 1469\\u20131472, 2010.\\n[28] M. Welling, M. Rosen-Zvi, and G. Hinton.\\n\\nExponential family harmoniums with an application\\nto information retrieval. In NIPS, pages 501\\u2013508,\\nVancouver, 2004. Morgan Kaufmann.\\n\\n[29] J. Weston, S. Bengio, and N. Usunier. Large scale\\n\\nimage annotation: Learning to rank with joint\\nword-image embeddings. In ECML, pages 21\\u201335, 2010.\\n[30] Y. Zhuang, Y. F. Wang, F. Wu, Y. Zhang, and W. Lu.\\n\\nSupervised coupled dictionary learning with group\\nstructures for multi-modal retrieval. In AAAI, 2013.\\n\\nY.-T. Zheng. Nus-wide: A real-world web image\\ndatabase from national university of singapore. In\\nCIVR, Santorini, Greece., 2009.\\n\\n[8] A. Farhadi, S. M. M. Hejrati, M. A. Sadeghi,\\n\\nP. Young, C. Rashtchian, J. Hockenmaier, and D. A.\\nForsyth. Every picture tells a story: Generating\\nsentences from images. In K. Daniilidis, P. Maragos,\\nand N. Paragios, editors, ECCV (4), volume 6314 of\\nLecture Notes in Computer Science, pages 15\\u201329.\\nSpringer, 2010.\\n\\n[9] A. Frome, G. Corrado, J. Shlens, S. Bengio, J. Dean,\\n\\nM. Ranzato, and T. Mikolov. Devise: A deep\\nvisual-semantic embedding model. In NIPS, pages\\n2121\\u20132129, 2013.\\n\\n[10] D. R. Hardoon, S. Szedm\\xb4l\\xb4ck, and J. Shawe-Taylor.\\n\\nCanonical correlation analysis; an overview with\\napplication to learning methods. Neural Computation,\\n16:2639\\u20132664, 2004.\\n\\n[11] G. Hinton and R. Salakhutdinov. Reducing the\\n\\ndimensionality of data with neural networks. Science,\\n313(5786):504\\u2013507, 2006.\\n\\n[12] G. E. Hinton. Training products of experts by\\n\\nminimizing contrastive divergence. Neural\\nComputation, 14(8):1771\\u20131800, 2002.\\n\\n[13] G. E. Hinton, S. Osindero, and Y. Teh. A fast learning\\n\\nalgorithm for deep belief nets. Neural Computation,\\n18(7):1527\\u20131554, 2006.\\n\\n[14] Y. Jia, M. Salzmann, and T. Darrell. Learning\\n\\ncross-modality similarity for multinomial data. ICCV,\\npages 2407\\u20132414, 2011.\\n\\n15\\x0cNorwich  City  F.C.  was  formed  following  a  meeting  at  the  Criterion  Cafe  in \\nNorwich  on  17  June  1902  by  a  group  of  friends  led  by  two  former  Norwich \\nCEYMS  players,  Norwich  City  FC  and  played  their  first  competitive  match \\nagainst  Harwich  &  Parkeston,  at  Newmarket  Road  on  6  September  1902. \\nOriginally, the club was nicknamed the \\'\\'Citizens\\'\\', and played in light blue and \\nwhite halved shirts. The popular pastime of canary rearing had given rise to the \\nteam\\'s nickname of \"The Canaries\" by April 1905, and by February 1907 this \\nmoniker had been adopted by the national press.  (cid:258)(cid:258)\\n\\n(a) Wikipedia\\n\\nA Das Air Cargo plane sits on the runway.\\nA large airplane that is parked.\\nA passenger plane parked on a runway.\\nA white airplane with the words Das Air Cargo is on a runway.\\nPlane on the ground on a runway.\\n\\n(b) Pascal\\n\\nnature blue green landscape spring leaves lines country hills\\n\\n(c) NUS-WIDE-10k\\n\\nFigure 7: Three examples of text-based cross-modal\\nusing our Corr-Full-AE and best baseline method.\\nIn each example, the query text and its correspon-\\ndence image are shown on the top; retrieved images\\nof our Corr-Full-AE are presented in the middle;\\nretrieved images of the best baseline model are pre-\\nsented at the bottom. Relevant matches are shown\\nwith green bounding box.\\nIrrelevant matches are\\nshown with red bounding box. The text queries\\ncome from sport, aeroplane and grass category re-\\nspectively.\\n\\nWar broke out again when Armenia and Iberia revolted against Sassanid \\nrule in 571 AD, following clashes involving Roman and Persian proxies in \\nYemen  and  the  Syrian  desert,  and  Roman  negotiations  for  an  alliance \\nwith  the  Turks  against  Persia.John  of  Epiphania,  \\'\\'History\\'\\',  ,  gives  an \\nadditional for the outbreak of the war: \" contentiousness increased even \\nfurther when Justin did not deem to pay the Medians the five hundred \\npounds of gold each year previously agreed to under the peace treaties \\nand let the Roman State remain forever a tributary of the Persians.\"(cid:258)(cid:258)\\nIt  is  a  commercially  important  species  to  the  extent  that  its  fins  are \\nprized  for  soup  and  its  meat  and  oil  frequently  used.  It  is  used  fresh, \\nsmoked, dried and salted for human consumption and its hide is used for \\nleather.  It  is  subject  to  fishing  pressure  throughout  virtually  its  whole \\nrange  &mdash;  although  it  is  more  often  taken  as  by-catch  than  by \\ndesign,  since  it  takes  bait  from  longlines  intended  for  other  species. \\nFamed  oceanographic  researcher  Jacques  Cousteau  described  the \\noceanic whitetip as \"the most dangerous of all sharks\". (cid:258)(cid:258)\\nAndean Peru has been recognized as one of six global areas that saw the \\nindigenous  development  of  civilization,  and  one  of  two,  along  with \\nMesoamerica, in the Western Hemisphere. Norte Chico has pushed back \\nthe horizon for complex societies in the Peruvian region by centuries. The \\nChav\\xedn  culture,  circa  900  BC,  had  long  been  considered  the  first \\ncivilization  of  the  area  and  is  still  regularly  cited  as  such  in  general \\nworks. (cid:258)(cid:258)\\n\\n(a) Wikipedia\\n\\nTwo bicyclists and a pedestrian are waiting to cross the road.\\nTwo bicyclists wait to cross a busy road. \\nTwo blonde girls attempt to cross a busy street in town with their bicycles. \\nTwo women watching the street with their bikes to see when they may cross. \\nTwo women with bicycles are trying to cross the street.\\n\\nA black and white cow in a grassy field stares at the camera. \\nA black and white cow standing in a grassy field. \\nA black and white cow stands on grass against a partly cloudy blue sky. \\nA cow is gazing over the grass he is about to graze.\\nBlack and white cow standing in grassy field.\\n\\nA group of friends pose with furry hats.\\nA group of people, four of which are women, two of which are men, pose \\nwearing fuzzy hats with earflaps.\\nA group of peoples posing in fur hats.\\nGroup of teenagers being silly.\\nVisitors pose wearing fur hats.\\n\\n(b) Pascal\\n\\nnature green quality fly spider\\n\\nwhite macro flowers pink rxcellence balcony\\n\\nfood fish mexico\\n\\n(c) NUS-WIDE-10k\\n\\nFigure 9: Several image-text pairs of three data set-\\ns.\\nIn these pairs, the closest images(texts) to the\\ntext(image) queries are all ground truth. In \\ufb01gure\\n(a), three image-text pairs come from warfare, bi-\\nology, history category respectively. In \\ufb01gure (b),\\nthree pairs come from bicycle, cow, person category\\nrespectively.\\nIn \\ufb01gure (c), three pairs come from\\nanimal, \\ufb02ower, food category respectively. Various\\nimages(texts) can be retrieved e\\ufb00ectively.\\n\\n16\\x0c', u'Multimodal Neural Language Models\\n\\nRyan Kiros\\nRuslan Salakhutdinov\\nRichard Zemel\\nDepartment of Computer Science, University of Toronto\\nCanadian Institute for Advanced Research\\n\\nRKIROS@CS.TORONTO.EDU\\nRSALAKHU@CS.TORONTO.EDU\\nZEMEL@CS.TORONTO.EDU\\n\\nAbstract\\n\\nWe introduce two multimodal neural language\\nmodels: models of natural language that can\\nbe conditioned on other modalities. An image-\\ntext multimodal neural language model can be\\nused to retrieve images given complex sentence\\nqueries, retrieve phrase descriptions given image\\nqueries, as well as generate text conditioned on\\nimages. We show that in the case of image-text\\nmodelling we can jointly learn word representa-\\ntions and image features by training our models\\ntogether with a convolutional network. Unlike\\nmany of the existing methods, our approach can\\ngenerate sentence descriptions for images with-\\nout the use of templates, structured prediction,\\nand/or syntactic trees. While we focus on image-\\ntext modelling, our algorithms can be easily ap-\\nplied to other modalities such as audio.\\n\\n1. Introduction\\nDescriptive language is almost never isolated from other\\nmodalities. Advertisements come with images of the prod-\\nuct that is being sold, social media pro\\ufb01les contain both\\ndescriptions and images of the user while multimedia web-\\nsites that play audio and video have associated descriptions\\nand opinions of the content. Consider the task of creat-\\ning an advertisement to sell an item. An algorithm that can\\nmodel both text descriptions and pictures of the item would\\nallow a user to (a): search for pictures given a text descrip-\\ntion of the desired content; (b): \\ufb01nd similar item descrip-\\ntions given uploaded images; and (c): automatically gen-\\nerate text to describe the item given pictures. What these\\ntasks have in common is the need to go beyond simple bag-\\nof-word representations of text alone to model complex de-\\nscriptions with an associated modality.\\n\\nProceedings of the 31 st International Conference on Machine\\nLearning, Beijing, China, 2014. JMLR: W&CP volume 32. Copy-\\nright 2014 by the author(s).\\n\\nIn this paper we introduce multimodal neural language\\nmodels, models of natural language that can be condi-\\ntioned on other modalities. A multimodal neural language\\nmodel represents a \\ufb01rst step towards tackling the previ-\\nously described modelling challenges. Unlike most pre-\\nvious approaches to generating image descriptions, our\\nmodel makes no use of templates, structured models, or\\nsyntactic trees. Instead, it relies on word representations\\nlearned from millions of words and conditioning the model\\non high-level image features learned from deep neural net-\\nworks. We introduce two methods based on the log-bilinear\\nmodel of Mnih & Hinton (2007): the modality-biased log-\\nbilinear model and the factored 3-way log-bilinear model.\\nWe then show how to learn word representations and im-\\nage features together by jointly training our language mod-\\nels with a convolutional network. Experimentation is\\nperformed on three datasets with image-text descriptions:\\nIAPR TC-12, Attributes Discovery, and the SBU datasets.\\nWe further illustrate capabilities of our models through\\nquantitative retrieval evaluation and visualizations of our\\nresults.\\n\\n2. Related Work\\nOur related work can largely be separated into three groups:\\nneural language models, image content description and\\nmultimodal representation learning.\\nNeural Language Models: A neural language model im-\\nproves on n-gram language models by reducing the curse\\nof dimensionality through the use of distributed word rep-\\nresentations. Each word in the vocabulary is represented as\\na real-valued feature vector such that the cosine of the an-\\ngles between these vectors is high for semantically similar\\nwords. Several models have been proposed based on feed-\\nforward networks (Bengio et al., 2003), log-bilinear mod-\\nels (Mnih & Hinton, 2007), skip-gram models (Mikolov\\net al., 2013) and recurrent neural networks (Mikolov et al.,\\n2010; 2011). Training can be sped up through the use of\\nhierarchical softmax (Morin & Bengio, 2005) or noise con-\\ntrastive estimation (Mnih & Teh, 2012).\\n\\n\\x0cMultimodal Neural Language Models\\n\\nFigure 1. Left two columns: Sample description retrieval given images. Right two columns: description generation. Each description\\nwas initialized to \\u2018in this picture there is\\u2019 or \\u2018this product contains a\\u2019, with 50 subsequent words generated.\\n\\nImage Description Generation: A growing body of re-\\nsearch has explored how to generate realistic text descrip-\\ntions given an image. Farhadi et al. (2010) consider learn-\\ning an intermediate meaning space to project image and\\nsentence features allowing them to retrieve text from im-\\nages and vice versa. Kulkarni et al. (2011) construct a\\nCRF using unary potentials from objects, attributes and\\nprepositions and high-order potentials from text corpora,\\nusing an n-gram model for decoding and templates for con-\\nstraints. To allow for more descriptive and poetic gen-\\neration, Mitchell et al. (2012) propose the use of syntac-\\ntic trees constructed from 700,000 Flickr images and text\\ndescriptions. For large scale description generation, Or-\\ndonez et al. (2011) showed that non-parametric approaches\\nare effective on a dataset of one million image-text cap-\\ntions. More recently, Socher et al. (2014) show how to\\nmap sentence representations from recursive networks into\\nthe same space as images. We note that unlike most exist-\\ning work, our generated text comes directly from language\\nmodel samples without any additional templates, structure,\\nor constraints.\\nMultimodal Representation Learning: Deep learning\\nmethods have been successfully used to learn representa-\\ntions from multiple modalities. Ngiam et al. (2011) pro-\\nposed using deep autoencoders to learn features from audio\\nand video, while Srivastava & Salakhutdinov (2012) intro-\\n\\nduced the multimodal deep Boltzmann machine as a joint\\nmodel of images and text. Unlike Srivastava & Salakhut-\\ndinov (2012), our proposed models are conditional and go\\nbeyond bag-of-word features. More recently, Socher et al.\\n(2013) and Frome et al. (2013) propose methods for map-\\nping images into a text representation space learned from\\na language model that incorporates global context (Huang\\net al., 2012) or a skip-gram model (Mikolov et al., 2013),\\nrespectively . This allowed Socher et al. (2013); Frome\\net al. (2013) to perform zero-shot learning, generalizing to\\nclasses the model has never seen before. Similar to our\\nwork, the authors combine convolutional networks with a\\nlanguage model but our work instead focuses on text gen-\\neration and retrieval as opposed to object classi\\ufb01cation.\\nThe remainder of the paper is structured as follows. We \\ufb01rst\\nreview the log-bilinear model of Mnih & Hinton (2007) as\\nit forms the foundation for our work. We then introduce\\nour two proposed models as well as how to perform joint\\nimage-text feature learning. Finally, we describe our exper-\\niments and results.\\n\\n3. The Log-Bilinear Model (LBL)\\nThe log-bilinear language model (LBL) (Mnih & Hinton,\\n2007) is a deterministic model that may be viewed as a\\nfeed-forward neural network with a single linear hidden\\n\\n\\x0cMultimodal Neural Language Models\\n\\nlayer. As a neural language model, the LBL operates on\\nword representation vectors. Each word w in the vocabu-\\nlary is represented as a D-dimensional real-valued vector\\nrw \\u2208 RD. Let R denote the K \\xd7 D matrix of word rep-\\nresentation vectors where K is the vocabulary size. Let\\n(w1, . . . wn\\u22121) be a tuple of n\\u2212 1 words where n\\u2212 1 is the\\ncontext size. The LBL model makes a linear prediction of\\nthe next word representation as\\n\\nn\\u22121(cid:88)\\n\\ni=1\\n\\n\\u02c6r =\\n\\nC(i)rwi,\\n\\n(1)\\n\\nwhere C(i), i = 1, . . . , n \\u2212 1 are D \\xd7 D context param-\\neter matrices. Thus, \\u02c6r is the predicted representation of\\nrwn. The conditional probability P (wn = i|w1:n\\u22121) of wn\\ngiven w1, . . . , wn\\u22121 is\\n\\nP (wn = i|w1:n\\u22121) =\\n\\n(cid:80)K\\n\\nexp(\\u02c6rT ri + bi)\\nj=1 exp(\\u02c6rT rj + bj)\\n\\n,\\n\\n(2)\\n\\nwhere b \\u2208 RK is a bias vector with a word-speci\\ufb01c bias\\nbi. Eq. 2 may be seen as scoring the predicted representa-\\ntion \\u02c6r of wn against the actual representation rwn through\\nan inner product, followed by normalization based on the\\ninner products amongst all other word representations in\\nthe vocabulary. In the context of a feed-forward neural net-\\nwork, the weights between the output layer and linear hid-\\nden layer is the word representation matrix R where the\\noutput layer uses a softmax activation. Learning can be\\ndone with standard backpropagation.\\n\\n4. Multimodal Log-Bilinear Models\\nSuppose that along with each training tuple of words\\n(w1, . . . wn) there is an associated vector x \\u2208 RM cor-\\nresponding to the feature representation of the modality to\\nbe conditioned on, such as an image. Assume for now that\\nthese features are computed in advance. In Section 5 we\\nshow how to jointly learn both text and image features.\\n\\n4.1. Modality-Biased Log-Bilinear Model (MLBL-B)\\nOur \\ufb01rst proposed model\\nis the modality-biased log-\\nbilinear model (MLBL-B) which is a straightforward ex-\\ntension of the LBL model. The MLBL-B model adds an\\nadditive bias to the next predicted word representation \\u02c6r\\nwhich is computed as\\n\\n(cid:32)n\\u22121(cid:88)\\n\\n(cid:33)\\n\\n\\u02c6r =\\n\\nC(i)rwi\\n\\n+ C(m)x,\\n\\n(3)\\n\\ni=1\\n\\nwhere C(m) is a D \\xd7 M context matrix. Given the pre-\\ndicted next word representation \\u02c6r, computing the con-\\nditional probability P (wn = i|w1:n\\u22121, x) of wn given\\nw1, . . . , wn\\u22121 and x remains unchanged from the LBL\\n\\nM(cid:88)\\n\\nmodel. The MLBL-B can be viewed as a feedforward\\nnetwork by taking the LBL network and adding a context\\nchannel based on the modality x, as shown in Fig. 2a. This\\nmodel also shares resemblance to the quadratic model of\\nGrangier et al. (2006). Learning in this model involves\\na straightforward application of backpropagation as in the\\nLBL model.\\n\\n4.2. The Factored 3-way Log-Bilinear Model\\n\\n(MLBL-F)\\n\\nA more powerful model to incorporate modality condition-\\ning is to gate the word representation matrix R by the fea-\\ntures x. By doing this, R becomes a tensor for which each\\nfeature x can specify its own hidden to output weight ma-\\ntrix. More speci\\ufb01cally, let R(1), . . . , R(m) be K \\xd7 D ma-\\ntrices speci\\ufb01ed by feature components 1, . . . , M of x. The\\nhidden to output weights corresponding to x are computed\\nas\\n\\nRx =\\n\\nxiR(i),\\n\\n(4)\\n\\ni=1\\n\\nwhere Rx denotes the word representations with respect to\\nx. The motivation for using a modality speci\\ufb01c word rep-\\nresentation is as follows. Suppose x is an image containing\\na picture of a cat, with context words (there, is, a). A lan-\\nguage model that is trained without knowledge of image\\nfeatures would score the predicted next word representa-\\ntion \\u02c6r high with words such as dog, building or car. If each\\nimage has a corresponding word representation matrix Rx,\\nthe representations for attributes that are not present in the\\nimage would be modi\\ufb01ed such that the inner product of \\u02c6r\\nwith the representation of cat would score higher than the\\ninner product of \\u02c6r with the representations of dog, building\\nor car.\\nAs is, the tensor R requires K \\xd7 D \\xd7 M parameters\\nwhich makes using a general 3-way tensor impractical even\\nfor modest vocabulary sizes. A common solution to this\\napproach (Memisevic & Hinton, 2007; Krizhevsky et al.,\\n2010) is to factor R into three lower-rank matrices Wf \\u02c6r \\u2208\\nRF\\xd7D, Wf x \\u2208 RF\\xd7M and Wf h \\u2208 RF\\xd7K, such that\\n\\nRx = (Wf h)(cid:62) \\xb7 diag(Wf xx) \\xb7 Wf \\u02c6r,\\n\\n(5)\\nwhere diag(\\xb7) denotes the matrix with its argument on the\\ndiagonal. These matrices are parametrized by F , the num-\\nber of factors, as shown in Fig. 2b.\\nLet E = (Wf \\u02c6r)(cid:62)Wf h denote the D \\xd7 K matrix of word\\nembeddings. Given the context w1, . . . , wn\\u22121, the pre-\\ndicted next word representation \\u02c6r is given by:\\n\\n(cid:32)n\\u22121(cid:88)\\n\\n(cid:33)\\n\\n\\u02c6r =\\n\\nC(i)E(:, wi)\\n\\n+ C(m)x,\\n\\n(6)\\n\\ni=1\\n\\nwhere E(:, wi) denotes the column of E for the word repre-\\nsentation of wi. Given a predicted next word representation\\n\\n\\x0cMultimodal Neural Language Models\\n\\n(a) Modality-Biased Log-Bilinear Model (MLBL-B)\\n\\n(b) Factored 3-way Log-Bilinear Model (MLBL-F)\\n\\nFigure 2. Our proposed models. Left: The predicted next word representation \\u02c6r is a linear prediction of word features rw1 , rw2 , rw3\\n(blue connections) biased by image features x. Right: The word representation matrix R is replaced by a factored tensor for which the\\nhidden-to-output connections are gated by x.\\n\\n\\u02c6r, the factor outputs are\\n\\nf = (Wf \\u02c6r\\u02c6r) \\u2022 (Wf xx),\\nwhere \\u2022 is a component-wise product.\\nThe condi-\\ntional probability P (wn = i|w1:n\\u22121, x) of wn given\\nw1, . . . , wn\\u22121 and x can be written as\\n\\n(7)\\n\\nP (wn = i|w1:n\\u22121, x) =\\n\\nexp(cid:0)(Wf h(:, i))(cid:62)f + bi\\n(cid:1)\\nj=1 exp(cid:0)(Wf h(:, j))(cid:62)f + bj\\n(cid:80)K\\n\\n(cid:1) ,\\n\\nwhere Wf h(:, i) denotes the column of Wf h correspond-\\ning to word i. We call this the MLBL-F model. As with the\\nLBL and MLBL-B models, training can be achieved using\\na straightforward application of backpropagation. Unlike\\nthe other models, extra care needs to be taken when adjust-\\ning the learning rates for the matrices of the factored tensor.\\nIt is sensible that pre-computed word embeddings could be\\nused as a starting point to training, as opposed to random\\ninitialization of the word representations. Indeed, all of our\\nexperiments use the embeddings of Turian et al. (2010) for\\ninitialization. In the case of the LBL and MLBL-B models,\\neach pre-trained word embedding can be used to initialize\\nthe rows of R. In the case of the MLBL-F model where R\\nis a factored tensor, we can let E be the D \\xd7 K matrix of\\npre-trained embeddings. Since E = (Wf \\u02c6r)(cid:62)Wf h, we can\\ninitialize the MLBL-F model with pre-trained embeddings\\nby simply applying an SVD to E.\\n\\n5. Joint Image-Text Feature Learning\\nUp until now we have not made any assumptions on the\\ntype of modality being used for the feature representation\\nx. In this section, we consider the case where the condi-\\ntioned modality consists of images and show how to jointly\\nlearn image and word features along with the model param-\\neters.\\nOne way of incorporating image representation learning is\\nto use a convolutional network for which the outputs are\\n\\nused either as an additive bias or for gating. Gradients from\\nthe loss could then be backpropagated from the language\\nmodel through the convolutional network to update \\ufb01lter\\nweights. Unfortunately, learning on every image in this ar-\\nchitecture is computationally demanding. Since each train-\\ning tuple of words comes with an associated image, then\\nthe number of training elements becomes large even with\\na modest size training set. For example, if the training set\\nconsisted of 10,000 images and each image had a text de-\\nscription of 20 words, then the number of training elements\\nfor the model becomes 200,000. For large image databases\\nthis could quickly scale to millions of training instances.\\nTo speed up computation, we follow Wang et al. (2012);\\nSwersky et al. (2013) and learn our convolutional networks\\non small feature maps learned using k-means as opposed\\nto the original images. We follow the pipeline of Coates &\\nNg (2011). Given training images, r \\xd7 r patches are ran-\\ndomly extracted, contrast normalized and whitened. These\\nare used for training a dictionary with spherical k-means.\\nThese \\ufb01lters are convolved with the image and a soft acti-\\nvation encoding is applied. If the image is of dimensions\\nnV \\xd7nH \\xd73 and kf \\ufb01lters are learned, the resulting feature\\nmaps are of size (nV \\u2212 r + 1) \\xd7 (nH \\u2212 r + 1) \\xd7 kf . Each\\nslice of this region is then split into a G\\xd7 G grid for which\\nfeatures within each region are max-pooled. This results in\\nan output of size G \\xd7 G \\xd7 kf . It is these outputs that are\\nused as inputs to the convolutional network. For all of our\\nexperiments, we use G = 9 and kf = 128.\\nEach 9 \\xd7 9 \\xd7 128 input is convolved with 64 3 \\xd7 3 \\ufb01lters\\nresulting in feature maps of size 7 \\xd7 7 \\xd7 64. Recti\\ufb01ed lin-\\near units (ReLUs) are used for activation followed by a re-\\nsponse normalization layer (Krizhevsky et al., 2012). The\\nresponse-normalized feature maps are then max-pooled\\nwith a pooling window of 3 \\xd7 3 and a stride of 2, result-\\ning in outputs of size 3\\xd7 3\\xd7 64. One fully-connected layer\\nwith ReLU activation is added. It is the feature responses at\\nthis layer that are used either for additive biasing or gating\\nin the MLBL-B and MLBL-F models, respectively.\\n\\nrw1rw2rw3\\u02c6rC1C2C3Cmhsteamshipinaardvarkabacus...zebraRrw1rw2rw3\\u02c6rC1C2C3Cmhsteamshipinaardvarkabacus...zebraWfxWfhWf\\u02c6r\\x0cMultimodal Neural Language Models\\n\\n6. Generation and Retrieval\\nThe standard approach to evaluating language models is\\nthrough perplexity\\n\\n(cid:88)\\n\\nw1:n\\n\\nlog2C(w1:n|x) = \\u2212 1\\nN\\n\\nlog2P (wn = i|w1:n\\u22121, x),\\n\\nwhere w1:n\\u22121 runs through each subsequence of length\\nn \\u2212 1 and N is the length of the sequence. Here we use\\nperplexity not only as a measure of performance but also\\nas a link between text and the additional modality.\\nFirst, consider the task of retrieving training images from\\na text query w1:N . For each image x in the training set,\\nwe compute C(w1:N|x) and return the images for which\\nC(w1:N|x) is lowest. Intuitively, images when conditioned\\non by the model that achieve low perplexity are those that\\nare a good match to the query description.\\nThe task of retrieving text from an image query is trick-\\nier for the following reasons.\\nIt is likely that there are\\nmany \\u2018easy\\u2019 sentences for which the language model will\\nassign low perplexity to independent of the query image\\nbeing conditioned on. Thus, instead of retrieving text\\nfrom the training set for which C(w1:N|x) is lowest con-\\nditioned on the query image x, we instead look at the ratio\\nC(w1:N|x)/C(w1:N|\\u02dcx) where \\u02dcx denotes the mean image in\\nthe training set (computed in feature space). Thus, if w1:N\\nis a good explanation of x, then C(w1:N|x) < C(w1:N|\\u02dcx)\\nand we can simply retrieve the text for which this ratio is\\nsmallest.\\nWhile this leads to better search results, it is conceivable\\nthat using the image itself as a query for other images\\nand returning their corresponding descriptions may in itself\\nwork well as a query strategy. For example, an image taken\\nat night would ideally return a description describing this,\\nwhich would be more likely to occur if we \\ufb01rst retrieved\\nnearby images which were also taken at night. We found\\nthe most effective way of performing description retrieval\\nis as follows: \\ufb01rst retrieve the top kr training images as\\na shortlist based on the Euclidean distance between x and\\nimages in the training set. Then retrieve the descriptions for\\nwhich C(w1:N|x)/C(w1:N|\\u02dcx) is smallest for each descrip-\\ntion w1:N in the shortlist. We found that combining these\\ntwo strategies is more effective than using either alone. In\\nthe case when a convolutional network is used, we \\ufb01rst map\\nthe images through the convolutional network and use the\\noutput representations for computing distances.\\nFinally, we generate text given an image as follows: Sup-\\npose we are given an initialization w1:n\\u22121, where n \\u2212 1 is\\nthe context size. We compute P (wn = i|w1:n\\u22121, x) and\\nobtain a sample \\u02dcw from this distribution, appending \\u02dcw to\\nour initialization. This procedure is then repeated for as\\nlong as desired.\\n\\n7. Experiments\\nWe perform experimental evaluation of our proposed mod-\\nels on three publicly available datasets:\\nIAPR TC-12 This data set consists of 20,000 images\\nacross various domains, such as landscapes, portraits, in-\\ndoor and sports scenes. Accompanying each image is a text\\ndescription of one to three sentences describing the content\\nof the image. The dataset was initially released for cross-\\nlingual retrieval (Grubinger et al., 2006) but has since been\\nused extensively for other tasks such as image annotation.\\nWe used a publicly available train/test split for our experi-\\nments.\\nAttribute Discovery This dataset contains roughly 40,000\\nimages related to products such as bags, clothing and shoes\\nas well as subcategories of each product, such as high-\\nheels and sneakers. Each image is accompanied by a web-\\nretrieved text description which often reads as an advertise-\\nment for the product. Unlike the IAPR dataset, the text de-\\nscriptions are not guaranteed to be descriptive of the image\\nand often contain noisy, unrelated text. This dataset was\\nproposed as a means of discovering visual attributes from\\nnoisy text (Berg et al., 2010). We used a random train/test\\nsplit for our experiments which will be made publicly avail-\\nable.\\nSBU Captioned Photos We obtained a subset of roughly\\n400,000 images from the SBU dataset (Ordonez et al.,\\n2011) which contain images and short text descriptions.\\nThis dataset is used to induce word embeddings learned\\nfrom both images and text for qualitative comparison.\\n\\n7.1. Details of Experiments\\nWe perform four experiments, three of which are quantita-\\ntive and one of which is qualitative:\\nBleu Evaluation Our main evaluation criteria is based on\\nBleu (Papineni et al., 2002). Bleu was designed for auto-\\nmated evaluation of statistical machine translation and can\\nbe used in our setting to measure similarity of descriptions.\\nPrevious work on generating text descriptions for images\\nuse Bleu as a means of evaluation, where the generated\\nsentence is used as a candidate for the gold standard ref-\\nerence generation. Given the diversity of possible image\\ndescriptions, Bleu may penalize candidates which are ar-\\nguably descriptive of image content as noted by Kulkarni\\net al. (2011) and may not always be the most effective eval-\\nuation (Hodosh et al., 2013), though Bleu remains the stan-\\ndard evaluation criteria for such models. Given a model,\\nwe generate a candidate description as described in Sec-\\ntion 6, generating as many words as there are in the refer-\\nence sentence and compute the Bleu score of the candidate\\nwith the reference. This is repeated over all test points ten\\ntimes, in order to account for the variability in the gener-\\nated sentences. For baselines, we also compare against the\\n\\n\\x0cMultimodal Neural Language Models\\n\\nTable 1. Sample neighbors (by cosine similarity) of words learned from the SBU dataset. First row: neighbors from Collobert & Weston\\n(2008) (C&W). Second row: neighbors from a LBL model (without images). Third row: neighbors from a MLBL-F model (with\\nimages).\\n\\ngloomy\\n\\nclassroom\\n\\n\\ufb02ower\\n\\nlighthouse\\n\\ncup\\n\\nterrain\\n\\ntranquil\\ndismal\\nhazy\\nlaptop\\npub\\nlibrary\\nbamboo\\n\\nbird\\nplant\\n\\nbreakwater\\nmonument\\n\\nchampionship\\n\\npier\\n\\ncider\\nbag\\n\\nshorelines\\n\\nseas\\n\\nheadland\\n\\nsensuous\\nslower\\nstormy\\ndorm\\ncabin\\ndesk\\nsilk\\ntiger\\n\\ufb02owers\\nice\\ufb01eld\\nlagoon\\nship\\ntrophy\\nbottle\\nbottle\\n\\ntopography\\n\\npaces\\nchasm\\n\\nsomber\\nfeeble\\nfoggy\\ndesk\\nlibrary\\nrestroom\\n\\ngold\\n\\nmonster\\n\\nfruit\\nlagoon\\nkingdom\\n\\ndock\\nbowl\\nneedle\\ncontainer\\nvegetation\\ndescent\\ncreekbed\\n\\nbleak\\nrealistic\\n\\ncrisp\\n\\ncomputer\\nbedroom\\n\\nof\\ufb01ce\\nbark\\ncow\\ngreen\\nnunnery\\nmosque\\ncastle\\nleague\\nbox\\noil\\n\\nconvection\\n\\nyards\\nranges\\n\\ncheerful\\nbrighter\\ncloudless\\ncanteen\\nof\\ufb01ce\\ncabinet\\n\\ufb02esh\\n\\ufb01sh\\nplants\\n\\nwaterway\\nskyline\\nmarina\\n\\ntournament\\n\\nfashion\\n\\nnet\\n\\ncanyons\\n\\nrays\\ncrest\\n\\ndreary\\nstrong\\ndull\\n\\ndarkroom\\ncottage\\nkitchen\\n\\nwalkway\\n\\ncrab\\nleaf\\nrose\\n\\ntruck\\npool\\ncups\\nshoe\\njam\\nslopes\\n\\ufb02oors\\n\\npamagirri\\n\\nlog-bilinear model was well as image-conditioned models\\nconditioned on random images. This allows us to obtain\\nfurther evidence of the relevance of generated text. Finally,\\nwe compare against the models of Gupta et al. (2012) and\\nGupta & Mannem (2012) who report Bleu scores for their\\nmodels on the IAPR dataset.1\\nPerplexity Evaluation Each of our proposed models are\\ntrained on both datasets and the perplexity of the language\\nmodels are evaluated. As baselines, we also include the\\nbasic log-bilinear model as well as two n-gram models. To\\nevaluate the effectiveness of using pre-trained word embed-\\ndings, we also train a log-bilinear model where the word\\nrepresentations are randomly initialized. We hypothesize\\nthat image-conditioned models should result in lower per-\\nplexity than models which are only trained on text without\\nknowledge of their associated images.\\nRetrieval Evaluation We quantitatively evaluate the per-\\nformance of our model for doing retrieval. First consider\\nthe task of retrieving images from sentence queries. Given\\na test sentence, we compute the model perplexity condi-\\ntioned on each test image and rank each image accordingly.\\nLet kr denote the number of retrieved images. We de\\ufb01ne a\\nsentence to be correctly matched if the matching image to\\nthe sentence query is ranked in the top kr images sorted by\\nmodel perplexity. Retrieving sentences from image queries\\nis performed equivalently. Since our models use a shortlist\\n(see Section 6) of nearest images for retrieving sentences,\\nwe restrict our search to images within the shortlist, for\\n\\n1We note that an exact comparison cannot be made with these\\nmethods since Gupta & Mannem (2012) assume tags are given\\nas input along with images and both methods apply 10-fold CV.\\nThe use of tags can substantially boost the relevance of generated\\nsentences. Nonetheless, these methods provide context for our\\nresults.\\n\\nN\\n\\nw1:n\\n\\n(cid:80)\\n\\nwhich the matching sentence is guaranteed to be in.\\nFor additional comparison, we include a strong image-\\nbased bag-of-words baseline to determine whether a lan-\\nguage model (and word ordering) is necessary for image-\\ndescription retrieval tasks. This model works as follows:\\ngiven image features, we learn a linear transformation onto\\nindependent logistic units, one for each word in the descrip-\\ntion. Descriptions are scored as \\u2212 1\\nlogP (wn =\\nw|x). For retrieving images, we project each image and\\nrank those which result in the highest description score.\\nFor retrieving sentences, we return those which result in\\nthe highest score given the word probabilities computed\\nfrom the image. Since we use a shortlist for our models\\nwhen performing sentence retrieval, we also use the same\\nshortlist (relative to the image features used) to allow for\\nfair comparison. A validation batch was used to tune the\\nweight decay.\\nQualitative Results We trained a LBL model and a\\nMLBL-F model on the SBU examples. Both language\\nmodels were trained on the same text, but the MLBL-F\\nalso conditioned on images using DeCAF features (Don-\\nahue et al., 2013). Both models were trained using per-\\nplexity as a criteria for early stopping, and with the same\\ncontext size and vocabulary. Table 1 shows sample nearest\\nneighbors from both models. When trained on images and\\ntext, the MLBL-F model can learn to capture both visual\\nand semantic similarities, resulting in very different near-\\nest neighbors than the LBL model and C&W embeddings.\\nThese word embeddings will be made publicly available.\\nWe use three types of image features in our experiments:\\nGist (Oliva & Torralba, 2001), DeCAF (Donahue et al.,\\n2013), and features learned jointly with a convolutional net.\\n\\n\\x0cMultimodal Neural Language Models\\n\\nTable 2. Results on IAPR TC-12. PPL refers to perplexity while\\nB-n indicates Bleu scored with n-grams. Back-off GTn refers to\\nn-grams with Katz backoff and Good-Turing discounting. Mod-\\nels which use a convolutional network are indicated by -conv,\\nwhile -conv-R indicates using random images for conditioning.\\nskmeans refers to the features of Kiros & Szepesv\\xb4ari (2012).\\n\\nMODEL TYPE\\n\\nPPL.\\n\\nB-1\\n\\nB-2\\n\\nB-3\\n\\nBACK-OFF GT2\\nBACK-OFF GT3\\nLBL\\nMLBL-B-CONV-R\\nMLBL-B-SKMEANS\\nMLBL-F-SKMEANS\\nMLBL-B-GIST\\nMLBL-F-GIST\\nMLBL-B-CONV\\nMLBL-F-CONV\\nMLBL-B-DECAF\\nMLBL-F-DECAF\\nGUPTA ET AL.\\nGUPTA & MANNEM\\n\\n54.5\\n55.6\\n20.1\\n28.7\\n18.0\\n20.3\\n20.8\\n28.8\\n20.6\\n21.7\\n24.7\\n21.8\\n\\n-\\n-\\n\\n0.323\\n0.312\\n0.327\\n0.325\\n0.349\\n0.348\\n0.348\\n0.341\\n0.349\\n0.341\\n0.373\\n0.361\\n0.15\\n0.33\\n\\n0.145\\n0.131\\n0.144\\n0.143\\n0.161\\n0.165\\n0.164\\n0.151\\n0.165\\n0.156\\n0.187\\n0.176\\n0.06\\n0.18\\n\\n0.059\\n0.059\\n0.068\\n0.069\\n0.079\\n0.085\\n0.083\\n0.074\\n0.085\\n0.073\\n0.098\\n0.092\\n0.01\\n0.07\\n\\n7.2. Details of Training\\nEach of our language models were trained using the follow-\\ning hyperparameters: all context matrices used a weight de-\\ncay of 1.0\\xd710\\u22124 while word representations used a weight\\ndecay of 1.0 \\xd7 10\\u22125. All other weight matrices, includ-\\ning the convolutional network \\ufb01lters use a weight decay of\\n1.0 \\xd7 10\\u22124. We used batch sizes of 20 and an initial learn-\\ning rate of 0.2 (averaged over the minibatch) which was\\nexponentially decreased at each epoch by a factor of 0.998.\\nGated methods used an initial learning rate of 0.02. Ini-\\ntial momentum was set to 0.5 and was increased linearly to\\n0.9 over 20 epochs. The word representation matrices were\\ninitialized to the 50 dimensional pre-trained embeddings of\\nTurian et al. (2010). We used a context size of 5 for each of\\nour models. Perplexity was computed starting with word\\nC + 1 for all methods where C is the largest context size\\nused in comparison (5 in our experiments). Perplexity was\\nnot evaluated on descriptions shorter than C + 3 words for\\nall models. Since features used have varying dimension-\\nality, an additional layer was added to map images to 256\\ndimensions, so that across all experiments the input size to\\nthe bias and gating units are equivalent. Note that we did\\nnot explore varying the word embedding dimensionalities,\\ncontext sizes or number of factors.\\nFor each of our experiments, we split the training set into\\n80% training and 20% validation. Each model was trained\\nwhile monitoring the perplexity on the validation set. Once\\nthe perplexity no longer improved for 5 epochs, the objec-\\ntive value on the training set was recorded. The training and\\nvalidation sets were then fused and training continued un-\\ntil the objective value on the validation batch matched the\\nrecorded training objective. At this point, training stopped\\n\\nTable 3. Results on the Attributes Discovery dataset.\\n\\nMODEL TYPE\\n\\nPPL.\\n\\nB-1\\n\\nB-2\\n\\nB-3\\n\\nBACK-OFF GT2\\nBACK-OFF GT3\\nLBL\\nMLBL-B-CONV-R\\nMLBL-B-GIST\\nMLBL-F-GIST\\nMLBL-B-CONV\\nMLBL-F-CONV\\nMLBL-B-DECAF\\nMLBL-F-DECAF\\n\\n117.7\\n93.4\\n97.6\\n154.4\\n95.7\\n115.1\\n99.2\\n113.2\\n98.3\\n133.0\\n\\n0.163\\n0.166\\n0.161\\n0.166\\n0.185\\n0.182\\n0.189\\n0.175\\n0.186\\n0.178\\n\\n0.033\\n0.032\\n0.031\\n0.035\\n0.044\\n0.042\\n0.048\\n0.042\\n0.045\\n0.041\\n\\n0.009\\n0.011\\n0.009\\n0.012\\n0.013\\n0.013\\n0.017\\n0.014\\n0.014\\n0.012\\n\\nand evaluation was performed on the test set.\\n\\n7.3. Generation and Perplexity Results\\nTables 2 and 3 show results on the IAPR and Attributes\\ndataset, respectively. On both datasets, each of our mul-\\ntimodal models outperforms both the log-bilinear and n-\\ngram models on Bleu scores. Our multimodal models also\\noutperform Gupta et al. (2012) and result in comparable\\nperformance to Gupta & Mannem (2012).\\nIt should be\\nnoted that Gupta & Mannem (2012) assumes that both im-\\nages and tags are given as input, where the presence of\\ntags give substantial information about general image con-\\ntent. What is perhaps most surprising is that simple lan-\\nguage models independent of images can also achieve non-\\ntrivial Bleu scores. For further comparison, we computed\\nBleu scores on the convolutional MLBL-B model when\\nrandom images are used for conditioning. Moreover, we\\nalso computed Bleu scores on IAPR with LBL and MLBL-\\nB-DeCAF when stopwords are removed, obtaining (0.166,\\n0.052, 0.013) and (0.224, 0.082, 0.028) respectively. This\\ngives us strong evidence that the gains in Bleu scores are\\nobtained directly from capturing and associating word rep-\\nresentations from image content.\\nOne observation from our results is that perplexity does not\\nappear to be correlated with Bleu scores.2 On the IAPR\\ndataset, the best perplexity is obtained using the MLBL-B\\nmodel with \\ufb01xed features, even though the best Bleu scores\\nare obtained with a convolutional model. Similarly, both\\nBack-off GT3 and LBL have the lowest perplexities on the\\nAttributes dataset but are worse with respect to Bleu. Us-\\ning more than 3-grams did not improve results on either\\ndataset. For additional comparison, we also ran an experi-\\nment training LBL on both datasets using random word ini-\\ntialization, achieving perplexity scores of 23.4 and 109.6.\\nThis indicates the bene\\ufb01t of initialization from pre-trained\\nword representations. Perhaps unsurprisingly, perplexity\\n\\n2This is likely due to high variance on held-out perplexities\\ndue to the shortness of text. We note that perplexity is lower on\\nthe training set with multimodal models.\\n\\n\\x0cMultimodal Neural Language Models\\n\\nTable 4. F-scores for retrieval on IAPR TC-12 when a text query\\nis used to retrieve images (T \\u2192 I) or when an image query is\\nused to retrieve text (I \\u2192 T ). Each row corresponds to DeCAF,\\nConv and Gist features, respectively.\\n\\nTable 5. F-scores for retrieval on Attributes Discovery when a text\\nquery is used to retrieve images (T \\u2192 I) or when an image query\\nis used to retrieve text (I \\u2192 T ). Each row corresponds to DeCAF,\\nConv and Gist features, respectively.\\n\\nT \\u2192 I\\n\\nI \\u2192 T\\n\\nT \\u2192 I\\n\\nI \\u2192 T\\n\\nBOW\\n\\nMLBL-B MLBL-F\\n\\nBOW\\n\\nMLBL-B MLBL-F\\n\\nBOW\\n\\nMLBL-B MLBL-F\\n\\nBOW\\n\\nMLBL-B MLBL-F\\n\\n0.890\\n0.726\\n0.832\\n\\n0.889\\n0.788\\n0.799\\n\\n0.899\\n0.851\\n0.792\\n\\n0.755\\n0.687\\n0.599\\n\\n0.731\\n0.719\\n0.675\\n\\n0.568\\n0.736\\n0.612\\n\\n0.808\\n0.730\\n0.826\\n\\n0.852\\n0.839\\n0.844\\n\\n0.835\\n0.815\\n0.818\\n\\n0.579\\n0.607\\n0.555\\n\\n0.580\\n0.590\\n0.621\\n\\n0.504\\n0.576\\n0.579\\n\\nis much worse on the convolutional MLBL-B model when\\nrandom images are used for conditioning.\\n\\n7.4. Retrieval Results\\nTables 4 and 5 illustrate the results of our retrieval ex-\\nperiments.\\nIn the majority of our experiments either the\\nmultimodal models outperform or are competitive with the\\nbag-of-words baseline. The baseline when combined with\\nDeCAF features is exceptionally strong. Perhaps this is\\nunsurprising, given that these features were trained to pre-\\ndict object classes on ImageNet. The generality of these\\nfeatures also make it effective for predicting word occur-\\nrences, particularly if they are visual. For non-DeCAF ex-\\nperiments, our models improve on the baseline for 6 out of\\n8 tasks and result in near similar performance on another.\\nThe MLBL-F model performed best when combined with\\na convolutional net on IAPR while the MLBL-B model\\nperformed better on the remaining tasks. All 12 retrieval\\ncurves are included in the supplementary material.\\n\\n7.5. Qualitative results\\nThe supplementary material contains qualitative results\\nfrom our models. In general, the model does a good job\\nat retrieving text with general characteristics of a scene or\\nretrieving the correct type of product on the Attributes Dis-\\ncovery dataset, being able to distinguish between different\\nkinds of sub-products, such as shoes and boots. The most\\ncommon mistakes that the model makes are retrieving text\\nwith extraneous descriptions that do not exist in the image,\\nsuch as describing people that are not present. We also ob-\\nserved errors on shorter queries where single words, such\\nas sunset and lake, indicate key visual concepts that the\\nmodel is not able to pick up on.\\nFor generating text, the model was initialized with \\u2018in this\\npicture there is\\u2019 or \\u2019this product contains a\\u2019 and proceeded\\nto generate 50 words conditioned on the image. The model\\nis often able to describe the general content of the image,\\neven if it does not get speci\\ufb01cs correct such as colors of\\nclothing. This gives visual con\\ufb01rmation of the increased\\nBleu scores from our models. Several additional results are\\n\\nincluded on the web page of the \\ufb01rst author.\\n\\n8. Conclusion\\nIn this paper we proposed multimodal neural language\\nmodels. We described two novel language models and\\nshowed in the context of image-text learning how to jointly\\nlearn word representations and image features. Our models\\ncan obtain improved Bleu scores to existing approaches for\\nsentence generation while generally outperforming a strong\\nbag-of-words baseline for description and image retrieval.\\nTo our surprise, we found additive biasing with high-level\\nimage features to be quite effective. A key advantage of the\\nmultiplicative model though is speed of training: even with\\nlearning rates an order of magnitude smaller these mod-\\nels typically required substantially fewer epochs to achieve\\nthe same performance. Unlike MLBL-B, MLBL-F requires\\nadditional care in early stopping and learning rate selection.\\nThis work takes a \\ufb01rst step towards generating image de-\\nscriptions with a multimodal language model and sets a\\nbaseline when no additional structures are used. For future\\nwork, we intend to explore adding additional structures to\\nimprove syntax as well as combining our methods with a\\ndetection algorithm.\\n\\nACKNOWLEDGMENTS\\n\\nWe would also like to thank the anonymous reviewers for\\ntheir valuable comments and suggestions. This work was\\nsupported by Google and ONR Grant N00014-14-1-0232.\\n\\nReferences\\nBengio, Yoshua, Ducharme, R\\xb4ejean, Vincent, Pascal, and Janvin,\\nChristian. A neural probabilistic language model. J. Mach.\\nLearn. Res., 3:1137\\u20131155, March 2003.\\n\\nBerg, Tamara L, Berg, Alexander C, and Shih, Jonathan. Auto-\\nmatic attribute discovery and characterization from noisy web\\ndata. In ECCV, pp. 663\\u2013676. Springer, 2010.\\n\\nCoates, Adam and Ng, Andrew. The importance of encoding ver-\\nIn\\n\\nsus training with sparse coding and vector quantization.\\nICML, pp. 921\\u2013928, 2011.\\n\\n\\x0cMultimodal Neural Language Models\\n\\nCollobert, Ronan and Weston, Jason. A uni\\ufb01ed architecture for\\nnatural language processing: Deep neural networks with mul-\\ntitask learning. In ICML, pp. 160\\u2013167. ACM, 2008.\\n\\nMikolov, Tomas, Chen, Kai, Corrado, Greg, and Dean, Jeffrey.\\nEf\\ufb01cient estimation of word representations in vector space.\\narXiv preprint arXiv:1301.3781, 2013.\\n\\nDonahue, Jeff, Jia, Yangqing, Vinyals, Oriol, Hoffman, Judy,\\nZhang, Ning, Tzeng, Eric, and Darrell, Trevor. Decaf: A deep\\nconvolutional activation feature for generic visual recognition.\\narXiv preprint arXiv:1310.1531, 2013.\\n\\nFarhadi, Ali, Hejrati, Mohsen, Sadeghi, Mohammad Amin,\\nYoung, Peter, Rashtchian, Cyrus, Hockenmaier, Julia, and\\nForsyth, David. Every picture tells a story: Generating sen-\\ntences from images. In ECCV, pp. 15\\u201329. Springer, 2010.\\n\\nFrome, Andrea, Corrado, Greg S, Shlens, Jon, Bengio, Samy,\\nDean, Jeffrey, and MarcAurelio Ranzato, Tomas Mikolov. De-\\nvise: A deep visual-semantic embedding model. 2013.\\n\\nGrangier, David, Monay, Florent, and Bengio, Samy. A discrimi-\\nnative approach for the retrieval of images from text queries. In\\nMachine Learning: ECML 2006, pp. 162\\u2013173. Springer, 2006.\\n\\nGrubinger, Michael, Clough, Paul, M\\xa8uller, Henning, and Dese-\\nlaers, Thomas. The iapr tc-12 benchmark: A new evaluation\\nresource for visual information systems. In International Work-\\nshop OntoImage, pp. 13\\u201323, 2006.\\n\\nGupta, Ankush and Mannem, Prashanth. From image annotation\\n\\nto image description. In NIP, pp. 196\\u2013204. Springer, 2012.\\n\\nGupta, Ankush, Verma, Yashaswi, and Jawahar, CV. Choosing\\n\\nlinguistics over vision to describe images. In AAAI, 2012.\\n\\nMitchell, Margaret, Han, Xufeng, Dodge, Jesse, Mensch, Alyssa,\\nGoyal, Amit, Berg, Alex, Yamaguchi, Kota, Berg, Tamara,\\nStratos, Karl, and Daum\\xb4e III, Hal. Midge: Generating image\\nIn EACL, pp.\\ndescriptions from computer vision detections.\\n747\\u2013756, 2012.\\n\\nMnih, Andriy and Hinton, Geoffrey. Three new graphical mod-\\nels for statistical language modelling. In ICML, pp. 641\\u2013648.\\nACM, 2007.\\n\\nMnih, Andriy and Teh, Yee Whye. A fast and simple algorithm for\\ntraining neural probabilistic language models. arXiv preprint\\narXiv:1206.6426, 2012.\\n\\nMorin, Frederic and Bengio, Yoshua. Hierarchical probabilistic\\nIn AISTATS, pp. 246\\u2013252,\\n\\nneural network language model.\\n2005.\\n\\nNgiam, Jiquan, Khosla, Aditya, Kim, Mingyu, Nam, Juhan, Lee,\\nIn\\n\\nHonglak, and Ng, Andrew. Multimodal deep learning.\\nICML, pp. 689\\u2013696, 2011.\\n\\nOliva, Aude and Torralba, Antonio. Modeling the shape of the\\nscene: A holistic representation of the spatial envelope. IJCV,\\n42(3):145\\u2013175, 2001.\\n\\nOrdonez, Vicente, Kulkarni, Girish, and Berg, Tamara L. Im2text:\\nDescribing images using 1 million captioned photographs. In\\nNIPS, pp. 1143\\u20131151, 2011.\\n\\nHodosh, Micah, Young, Peter, and Hockenmaier, Julia. Framing\\nimage description as a ranking task: data, models and evalua-\\ntion metrics. JAIR, 47(1):853\\u2013899, 2013.\\n\\nPapineni, Kishore, Roukos, Salim, Ward, Todd, and Zhu, Wei-\\nJing. Bleu: a method for automatic evaluation of machine\\ntranslation. In ACL, pp. 311\\u2013318. ACL, 2002.\\n\\nHuang, Eric H, Socher, Richard, Manning, Christopher D, and\\nNg, Andrew Y. Improving word representations via global con-\\ntext and multiple word prototypes. In ACL, pp. 873\\u2013882, 2012.\\n\\nKiros, Ryan and Szepesv\\xb4ari, Csaba. Deep representations and\\ncodes for image auto-annotation. In NIPS, pp. 917\\u2013925, 2012.\\n\\nKrizhevsky, Alex, Hinton, Geoffrey E, et al. Factored 3-way re-\\nstricted boltzmann machines for modeling natural images. In\\nAISTATS, pp. 621\\u2013628, 2010.\\n\\nKrizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoff. Imagenet\\nIn\\n\\nclassi\\ufb01cation with deep convolutional neural networks.\\nNIPS, pp. 1106\\u20131114, 2012.\\n\\nKulkarni, Girish, Premraj, Visruth, Dhar, Sagnik, Li, Siming,\\nChoi, Yejin, Berg, Alexander C, and Berg, Tamara L. Baby\\ntalk: Understanding and generating simple image descriptions.\\nIn CVPR, pp. 1601\\u20131608. IEEE, 2011.\\n\\nMemisevic, Roland and Hinton, Geoffrey. Unsupervised learning\\n\\nof image transformations. In CVPR, pp. 1\\u20138. IEEE, 2007.\\n\\nMikolov, Tomas, Kara\\ufb01\\xb4at, Martin, Burget, Lukas, Cernock`y, Jan,\\nand Khudanpur, Sanjeev. Recurrent neural network based lan-\\nguage model. In INTERSPEECH, pp. 1045\\u20131048, 2010.\\n\\nMikolov, Tomas, Deoras, Anoop, Povey, Daniel, Burget, Lukas,\\nand Cernocky, Jan. Strategies for training large scale neural\\nnetwork language models. In ASRU, pp. 196\\u2013201. IEEE, 2011.\\n\\nSocher, Richard, Ganjoo, Milind, Manning, Christopher D, and\\nNg, Andrew. Zero-shot learning through cross-modal transfer.\\nIn NIPS, pp. 935\\u2013943, 2013.\\n\\nSocher, Richard, Le, Quoc V, Manning, Christopher D, and Ng,\\nAndrew Y. Grounded compositional semantics for \\ufb01nding and\\ndescribing images with sentences. TACL, 2014.\\n\\nSrivastava, Nitish and Salakhutdinov, Ruslan. Multimodal learn-\\ning with deep boltzmann machines. In NIPS, pp. 2231\\u20132239,\\n2012.\\n\\nSwersky, Kevin, Snoek, Jasper, and Adams, Ryan. Multi-task\\n\\nbayesian optimization. In NIPS, 2013.\\n\\nTurian, Joseph, Ratinov, Lev, and Bengio, Yoshua. Word repre-\\nsentations: a simple and general method for semi-supervised\\nlearning. In ACL, pp. 384\\u2013394. Association for Computational\\nLinguistics, 2010.\\n\\nWang, Tao, Wu, David J, Coates, Adam, and Ng, Andrew Y. End-\\nto-end text recognition with convolutional neural networks. In\\nICPR, pp. 3304\\u20133308. IEEE, 2012.\\n\\n\\x0c', u'2013 IEEE International Conference on Computer Vision\\n\\nPedestrian Parsing via Deep Decompositional Network\\n\\nPing Luo1,3\\n\\nXiaogang Wang2\\n\\n1Department of Information Engineering, The Chinese University of Hong Kong\\n2Department of Electronic Engineering, The Chinese University of Hong Kong\\n3Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences\\n\\nXiaoou Tang1,3\\u2217\\n\\npluo.lhi@gmail.com\\n\\nxgwang@ee.cuhk.edu.hk\\n\\nxtang@ie.cuhk.edu.hk\\n\\nAbstract\\n\\nWe propose a new Deep Decompositional Network\\n(DDN) for parsing pedestrian images into semantic regions,\\nsuch as hair, head, body, arms, and legs, where the pedestri-\\nans can be heavily occluded. Unlike existing methods based\\non template matching or Bayesian inference, our approach\\ndirectly maps low-level visual features to the label maps of\\nbody parts with DDN, which is able to accurately estimate\\ncomplex pose variations with good robustness to occlusions\\nand background clutters. DDN jointly estimates occluded\\nregions and segments body parts by stacking three types\\nof hidden layers: occlusion estimation layers, completion\\nlayers, and decomposition layers. The occlusion estimation\\nlayers estimate a binary mask, indicating which part of a\\npedestrian is invisible. The completion layers synthesize\\nlow-level features of the invisible part from the original\\nfeatures and the occlusion mask. The decomposition layers\\ndirectly transform the synthesized visual features to label\\nmaps. We devise a new strategy to pre-train these hidden\\nlayers, and then \\ufb01ne-tune the entire network using the\\nstochastic gradient descent. Experimental results show that\\nour approach achieves better segmentation accuracy than\\nthe state-of-the-art methods on pedestrian images with or\\nwithout occlusions. Another important contribution of this\\npaper is that it provides a large scale benchmark human\\nparsing dataset1 that includes 3, 673 annotated samples\\ncollected from 171 surveillance videos. It is 20 times larger\\nthan existing public datasets.\\n\\n1. Introduction\\n\\nvision,\\n\\nPedestrian analysis is an important topic in computer\\nincluding pedestrian detection, pose estimation,\\n\\u2217This work is supported by the General Research Fund sponsored by\\nthe Research Grants Council of Hong Kong (Project No. CUHK 417110,\\nCUHK 417011, CUHK 429412) and National Natural Science Foundation\\nof China (Project No. 61005057).\\n\\n1http://mmlab.ie.cuhk.edu.hk/datasets.html.\\n\\nFigure 1. Pedestrian parsing is dif\\ufb01cult due to the appearance and pose\\nvariations, occlusion, and background clutters. We illustrate the results of\\nDDN compared to P&S [20], SBP [1], and PbOS [6].\\n\\nand body segmentation.\\nIt has important applications to\\nimage and video search, and video surveillance. This\\npaper focuses on parsing a pedestrian \\ufb01gure into different\\nsemantic parts, such as hair, head, body, arms, and legs.\\nThis problem is challenging because of the large variations\\nof appearance, poses and shapes of pedestrians, as well as\\nthe presence of occlusions and background clutters. Some\\nexamples are shown in the \\ufb01rst column of Fig.1.\\n\\nExisting studies of pedestrian parsing [2, 1, 6, 20]\\ngenerally fall into two categories: template matching and\\nBayesian inference. The pixel-level segmentation of body\\nparts was \\ufb01rst proposed in [2], which searches for templates\\nof body parts (poselets) in the training set by incorporating\\nthe 3D skeletons of humans. The identi\\ufb01ed templates are\\ndirectly used as segmentation results and cannot accurately\\n\\ufb01t body boundaries of pedestrians in tests. No quatitative\\nexperimental evaluation was provided in [2]. Bo et al. [1]\\n(SBP) provided the ground truth annotations of the Penn-\\nFudan pedestrian database [27] and used it to evaluate the\\n\\n1550-5499/13 $31.00 \\xa9 2013 IEEE\\nDOI 10.1109/ICCV.2013.329\\n\\n2648\\n\\n\\x0csegmentation accuracy of their algorithm. Their method\\nsegments an image into superpixels and then merges the\\nsuperpixels into candidate body parts by comparing their\\nshapes and positions with templates in the training set.\\nThese approaches rely heavily on training templates. Some\\nexamples are shown in the fourth column of Fig.1.\\n\\nRauschert et al. [20] (P&S) and Eslami et al. [6] (PbOS)\\ntreated human parsing as a Bayesian inference problem.\\nPriors are learned to factorize shape and appearance of\\npedestrians. They model the appearance prior as Gaussian\\nmixture of pixel colors, and the body shape prior is modeled\\nby the pose skeleton in [20] and the multinomial shape\\nboltzmann machine in [6]. Body shapes and appearance are\\n\\ufb01rst generated from the prior models and then veri\\ufb01ed by\\nbeing matched with the observed images. The drawback\\nof these approaches is that their appearance models are\\nrelatively weak. Their results are sensitive to background\\nclutters, complex poses, and many possible cloth styles.\\nAlso, the inference through MCMC is slow. Some examples\\nare shown in the third and the \\ufb01fth columns of Fig.1.\\n\\nNo existing works consider the factor of occlusions,\\nwhich occur frequently in video surveillance and can\\nseriously deteriorate the performance of human parsing.\\n\\nThis paper addresses the aforementioned limitations by\\nproposing a new deep model, the Deep Decompositional\\nNetwork (DDN), which utilizes HOG features [3] as input\\nand outputs the segmentation label maps. HOG features\\ncan effectively characterize the boundaries of body parts\\nand estimate human poses. In order to explicitly handle the\\nocclusion problem, DDN stacks three types of hidden layer-\\ns, including occlusion estimation layers, completion layers,\\nand decomposition layers (see Fig.2 (a)). Speci\\ufb01cally, the\\nocclusion estimation layers infer a binary mask, indicating\\nwhich part of the features is occluded. The completion\\nlayers then synthesize the missing features. Finally, the\\ndecomposition layers decompose the synthesized features\\nto the label maps by learning a mapping (transformation)\\nfrom the feature space to the space of label maps (see an\\nexample in Fig.2 (a)). Unlike CNN [11], whose weights\\nare shared and locally connected, we \\ufb01nd fully connecting\\nadjacent layers in DDN can capture the global structures of\\nhumans and can improve the parsing results.\\n\\nAt the training stage, we devise a new strategy based on\\nleast squares dictionary learning to pre-train the occlusion\\nestimation layers and the decomposition layers, while the\\ncompletion layers are pre-trained with a modi\\ufb01ed denoising\\nautoencoder [26]. The entire network is then \\ufb01ne-tuned by\\nthe stochastic gradient descent. At the testing stage, our\\nnetwork can ef\\ufb01ciently transform an image into label maps\\nwithout template matching or MCMC sampling.\\n\\nOur work has three key contributions.\\n\\n(1) This is\\nthe \\ufb01rst time that deep learning is studied speci\\ufb01cally for\\npedestrian parsing.\\n(2) To the best of our knowledge,\\n\\nthis is also the \\ufb01rst study to consider the presence of\\nocclusions in human parsing. We propose a novel deep\\nnetwork, where the models for occlusion estimation, data\\ncompletion, and data transformation are incorporated into a\\nuni\\ufb01ed deep architecture and jointly trained. This method\\nhas advantages over learning each module separately. (3)\\nBy carefully designing the architecture of the network and\\nproposing training strategies, the trained DDN not only\\nprovides accurate parsing results, but is also robust to\\nocclusions, background clutters, and complex variations of\\nposes and cloth styles, and signi\\ufb01cantly outperforms state-\\nof-the-art on benchmark datasets. (4) We provide a large-\\nscale benchmark human parsing dataset (refer to footnote\\n1) which includes 3, 673 annotated samples collected from\\n171 surveillance videos, making it 20 times larger than\\nexisting public datasets.\\n\\n1.1. Related Work\\n\\nWe review some related works on occlusion estimation\\n[28, 4, 7, 24, 17], data completion [5, 21, 8], and cross-\\nmodality data transformation [16, 14, 10].\\n\\nOcclusion estimation.\\n\\nIn [28, 4, 7], all the proposed\\napproaches estimated occlusions with SVM by using HOG\\nfeatures, depth maps and optical \\ufb02ows. Our DDN with\\ndeep structures are more powerful than SVM, which is a\\n\\ufb02at model [8]. Tang et al.\\n[24] employed two restricted\\nBoltzmann machines (RBM) [8] to model the patterns\\nof occlusions and uncorrupted images.\\nIn their network,\\nocclusion patterns are sampled from the models and then\\nveri\\ufb01ed with input images. Ouyang et al. [17, 18] took the\\npart detection scores as the input and used Deep Belief Net\\n(DBN) to estimate the visibility of body parts. In contrast,\\nour model directly maps the input features to occlusion\\nmasks.\\n\\nData completion. Deep networks are strong generative\\nmodels for data completion. The deep belief network\\n(DBN) [8] and the deep Boltzmann machine (DBM) [21]\\nboth consist of multiple layers of RBMs, and complete\\nthe corrupted data using probabilistic inference. Recently,\\nthe shape Boltzmann machine (SBM) [5] and multinomial\\nshape Boltzmann machine (MSBM) [6] were proposed\\nto complete discrete data. The denoising autoencoder\\n(DAE) [26] has shown excellent performance at recovering\\ncorrupted data, and we have integrated it as a module in\\nour DDN. Instead of completing the missing data, Luo et\\nal. [13] marginalized missing data with proposed deep sum\\nproduct network for facial attribute recognition.\\n\\nData transformation. Several studies have looked at\\ntransforming data from one modality, for example, an\\nimage, to another, for example, a label map. Ngiam et al.\\n[16] proposed a multimodel deep network that concatenates\\ndata across modalities as input and reconstructs them by\\nlearning a shared representation. Luo et al.\\n[12] learns\\n\\n2649\\n\\n\\x0cFigure 2. DDN architecture, which combines occlusion estimation, data completion, and data transformation in an uni\\ufb01ed deep network.\\n\\nthe joint representation of images and label maps for face\\nparsing. Zhu et al.\\n[29] proposed a deep network to\\ntransform a face image under arbitrary pose and lighting\\nto a canonical view. Mnih et al.\\n[10] used\\nconvolutional neural networks (CNN), which consider data\\nof one modality as input and the corresponding data of the\\nother modality as output. The decomposition layers in DDN\\nare similar to CNN, but with fully-connected layers that\\ncapture the global structures of the pedestrians.\\n\\nJain et al.\\n\\n2. Network Architecture\\n\\nFig.2 (a) shows the architecture of DDN, the input of\\nwhich is a feature vector x, and the output is a set of\\nlabel maps {y1, ..., yM} of body parts. Each layer is fully\\nconnected with the next upper layer, and there are one\\ndown-sampling layer, two occlusion estimation layers, two\\ncompletion layers, and two decomposition layers. This\\narchitecture works well for pedestrian parsing. More layers\\ncan be added for more complex problems.\\nAt the bottom of DDN, the input x is down-sampled to\\nxd. x is also mapped to a binary occlusion mask xo \\u2208\\n[0, 1]n through two weight matrices W o1, W o2, and biases\\nbo1 , bo2. Notice that xo is at the same size as xd in order to\\nreduce the number of parameters in the network. xo\\ni = 0\\nif the i-th element of the feature is occluded, and xo\\ni = 1\\notherwise. xo is computed as\\n\\nxo\\n\\n= \\u03c4 (W o2 \\u03c1(W o1 x + bo1) +b o2 ),\\n\\n(1)\\nwhere \\u03c4 (x) = 1/(1 + exp(\\u2212x)) and \\u03c1(x) = max(0, x).\\nThe \\ufb01rst occlusion estimation layer employs the recti\\ufb01ed\\nlinear function [15] \\u03c1(x) as the activation function. The\\nsecond layer models binary data with the sigmoid function.\\nIn the middle of DDN, the completion layers are modeled\\nas the denoising autoencoder (DAE) [26], which utilizes the\\nelement-wise product of xo and xd as input, and outputs\\nthe completed feature vector xc through four weight ma-\\ntrices W c1, W c2 , W c(cid:2)\\n2, and the corresponding biases\\nbc1, bc2, uc1, uc2, where W (cid:2) is the transpose of W . W c1\\n\\n1 , W c(cid:2)\\n\\nand W c2 are encoders that \\ufb01nd the compact representation\\nof noisy data by projecting high dimensional data into a\\nlow dimensional space. W c(cid:2)\\n2 are decoders that\\nreconstruct the data. xc is reconstructed from xo and xd as\\nfollows,\\n\\n1 and W c(cid:2)\\n\\n= \\u03c1(W c(cid:2)\\n\\n1 \\u03c1(W c(cid:2)\\n\\n) +b c1) +b c2),\\n\\n2z + uc2 ) +u c1),\\n\\nz = \\u03c1(W c2 \\u03c1(W c1(xo (cid:2) xd\\nxc\\n\\n(2)\\n(3)\\nwhere z is the compact representation and (cid:2) denotes the\\nelement-wise product.\\nOn the top of DDN, the completed feature xc is decom-\\nposed (transformed) into several label maps {y1, ..., yM}\\nwith the corresponding weight matrices W t1, W t2\\n1 , ..., W t2\\nM ,\\nM . Each label map yi \\u2208 [0, 1]n is\\nand biases bt1, bt2\\nestimated by\\n\\n1 , ..., bt2\\n\\nyi = \\u03c4 (W t2\\n\\ni \\u03c1(W t1 xc\\n\\n+ bt1 ) + bt2\\n\\ni ),\\n\\n(4)\\n\\nwhere yij = 0 indicates the pixel belongs to the background\\nand yij = 1 indicates the pixel is on the corresponding body\\npart.\\n\\n3. Training Algorithms\\n\\nTraining DDN is done by estimating a set of weight\\nmatrices and corresponding biases.\\nIt is challenging be-\\ncause of the huge amount of parameters. If the dimensions\\nof the input feature vector and the output label maps are\\n8, 000 and 10, 000. our network has millions of parameters.\\nWe pre-train DDN in a layer-wise manner to initialize the\\nparameters, and then \\ufb01ne-tune the entire network.\\n3.1. Pre-training Occlusion Estimation Layer\\n\\nThe occlusion estimation layers infer a binary mask xo\\nfrom an input feature x. We cannot employ RBMs as\\nin [8] to unsupervised pre-train these layers, because our\\ninput and output data are in different spaces. We devise\\na supervised method based on the least squares dictionary\\nlearning to pre-train these layers. We construct a training\\n\\n2650\\n\\n\\x0ci}, where the column vectors\\nset X = {xi} and X o = {xo\\nxi and xo\\ni denote a feature and its ground truth mask.\\nInitializing the weight matrices is done in order to optimize\\n\\narg min\\n\\nW o1 ,W o2\\n\\n(cid:3) X o \\u2212 \\u03c4 (W o2 H o1 ) (cid:3)2\\nF ,\\n\\n(5)\\n\\nwhere H o1 = \\u03c1(W o1 X) is the output of the \\ufb01rst layer as\\nshown in Fig.2, and (cid:3) \\xb7 (cid:3)F is the Frobenius norm. Note that\\nbe written as(cid:2)W(cid:3)x with(cid:2)W = [W b] and(cid:3)x = [x(cid:2)\\nwe drop the bias term b for simpli\\ufb01cation, since W x + b can\\n(cid:2). Solving\\nEq.5 is not trivial because of its nonlinearity. However, we\\ncan approximate W o1 , W o2 layer-wisely as\\n\\n1]\\n\\n(cid:3) X o \\u2212 W o1 X (cid:3)2\\nF ,\\n(cid:3) X o \\u2212 W o2 H o1 (cid:3)2\\nF .\\n\\narg min\\nW o1\\n\\narg min\\nW o2\\n\\n(6)\\n\\n(7)\\n\\n1 (H o1H o(cid:2)\\n1)\\n\\nWe \\ufb01rst directly use X to approximate X o with a linear\\ntransform W o1. Once W o1 has been learned, H o1 =\\n\\u03c1(W o1 X) is used to approximate X o again with another\\nlinear transform W o2. Eq.6 and Eq.7 have the closed-\\nform solutions, W o1 = X oX(cid:2)\\n\\u22121 and W o2 =\\n\\u22121. In the case when the data set is very\\nX oH o(cid:2)\\nlarge and it is hard to compute the matrix inversion, one\\ncan employ the on-line dictionary learning algorithm [23]\\ninstead of the closed form solutions to recursively update\\nthe weight matrices.\\n3.2. Pre-training Completion Layers\\n\\n(XX(cid:2)\\n\\n)\\n\\nOur purpose is to synthesize the occluded portion of\\nfeature xd. We cast it as a data reconstruction problem\\nusing a strategy similar to DAE [26], which initializes the\\nparameters by a RBM [8] with stochastically corrupted data\\nas input, and then \\ufb01ne-tunes them by minimizing the square\\nerror between the reconstructed data and the clean data.\\nThis strategy makes it possible to obtain the weight matrices\\nrobust to noise.\\n\\nwhich is the corruption of a clean sample vc, and hc =\\n\\nWe pre-train each layer with two steps: parameters\\ninitialization and reconstruction error minimization. In the\\n\\n\\ufb01rst step, for each completion layer, let (cid:3)vc be an input,\\n\\u03c1(W c(cid:3)vc + bc) be the output. The parameters of this layer\\nE((cid:3)vc, hc\\n\\nare initialized by RBM with an energy function\\n\\n(cid:4)\\n\\n(cid:4)\\n\\n(cid:4)\\n\\njW c\\nhc\\nij,\\n\\n((cid:3)vc\\ni \\u2212 bc\\ni )2\\n2\\u03c32\\ni\\n\\n(cid:3)vc\\n\\ni\\n\\u03c3i\\n\\nj \\u2212\\n\\nuc\\njhc\\n\\n\\u2212\\n\\n) =\\n\\ni\\n\\nj\\n\\ni,j\\n\\n(8)\\nwhere \\u03c3 is the standard deviation of the noise, and\\nW c, bc, uc are the weight matrix and biases described in\\nSec.2. Eq.8 can be minimized using contrastive diver-\\ngence [8]. The conventional DAE corrupts each training\\nsample with stochastic noise. However, we use the\\nstructured noises (as shown in Fig.3) to model occlusion\\n\\nFigure 3. The 40 structured noises templates.\\n\\npatterns. For each clean sample, we generate 40 corrupted\\nsamples by computing the element-wise product between\\nthe feature and the 40 templates in Fig.3. In the second step,\\nthe reconstructed data vc = \\u03c1(W c(cid:2)\\nhc + uc). We \\ufb01ne-tune\\nthe parameters by minimizing the square error between vc\\nand the clean data vc using gradient descent.\\n3.3. Pre-training Decomposition Layers\\n\\nThe \\ufb01rst decomposition layer transforms the output of\\nthe previous layer to a different space through the weight\\nmatrix W t1. The second layer projects the output of the \\ufb01rst\\nlayer to several subspaces through a set of weight matrices\\n{W t2\\n\\ni }. Therefore, we have\\n\\u23a1\\n\\u23a2\\u23a2\\u23a2\\u23a3\\n\\n\\u23a4\\n\\u23a5\\u23a5\\u23a5\\u23a6 = \\u03c4 (\\n\\n\\u23a1\\n\\u23a2\\u23a2\\u23a2\\u23a3\\n\\ny =\\n\\ny1\\ny2\\n...\\nyM\\n\\nW t2\\n1\\nW t2\\n2\\n...\\nW t2\\nM\\n\\n\\u23a4\\n\\u23a5\\u23a5\\u23a5\\u23a6 ht1 +\\n\\n\\u23a1\\n\\u23a2\\u23a2\\u23a2\\u23a3\\n\\n\\u23a4\\n\\u23a5\\u23a5\\u23a5\\u23a6),\\n\\nbt2\\n1\\nbt2\\n2\\n...\\nbt2\\nM\\n\\n(9)\\n\\nwhere ht1 is the output of the \\ufb01rst decomposition layer.\\nBoth decomposition layers can be pre-trained using the\\nstrategy introduced in Sec.3.1.\\n3.4. Fine-tuning\\n\\nWe \\ufb01ne-tune all the parameters of DDN by minimizing\\n\\nthe following loss function after pre-training\\nE(X; W, b) = (cid:3)Y \\u2212 Y (cid:3)2\\nF ,\\n\\n(10)\\nwhere X = {xi}, Y = {yi}, and Y = {yi} are a set of\\ninput features, a set of ground truth label maps, and a set\\nof outputs of our network. W and b are a set of weight\\nmatrices and biases. They are optimized with the stochastic\\ngradient descent. For example, the weight matrices can be\\nupdated as\\n\\n\\u0394i+1 = 0.9 \\xb7 \\u0394i \\u2212 0.001 \\xb7 \\x01 \\xb7 W (cid:2)\\ni+1 = W (cid:2)\\nW (cid:2)\\n\\ni + \\u0394i+1.\\n\\n(12)\\n(cid:7) \\u2208 {1, . . . , L} and i are the indices of layers and iterations.\\nL is the total number of layers. \\u0394 is the momentum variable\\n\\ni \\u2212 \\x01 \\xb7 \\u2202E\\n\\u2202W (cid:2)\\ni\\n\\n, (11)\\n\\n2651\\n\\n\\x0c[19], \\x01 is the learning rate, and \\u2202E\\nis the derivative.\\n\\u2202W (cid:2)\\n\\u2202W (cid:2) = h(cid:2)\\u22121(e(cid:2))\\n(cid:2) is computed as the outer product of the\\n\\u2202E\\nback-propagation error e(cid:2) and the output of the previous\\nlayer h(cid:2)\\u22121. In our network, the error e(cid:2) is computed in three\\ndifferent ways. For the output layer of DDN,\\n= diag(y \\u2212 y)diag(y)(1 \\u2212 y),\\n\\n(13)\\nwhere diag(\\xb7) is the diagonal matrix. For the (cid:7)-th lower\\nlayer with the sigmoid function, the backpropagation error\\nis denoted as e(cid:2),\\u03c4 ,\\n\\neL\\n\\ne(cid:2),\\u03c4\\n\\n= diag(W (cid:2)+1(cid:2)\\n\\ne(cid:2)+1\\n\\n)diag(h(cid:2)\\n\\n)(1 \\u2212 h(cid:2)\\n\\n),\\n\\n(14)\\n\\nwhere W (cid:2)+1 and e(cid:2)+1 are the weight matrix and the error\\nof the next layer, and h(cid:2) is the output of the (cid:7)-th layer.\\nFor a lower layer with the recti\\ufb01ed linear function, the\\nbackpropagation error is computed as\\n\\n(cid:11)\\n\\n[W (cid:2)+1(cid:2)\\n0,\\n\\ne(cid:2)+1]i,\\n\\ne(cid:2),\\u03c1\\ni =\\n\\n(15)\\ni = [W (cid:2)h(cid:2)\\u22121 + b(cid:2)]i. [\\xb7]i denotes the i-th element of\\n\\n,\\n\\n\\u03b4(cid:2)\\ni > 0\\ni \\u2264 0\\n\\u03b4(cid:2)\\n\\nwhere \\u03b4(cid:2)\\na vector.\\n\\n4. Experiments\\n\\nWe conduct two sets of experiments. Sec.4.1 evaluates\\nthe effectiveness of pre-training. The occlusion estimation\\nlayers are pre-trained with 600 images selected from the\\nCUHK occlusion dataset [17], where the ground truth\\nof occlusion masks was obtained as the overlapping re-\\ngions of the bounding boxes of neighboring pedestrians,\\ne.g. the second row of Fig.4. Both the completion and\\ndecomposition layers are pre-trained with the HumanEva\\ndataset [22], which contains 937 clean pedestrians with\\nthe ground truth of label maps annotated by [1]. Pre-\\ntraining the decomposition layers requires clean images\\nand their label maps. Pre-training the completion layers\\nrequires clean images, the corrupted data of which can\\nbe obtained by element-wise multiplication with the 40\\nocclusion templates shown in Fig.3.\\n\\nSec.4.2 shows the results of pedestrian parsing on two\\ndatasets:\\nthe Penn-Fudan dataset [27] and a new dataset\\nconstructed by us. The Penn-Fudan dataset includes 169\\npedestrians taken in campus without occlusions. Our\\npedestrian parsing dataset contains 3, 673 images from 171\\nvideos of different surveillance scenes (PPSS), where 2, 064\\nimages are occluded and 1, 609 are not. The ground truth\\nof label maps for all these images is provided. Some\\nexamples are shown in Fig.7, which shows that\\nlarge\\npose, illumination, and occlusion variations are present.\\nCompared with Penn-Fudan, PPSS is much larger and more\\ndiversi\\ufb01ed on scene coverage, and is therefore suitable to\\nevaluate the performance of pedestrian parsing algorithms\\nin practical applications.\\n\\nFigure 4. We show the images and the ground truth masks from the CUHK\\nocclusion dataset in the \\ufb01rst two rows. Estimated masks with DDN after\\npre-training are shown in the last row.\\n\\nSVM [7] RoBM [24] DDN\\n72.3\\n\\n62.3\\n\\n72.9\\n\\nTable 1. The per-pixel accuracies (%) of occlusion estimation.\\n\\n4.1. Effectiveness of Pre-training\\n\\nI. Occlusion Estimation Layers. We compare with struc-\\ntured SVM [7] and RoBM [24] for occlusion estimation on\\nthe CUHK dataset [17]. 500 images are selected for training\\nand another 100 images for testing. All of the methods\\nuse HOG/mask as input/output pairs for training. Each\\nimage and its mask have the size of [160, 80] and [80, 40]\\nrespectively. The cell size of HOG is 6, which means that\\nthe feature vector has 8, 525 dimensions. Both occlusion\\nestimation layers have 3, 200 neurons. The above settings\\nare adopted in all of the remaining experiments. We aug-\\nment the original training images by randomly disturbing\\nbounding boxes and randomly changing pixel values in the\\nsame way as [9]. Eventually, 50, 000 training samples are\\nobtained. In our method, we run gradient descent for several\\niterations after the closed-form initializations. Table 1\\nreports the per-pixel accuracies, while Fig.4 presents some\\nexamples. Our result is better than SVM and comparable\\nto RoBM. It is more ef\\ufb01cient than RoBM because its pre-\\ntraining has a closed-form solution.\\n\\nII. Completion Layers. We compare with PCA [25],\\nDBN [8], DBM [21], and MSBM [6] for data completion.\\nAll these approaches are trained on the HumanEva dataset\\n[22] and tested on 100 images with random noises as shown\\nin Fig.3. The two completion layers in DDN have neurons\\n104 and 3, 000, respectively. DBN and DBM have two\\nhidden layers. The architecture of MSBM is the same as\\n[6]. Table 2 reports the mean square errors of the completed\\nfeature values of invisible parts. Some exemplar results of\\nDDN are shown in Fig.5 on real occluded images.\\n\\nPCA [25] DBN [8] DBM [21] MSBM [6] DDN\\n89\\n\\n121\\nTable 2. The mean square errors of feature completions.\\n\\n216\\n\\n391\\n\\n420\\n\\nIII. Decomposition Layers. We compare with bimodel\\n\\n2652\\n\\n\\x0cbMAE [16]\\n\\n57.2\\n\\nCNN [11] DDN\\n72.9\\n\\n71.8\\n\\nTable 3. The per-pixel accuracies (%) of label maps.\\n\\nFigure 5. Examples of feature completion of DDN. The image is shown\\non the left, the original HOG features with the occluded region (black box)\\nin the center, and the completed features on the right. For instance, the\\nfeatures of the right leg of the woman in the \\ufb01rst image is well completed.\\n\\nautoencoder (bMAE) [16] and CNN [11] for data transfor-\\nmation on the HumanEva dataset. We randomly choose\\n100 images for test and the remainder for training. Table\\n3 reports the per-pixel accuracies for the above algorithms.\\nbMAE has two hidden layers. CNN has three convolutional\\nlayers, and each layer has 32 \\ufb01lters. DDN outperforms\\nbMAE and is slightly better than CNN.\\n\\nThis section shows that the pre-training results of DDN\\nhas achieved performance that is at least comparable to the\\nstate-of-the-art. However, the major advantage of DDN\\nis that all the three modules can be well integrated into\\nan uni\\ufb01ed deep architecture and jointly \\ufb01ne-tuned for the\\nultimate goal of human parsing. Sec.4.2 shows that the\\nperformance of human parsing is signi\\ufb01cantly improved\\nafter \\ufb01ne tuning. The performance also greatly outperforms\\nthe baseline of simply cascading the best performer from\\nthe state-of-the-art on each of the three tasks.\\n4.2. Pedestrian Parsing\\n\\nResults on Penn-Fudan Dataset. This experiment is\\nconducted on images without occlusions. DDN is \\ufb01ne-\\ntuned with the HumanEva dataset [22]. It takes two hours to\\ntrain our network on one NVIDIA GTX 670 GPU, and takes\\nless than 0.1 second to parse an image. We also show the\\nresults by using only the decomposition layers (DL), which\\nare trained with the HumanEva dataset.\\n\\nTable 4 (a) reports the human parsing accuracy of DDN\\ncompared with SBP [1], P&S [20], and PbOS [6]. The table\\nincludes segmentation results on six \\ufb01ne-scale regions:\\n\\u201chair\\u201d, \\u201cface\\u201d, \\u201cup-cloth\\u201d (upper clothes), \\u201carms\\u201d, \\u201clo-\\ncloth\\u201d (lower clothes), and \\u201clegs\\u201d; and also on \\ufb01ve coarse-\\nscale regions: \\u201chead\\u201d, \\u201cup-body\\u201d (upper-body), \\u201clo-body\\u201d\\n(lower body), \\u201cFB\\u201d (foreground), and \\u201cBG\\u201d (background).\\nThe de\\ufb01nitions of these regions are illustrated in Fig.2 (b).\\nPbOS [6] did not report its result on the \\ufb01ne-scale regions.\\nFor \\ufb01ne-scale regions, DDN outperforms both P&S and\\nIt achieves the best\\nSBP on the averaged accuracies.\\nresults on four regions except \\u201cface\\u201d and \\u201carms\\u201d. SBP has\\nthe best accuracy on \\u201cface\\u201d because its template matching\\nworks well in this case, as \\u201cface\\u201d has similar shape and\\n\\nappearance. For the coarse-scale regions, DDN performs\\nbest on all the regions. DDN adopts HOG features and the\\nfully-connected network architecture. This design enables\\nit to effectively capture global pose variations. On the other\\nhand, it may lose certain \\ufb01ne-grained descriptive power\\nbecause HOG is not sensitive to small local changes. This\\npartially explains DDN does not outperform SBP on \\u201cface\\u201d\\nand \\u201carms\\u201d. Some segmentation examples of DDN are\\nshown in Fig.6. For our methods, using DL alone achieves\\nbetter results than DDN, since this dataset has no occlusion,\\nwhich means that the occlusion estimation and completion\\nlayers may slightly induce noise to the DL in the DDN.\\n\\nResults on PPSS Dataset. We evaluate the robustness\\nof DDN to occlusions with the PPSS data set. Images from\\nthe \\ufb01rst 100 surveillance scenes are used for training, and\\nthose from the remaining 71 scenes for testing. We pre-train\\nDDN as described in Sec.4.1, and \\ufb01ne-tune the network\\nwith the training data of PPSS. We also report results of\\nDDN without \\ufb01ne-tining and using DL alone, which are\\ntrained on PPSS.\\n\\nBaselines. Since the implementations of SBP, P&S, and\\nPbOS are unavailable, we cannot evaluate their performance\\nunder occlusions.\\nInstead, we cascade RoBM [24], PCA\\n[25], and CNN [11] described in Sec.4.1 as our baseline.\\nThese three methods have the best performance among\\nstate-of-the-art methods on occlusion estimation, data com-\\npletion, and data transformation. The RoBM and CNN are\\ntuned on PPSS for fair comparison.\\n\\nTable 4 (b) reports the parsing accuracy. First,\\n\\nthe\\nperformance of DL drops signi\\ufb01cantly when occlusion\\nis present. DL essentially transform the occluded HOG\\nfeatures to the label maps, which is dif\\ufb01cult since the\\nfeature space of occlusion is extremely large. Fig.8 presents\\nsome segmentation examples of DDN compared to DL,\\nand shows that DDN can effectively capture the pose of\\nthe human when large occlusion is present, because our\\nnetwork is carefully designed to handle occlusion.\\n\\nSecond, DDN outperforms the baseline and improves\\nthe pre-training because it jointly optimizes three types of\\nhidden layers. Fig.7 presents some segmentation results\\nof DDN, and shows that DDN can recover the \\u201cupper\\nbody\\u201d and \\u201clower body\\u201d even with the presence of large\\nocclusions, because DDN can capture the global structures\\nand poses of the pedestrians. Note that some small\\nbody parts can still be estimated, such as \\u201cshoes\\u201d and\\n\\u201carms\\u201d, since the correlations between pixels are implicitly\\nmaintained by our network structure.\\n\\nFig.9 presents some incorrect results, which show that\\nDDN fails to distinguish some subtle pose variations some-\\ntimes. This is partially due to the limitation of HOG as\\ndiscussed above.\\n\\n2653\\n\\n\\x0cr\\ni\\na\\nh\\n\\n44.9\\n40.0\\n44.7\\n43.2\\n\\nd\\na\\ne\\nh\\n\\n51.8\\n58.2\\n54.1\\n60.2\\n60.0\\n\\nSBP [1]\\nP&S [20]\\n\\nDDN\\nDL\\n\\nSBP [1]\\nP&S [20]\\nPbOS [6]\\n\\nDDN\\nDL\\n\\nh\\nt\\no\\nl\\nc\\n-\\np\\nu\\n\\n74.8\\n75.2\\n78.1\\n77.5\\n\\ny\\nd\\no\\nb\\n-\\no\\nl\\n\\ns\\n\\nm\\nr\\na\\n\\n26.2\\n24.7\\n25.3\\n27.4\\n\\nG\\nF\\n\\ne\\nc\\na\\nf\\n\\n60.8\\n42.8\\n54.2\\n57.1\\n\\ny\\nd\\no\\nb\\n-\\np\\nu\\n\\n73.6\\n72.5\\n69.9\\n75.7\\n76.3\\n\\n71.6\\n72.9\\n68.5\\n73.1\\n75.6\\n\\n73.3\\n76.2\\n71.6\\n78.4\\n78.7\\n\\nh\\nt\\no\\nl\\nc\\n-\\no\\nl\\n\\n71.2\\n73.0\\n75.0\\n75.3\\n\\ns\\ng\\ne\\nl\\n\\n42.0\\n46.6\\n49.8\\n52.3\\n\\ng\\nv\\nA\\n53.3\\n50.4\\n54.7\\n56.2\\n\\nG\\nB\\n81.0\\n83.0\\n73.8\\n85.0\\n86.3\\n\\ng\\nv\\nA\\n70.3\\n72.6\\n66.6\\n74.5\\n75.4\\n\\nr\\ni\\na\\nh\\n\\n29.1\\n22.0\\n\\n29.5\\n\\n35.5\\n\\nd\\na\\ne\\nh\\n\\n38.3\\n30.2\\n\\n39.1\\n\\ne\\nc\\na\\nf\\n\\n38.7\\n29.1\\n\\n39.0\\n\\n44.1\\n\\ny\\nd\\no\\nb\\n-\\np\\nu\\n\\n62.6\\n51.5\\n\\n60.5\\n\\nh\\nt\\no\\nl\\nc\\n-\\np\\nu\\n\\n60.2\\n57.3\\n\\n61.7\\n\\n68.4\\n\\ny\\nd\\no\\nb\\n-\\no\\nl\\n\\n60.0\\n52.8\\n\\n57.9\\n\\ns\\n\\nm\\nr\\na\\n\\n17.2\\n10.6\\n\\n16.2\\n\\n17.0\\n\\nG\\nF\\n\\n67.1\\n59.1\\n\\n68.4\\n\\nBaseline\\n\\nDL\\nDDN\\n\\n(pre-train)\\n\\nDDN\\n\\nBaseline\\n\\nDL\\nDDN\\n\\n(pre-train)\\n\\nh\\nt\\no\\nl\\nc\\n-\\no\\nl\\n\\n53.0\\n46.1\\n\\n54.6\\n\\n61.7\\n\\nG\\nB\\n75.0\\n68.6\\n\\n74.3\\n\\n80.0\\n\\ns\\ng\\ne\\nl\\n\\n21.5\\n12.9\\n\\n21.9\\n\\n23.8\\n\\ng\\nv\\nA\\n36.7\\n30.0\\n\\n37.1\\n\\n41.8\\n\\ng\\nv\\nA\\n60.6\\n52.4\\n\\n59.6\\n\\n65.5\\n\\n(a) Segmentation accuracies on Penn-Fudan.\\n\\nFigure 6. More results of DDN on the Penn-Fudan data set [27].\\n\\n(b) Segmentation accuracies on PPSS.\\n\\n71.4\\nTable 4. Per-pixel segmentation accuracies (%) on the Penn-Fudan [27] (a) and PPSS (b) datasets.\\n\\nDDN\\n\\n65.5\\n\\n41.2\\n\\n69.3\\n\\nFigure 8. The image, ground truth, the result of DDN, and DL are shown.\\n\\nFigure 9. Some incorrect results on the PPSS dataset.\\n\\n5. Conclusions\\n\\nWe present a new Deep Decompositional Network\\n(DDN) for pedestrian parsing. DDN combines the occlu-\\nsion estimation layers, completion layers, and the decompo-\\nsition layers in an uni\\ufb01ed network, which can handle large\\n\\nocclusions. We construct a large benchmark parsing dataset\\nthat is larger and more dif\\ufb01cult than the existing dataset.\\nOur method outperforms the state-of-the-art on pedestrian\\nparsing, both with and without occlusions.\\n\\nReferences\\n[1] Y. Bo and C. C. Fowlkes. Shape-based pedestrian parsing.\\n\\nCVPR, 2011.\\n\\n[2] L. Bourdev and J. Malik. Poselets: Body part detectors\\n\\ntrained using 3d human pose annotations. ICCV, 2009.\\n\\n[3] N. Dalal and B. Triggs. Histograms of oriented gradients for\\n\\nhuman detection. CVPR, 2005.\\n\\n[4] M. Enzweiler, A. Eigenstetter, B. Schiele, and D. M. Gavrila.\\nMulti-cue pedestrian classi\\ufb01cation with partial occlusion\\nhandling. CVPR, 2010.\\n\\n[5] S. Eslami, N. Heess, and J. Winn. The shape boltzmann\\n\\nmachine: a strong model of object shape. CVPR, 2012.\\n\\n[6] S. Eslami and C. Williams. A generative model for parts-\\n\\nbased object segmentation. NIPS, 2012.\\n\\n[7] T. Gao, B. Packer, and D. Koller. A segmentation-aware\\nobject detection model with occlusion handling. CVPR,\\n2011.\\n\\n2654\\n\\n\\x0cFigure 7. Results of DDN on PPSS dataset. The above three rows are for the unoccluded pedestrians, and the below for the occluded pedestrians, where\\nthe images, predicted label maps and ground truthes are shown respectively.\\n\\n[8] G. E. Hinton and R. R. Salakhutdinov.\\n\\nReducing the\\ndimensionality of data with neural networks. Science, 2006.\\n[9] G. E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever,\\nImproving neural networks by\\narxiv.org,\\n\\nand R. R. Salakhutdinov.\\npreventing co-adaptation of feature detectors.\\n1207.0580, 2012.\\n\\n[10] V. Jain and H. S. Seung. Natural image denoising with\\n\\nconvolutional networks. NIPS, 2008.\\n\\n[11] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-\\nbased learning applied to document recognition. Proceed-\\nings of the IEEE, 1998.\\n\\n[12] P. Luo, X. Wang, and X. Tang. Hierachical face parsing via\\n\\ndeep learning. CVPR, 2012.\\n\\n[13] P. Luo, X. Wang, and X. Tang. A deep sum-product\\narchitecture for robust facial attributes analysis. ICCV, 2013.\\n[14] V. Mnih and G. E. Hinton. Learning to label aerial images\\n\\nfrom noisy data. ICML, 2012.\\n\\n[15] V. Nair and G. E. Hinton. Recti\\ufb01ed linear units improve\\n\\nrestricted boltzmann machines. ICML, 2010.\\n\\n[16] J. Ngiam, A. Khosla, M. Kim, J. Nam, H. Lee, and A. Y. Ng.\\n\\nMultimodal deep learning. ICML, 2011.\\n\\n[17] W. Ouyang and X. Wang. A discriminative deep model for\\npedestrian detection with occlusion handling. CVPR, 2012.\\n[18] W. Ouyang and X. Wang. Modeling mutual visibility\\nin pedestrian detection.\\n\\nrelationship with a deep model\\nCVPR, 2013.\\n\\n[19] N. Qian. On the momentum term in gradient descent learning\\n\\nalgorithms. Neural Networks, 1999.\\n\\n[20] I. Rauschert and R. T. Collins. A generative model for\\nsimultaneous estimation of human body shape and pixel-\\nlevel segmentation. ECCV, 2012.\\n\\n[21] R. Salakhutdinov and G. Hinton. Deep boltzmann machines.\\n\\nAISTATS, 2009.\\n\\n[22] L. Sigal and M. J. Black. Humaneva: Synchronized video\\nand motion capture dataset for evaluation of articulated hu-\\nman motion. Technical Report CS-06-08, Brown University,\\n2006.\\n\\n[23] K. Skretting and K. Engan. Recursive least squares dictio-\\nnary learning algorithm. IEEE Trans. Signal Process, 2010.\\n[24] Y. Tang, R. Salakhutdinov, and G. Hinton. Robust boltzmann\\n\\nmachines for recognition and denoising. CVPR, 2012.\\n\\n[25] M. Turk and A. Pentland. Eigenfaces for recognition. J.\\n\\nCognitive Neuroscience, 1991.\\n\\n[26] P. Vincent, H. Larochelle, I. Lajoie, Y. Bengio, and P.-A.\\nManzagol. Stacked denoising autoencoders: Learning useful\\nrepresentations in a deep network with a local denoising\\ncriterion. JMLR, 2010.\\n\\n[27] L. Wang, J. Shi, G. Song, and I. fan Shen. Object detection\\n\\ncombining recognition and segmentation. ACCV, 2007.\\n\\n[28] X. Wang, T. X. Han, and S. Yan. An hog-lbp human detector\\n\\nwith partial occlusion handling. ICCV, 2009.\\n\\n[29] Z. Zhu, P. Luo, X. Wang, and X. Tang. Deep learning identity\\n\\npreserving face space. ICCV, 2013.\\n\\n2655\\n\\n\\x0c', u'Hierarchical Face Parsing via Deep Learning\\n\\nPing Luo1,3\\n\\nXiaogang Wang2,3\\n\\nXiaoou Tang1,3\\n\\n1Department of Information Engineering, The Chinese University of Hong Kong\\n2Department of Electronic Engineering, The Chinese University of Hong Kong\\n3Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences\\n\\npluo.lhi@gmail.com\\n\\nxgwang@ee.cuhk.edu.hk\\n\\nxtang@ie.cuhk.edu.hk\\n\\nAbstract\\n\\nThis paper investigates how to parse (segment) facial\\ncomponents from face images which may be partially oc-\\ncluded. We propose a novel face parser, which recasts\\nsegmentation of face components as a cross-modality data\\ntransformation problem, i.e., transforming an image patch\\nto a label map. Speci\\ufb01cally, a face is represented hierarchi-\\ncally by parts, components, and pixel-wise labels. With this\\nrepresentation, our approach \\ufb01rst detects faces at both the\\npart- and component-levels, and then computes the pixel-\\nwise label maps (Fig.1). Our part-based and component-\\nbased detectors are generatively trained with the deep belief\\nnetwork (DBN), and are discriminatively tuned by logistic\\nregression. The segmentators transform the detected face\\ncomponents to label maps, which are obtained by learning\\na highly nonlinear mapping with the deep autoencoder. The\\nproposed hierarchical face parsing is not only robust to par-\\ntial occlusions but also provide richer information for face\\nanalysis and face synthesis compared with face keypoint de-\\ntection and face alignment. The effectiveness of our algo-\\nrithm is shown through several tasks on 2, 239 images se-\\nlected from three datasets (e.g., LFW [12], BioID [13] and\\nCUFSF [29]).\\n\\n1. Introduction\\n\\nExplicitly parsing face images into different facial com-\\nponents implies analyzing the semantic constituents (e.g.,\\nmouth, nose, and eyes) of human faces, and is useful for a\\nvariety of tasks, including recognition, animation, and syn-\\nthesis. All these applications bring new requirements on\\nface analysis\\u2014robustness to pose, background, and occlu-\\nsions. Existing works, including both face keypoint detec-\\ntion and face alignment, focus on localizing a number of\\nlandmarks, which implicitly cover the regions of interest.\\nThe main idea of these methods is to \\ufb01rst initialize the lo-\\ncations of the landmarks (i.e., mean shape) by classi\\ufb01cation\\nor regression, and then to re\\ufb01ne them by template match-\\n\\nFigure 1. Hierarchical representation of face parsing (a). A face image\\nis parsed by combining part-based face detection (b), component-based\\nface detection (c), and component segmentation (d). There are four part-\\nbased detectors (left/right/upper/lower-half face) and six component-based\\ndetectors (left/right eye, left/right eyebrow, nose and mouth). Each compo-\\nnent detector links to a novel component segmentator. (d) shows that the\\nsegmentators can transform the detected patches to label maps (1st, 2nd\\ncolumns) and we obtain the \\ufb01ne-label maps after hysteresis thresholding\\n(3rd). (e) is the image segmentation result (i.e., groups into 2 and 10 clus-\\nters) obtained by normalized cut.\\n\\ning [2, 3, 17, 16, 5, 30, 14] or graphical models (e.g., M-\\nRF) [23, 15]. In this work, we study the problem from a\\nnew point of view and focus on computing the pixel-wise\\nlabel map of a face image as shown in Fig.1 (d). It provides\\nricher information for further face analysis and synthesis\\nsuch as 3D modeling [1] and face sketching [20, 19, 26],\\ncomparing to the results obtained by face keypoint detec-\\ntion and face alignment. This task is challenging and ex-\\nisting image segmentation approaches cannot achieve satis-\\nfactory results without human interaction. An example is\\nshown in Fig.1(e). Inspired by the success of deep autoen-\\ncoder [11], which can transform high-dimensional data into\\nlow-dimensional code and then recover the data from the\\ncode within single modality, we recast face component seg-\\nmentation as a cross-modality data transformation problem,\\nand propose an alternative deep learning strategy to direct-\\nly learn a highly non-linear mapping from images to label\\nmaps.\\n\\n978-1-4673-1228-8/12/$31.00 \\xa92012 IEEE\\n\\n2480\\n\\n\\x0cFigure 2. (a) and (b) de\\ufb01ne parts and components, which correspond to\\nthe nodes in Fig.1 (a). Red points are the positions of parts (a) or compo-\\nnents (b). Red boxes are extracted image patches for training. (c) shows\\nthe spatial relationship (i.e., orientation and position) between parts and\\ncomponents.\\n\\nIn this paper, we propose a hierarchical face parser. A\\nface is represented hierarchically by parts, components, and\\npixel-wise labels (Fig.1 (a)). With this representation, our\\napproach \\ufb01rst detects faces at both the part- and component-\\nlevels, and then computes the pixel-wise label maps. The\\ndeformable part- and component-based detectors are incor-\\nporated by extending the conventional Pictorial model [9] in\\na hierarchical fashion. As shown in Fig.1 (a), each node at\\nthe upper three layers is a detector generatively pre-trained\\nby Deep Belief Network (DBN) [10] and discriminative-\\nly tuned with logistic regression (sec.3.2). At the bottom\\nlayer, each node is associated with a component segmenta-\\ntor, which is fully trained on a dataset with patch-label map\\npairs by a modi\\ufb01ed deep autoencoder [11] (sec.3.3). Then,\\nwe demonstrate that with this model, a greedy search algo-\\nrithm with a data-driven strategy is suf\\ufb01cient to ef\\ufb01ciently\\nyield good parsing results (sec.3.1). The effectiveness of\\nhierarchical face parsing is demonstrated through the appli-\\ncations of face alignment and detection of facial keypoints,\\nand it outperforms the state-of-the-art approaches.\\n\\n1.1. Related Work\\n\\nRecent approaches of scene parsing [22] provide an al-\\nternative view for face analysis, which is to compute the\\npixel-wise label maps [27]. This representation offers rich-\\ner information and robustness compared with the existing\\nface alignment and key point detection methods.\\n\\nActive Shape Model (ASM) [3] is a well established and\\nrepresentative face alignment algorithm, and has many vari-\\nants [2, 17, 30]. They heavily rely on good initialization\\nand do not work well on images taken in unconstrained en-\\nvironments, where shape and appearance may vary great-\\nly. Starting with multiple initial shapes is a natural way to\\novercome this problem [21, 15]. For instance, in order to be\\nrobust to noise, the Markov Shape Model [15, 14] samples\\nmany shapes by combining the local line segments and ap-\\npearances as constraints. Although such methods reduce the\\ndependence on initialization, they are computationally ex-\\npensive since a large number of examples have to be drawn;\\notherwise, the matching may get stuck at local minimum.\\n\\nTo improve computational ef\\ufb01ciency and to be robust\\nto pose variations and background clutters, it has become\\n\\nFigure 3. Compare the alignment results of [16] ((a)-(d)) and ours ((e)-\\n(h)) when face images are partially occluded. Landmarks can be easily\\nobtained from the label maps obtained by our approach. The white boxes\\nindicate the initial face detection results employed in [16]. It is not accurate\\nin (a) due to occlusion. The blues boxes indicate the results of component\\ndetection. The red boxes indicate the results of part-based detectors em-\\nployed in our approach.\\n\\npopular to adopt discriminative approaches in facial analy-\\nsis [23, 16, 5, 28]. For example, the BoRMaN method [23]\\nand the Regression ASM [5] were proposed to detect facial\\nfeatures or components using boosting classi\\ufb01ers on small\\nimage patches. A component-based discriminative search\\nalgorithm [16] extended the Active Appearance Model\\n(AAM) [2] by combining the facial-component detectors\\nand the direction classi\\ufb01ers, which predict the shifting di-\\nrections of the detected components.\\n\\nHowever,\\n\\nthe aforementioned approaches have two\\ndrawbacks. First, it is dif\\ufb01cult for a single detector [25] to\\naccurately locate an occluded face for initialization. Thus,\\nthe matching process fails to converge since the initial shape\\nis far away from the optimal solution as shown in Fig.3\\n(a). Our method adopts multiple hierarchical deformable\\npart- and component-based detectors and is more robust to\\npartial occlusions. Second, since face detection and shape\\nmatching are optimized alternately, we empirically observe\\nthat even though the face components can be well detected,\\nshape matching may still converge to a local minima be-\\ncause the correlation between shape and image appearance\\nis not well captured as shown in Fig.3 (a)(c). Our method\\nemploys DBN to establish strong correlations between im-\\nages and shapes by estimating the label maps directly from\\nthe detected image patches. See Fig.1 (d) and Fig.3 (f)(h)\\nfor details.\\n\\n2. A Bayesian Formulation\\n\\nOur hierarchical face parsing can be formulated under\\na Bayesian framework, under which the detectors and seg-\\nmentators can be explained as likelihoods, and spatial con-\\nsistency can be explained as priors.\\nall the detectors. Here, L = {(cid:2)r, (cid:2)p\\n\\nLet I be an input image and L be a set of class labels of\\n}j=1..6\\ni=1..4 (see the upper\\n\\ni , (cid:2)c\\nj\\n\\n2481\\n\\n\\x0cthree layers of Fig.1 (a))1. Under the Bayesian Framework,\\nour objective is to compute a solution \\u03b8 that maximizes a\\nposterior probability, p(\\u03b8|I, L). Therefore,\\n= arg max p(I, L|\\u03b8)p(\\u03b8)\\n= arg max log p(I, L|\\u03b8) + log p(\\u03b8).\\n\\n(1)\\n\\n\\u2217\\n\\n\\u03b8\\n\\ni\\n\\n= (br/p/c\\n\\nAfter taking \\u201clog\\u201d, the objective value is equivalent to a\\nsummation of a set of scores. In other words, our problem\\nis, given a facial image I, to hierarchically \\ufb01nd the most\\npossible parsing representation \\u03b8 = (V r, V p, V c, V s, E),\\nwhich contains a set of nodes and edges. More specif-\\nically, V r/p/c = {vr/p/c\\n)}K\\ni=1\\nare the root detector (K = 1), part detectors (K = 4),\\nand component detectors (K = 6) respectively. E =\\n(Erp, Epc) indicates the spatial relations among the upper\\nthree layers. Here, we denote the component segmentators\\nas V s = {vs\\ni=1. In particular, a node is\\ndescribed by a bounding box b, a binary variable \\u03bb \\u2208 {0, 1}\\nthat indicates whether this node is occluded (\\u201co\\ufb00 \\u201d) or not\\n(\\u201con\\u201d), and a set of deep learning parameters \\u03c1 and \\u03c62. \\u039b\\ndenotes the label map of the corresponding component.\\n\\ni , \\u03c6i, \\u039bi)}6\\n\\ni = (bs\\n\\n, \\u03bbr/p/c\\n\\n, \\u03c1r/p/c\\n\\ni , \\u03bbs\\n\\ni\\n\\ni\\n\\ni\\n\\n2.1. The Scores of Spatial Consistency\\n\\nHere, log p(\\u03b8) in Eq.1 is the score of spatial consistency,\\n\\nwhich is modeled as a hierarchical pictorial structure.\\n\\ni\\n\\ni\\n\\nj\\n\\ni , vc\\n\\nj > |\\u2200vp\\n\\np(\\u03b8) is the prior probability, measuring the spatial com-\\npatibility between a face and its parts, and also between\\neach part and the components. Hence, we have p(\\u03b8) =\\n\\u2208\\np(Erp|vr)p(Epc|V p), in which Erp = {< vr, vp\\ni > |\\u2200vp\\nV p}4\\n\\u2208\\n\\u2208 V p,\\u2200vc\\ni=1 and Epc = {< vp\\nV c}j=1..6\\ni=1..4 indicate two edge set (Fig.1 (a)). In this paper,\\nwe consider orientations and locations as two kinds of s-\\npatial constraints. Therefore, the prior p(Erp|vr) can be\\nfactorized as,\\np(Erp|vr) =\\n\\ni )|\\u03bbr).\\n(2)\\nHere, the functions o(\\xb7,\\xb7) and d (\\xb7,\\xb7) respectively calcu-\\nlate the relative angle and the normalized Euclidean dis-\\ntance between the centers of two bounding boxes br and\\nbp. For example, in Fig.2 (c), we illustrate the spatial\\nconstraints of the right eye related to the left-half face.\\nWe model the probabilities of these two spatial relations\\nas Gaussian distributions. For instance, p(o(br, bp)|\\u03bbr =\\n\\n(cid:2)\\ni >\\u2208Erp\\n\\ni )|\\u03bbr)p(d (br, bp\\n\\np(o(br, bp\\n\\n<vr,v\\n\\np\\n\\np\\n\\n2, \\u2200(cid:2)\\n\\ni \\u2208 R\\n\\n1In our experiment (sec.3.4), (cid:2)r \\u2208 R\\n\\n7, each\\nvector indicates class label of the node and has a 1-of-K representation.\\nConsider \\u201cmouth ((cid:2)c\\n5)\\u201d as an example, the 5-th element is set to be 1 and\\nall the other are 0, and the 7-th element denotes the class of background.\\n2\\u03c1 = (\\u03c1dbn , \\u03c1reg ) includes the parameters for DBN and logistic re-\\ngression. \\u03c6 will be written as \\u03c6ae in later derivation which denotes the\\nparameters for the deep autoencoder.\\n\\nj \\u2208 R\\n\\n5, \\u2200(cid:2)c\\n\\n1) = N (o(br, bp); \\u03bcbrbp , \\u03a3brbp )3. Similarly, the prior of\\np(Epc|V p) can be factorized in the same way.\\n2.2. The Scores of Detectors and Segmentators\\n\\nlog p(I, L|\\u03b8) in Eq.1 can be explained as the sum of the\\nscores of detectors and segmentators, and they are modeled\\nthrough DBNs and deep autoencoders respectively.\\nLet Ib be the image patch occupied by the bounding box\\nb. The likelihood probability, p(I, L|\\u03b8), can be factorized\\n(cid:2)\\ninto the likelihood of each node as,\\n\\np(I, L|\\u03b8) =\\n\\np(Ibi, (cid:2)i|vi)\\n(cid:3)\\n(cid:6)\\n\\n(cid:4)(cid:5)\\n\\ndetector\\n\\n(3)\\n\\ni\\u2208{r,p1,...,p4,c1,...,c6}\\n\\n\\xd7 6(cid:2)\\n\\nj=1\\n\\n|vs\\n(cid:3)\\n(cid:4)(cid:5)\\n(cid:6)\\np(Ibs\\nj )\\n\\nj\\n\\n.\\n\\nsegmentator\\n\\nBy applying the Bayes rule, we formulate the likeli-\\nhood of each detector as p(Ib, (cid:2)|v) =p (Ib; \\u03c1dbn ) \\xd7\\np((cid:2)|Ib, \\u03c1dbn ; \\u03c1reg )4, and the likelihood of each component\\nsegmentator as p(Ibs|vs) = p(Ibs|\\u039bs; \\u03c6s\\n\\nae )5.\\n\\nWe model the \\ufb01rst term of the detector\\u2019s likelihood by\\nDBN and discuss details in sec.3.2, and the second term\\nevaluates how likely a node should be located on a certain\\nimage patch, is derived as below,\\np((cid:2)|Ib, b, \\u03bb, \\u03c1dbn ; \\u03c1reg ) \\u221d exp{\\u2212 (cid:5) (cid:2)\\u2212f (Ib, \\u03c1dbn ; \\u03c1reg ) (cid:5)1},\\n(4)\\nwhere, given an image patch, f (\\xb7,\\xb7) is a softmax function\\nthat predicts its class label based on the learned parameters\\n\\u03c1dbn and \\u03c1reg. Furthermore, the likelihood of the segmen-\\ntator has the following form6 and its parameters are learned\\nby deep autoencoders (sec.3.3),\\n\\np(Ibs|\\u039bs, bs, \\u03bbs; \\u03c6s\\nae ), ..., gk(Ibs|\\u039bs\\nk; \\u03c6s\\n\\nae ) \\u221d (5)\\nae )}}.\\n\\nexp{\\u2212 min{g1(Ibs|\\u039bs; \\u03c6s\\nHere, we learn k deep autoencoders to estimate the label\\nmaps of a facial component and return the one with the min-\\nimal cross-entropy error gk(\\xb7,\\xb7)7.\\n3. Hierarchical Face Parsing\\n\\nBefore we discuss the details, we \\ufb01rst give an overview\\nof our algorithm in sec.3.1. After that, we describe our\\nmethods for learning the detectors in sec.3.2 and the seg-\\nmentators in sec.3.3.\\n\\n3When a node is \\u201co\\ufb00 \\u201d, log p(o(br, bp)|\\u03bbr = 0) = \\x01, where \\x01 is a\\n\\nsuf\\ufb01ciently small negative number.\\n\\n4We drop the superscripts r/p/c here.\\n5Note that the random variables b and \\u03bb are omitted for simplicity in\\n\\nthe derivation of the likelihoods.\\n\\nthe situation that \\u03bb = 0.\\n\\n6Eq.4 and Eq.5 are de\\ufb01ned when \\u03bb = 1. Please refer to footnote 3 for\\n7gk(\\xb7, \\xb7) evaluates the cross-entropy error between the input image\\n\\npatch and the reconstructed one under the k-th deep autoencoder.\\n\\n2482\\n\\n\\x0cAlgorithm 1: Hierarchical Face Parsing\\nInput: an image I and the class label set L\\nOutput: label maps of facial components\\n1) Part-based detection:\\n\\n(1) evaluate face or part detectors on I according to sec.3.2 in a\\n\\ndata-driven fashion\\n\\n(2) hypothesize the face\\u2019s or parts\\u2019s position by calculating the\\n\\nscores of spatial constraints (Eq.2) and detection (Eq.4)\\n2) Component segmentation:\\n\\n(1) detect the components around the location proposed by 1)\\n(2) if a component is detected, then estimate its label map\\n(3) compute the scores according to sec.2.1 and Eq.5\\n\\n3) Combine the scores of 1) and 2) together, if the \\ufb01nal score is\\nlarger than a threshold, then output the result.\\n\\nFigure 4. The parsing process. In practice, we adopt the HOG features\\nfrom [6] and each testing image (a) is described by a HOG feature pyramid\\nsimilar to [8]. (b) shows the scores of the \\u201cface\\u201d and the \\u201cright-half face\\u201d\\ndetectors. Note that the former one evaluates all positions while the latter\\none evaluates only a small portion. (c) illustrates the scores of component\\nsegmentators and the transformed label maps.\\n\\n3.1. Data-driven Greedy Search Algorithm\\n\\nOur data-driven greedy search algorithm can be separat-\\ned into two steps: part-based face detection and component\\nsegmentation. For the \\ufb01rst step, we assume that the nodes of\\nroot and parts are visible (i.e., \\u03bb = 1), then we sequential-\\nly evaluate their detectors with the sliding window scheme.\\nOnce a node has been activated, the other nodes, guided by\\nthe data-driven information, will only search within a cer-\\ntain range. For instance, as shown in Fig.4 (b), the root de-\\ntector tests all positions while the detector of the right-half\\nface tests only a small portion. After running all \\ufb01ve de-\\ntectors, we combine the scores together, resulting in a good\\nhypothesis of the face\\u2019s position. Such strategy for object\\ndetection is fully evaluated in [8], where convincing results\\nare reported. For the second step, we use the component\\ndetectors to search the components on the previously pro-\\nposed location (Fig.4 (c)). If a component is activated, its\\ncorresponding segmentator is performed to output the label\\nmap. Eventually, the \\ufb01nal score of the parsing is achieved\\nby summing up the scores of spatial constraints (sec.2.1),\\ndetection (Eq.4), and segmentation (Eq.5). Moreover, the\\nresult will be pruned by a threshold learned on a validation\\nset. We summarize the algorithm in Algorithm.1.\\n\\nData-driven strategy 1. To improve the search algorith-\\nm, we solve a localization problem that is to determine the\\nangle and distance between a detected and an undetected\\nnode. For example, as illustrated in Fig.2 (c), given the lo-\\ncation of the detected left-half face, we decide to predict the\\ncoordinates of the undetected right eye. We deal with this\\nproblem by training two regressors: the \\ufb01rst one estimates\\nthe angle \\u03b1, and the second one \\ufb01nds the distance \\u03b2. The\\nSupport Vector Regression [4] is adopted to learn these two\\nregressors.\\n\\nData-driven strategy 2. It is not necessary for us to enu-\\nmerate all the combinations of the binary variable \\u03bb. If a\\nnode is not detectable, its \\u03bb is set to zero8.\\n3.2. Learning Detectors\\n\\nIn this paper, we model our detectors by deep belief\\nnetwork (DBN), which is unsupervisedly pre-trained using\\nlayer-wise Restricted Boltzmann Machine (RBM) [10] and\\nsupervisedly \\ufb01ne-tuned for classi\\ufb01cation using logistic re-\\ngression. Here, given image patches Ib as the training sam-\\nples (i.e., inputs in Fig.5 (a)), a DBN with K layers mod-\\nels the joint distribution between Ib and K hidden layers\\nh1, ..., hk as follows:\\n\\np(Ib, h1, ..., hk; \\u03c1dbn ) =\\np(hk|hk+1; \\u03c1dbn ))p(hK\\u22121, hK ; \\u03c1dbn ),\\n\\n(6)\\n\\nK\\u22122(cid:2)\\n\\n(\\n\\nk=0\\n\\nwhere Ib = h0, p(hk|hk+1; \\u03c1dbn )) is a visible-given-\\nhidden conditional distribution of the RBM at level k, and\\np(hK\\u22121, hK; \\u03c1dbn ) is the joint distribution at the top-level\\nRBM. Speci\\ufb01cally, as illustrated in Fig.5 (a), learning the\\nparaments \\u03c1 = (\\u03c1dbn, \\u03c1reg) of each detector includes two\\nstages: \\ufb01rst, \\u03c1dbn = {Wi, ui, zi}i=1..3 are estimated by\\npre-training the DBN using three RBMs layer-wisely. Then,\\nwe randomly initialize \\u03c1reg = (Wr, ur), and the initial-\\nized parameters \\u03c1reg along with the pre-trained parameters\\n\\u03c1dbn are \\ufb01ne-tuned by logistic regression. This logistic re-\\ngression layer maps the outputs of the last hidden layer to\\nclass labels and is optimized by minimizing the loss func-\\n\\ntion between label hypothesis ((cid:7)(cid:2)) and ground truth ((cid:2)). In\\n\\nthe following, we discuss how to estimate \\u03c1dbn by RBM in\\ndetails.\\n\\nAs the building block of DBN (see Fig.5 (a)), a RBM is\\nan undirected two-layer graphical model with hidden units\\n(h) and input units (Ib). There are symmetric connections\\n(i.e., weights W) between the hidden and visible units, but\\n\\n8No need to evaluate the scores related to them. This is different\\n\\nfrom [8], where all part \\ufb01lters are visible.\\n\\n2483\\n\\n\\x0cFigure 5. Illustration of learning process. We employ a four-layer DBN to model our detectors (a). The DBN is trained by layer-wise RBMs and tuned by\\nlogistic regression. Then, we propose a deep training strategy containing two steps to train the segmentators: \\ufb01rst, we train a deep autoencoder (c), whose\\n\\ufb01rst layer are replaced by a one-layer denoising autoencoder (b). The deep autoencoder is tuned in one modality, while the one-layer autoencoder is tuned\\nin both modalities. Each trained segmentator can directly output the label map using an image patch as input (d).\\n\\nno connections within them. It de\\ufb01nes a marginal probabil-\\nity over Ib using an energy model as below,\\n\\n(cid:8)\\n\\nexp{zT Ib + uT h + hT WIb}\\n\\np(Ib; \\u03c1dbn ) =\\n\\nh\\n\\nZ\\n\\n,\\n\\n(7)\\n\\nwhere Z is the partition function and z, u are the offset vec-\\ntor for input units and hidden units respectively. In our case,\\nthe conditional probabilities of p(h|I) and p(I|h) can be\\nsimply modeled by products of Bernoulli distributions:\\n\\np(hi = 1|I) = sigm(ui + Wi\\xb7I),\\np(Ij = 1|h) = sigm(zj + WT\\xb7j\\nh).\\n\\n(8)\\n\\nsigm(\\xb7) is the sigmoid function. The parameters \\u03c1dbn can\\nbe estimated by taking gradient steps determined by the\\ncontrastive divergence [10].\\n3.3. Learning Segmentators\\n\\nIn this section, we introduce a deep learning approach\\nfor training the component segmentators, which transfor-\\nm image patches to label maps. The data transforma-\\ntion problem has been well examined by deep architectures\\n(i.e., multilayer network) in previous methods. Neverthe-\\nless, they mainly focus on single modality, such as deep\\nautoencoder [11] and deep denoising autoencoder [24].\\nThe former one encodes high-dimensional data into low-\\ndimensional code and decodes the original data from it,\\nwhile the latter one learns a more robust encoder and de-\\ncoder which can recover the data even though they are heav-\\nily corrupted by noise. By combining and extending the ex-\\nisting works, we propose a deep training strategy containing\\ntwo portions: we train 1) a deep autoencoder, whose \\ufb01rst\\nlayer is replaced by 2) a one-layer denoising autoencoder.\\nIn the following, we explain how to generate the training\\ndata \\ufb01rst, and then discuss the above two steps in detail.\\n\\nTo learn a mapping from images to label maps, we must\\nexplore the correlations between them. Therefore, unlike\\nsec.3.2 where only image data are used for training, here\\nwe concatenate the images and ground truth label maps to-\\ngether as a training set. Since our purpose is to output label\\nmap given only image as input, we augment this training set\\nby adding samples that have zero values of the label map\\nand original values of the image (see Fig.5 (b)(c)). In other\\nwords, half of the training data has only image (i.e., (Ib, 0)),\\nwhile the other half has both image and ground truth la-\\nbel map (i.e., (Ib, \\u039b)). Similar strategy is adopted by [18],\\nwhich learns features by using data from different modal-\\nities in order to improve the performance of classi\\ufb01cation\\nperformed in single modality.\\n\\n1) Deep Autoencoder. As shown in Fig.5 (c), we estab-\\nlish a four-layer deep autoencoder, whose parameters \\u03c1ae\\ncan be de\\ufb01ned as {Wi, ui, zi}i=1..2. The weights and off-\\nset vector for the \\ufb01rst layer are achieved by a one-layer de-\\nnoising autoencoder introduced in step 2). We estimate the\\nweights of the second layer by RBM, and the weights of the\\nupper two layers are tied similar to [11] (i.e., W3 = W2T ,\\nW4 = W1T ). Then, the whole network is tuned in single\\nmodality, that is minimizing the cross-entropy error LH be-\\nmaps (cid:7)\\u039b) and the targets (i.e., ground truth label maps \\u039b).\\ntween the outputs at the top layer (i.e., reconstructed label\\n\\n2) One-layer denoising autoencoder. Modeling the low-\\nlevel relations between data from different modalities is cru-\\ncial but not a trivial task. Therefore, to improve the per-\\nformance of the deep network, we specially learn its \\ufb01rst\\nlayer with a denoising autoencoder [24] as shown in Fig.5\\n(b). Such shallow network is again pre-trained by RBM, but\\ntuned in both modalities (i.e., images and label maps). Note\\nthat in the \\ufb01ne-tuning stage, only the images and the ground\\ntruth label maps are used as the targets as shown at the top\\nlayer of Fig.5 (b).\\n\\n2484\\n\\n\\x0cOverall, with these two steps, each component segmen-\\ntator learns a highly non-linear mapping from images to la-\\nbel maps. The testing procedure is illustrated in Fig.5 (d),\\nwhere we delete the unused image data in the output. Thus,\\nthe deep network indeed outputs a label map given an image\\npatch.\\n3.4. Implementation Details\\n\\nIn this section, we sketch several details for the sake of\\n\\nreproduction.\\n\\nTraining Detectors. We randomly select 3, 500 images\\nfrom the Labeled Faces in the Wild (LFW) [12] database.\\nWe then randomly perturb each extracted image patch (see\\nred boxes in Fig.2 (a)(b)) by translation, rotation, and scal-\\ning. Therefore, for each category, we totaly have 42, 000\\ntraining patches. Multi-label classi\\ufb01cation strategy is em-\\nployed so that training three DBNs are enough (i.e., one\\nfor face detector, one for part detectors, and the other for\\ncomponent detectors). More speci\\ufb01cally, as shown in Fig.5\\n(a), we construct a 3-layers deep architecture and the unit\\nnumbers are 2 times, 3 times, and 4 times of the input size\\nrespectively. We supervisedly tune three DBNs with 2 out-\\nputs, 5 outputs, and 7 outputs at the top layer respectively.\\nTo construct the examples of the background category, we\\ncrop 105, 000 patches from the background for each DBN.\\nAll examples of the three DBNs are normalized to 64 \\xd7 64,\\n64 \\xd7 32, and 32 \\xd7 32 respectively. We use 9 gradient orien-\\ntations and 6 \\xd7 6 cell size to extract the HOG feature.\\n\\ncollect a dataset from internet and compare the face align-\\nment results with a state-of-the-art method, the Component-\\nbased Discriminative Search (CBDS) [16]; Third, we com-\\npare with two leading approaches (i.e., BoRMaN [23] and\\nExtended ASM (eASM) [17]) of feature point extractions.\\nThis experiment is conducted on the BioID [13] database;\\nFinally, to further evaluate the generalization power, we\\ncarry out a segmentation task on the CUHK Face Sketch\\nFERET Database (CUFSF) [29]. Note that for all these ex-\\nperiments, our model is trained on the LFW as outlined in\\nsec.3.4.\\n\\nExperiment I: performance of segmentators. We\\ntest our segmentators with a 7-classes facial image pars-\\ning experiment, which is to assign each pixel a class label\\n(e.g., left/right eye, left/right brow, nose, mouth, and back-\\nground). First, we randomly select 300 images from the\\nLFW database, and all the label maps of these images are\\nwell-labeled by hand. Then, our data-driven greedy search\\nalgorithm is performed on these images to achieve the re-\\nconstructed label maps, from which we obtain the \\ufb01nal re-\\nsults after hysteresis thresholding. Fig.7 (a) shows the con-\\nfusion matrix, in which accuracy values are computed as the\\npercentage of image pixels assigned with the correct class\\nlabel. The overall pixel-wise labeling accuracy is 90.86%.\\nIt demonstrates the ability of the learned segmentators on\\nthe segmentation of facial components. Several parsing re-\\nsults are visualized in Fig.10 (a).\\n\\nFigure 6. (a) Different translations and orientations are imposed in our\\ntraining data. (b) shows how to deal with pose variations. We \\ufb01rst separate\\nthe training data by K-means, then learn one deep autoencoder on each\\ncluster.\\n\\nTraining Segmentators. We choose anther 500 images\\nfrom the LFW [12] database, whose label maps are manual-\\nly annotated. Nevertheless, in order to cover variant poses,\\nwe import \\ufb02uctuations on position and orientation for each\\nexample as illustrated in Fig.6 (a). Similarly, all training\\npatches are \\ufb01xed at 32 \\xd7 32 and described by HOG fea-\\nture. In order to account for pose variations, we train a set\\nof 4-layers deep autoencoders for each component, which is\\nobtained by \\ufb01rst applying K-means on the label maps and\\nthen training one autoencoder on each cluster. Empirically,\\nwe set K = 15 (Fig.6 (b)).\\n4. Experiments\\n\\nWe conduct four experiments to test our algorithm. First,\\nwe perform face parsing on the LFW [12] database to e-\\nvaluate the performance of our segmentators; Second, we\\n\\nFigure 7. (a) shows the confusion matrix of Experiment I. (b) and (c) are\\nthe face models for Experiment II and Experiment III respectively.\\n\\nExperiment II: face alignment. The purpose of our\\nsecond experiment is to test our algorithm on the task of\\nface alignment compared with the CBDS method, which\\nwas trained on many public databases including LFW over\\n4, 000 images.\\nIn the spirit of performing a database in-\\ndependent test, we collect a testing dataset containing 118\\nfacial images from Google Image Search and Flickr. This\\ndataset is challenging due to occlusion, background clutters,\\npose and appearance variations. Some examples are given\\nin Fig.8.\\n\\nTo apply our method to face alignment, we \\ufb01rst obtain\\nthe label map with the data-driven greedy search algorithm.\\nThen a mean shape as illustrated in Fig.7 (b) is \\ufb01tted to the\\nlabel map by minimizing the Procrustes distance [7]. Note,\\nthe mean shape we used is different from the CBDS\\u2019s. To\\nallow a fair comparison, we thus exclude the bridge of the\\n\\n2485\\n\\n\\x0c1\\n\\n0.9\\n\\n0.8\\n\\n0.7\\n\\n1\\n\\n0.9\\n\\n0.8\\n\\n0.7\\n\\nn\\no\\ni\\nt\\nr\\no\\nP\\ns\\ne\\ng\\na\\nm\\n\\n \\n\\n0.6\\n\\n0.5\\n\\n0.4\\n\\nI\\n\\n0.3\\n\\n0.2\\n\\n0.1\\n\\n0\\n\\n0\\n\\n0.6\\n\\n0.5\\n\\n0.4\\n\\nn\\no\\ni\\nt\\nr\\no\\nP\\ns\\ne\\ng\\na\\nm\\n\\n \\n\\nI\\n\\nBoRMaN\\neASM\\nOurs\\n\\n0.02 0.04 0.06 0.08 0.1 0.12 0.14\\n\\nDistance\\n\\n0.3\\n\\n0.2\\n\\n0.1\\n\\n0\\n\\n0\\n\\nOurs\\nBoRMaN\\n\\n0.02 0.04 0.06 0.08 0.1 0.12 0.14\\n\\nDistance\\n\\nFigure 8. Some samples in the testing dataset of Experiment II.\\n\\nFigure 9. Comparisons of the cumulative error distribution of point-wise\\nerror measured on set A (left) and set B (right) respectively.\\n\\nnose in our shape, and the inner lips and pro\\ufb01les of CBDS\\nare also excluded. To yield a better alignment, we permit\\nsightly non-rigid deformation of each landmark during the\\nmatching procedure. We summarize the results in Table 1,\\nwhich shows the percentage of images with the root mean\\nsquared error (RMSE) less than the speci\\ufb01c thresholds. As\\nwe can see, our algorithm achieves better results than CBD-\\nS. This is because our part-based detectors are very robust\\nto occlusions and pose variations, and the segmentators can\\ndirectly estimate the label map without a separated template\\nmatching process. The results of our method on several par-\\ntially occluded faces are shown in Fig.10 (b).\\n\\nMethods \\\\ RMSE <7 pixels <9.5 pixels <12 pixels\\n99.3%\\n98.1%\\n\\n85.8%\\n76.9%\\n\\n92.5%\\n88.3%\\n\\nCBDS [16]\\n\\nOurs\\n\\nTable 1. Comparisons the alignment results with CBDS [16]. Each col-\\numn is the percentage of images with RMSE less than a threshold.\\n\\nExperiment III: feature point extraction. The goal of\\nour third experiment is to compare our approach to BoR-\\nMaN and eASM with a facial feature points detection task.\\nFig.7 (c) plots the model of 21 feature points we use, which\\nis de\\ufb01ned similar to the BoRMaN method except the point\\nat the facial pro\\ufb01le. Our algorithm can be easily extended to\\nconsider the pro\\ufb01le. Our feature points are extracted on the\\nreconstructed label map, where two eye center points are e-\\nquivalent to the centers of the eye detectors and the other\\nboundary points are achieved by running fast corner detec-\\ntion.\\nIn this experiment, we collect two testing sets and\\ndenote them as A and B. Set A consists of all the original\\n1, 521 images in the BioID database, while set B consists\\nof 200 randomly occluded images selected from set A. We\\nseparate this experiment into two parts: 1) we \\ufb01rst com-\\npare with both BoRMaN and eASM on the set A, and 2) we\\ncompare with BoRMaN on the set B9.\\n\\nPart 1. Both BoRMaN and eASM evaluated their meth-\\nods on the whole BioID database. We therefore can com-\\npare the performance with these two methods on the set A.\\nThe cumulated error distributions of the me17 error mea-\\nsure [23] are illustrated in Fig.9 (left). The me17 measure\\ncomputes the mean error over all internal points, which are\\n\\n9Since the program of eASM is not publicly obtainable, we only com-\\npare with BoRMaN on set B. Its executable program is available at:\\nhttp://ibug.doc.ic.ac.uk/resources/facial-point-detector-2010/.\\n\\nall the points except for the facial pro\\ufb01le and eyelids. Fig.9\\n(left) shows that our approach outperforms these two meth-\\nods.\\n\\nPart 2. We then compare our method with BoRMaN on\\nset B, where the images are partially occluded. The results\\nare shown in Fig.9 (right). When occlusions are present-\\ned, we demonstrate that our approach provides signi\\ufb01cantly\\nbetter results. Some results are plotted in Fig.10 (c).\\n\\nExperiment IV: facial component segmentation. To\\nfurther show that our algorithm can generalize to different\\nfacial modality, we conduct a 7-classes segmentation test on\\n100 face sketch images selected from the CUFSF database.\\nThe de\\ufb01nition of the 7 classes is similar to Experiment I.\\nWe evaluate by computing the accuracy values, which are\\nthe percentage of image pixels assigned to the correct la-\\nbel. Several results are visualized in Fig.10 (d). Our overall\\npixel-wise labeling accuracy is 92.9%, which is better than\\n84.1% of the CBDS method.\\n\\n5. Conclusion\\n\\nIn this paper, we propose a hierarchial face parser,\\nwhere face parsing is achieved by part-based face detec-\\ntors, component-based detectors, and component segmen-\\ntators. For accurate face parsing, we recast segmentation of\\nface components as the cross-modality data transformation\\nproblem, and solve it by a new deep learning strategy, which\\ncan output the label map given an image patch as input. By\\nincorporating the deformable part-based detectors and the\\nsegmentators, our parser is very robust to occlusions, pose\\nvariations, and background clutters. We test our method on\\nseveral applications and demonstrate great improvement.\\n\\n6. Acknowledgement\\n\\nThis work is partially supported by Hong Kong SAR\\nthrough RGC project 416510, and by Guangdong Province\\nthrough Introduced Innovative R&D Team of Guangdong\\nProvince 201001D0104648280.\\n\\nReferences\\n[1] B. Amberg, A. Blake, A. Fitzgibbon, S. Romdhani, and T. Vetter.\\nReconstructing high quality face-surfaces using model based stereo.\\nICCV, 2007.\\n\\n2486\\n\\n\\x0cFigure 10. Demonstration of our results. The parsing results of Experiment I are shown in (a), where the 1st column is the original images, and the\\n2nd, 3rd columns are the transformed label maps and the ground truth respectively. (b) is the aligned faces of Experiment II. (c) visualizes some results of\\nfacial feature extraction on the partially occluded set B. (d) is the segmentation results on face sketches.\\n\\n[2] T. Cootes, G. Edwards, and C. Taylor. Active appearance models.\\n\\nECCV, 1998.\\n\\n[3] T. Cootes, C. Taylor, and J. Graham. Active shape models their train-\\ning and application. Computer Vision and Image Understanding,\\n1995.\\n\\n[4] N. Cristianini and J. Shawe-Taylor. An introduction to support vec-\\ntor machines: and other kernel-based learning methods. Cambridge\\nUniversity Press, 2000.\\n\\n[5] D. Cristinacce and T. Cootes. Boosted regression active shape mod-\\n\\nels. BMVC, 2007.\\n\\n[6] N. Dalal and B. Triggs. Histograms of oriented gradients for human\\n\\ndetection. CVPR, 2005.\\n\\n[7] I. Dryden and K. Mardia. Statistical shape analysis. John Wiley and\\n\\nSon, Chichester, 1998.\\n\\n[8] P. Felzenszwalb, R. Girshick, and D. McAllester. Cascade object\\n\\ndetection with deformable part models. CVPR, 2010.\\n\\n[9] P. Felzenszwalb and D. Huttenlocher. Pictorial structures for object\\n\\nrecognition. International Journal of Computer Vision, 2005.\\n\\n[10] G. Hinton, S. Osindero, and Y. Teh. A fast learning algorithm for\\n\\ndeep belief nets. Neural Computation, 2006.\\n\\n[11] G. Hinton and R. Salakhutdinov. Reducing the dimensionality of\\n\\ndata with neural networks. Science, 2006.\\n\\n[12] G. Huang, M. Ramesh, T. Berg, and E. Miller. Labeled faces in\\nthe wild: A database for studying face recognition in unconstrained\\nenvironments. Technical Report, 2007.\\n\\n[13] O. Jesorsky, K. Kirchberg, and R. Frischholz. Robust face detection\\nusing the hausdorff distance. Lecture Notes in Computer Science,\\n2001.\\n\\n[14] L. Liang, F. Wen, X. Tang, and Y. Xu. An integrated model for\\n\\naccurate shape alignment. ECCV, 2006.\\n\\n[15] L. Liang, F. Wen, Y. Xu, X. Tang, and H. Shum. Accurate face\\n\\nalignment using shape constrained markov network. CVPR, 2006.\\n\\n[16] L. Liang, R. Xiao, F. Wen, and J. Sun.\\n\\nFace alignment via\\n\\ncomponent-based discriminative search. ECCV, 2008.\\n\\n[17] S. Milborrow and F. Nicolls. Locating facial features with an extend-\\n\\ned active shape model. ECCV, 2008.\\n\\n[18] J. Ngiam, A. Khosla, M. Kim, J. Nam, H. Lee, and A. Y. Ng. Multi-\\n\\nmodal deep learning. ICML, 2011.\\n\\n[19] X. Tang and X. Wang. Face sketch synthesis and recognition. ICCV,\\n\\n2003.\\n\\n[20] X. Tang and X. Wang. Face sketch recognition. IEEE Transactions\\n\\non Circuits and Systems for Video Technology, 2004.\\n\\n[21] J. Tu, Z. Zhang, Z. Zeng, and T. Huang. Face localization via hier-\\narchical condensation with \\ufb01sher boosting feature selection. CVPR,\\n2004.\\n\\n[22] Z. Tu, X. Chen, A. Yuille, and S. Zhu.\\n\\nImage parsing: unifying\\n\\nsegmentation, detection and recognition. IJCV, 2005.\\n\\n[23] M. Valstar, B. Martinez, X. Binefa, and M. Pantic. Facial point de-\\n\\ntection using boosted regression and graph models. CVPR, 2010.\\n\\n[24] P. Vincent, H. Larochelle, I. Lajoie, Y. Bengio, and P. Manzagol.\\nStacked denoising autoencoders: Learning useful representations in\\na deep network with a local denoising criterion. Journal of Machine\\nLearning Research, 2010.\\n\\n[25] P. Viola and M. Jones. Robust real-time face detection. IJCV, 2004.\\n[26] X. Wang and X. Tang. Face photo-sketch synthesis and recognition.\\n\\nTPAMI, 2009.\\n\\n[27] J. Warrell and S. Prince. Labelfaces: Parsing facial features by mul-\\n\\nticlass labeling with an epitome prior. ICIP, 2009.\\n\\n[28] H. Wu, X. Liu, and G. Doretto. Face alignment via boosted ranking\\n\\nmodel. CVPR, 2008.\\n\\n[29] W. Zhang, X. Wang, and X. Tang. Coupled information-theoretic\\n\\nencoding for face photo-sketch recognition. CVPR, 2011.\\n\\n[30] Y. Zhou, W. Zhang, X. Tang, and H. Shum. A bayesian mixture\\n\\nmodel for multi-view face alignment. CVPR, 2005.\\n\\n2487\\n\\n\\x0c', u'Multimodal Compact Bilinear Pooling\\n\\nfor Visual Question Answering and Visual Grounding\\n\\nAkira Fukui*1,2\\nAnna Rohrbach*1,3\\n\\nDong Huk Park*1\\nTrevor Darrell1 Marcus Rohrbach1\\n\\nDaylen Yang*1\\n\\n1UC Berkeley EECS, CA, United States\\n\\n2Sony Corp., Tokyo, Japan\\n\\n3Max Planck Institute for Informatics, Saarbr\\xa8ucken, Germany\\n\\n6\\n1\\n0\\n2\\n\\n \\n\\np\\ne\\nS\\n4\\n2\\n\\n \\n\\n \\n \\n]\\n\\nV\\nC\\n.\\ns\\nc\\n[\\n \\n \\n\\n3\\nv\\n7\\n4\\n8\\n1\\n0\\n\\n.\\n\\n6\\n0\\n6\\n1\\n:\\nv\\ni\\nX\\nr\\na\\n\\nAbstract\\n\\nModeling textual or visual information with\\nvector representations trained from large lan-\\nguage or visual datasets has been successfully\\nexplored in recent years. However, tasks such\\nas visual question answering require combin-\\ning these vector representations with each other.\\nApproaches to multimodal pooling include\\nelement-wise product or sum, as well as con-\\ncatenation of the visual and textual represen-\\ntations. We hypothesize that these methods\\nare not as expressive as an outer product of\\nthe visual and textual vectors. As the outer\\nproduct is typically infeasible due to its high\\ndimensionality, we instead propose utilizing\\nMultimodal Compact Bilinear pooling (MCB)\\nto ef\\ufb01ciently and expressively combine multi-\\nmodal features. We extensively evaluate MCB\\non the visual question answering and ground-\\ning tasks. We consistently show the bene\\ufb01t of\\nMCB over ablations without MCB. For visual\\nquestion answering, we present an architec-\\nture which uses MCB twice, once for predict-\\ning attention over spatial features and again\\nto combine the attended representation with\\nthe question representation. This model out-\\nperforms the state-of-the-art on the Visual7W\\ndataset and the VQA challenge.\\n\\nIntroduction\\n\\n1\\nRepresentation learning for text and images has been\\nextensively studied in recent years. Recurrent neural\\nnetworks (RNNs) are often used to represent sen-\\ntences or phrases (Sutskever et al., 2014; Kiros et al.,\\n\\n* indicates equal contribution\\n\\nFigure 1: Multimodal Compact Bilinear Pooling for\\nvisual question answering.\\n\\n2015), and convolutional neural networks (CNNs)\\nhave shown to work best to represent images (Don-\\nahue et al., 2013; He et al., 2015). For tasks such as\\nvisual question answering (VQA) and visual ground-\\ning, most approaches require joining the represen-\\ntation of both modalities. For combining the two\\nvector representations (multimodal pooling), current\\napproaches in VQA or grounding rely on concatenat-\\ning vectors or applying element-wise sum or product.\\nWhile this generates a joint representation, it might\\nnot be expressive enough to fully capture the complex\\nassociations between the two different modalities.\\n\\nIn this paper, we propose to rely on Multimodal\\nCompact Bilinear pooling (MCB) to get a joint repre-\\nsentation. Bilinear pooling computes the outer prod-\\nuct between two vectors, which allows, in contrast\\nto element-wise product, a multiplicative interaction\\nbetween all elements of both vectors. Bilinear pool-\\ning models (Tenenbaum and Freeman, 2000) have\\nrecently been shown to be bene\\ufb01cial for \\ufb01ne-grained\\nclassi\\ufb01cation for vision only tasks (Lin et al., 2015).\\nHowever, given their high dimensionality (n2), bi-\\nlinear pooling has so far not been widely used. In\\n\\nCNN WE, LSTM What are all the people doing? \\u03a8 \\u03a8 FFT FFT-1 Convolution \\u2a00 FFT \\u201cflying kites\\u201d Classifier Multimodal Compact Bilinear Count Sketch Signed Sqrt L2 Norm \\x0cthis paper, we adopt the idea from Gao et al. (2016)\\nwhich shows how to ef\\ufb01ciently compress bilinear\\npooling for a single modality. In this work, we dis-\\ncuss and extensively evaluate the extension to the\\nmultimodal case for text and visual modalities. As\\nshown in Figure 1, Multimodal Compact Bilinear\\npooling (MCB) is approximated by randomly pro-\\njecting the image and text representations to a higher\\ndimensional space (using Count Sketch (Charikar\\net al., 2002)) and then convolving both vectors ef\\ufb01-\\nciently by using element-wise product in Fast Fourier\\nTransform (FFT) space. We use MCB to predict an-\\nswers for the VQA task and locations for the visual\\ngrounding task. For open-ended question answering,\\nwe present an architecture for VQA which uses MCB\\ntwice, once to predict spatial attention and the second\\ntime to predict the answer. For multiple-choice ques-\\ntion answering we introduce a third MCB to relate the\\nencoded answer to the question-image space. Addi-\\ntionally, we discuss the bene\\ufb01t of attention maps and\\nadditional training data for the VQA task. To sum-\\nmarize, MCB is evaluated on two tasks, four datasets,\\nand with a diverse set of ablations and comparisons\\nto the state-of-the-art.\\n\\n2 Related Work\\n\\nMultimodal pooling. Current approaches to mul-\\ntimodal pooling involve element-wise operations or\\nvector concatenation. In the visual question answer-\\ning domain, a number of models have been proposed.\\nSimpler models such as iBOWIMG baseline (Zhou\\net al., 2015) use concatenation and fully connected\\nlayers to combine the image and question modali-\\nties. Stacked Attention Networks (Yang et al., 2015)\\nand Spatial Memory Networks (Xu et al., 2015) use\\nLSTMs or extract soft-attention on the image fea-\\ntures, but ultimately use element-wise product or\\nelement-wise sum to merge modalities. D-NMN (An-\\ndreas et al., 2016a) introduced REINFORCE to dy-\\nnamically create a network and use element-wise\\nproduct to join attentions and element-wise sum pre-\\ndict answers. Dynamic Memory Networks (DMN)\\n(Xiong et al., 2016) pool the image and question\\nwith element-wise product and sum, attending to part\\nof the image and question with an Episodic Mem-\\nory Module (Kumar et al., 2016). DPPnet (Noh et\\nal., 2015) creates a Parameter Prediction Network\\n\\nwhich learns to predict the parameters of the second\\nto last visual recognition layer dynamically from the\\nquestion. Similar to this work, DPPnet allows mul-\\ntiplicative interactions between the visual and ques-\\ntion encodings. Lu et al. (2016) recently proposed\\na model that extracts multiple co-attentions on the\\nimage and question and combines the co-attentions\\nin a hierarchical manner using element-wise sum,\\nconcatenation, and fully connected layers.\\n\\nFor the visual grounding task, Rohrbach et al.\\n(2016) propose an approach where the language\\nphrase embedding is concatenated with the visual\\nfeatures in order to predict the attention weights over\\nmultiple bounding box proposals. Similarly, Hu et\\nal. (2016a) concatenate phrase embeddings with vi-\\nsual features at different spatial locations to obtain a\\nsegmentation.\\n\\nBilinear pooling. Bilinear pooling has been ap-\\nplied to the \\ufb01ne-grained visual recognition task. Lin\\net al. (2015) use two CNNs to extract features from\\nan image and combine the resulting vectors using an\\nouter product, which is fully connected to an output\\nlayer. Gao et al. (2016) address the space and time\\ncomplexity of bilinear features by viewing the bilin-\\near transformation as a polynomial kernel. Pham and\\nPagh (2013) describe a method to approximate the\\npolynomial kernel using Count Sketches and convo-\\nlutions.\\n\\nJoint multimodal embeddings.\\nIn order to model\\nsimilarities between two modalities, many prior\\nworks have learned joint multimodal spaces, or em-\\nbeddings. Some of such embeddings are based\\non Canonical Correlation Analysis (Hardoon et al.,\\n2004) e.g. (Gong et al., 2014; Klein et al., 2015;\\nPlummer et al., 2015), linear models with ranking\\nloss (Frome et al., 2013; Karpathy and Fei-Fei, 2015;\\nSocher et al., 2014; Weston et al., 2011) or non-linear\\ndeep learning models (Kiros et al., 2014; Mao et al.,\\n2015; Ngiam et al., 2011). Our multimodal com-\\npact bilinear pooling can be seen as a complementary\\noperation that allows us to capture different interac-\\ntions between two modalities more expressively than\\ne.g. concatenation. Consequently, many embedding\\nlearning approaches could bene\\ufb01t from incorporating\\nsuch interactions.\\n\\n\\x0cAlgorithm 1 Multimodal Compact Bilinear\\n1: input: v1 \\u2208 Rn1 , v2 \\u2208 Rn2\\n2: output: \\u03a6(v1, v2) \\u2208 Rd\\n3: procedure MCB(v1, v2, n1, n2, d)\\nfor k \\u2190 1 . . . 2 do\\n4:\\n5:\\n6:\\n7:\\n8:\\n9:\\n10:\\n11:\\n12: procedure \\u03a8(v, h, s, n)\\n13:\\ny = [0, . . . , 0]\\nfor i \\u2190 1 . . . n do\\n14:\\n15:\\n16:\\n\\nsample hk[i] from {1, . . . , d}\\nsample sk[i] from {\\u22121, 1}\\n\\n\\u03a6 = FFT\\u22121(FFT(v(cid:48)\\nreturn \\u03a6\\n\\ny[h[i]] = y[h[i]] + s[i] \\xb7 v[i]\\n\\nv(cid:48)\\nk = \\u03a8(vk, hk, sk, nk)\\n\\nreturn y\\n\\nif hk, sk not initialized then\\n\\nfor i \\u2190 1 . . . nk do\\n\\n1) (cid:12) FFT(v(cid:48)\\n2))\\n\\nway. However, the high dimensional representation\\n(i.e. when n1 and n2 are large) leads to an infeasible\\nnumber of parameters to learn in W . For example,\\nwe use n1 = n2 = 2048 and z \\u2208 R3000 for VQA.\\nW thus would have 12.5 billion parameters, which\\nleads to very high memory consumption and high\\ncomputation times.\\n\\nWe thus need a method that projects the outer prod-\\nuct to a lower dimensional space and also avoids\\ncomputing the outer product directly. As suggested\\nby Gao et al. (2016) for a single modality, we rely\\non the Count Sketch projection function \\u03a8 (Charikar\\net al., 2002), which projects a vector v \\u2208 Rn to\\ny \\u2208 Rd. We initialize two vectors s \\u2208 {\\u22121, 1}n and\\nh \\u2208 {1, ..., d}n: s contains either 1 or \\u22121 for each\\nindex, and h maps each index i in the input v to an\\nindex j in the output y. Both s and h are initialized\\nrandomly from a uniform distribution and remain\\nconstant for future invocations of count sketch. y is\\ninitialized as a zero vector. For every element v[i] its\\ndestination index j = h[i] is looked up using h, and\\ns[i] \\xb7 v[i] is added to y[j]. See lines 1-9 and 12-16 in\\nAlgorithm 1.\\n\\nThis allows us to project the outer product to a\\nlower dimensional space, which reduces the number\\nof parameters in W . To avoid computing the outer\\nproduct explicitly, Pham and Pagh (2013) showed\\nthat the count sketch of the outer product of two\\nvectors can be expressed as convolution of both count\\nsketches: \\u03a8(x \\u2297 q, h, s) = \\u03a8(x, h, s) \\u2217 \\u03a8(q, h, s),\\n\\nFigure 2: Multimodal Compact Bilinear Pooling\\n(MCB)\\n\\n3 Multimodal Compact Bilinear Pooling\\n\\nfor Visual and Textual Embeddings\\n\\nFor the task of visual question answering (VQA) or\\nvisual grounding, we have to predict the most likely\\nanswer or location \\u02c6a for a given image x and question\\nor phrase q. This can be formulated as\\np(a|x, q; \\u03b8)\\n\\n\\u02c6a = argmax\\n\\n(1)\\n\\na\\u2208A\\n\\nwith parameters \\u03b8 and the set of answers or loca-\\ntions A. For an image embedding x = \\u039e(x) (i.e. a\\nCNN) and question embedding q = \\u2126(q) (i.e. an\\nLSTM), we are interested in getting a good joint rep-\\nresentation by pooling both representations. With a\\nmultimodal pooling \\u03a6(x, q) that encodes the relation-\\nship between x and q well, it becomes easier to learn\\na classi\\ufb01er for Equation (1).\\n\\nIn this section, we \\ufb01rst discuss our multimodal\\npooling \\u03a6 for combining representations from differ-\\nent modalities into a single representation (Sec. 3.1)\\nand then detail our architectures for VQA (Sec. 3.2)\\nand visual grounding (Sec. 3.3), further explaining\\nhow we predict \\u02c6a with the given image representation\\n\\u039e and text representation \\u2126.\\n\\n3.1 Multimodal Compact Bilinear\\n\\nPooling (MCB)\\n\\nBilinear models (Tenenbaum and Freeman, 2000)\\ntake the outer product of two vectors x \\u2208 Rn1 and\\nq \\u2208 Rn2 and learn a model W (here linear), i.e.\\nz = W [x \\u2297 q], where \\u2297 denotes the outer product\\n(xqT ) and [ ] denotes linearizing the matrix in a vec-\\ntor. As discussed in the introduction, bilinear pooling\\nis interesting because it allows all elements of both\\nvectors to interact with each other in a multiplicative\\n\\n0 -xn1 ... 0 -x1 00 x2 -q2  0 ... q4  0 0qn2 q9 q1 q2 ... qn2 x1 x2 ... xn1 Visual VectorTextual Vector\\u03a8\\u03a8Count Sketch of Visual VectorCount Sketch of Textual VectorFFTFFTFFT-1Convolution\\x0cFigure 3: Our architecture for VQA: Multimodal Compact Bilinear (MCB) with Attention. Conv implies\\nconvolutional layers and FC implies fully connected layers. For details see Sec. 3.2.\\n\\nwhere \\u2217 is the convolution operator. Additionally, the\\nconvolution theorem states that convolution in the\\ntime domain is equivalent to element-wise product\\nin the frequency domain. The convolution x(cid:48) \\u2217 q(cid:48) can\\nbe rewritten as FFT\\u22121(FFT(x(cid:48)) (cid:12) FFT(q(cid:48))), where\\n(cid:12) refers to element-wise product. These ideas are\\nsummarized in Figure 2 and formalized in Algorithm\\n1, which is based on the Tensor Sketch algorithm of\\nPham and Pagh (2013). We invoke the algorithm with\\nv1 = x and v2 = q. We note that this easily extends\\nand remains ef\\ufb01cient for more than two multi-modal\\ninputs as the combination happens as element-wise\\nproduct.\\n\\n3.2 Architectures for VQA\\nIn VQA, the input to the model is an image and a\\nquestion, and the goal is to answer the question. Our\\nmodel extracts representations for the image and the\\nquestion, pools the vectors using MCB, and arrives\\nat the answer by treating the problem as a multi-class\\nclassi\\ufb01cation problem with 3,000 possible classes.\\n\\nWe extract image features using a 152-layer Resid-\\nual Network (He et al., 2015) that is pretrained on\\nImageNet data (Deng et al., 2009). Images are re-\\nsized to 448\\xd7 448, and we use the output of the layer\\n(\\u201cpool5\\u201d) before the 1000-way classi\\ufb01er. We then\\nperform L2 normalization on the 2048-D vector.\\n\\nInput questions are \\ufb01rst tokenized into words, and\\nthe words are one-hot encoded and passed through\\na learned embedding layer. The tanh nonlinearity\\nis used after the embedding. The embedding layer\\nis followed by a 2-layer LSTM with 1024 units in\\neach layer. The outputs of each LSTM layer are\\nconcatenated to form a 2048-D vector.\\n\\nThe two vectors are then passed through MCB.\\nThe MCB is followed by an element-wise signed\\nsquare-root and L2 normalization. After MCB pool-\\ning, a fully connected layer connects the resulting\\n16,000-D multimodal representation to the 3,000 top\\nanswers.\\nAttention. To incorporate spatial information, we\\nuse soft attention on our MCB pooling method. Ex-\\nplored by (Xu et al., 2015) for image captioning and\\nby (Xu and Saenko, 2016) and (Yang et al., 2015)\\nfor VQA, the soft attention mechanism can be easily\\nintegrated in our model.\\n\\nFor each spatial grid location in the visual rep-\\nresentation (i.e. last convolutional layer of ResNet\\n[res5c], last convolutional layer of VGG [conv5]),\\nwe use MCB pooling to merge the slice of the visual\\nfeature with the language representation. As depicted\\nin Figure 3, after the pooling we use two convolu-\\ntional layers to predict the attention weight for each\\ngrid location. We apply softmax to produce a nor-\\nmalized soft attention map. We then take a weighted\\nsum of the spatial vectors using the attention map to\\ncreate the attended visual representation. We also ex-\\nperiment with generating multiple attention maps to\\nallow the model to make multiple \\u201cglimpses\\u201d which\\nare concatenated before being merged with the lan-\\nguage representation through another MCB pooling\\nfor prediction. Predicting attention maps with MCB\\npooling allows the model to effectively learn how to\\nattend to salient locations based on both the visual\\nand language representations.\\nAnswer Encoding. For VQA with multiple\\nchoices, we can additionally embed the answers. We\\n\\n1 x 14 x 14 512 x 14 x 14 CNN (ResNet152) 16k x14x14 2048x14x14 2048x14x14 Conv, Relu Conv \\u201cCarrot\\u201d 16k 3000 2048 2048 WE, LSTM Softmax Weighted Sum Tile 2048 What is the woman feeding the giraffe? Multimodal Compact Bilinear Multimodal Compact Bilinear FC Softmax \\x0cFigure 4: Our architecture for VQA: MCB with At-\\ntention and Answer Encoding\\n\\nFigure 5: Our Architecture for Grounding with MCB\\n(Sec. 3.3)\\n\\nbase our approach on the proposed MCB with atten-\\ntion. As can be seen from Figure 4, to deal with\\nmultiple variable-length answer choices, each choice\\nis encoded using a word embedding and LSTM lay-\\ners whose weights are shared across the candidates.\\nIn addition to using MCB with attention, we use an\\nadditional MCB pooling to merge the encoded an-\\nswer choices with the multimodal representation of\\nthe original pipeline. The resulting embedding is\\nprojected to a classi\\ufb01cation vector with a dimension\\nequal to the number of answers.\\n\\n3.3 Architecture for Visual Grounding\\n\\nWe base our grounding approach on the fully-\\nsupervised version of GroundeR (Rohrbach et al.,\\n2016). The overview of our model is shown in Fig-\\nure 5. The input to the model is a query natural\\nlanguage phrase and an image along with multiple\\nproposal bounding boxes. The goal is to predict a\\nbounding box which corresponds to the query phrase.\\nWe replace the concatenation of the visual representa-\\ntion and the encoded phrase in GroundeR with MCB\\nto combine both modalities. In contrast to Rohrbach\\net al. (2016), we include a linear embedding of the\\nvisual representation and L2 normalization of both in-\\nput modalities, instead of batch normalization (Ioffe\\nand Szegedy, 2015), which we found to be bene\\ufb01cial\\nwhen using MCB for the grounding task.\\n\\n4 Evaluation on Visual Question\\n\\nAnswering\\n\\nWe evaluate the bene\\ufb01t of MCB with a diverse set of\\nablations on two visual question answering datasets.\\n\\n4.1 Datasets\\nThe Visual Question Answering (VQA) real-image\\ndataset (Antol et al., 2015) consists of approximately\\n200,000 MSCOCO images (Lin et al., 2014), with\\n3 questions per image and 10 answers per question.\\nThere are 3 data splits: train (80K images), validation\\n(40K images), and test (80K images). Additionally,\\nthere is a 25% subset of test named test-dev. Ac-\\ncuracies for ablation experiments in this paper are\\nreported on the test-dev data split. We use the VQA\\ntool provided by Antol et al. (2015) for evaluation.\\nWe conducted most of our experiments on the open-\\nended real-image task. In Table 4, we also report our\\nmultiple-choice real-image scores.\\n\\nThe Visual Genome dataset (Krishna et al.,\\n2016) uses 108,249 images from the intersection of\\nYFCC100M (Thomee et al., 2015) and MSCOCO.\\nFor each image, an average of 17 question-answer\\npairs are collected. There are 1.7 million QA pairs\\nof the 6W question types (what, where, when, who,\\nwhy, and how). Compared to the VQA dataset, Vi-\\nsual Genome represents a more balanced distribu-\\ntion of the 6W question types. Moreover, the aver-\\nage question and answer lengths for Visual Genome\\nare larger than the VQA dataset. To leverage the\\nVisual Genome dataset as additional training data,\\nwe remove all the unnecessary words such as \\u201da\\u201d,\\n\\u201dthe\\u201d, and \\u201dit is\\u201d from the answers to decrease the\\nlength of the answers and extract QA pairs whose\\nanswers are single-worded. The extracted data is \\ufb01l-\\ntered again based on the answer vocabulary space\\ncreated from the VQA dataset, leaving us with addi-\\ntional 1M image-QA triplets.\\n\\nThe Visual7W dataset (Zhu et al., 2016) is a part\\nof the Visual Genome. Visual7W adds a 7th which\\nquestion category to accommodate visual answers,\\n\\nQ  : \\u201cWhat do you see?\\u201d  (Ground Truth : a3) a1 : \\u201cA courtyard with flowers\\u201d a2 : \\u201cA restaurant kitchen\\u201d a3 : \\u201cA family with a stroller, tables for dining\\u201d a4 : \\u201cPeople waiting on a train\\u201d a1 a2 a3 a4 Attention  I MCB Q a2 encoded a3 encoded a4 encoded Conv a3 Multimodal Compact Bilinear a1 encoded WE LSTM Tile Relu Conv Softmax Q: \\u201cPerson in blue checkered shirt\\u201db1b2b3b4QConvb4MultimodalCompactBilinearTileReluConvSoftmaxCNNCNNCNNCNNb3b2b1b3WELSTML2 normConvL2 normConvL2 normConvL2 normConvL2 norm\\x0cMethod\\nElement-wise Sum\\nConcatenation\\nConcatenation + FC\\nConcatenation + FC + FC\\nElement-wise Product\\nElement-wise Product + FC\\nElement-wise Product + FC + FC\\nMCB (2048 \\xd7 2048 \\u2192 16K)\\nFull Bilinear (128 \\xd7 128 \\u2192 16K)\\nMCB (128 \\xd7 128 \\u2192 4K)\\nElement-wise Product with VGG-19\\nMCB (d = 16K) with VGG-19\\nConcatenation + FC with Attention\\nMCB (d = 16K) with Attention\\n\\nAccuracy\\n\\n56.50\\n57.49\\n58.40\\n57.10\\n58.57\\n56.44\\n57.88\\n59.83\\n58.46\\n58.69\\n55.97\\n57.05\\n58.36\\n62.50\\n\\nTable 1: Comparison of multimodal pooling methods.\\nModels are trained on the VQA train split and tested\\non test-dev.\\n\\nCompact Bilinear d Accuracy\\n1024\\n2048\\n4096\\n8192\\n16000\\n32000\\n\\n58.38\\n58.80\\n59.42\\n59.69\\n59.83\\n59.71\\n\\nTable 2: Accuracies for different values of d, the\\ndimension of the compact bilinear feature. Models\\nare trained on the VQA train split and tested on test-\\ndev. Details in Sec. 4.3.\\n\\nMethod\\nZhu et al.\\nConcat+Att.\\nMCB+Att.\\n\\nWhat Where When Who Why How Avg\\n51.5 57.0\\n75.0 59.5 55.5 49.8 54.3\\n74.1 62.3 52.7 51.2 52.8\\n47.8 56.9\\n60.3 70.4\\n79.5 69.2 58.2 51.1 62.2\\n\\nTable 3: Multiple-choice QA tasks accuracy (%) on\\nVisual7W test set.\\n\\nbut we only evaluate the models on the Telling task\\nwhich involves 6W questions. The natural language\\nanswers in Visual7W are in a multiple-choice format\\nand each question comes with four answer candidates,\\nwith only one being the correct answer. Visual7W\\nis composed of 47,300 images from MSCOCO and\\nthere are a total of 139,868 QA pairs.\\n\\n4.2 Experimental Setup\\nWe use the Adam solver with \\x01 = 0.0007, \\u03b21 = 0.9,\\n\\u03b22 = 0.999. We use dropout after the LSTM layers\\nand in fully connected layers. For the experiments in\\nTable 1 and 2, we train on the VQA train split, vali-\\ndate on the VQA validation split, and report results\\non the VQA test-dev split. We use early stopping: if\\nthe validation score does not improve for 50,000 iter-\\nations, we stop training and evaluate the best iteration\\non test-dev.\\n\\nFor the Visual7W task, we use the same hyperpa-\\nrameters and training settings as in the VQA exper-\\niments. We use the splits from (Zhu et al., 2016) to\\ntrain, validate, and test our models. We also compute\\naccuracies on this data using their evaluation code.\\nFor VQA multiple choice, we train the open-ended\\nmodels and take the argmax over the multiple choice\\n\\nanswers at test time. For Visual7W, we use the an-\\nswer encoding as described in Sec. 3.2.\\n\\n4.3 Ablation Results\\nWe compare the performance of non-bilinear and\\nbilinear pooling methods in Table 1. We see that\\nMCB pooling outperforms all non-bilinear pooling\\nmethods, such as eltwise sum, concatenation, and\\neltwise product.\\n\\nOne could argue that the compact bilinear method\\nsimply has more parameters than the non-bilinear\\npooling methods, which contributes to its perfor-\\nmance. We compensated for this by stacking fully\\nconnected layers (with 4096 units per layer, ReLU\\nactivation, and dropout) after the non-bilinear pool-\\ning methods to increase their number of parameters.\\nHowever, even with similar parameter budgets, non-\\nbilinear methods could not achieve the same accuracy\\nas the MCB method. For example, the \\u201cConcatena-\\ntion + FC + FC\\u201d pooling method has approximately\\n40962 + 40962 + 4096 \\xd7 3000 \\u2248 46 million pa-\\nrameters, which matches the 48 million parameters\\navailable in MCB with d = 16000. However, the per-\\nformance of the \\u201cConcatenation + FC + FC\\u201d method\\nis only 57.10% compared to MCB\\u2019s 59.83%.\\n\\nSection 2 in Table 1 also shows that compact bi-\\n\\n\\x0cTest-dev\\n\\nOpen Ended\\n\\nY/N No. Other All\\n60.8\\n81.2\\n62.3\\n81.7\\n64.2\\n82.2\\n82.5\\n64.7\\n65.1\\n81.7\\n65.4\\n82.3\\n83.4\\n66.7\\n64.9\\n83.5\\n61.8\\n79.7\\n60.3\\n80.5\\n81.1\\n59.2\\n59.4\\n81.1\\n59.2\\n81.0\\n58.7\\n79.3\\n81.2\\n58.6\\n58.4\\n78.4\\n58.0\\n80.9\\n80.5\\n57.8\\n57.2\\n80.7\\n76.5\\n55.7\\n\\n49.3\\n51.5\\n54.8\\n55.6\\n57.0\\n57.4\\n58.5\\n54.8\\n51.7\\n48.3\\n45.8\\n45.5\\n45.2\\n46.1\\n44.0\\n46.3\\n43.1\\n43.1\\n41.7\\n42.6\\n\\n35.1\\n36.6\\n37.7\\n37.6\\n38.2\\n37.2\\n39.8\\n39.8\\n38.7\\n36.8\\n36.2\\n38.6\\n38.4\\n36.6\\n38.0\\n36.4\\n37.3\\n36.8\\n37.2\\n35.0\\n\\nMCB\\nMCB + Genome\\nMCB + Att.\\nMCB + Att. + GloVe\\nMCB + Att. + Genome\\nMCB + Att. + GloVe + Genome\\nEnsemble of 7 Att. models\\nNaver Labs (challenge 2nd)\\nHieCoAtt (Lu et al., 2016)\\nDMN+ (Xiong et al., 2016)\\nFDA (Ilievski et al., 2016)\\nD-NMN (Andreas et al., 2016a)\\nAMA (Wu et al., 2016)\\nSAN (Yang et al., 2015)\\nNMN (Andreas et al., 2016b)\\nAYN (Malinowski et al., 2016)\\nSMem (Xu and Saenko, 2016)\\nVQA team (Antol et al., 2015)\\nDPPnet (Noh et al., 2015)\\niBOWIMG (Zhou et al., 2015)\\n\\nTest-standard\\n\\nOpen Ended\\n\\nMC\\nAll Y/N No. Other All\\n-\\n65.4\\n-\\n66.4\\n-\\n68.6\\n69.1\\n-\\n-\\n69.5\\n69.9\\n-\\n70.2\\n69.4\\n65.8\\n\\n58.0\\n54.6\\n\\n39.5\\n38.7\\n\\n83.2\\n83.3\\n\\n-\\n-\\n-\\n-\\n-\\n-\\n\\n-\\n-\\n-\\n-\\n\\n-\\n-\\n-\\n-\\n-\\n-\\n\\n-\\n-\\n-\\n-\\n\\n-\\n-\\n-\\n-\\n-\\n-\\n\\n-\\n-\\n-\\n-\\n\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n\\n62.7\\n\\n-\\n-\\n\\n81.1\\n\\n37.1\\n\\n45.8\\n\\n-\\n\\n81.2\\n78.2\\n80.9\\n80.6\\n80.3\\n76.8\\n\\n-\\n\\n37.7\\n36.3\\n37.5\\n36.5\\n36.9\\n35.0\\n\\n-\\n\\n44.0\\n46.3\\n43.5\\n43.7\\n42.2\\n42.6\\n\\nMC\\nAll\\n-\\n-\\n-\\n-\\n-\\n-\\n\\n70.1\\n69.3\\n66.1\\n\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n\\n63.1\\n\\n-\\n\\n62.0\\n\\n66.5\\n64.8\\n62.1\\n60.4\\n59.5\\n59.4\\n59.4\\n58.9\\n58.7\\n58.4\\n58.2\\n58.2\\n57.4\\n55.9\\n\\nTable 4: Open-ended and multiple-choice (MC) results on VQA test set (trained on train+val set) compared\\nwith state-of-the-art: accuracy in %. See Sec. 4.4.\\n\\nlinear pooling has no impact on accuracy compared\\nto full bilinear pooling. Section 3 in Table 1 demon-\\nstrates that the MCB brings improvements regardless\\nof the image CNN used. We primarily use ResNet-\\n152 in this paper, but MCB also improves perfor-\\nmance if VGG-19 is used. Section 4 in Table 1 shows\\nthat our soft attention model works best with MCB\\npooling. In fact, attending to the Concatenation + FC\\nlayer has the same performance as not using attention\\nat all, while attending to the MCB layer improves\\nperformance by 2.67 points.\\n\\nTable 2 compares different values of d, the output\\ndimensionality of the multimodal compact bilinear\\nfeature. Approximating the bilinear feature with a\\n16,000-D vector yields the highest accuracy.\\n\\nWe also evaluated models with multiple atten-\\ntion maps or channels. One attenion map achieves\\n64.67%, two 65.08% and four 64.24% accuracy\\n(trained on train+val). Visual inspection of the gen-\\n\\nerated attention maps reveals that an ensembling or\\nsmoothing effect occurs when using multiple maps.\\nTable 3 presents results for the Visual7W multiple-\\nchoice QA task. The MCB with attention model out-\\nperforms the previous state-of-the-art by 7.9 points\\noverall and performs better in almost every category.\\n\\n4.4 Comparison to State-of-the-Art\\nTable 4 compares our approach with the state-of-the-\\nart on VQA test set. Our best single model uses\\nMCB pooling with two attention maps. Additionally,\\nwe augment our training data with images and QA\\npairs from the Visual Genome dataset. We also con-\\ncatenate the learned word embedding with pretrained\\nGloVe vectors (Pennington et al., 2014).\\n\\nEach model in our ensemble of 7 models uses\\nMCB with attention. Some of the models were\\ntrained with data from Visual Genome, and some\\nwere trained with two attention maps. This ensem-\\n\\n\\x0cMethod\\nPlummer et al. (2015)\\nHu et al. (2016b)\\nPlummer et al. (2016)1\\nWang et al. (2016)\\nRohrbach et al. (2016)\\nConcatenation\\nElement-wise Product\\nElement-wise Product + Conv\\nMCB\\n\\nAccuracy, %\\n\\n27.42\\n27.80\\n43.84\\n43.89\\n47.81\\n46.50\\n47.41\\n47.86\\n48.69\\n\\nTable 5: Grounding accuracy on Flickr30k Entities\\ndataset.\\n\\nMethod\\nHu et al. (2016b)\\nRohrbach et al. (2016)\\nConcatenation\\nElement-wise Product\\nElement-wise Product + Conv\\nMCB\\n\\nAccuracy, %\\n\\n17.93\\n26.93\\n25.48\\n27.80\\n27.98\\n28.91\\n\\nTable 6: Grounding accuracy on ReferItGame\\ndataset.\\n\\nble is 1.8 points above the next best approach on the\\nVQA open-ended task and 0.8 points above the next\\nbest approach on the multiple-choice task (on Test-\\ndev). Even without ensembles, our \\u201cMCB + Genome\\n+ Att. + GloVe\\u201d model still outperforms the next\\nbest result by 0.5 points, with an accuracy of 65.4%\\nversus 64.9% on the open-ended task (on Test-dev).\\n\\n5 Evaluation on Visual Grounding\\n\\n5.1 Datasets\\nWe evaluate our visual grounding approach on two\\ndatasets. The \\ufb01rst is Flickr30k Entities (Plummer\\net al., 2015) which consists of 31K images from\\nFlickr30k dataset (Hodosh et al., 2014) with 244K\\nphrases localized with bounding boxes. We follow\\nthe experimental setup of Rohrbach et al. (2016),\\ne.g. we use the same Selective Search (Uijlings et\\n\\n1Plummer et al. (2016) achieve higher accuracy of 50.89%\\nwhen taking into account box size and color. We believe our\\napproach would also bene\\ufb01t from such additional features.\\n\\nal., 2013) object proposals and the Fast R-CNN (Gir-\\nshick, 2015) \\ufb01ne-tuned VGG16 features (Simonyan\\nand Zisserman, 2014). The second dataset is Refer-\\nItGame (Kazemzadeh et al., 2014), which contains\\n20K images from IAPR TC-12 dataset (Grubinger et\\nal., 2006) with segmented regions from SAIAPR-12\\ndataset (Escalante et al., 2010) and 120K associated\\nnatural language referring expressions. For Refer-\\nItGame we follow the experimental setup of Hu et\\nal. (2016b) and rely on their ground-truth bound-\\ning boxes extracted around the segmentation masks.\\nWe use the Edge Box (Zitnick and Doll\\xb4ar, 2014) ob-\\nject proposals and visual features (VGG16 combined\\nwith the spatial features, which encode bounding box\\nrelative position) from Hu et al. (2016b).\\n\\n5.2 Experimental Setup\\n\\nIn all experiments we use Adam solver (Kingma and\\nBa, 2014) with learning rate \\x01 = 0.0001. The embed-\\nding size is 500 both for visual and language embed-\\ndings. We use d = 2048 in the MCB pooling, which\\nwe found to work best for the visual grounding task.\\nThe accuracy is measured as percentage of query\\nphrases which have been localized correctly. The\\nphrase is localized correctly if the predicted bound-\\ning box overlaps with the ground-truth bounding box\\nby more than 50% intersection over union (IOU).\\n\\n5.3 Results\\n\\nTables 5 and 6 summarize our results in the visual\\ngrounding task. We present multiple ablations of our\\nproposed architecture. First, we replace the MCB\\nwith simple concatenation of the embedded visual\\nfeature and the embedded phrase, resulting in 46.5%\\non the Flickr30k Entities and 25.48% on the Refer-\\nItGame datasets. The results can be improved by\\nreplacing the concatenation with the element-wise\\nproduct of both embedded features (47.41% and\\n27.80%). We can further slightly increase the per-\\nformance by introducing additional 2048-D convo-\\nlution after the element-wise product (47.86% and\\n27.98%). However, even with fewer parameters, our\\nMCB pooling signi\\ufb01cantly improves over this base-\\nline on both datasets, reaching state-of-the-art accu-\\nracy of 48.69% on Flickr30k Entities and 28.91%\\non ReferItGame dataset. Figure 6 (bottom) shows\\nexamples of improved phrase localization.\\n\\n\\x0cWhat vegetable is the dog\\nchewing on?\\nMCB: carrot\\nGT: carrot\\n\\nWhat kind of dog is this?\\nMCB: husky\\nGT: husky\\n\\nWhat kind of \\ufb02ooring does\\nthe room have?\\nMCB: carpet\\nGT: carpet\\n\\nWhat color is the traf\\ufb01c\\nlight?\\nMCB: green\\nGT: green\\nEltwise Product + Conv MCB\\n\\nIs this an urban area?\\nMCB: yes\\nGT: yes\\n\\nWhere are the buildings?\\nMCB: in background\\nGT: on left\\n\\nEltwise Product + Conv\\n\\nMCB\\n\\nA tattooed woman with a green dress and yellow back-\\npack holding a water bottle is walking across the street.\\n\\nA dog distracts his owner from working at her computer.\\n\\nFigure 6: Top: predicted answers and attention maps from MCB model on VQA images. Bottom: predicted\\ngrounding from MCB model (left) and Eltwise Product + Conv model (right) on Flickr30k Entities images.\\n\\n6 Conclusion\\n\\nWe propose the Multimodal Compact Bilinear Pool-\\ning (MCB) to combine visual and text representa-\\ntions. For visual question answering, our architecture\\nwith attention and multiple MCBs gives signi\\ufb01cant\\nimprovements on two VQA datasets compared to\\nstate-of-the-art.\\nIn the visual grounding task, in-\\ntroducing MCB pooling leads to improved phrase\\nlocalization accuracy, indicating better interaction\\nbetween query phrase representations and visual rep-\\n\\nresentations of proposal bounding boxes. The code\\nto replicate our experiments is available at https:\\n//github.com/akirafukui/vqa-mcb.\\nAcknowledgments\\nWe would like to thank Yang Gao and Oscar Beijbom\\nfor helpful discussions about Compact Bilinear Pool-\\ning. This work was supported by DARPA, AFRL,\\nDoD MURI award N000141110688, NSF awards\\nIIS-1427425 and IIS-1212798, and the Berkeley Ar-\\nti\\ufb01cial Intelligence Research (BAIR) Lab.\\n\\n\\x0cReferences\\n[Andreas et al.2016a] Jacob Andreas, Marcus Rohrbach,\\nTrevor Darrell, and Dan Klein. 2016a. Learning to\\ncompose neural networks for question answering. In\\nProceedings of the Conference of the North American\\nChapter of the Association for Computational Linguis-\\ntics (NAACL).\\n\\n[Andreas et al.2016b] Jacob Andreas, Marcus Rohrbach,\\nTrevor Darrell, and Dan Klein. 2016b. Neural module\\nnetworks. In Proceedings of the IEEE Conference on\\nComputer Vision and Pattern Recognition (CVPR).\\n\\n[Antol et al.2015] Stanislaw Antol, Aishwarya Agrawal,\\nJiasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence\\nZitnick, and Devi Parikh. 2015. Vqa: Visual question\\nanswering. In Proceedings of the IEEE International\\nConference on Computer Vision (ICCV).\\n\\n[Charikar et al.2002] Moses Charikar, Kevin Chen, and\\nMartin Farach-Colton. 2002. Finding frequent items\\nin data streams. In Automata, languages and program-\\nming, pages 693\\u2013703. Springer.\\n\\n[Deng et al.2009] J. Deng, W. Dong, R. Socher, L.-J. Li,\\nK. Li, and L. Fei-Fei. 2009.\\nImageNet: A Large-\\nScale Hierarchical Image Database. In Proceedings of\\nthe IEEE Conference on Computer Vision and Pattern\\nRecognition (CVPR).\\n\\n[Donahue et al.2013] Jeff Donahue, Yangqing Jia, Oriol\\nVinyals, Judy Hoffman, Ning Zhang, Eric Tzeng, and\\nTrevor Darrell. 2013. Decaf: A deep convolutional\\nactivation feature for generic visual recognition.\\nIn\\nProceedings of the International Conference on Ma-\\nchine Learning (ICML).\\n\\n[Escalante et al.2010] Hugo Jair Escalante, Carlos A\\nHern\\xb4andez, Jesus A Gonzalez, Aurelio L\\xb4opez-L\\xb4opez,\\nManuel Montes, Eduardo F Morales, L Enrique Sucar,\\nLuis Villase\\u02dcnor, and Michael Grubinger. 2010. The\\nsegmented and annotated iapr tc-12 benchmark. Com-\\nputer Vision and Image Understanding, 114(4):419\\u2013\\n428.\\n\\n[Frome et al.2013] Andrea Frome, Greg S Corrado, Jon\\nShlens, Samy Bengio, Jeff Dean, Tomas Mikolov, et al.\\n2013. Devise: A deep visual-semantic embedding\\nmodel. In Advances in Neural Information Process-\\ning Systems (NIPS).\\n\\n[Gao et al.2016] Yang Gao, Oscar Beijbom, Ning Zhang,\\nand Trevor Darrell. 2016. Compact bilinear pooling.\\nIn Proceedings of the IEEE Conference on Computer\\nVision and Pattern Recognition (CVPR).\\n\\n[Girshick2015] Ross Girshick. 2015. Fast R-CNN. In\\nProceedings of the IEEE International Conference on\\nComputer Vision (ICCV).\\n\\n[Gong et al.2014] Yunchao Gong, Liwei Wang, Micah Ho-\\ndosh, Julia Hockenmaier, and Svetlana Lazebnik. 2014.\\nImproving image-sentence embeddings using large\\n\\nweakly annotated photo collections. In Proceedings of\\nthe European Conference on Computer Vision (ECCV).\\n[Grubinger et al.2006] Michael Grubinger, Paul Clough,\\nHenning M\\xa8uller, and Thomas Deselaers. 2006. The\\niapr tc-12 benchmark: A new evaluation resource for\\nvisual information systems. In International Workshop\\nOntoImage, volume 5, page 10.\\n\\n[Hardoon et al.2004] David R Hardoon, Sandor Szedmak,\\nand John Shawe-Taylor. 2004. Canonical correlation\\nanalysis: An overview with application to learning\\nmethods. Neural computation, 16(12):2639\\u20132664.\\n\\n[He et al.2015] Kaiming He, Xiangyu Zhang, Shaoqing\\nRen, and Jian Sun. 2015. Deep residual learning for\\nimage recognition. In Proceedings of the IEEE Con-\\nference on Computer Vision and Pattern Recognition\\n(CVPR).\\n\\n[Hodosh et al.2014] Peter Hodosh, Alice Young, Micah\\nLai, and Julia Hockenmaier. 2014. From image de-\\nscriptions to visual denotations: New similarity met-\\nrics for semantic inference over event descriptions. In\\nTransactions of the Association for Computational Lin-\\nguistics (TACL).\\n\\n[Hu et al.2016a] Ronghang Hu, Marcus Rohrbach, and\\nTrevor Darrell. 2016a. Segmentation from natural\\nlanguage expressions. In Proceedings of the European\\nConference on Computer Vision (ECCV).\\n\\n[Hu et al.2016b] Ronghang Hu, Huazhe Xu, Marcus\\nRohrbach, Jiashi Feng, Kate Saenko, and Trevor Dar-\\nrell. 2016b. Natural language object retrieval. In Pro-\\nceedings of the IEEE Conference on Computer Vision\\nand Pattern Recognition (CVPR).\\n\\n[Ilievski et al.2016] Ilija Ilievski, Shuicheng Yan, and Ji-\\nashi Feng. 2016. A focused dynamic attention model\\nfor visual question answering. arXiv:1604.01485.\\n\\n[Ioffe and Szegedy2015] Sergey Ioffe\\n\\nand Christian\\nSzegedy. 2015. Batch normalization: Accelerating\\ndeep network training by reducing internal covariate\\nshift. In Proceedings of the International Conference\\non Machine Learning (ICML).\\n\\n[Karpathy and Fei-Fei2015] Andrej Karpathy and Li Fei-\\nFei. 2015. Deep visual-semantic alignments for gener-\\nating image descriptions. In Proceedings of the IEEE\\nConference on Computer Vision and Pattern Recogni-\\ntion (CVPR).\\n\\n[Kazemzadeh et al.2014] Sahar Kazemzadeh, Vicente Or-\\ndonez, Mark Matten, and Tamara L. Berg.\\n2014.\\nReferit game: Referring to objects in photographs of\\nnatural scenes. In Proceedings of the Conference on\\nEmpirical Methods in Natural Language Processing\\n(EMNLP).\\n\\n[Kingma and Ba2014] Diederik Kingma and Jimmy Ba.\\n2014. Adam: A method for stochastic optimization. In\\nProceedings of the International Conference on Learn-\\ning Representations (ICLR).\\n\\n\\x0c[Kiros et al.2014] Ryan Kiros, Ruslan Salakhutdinov, and\\nRich Zemel. 2014. Multimodal neural language mod-\\nels. In Proceedings of the International Conference on\\nMachine Learning (ICML), pages 595\\u2013603.\\n\\n[Kiros et al.2015] Ryan Kiros, Yukun Zhu, Ruslan\\nSalakhutdinov, Richard S Zemel, Antonio Torralba,\\nRaquel Urtasun, and Sanja Fidler. 2015. Skip-thought\\nvectors. In Advances in Neural Information Processing\\nSystems (NIPS).\\n\\n[Klein et al.2015] Benjamin Klein, Guy Lev, Gil Sadeh,\\nand Lior Wolf. 2015. Fisher vectors derived from\\nhybrid gaussian-laplacian mixture models for image\\nannotation. In Proceedings of the IEEE Conference on\\nComputer Vision and Pattern Recognition (CVPR).\\n\\n[Krishna et al.2016] Ranjay Krishna, Yuke Zhu, Oliver\\nGroth, Justin Johnson, Kenji Hata, Joshua Kravitz,\\nStephanie Chen, Yannis Kalantidis, Li-Jia Li, David A\\nShamma, Michael Bernstein, and Li Fei-Fei. 2016.\\nVisual genome:\\nConnecting language and vi-\\nsion using crowdsourced dense image annotations.\\narXiv:1602.07332.\\n\\n[Kumar et al.2016] Ankit Kumar, Ozan Irsoy, Jonathan Su,\\nJames Bradbury, Robert English, Brian Pierce, Peter\\nOndruska, Ishaan Gulrajani, and Richard Socher. 2016.\\nAsk me anything: Dynamic memory networks for natu-\\nral language processing. In Proceedings of the Interna-\\ntional Conference on Machine Learning (ICML).\\n\\n[Lin et al.2014] Tsung-Yi Lin, Michael Maire, Serge Be-\\nlongie, James Hays, Pietro Perona, Deva Ramanan,\\nPiotr Doll\\xb4ar, and C Lawrence Zitnick. 2014. Microsoft\\ncoco: Common objects in context. In Proceedings of\\nthe European Conference on Computer Vision (ECCV).\\n[Lin et al.2015] Tsung-Yu Lin, Aruni RoyChowdhury, and\\nSubhransu Maji. 2015. Bilinear cnn models for \\ufb01ne-\\ngrained visual recognition. In Proceedings of the IEEE\\nInternational Conference on Computer Vision (ICCV).\\n[Lu et al.2016] Jiasen Lu, Jianwei Yang, Dhruv Batra, and\\nDevi Parikh. 2016. Hierarchical Co-Attention for Vi-\\nsual Question Answering. In Advances in Neural Infor-\\nmation Processing Systems (NIPS).\\n\\n[Malinowski et al.2016] Mateusz Malinowski, Marcus\\nRohrbach, and Mario Fritz. 2016. Ask Your Neu-\\nrons: A Deep Learning Approach to Visual Question\\nAnswering. arXiv: 1605.02697.\\n\\n[Mao et al.2015] Junhua Mao, Wei Xu, Yi Yang, Jiang\\nWang, Zhiheng Huang, and Alan Yuille. 2015. Deep\\ncaptioning with multimodal recurrent neural networks\\n(m-rnn). In Proceedings of the International Confer-\\nence on Learning Representations (ICLR).\\n\\n[Ngiam et al.2011] Jiquan Ngiam, Aditya Khosla, Mingyu\\nKim, Juhan Nam, Honglak Lee, and Andrew Y Ng.\\n2011. Multimodal deep learning. In Proceedings of\\nthe International Conference on Machine Learning\\n(ICML), pages 689\\u2013696.\\n\\n[Noh et al.2015] Hyeonwoo Noh, Paul Hongsuck Seo, and\\nBohyung Han. 2015. Image question answering using\\nconvolutional neural network with dynamic parameter\\nprediction. In Proceedings of the IEEE Conference on\\nComputer Vision and Pattern Recognition (CVPR).\\n\\n[Pennington et al.2014] Jeffrey Pennington, Richard\\nSocher, and Christopher D. Manning. 2014. Glove:\\nGlobal vectors for word representation. In Proceedings\\nof the Conference on Empirical Methods in Natural\\nLanguage Processing (EMNLP).\\n\\n[Pham and Pagh2013] Ninh Pham and Rasmus Pagh.\\n2013. Fast and scalable polynomial kernels via ex-\\nplicit feature maps. In Proceedings of the 19th ACM\\nSIGKDD International Conference on Knowledge Dis-\\ncovery and Data Mining, KDD \\u201913, pages 239\\u2013247,\\nNew York, NY, USA. ACM.\\n\\n[Plummer et al.2015] Bryan Plummer, Liwei Wang, Chris\\nCervantes, Juan Caicedo, Julia Hockenmaier, and Svet-\\nlana Lazebnik. 2015. Flickr30k entities: Collecting\\nregion-to-phrase correspondences for richer image-to-\\nsentence models. In Proceedings of the IEEE Interna-\\ntional Conference on Computer Vision (ICCV).\\n\\n[Plummer et al.2016] Bryan Plummer, Liwei Wang, Chris\\nCervantes, Juan Caicedo, Julia Hockenmaier, and Svet-\\nlana Lazebnik. 2016. Flickr30k entities: Collecting\\nregion-to-phrase correspondences for richer image-to-\\nsentence models. arXiv:1505.04870v3.\\n\\n[Rohrbach et al.2016] Anna Rohrbach, Marcus Rohrbach,\\nRonghang Hu, Trevor Darrell, and Bernt Schiele. 2016.\\nGrounding of textual phrases in images by reconstruc-\\ntion. In Proceedings of the European Conference on\\nComputer Vision (ECCV).\\n\\n[Simonyan and Zisserman2014] Karen Simonyan and An-\\ndrew Zisserman. 2014. Very deep convolutional net-\\nworks for large-scale image recognition. In Proceed-\\nings of the International Conference on Learning Rep-\\nresentations (ICLR).\\n\\n[Socher et al.2014] Richard Socher, Andrej Karpathy,\\nQuoc V Le, Christopher D Manning, and Andrew Y Ng.\\n2014. Grounded compositional semantics for \\ufb01nding\\nand describing images with sentences. Transactions of\\nthe Association for Computational Linguistics, 2:207\\u2013\\n218.\\n\\n[Sutskever et al.2014] Ilya Sutskever, Oriol Vinyals, and\\nQuoc V. V Le. 2014. Sequence to sequence learning\\nwith neural networks. In Advances in Neural Informa-\\ntion Processing Systems (NIPS).\\n\\n[Tenenbaum and Freeman2000] Joshua B Tenenbaum and\\nWilliam T Freeman. 2000. Separating style and content\\nwith bilinear models. Neural computation, 12(6):1247\\u2013\\n1283.\\n\\n[Thomee et al.2015] Bart Thomee, David A. Shamma,\\nGerald Friedland, Benjamin Elizalde, Karl Ni, Dou-\\nglas Poland, Damian Borth, and Li-Jia Li. 2015. The\\n\\n\\x0cnew data and new challenges in multimedia research.\\nCoRR, abs/1503.01817.\\n\\nference on Computer Vision (ECCV), pages 391\\u2013405.\\nSpringer.\\n\\n[Uijlings et al.2013] Jasper RR Uijlings, Koen EA van de\\nSande, Theo Gevers, and Arnold WM Smeulders. 2013.\\nSelective search for object recognition. International\\nJournal of Computer Vision (IJCV), 104(2).\\n\\n[Wang et al.2016] Liwei Wang, Yin Li, and Svetlana\\nLazebnik. 2016. Learning deep structure-preserving\\nimage-text embeddings. In Proceedings of the IEEE\\nConference on Computer Vision and Pattern Recogni-\\ntion (CVPR).\\n\\n[Weston et al.2011] Jason Weston, Samy Bengio, and\\nNicolas Usunier. 2011. Wsabie: Scaling up to large\\nvocabulary image annotation. In Proceedings of the In-\\nternational Joint Conference on Arti\\ufb01cial Intelligence\\n(IJCAI).\\n\\n[Wu et al.2016] Qi Wu, Peng Wang, Chunhua Shen, An-\\nton van den Hengel, and Anthony Dick. 2016. Ask\\nMe Anything: Free-form Visual Question Answering\\nBased on Knowledge from External Sources. In Proc.\\nIEEE Conf. Computer Vision Pattern Recognition.\\n\\n[Xiong et al.2016] Caiming Xiong, Stephen Merity, and\\nRichard Socher. 2016. Dynamic memory networks for\\nvisual and textual question answering. In Proceedings\\nof the International Conference on Machine Learning\\n(ICML).\\n\\n[Xu and Saenko2016] Huijuan Xu and Kate Saenko. 2016.\\nAsk, attend and answer: Exploring question-guided spa-\\ntial attention for visual question answering. In Proceed-\\nings of the European Conference on Computer Vision\\n(ECCV).\\n\\n[Xu et al.2015] Kelvin Xu, Jimmy Ba, Ryan Kiros, Aaron\\nCourville, Ruslan Salakhutdinov, Richard Zemel, and\\nYoshua Bengio. 2015. Show, attend and tell: Neural\\nimage caption generation with visual attention. Pro-\\nceedings of the International Conference on Machine\\nLearning (ICML).\\n\\n[Yang et al.2015] Zichao Yang, Xiaodong He, Jianfeng\\nGao, Li Deng, and Alex Smola.\\n2015. Stacked\\nattention networks for image question answering.\\narXiv:1511.02274.\\n\\n[Zhou et al.2015] Bolei Zhou, Yuandong Tian, Sainba-\\nyar Sukhbaatar, Arthur Szlam, and Rob Fergus.\\n2015. Simple baseline for visual question answering.\\narXiv:1512.02167.\\n\\n[Zhu et al.2016] Yuke Zhu, Oliver Groth, Michael Bern-\\nstein, and Li Fei-Fei. 2016. Visual7W: Grounded\\nIn Proceedings of\\nQuestion Answering in Images.\\nthe IEEE Conference on Computer Vision and Pattern\\nRecognition (CVPR).\\n\\n[Zitnick and Doll\\xb4ar2014] C Lawrence Zitnick and Piotr\\nDoll\\xb4ar. 2014. Edge boxes: Locating object propos-\\nals from edges. In Proceedings of the European Con-\\n\\n\\x0c', u'Attribute2Image: Conditional Image Generation\\n\\nfrom Visual Attributes\\n\\nXinchen Yan1, Jimei Yang2, Kihyuk Sohn3 and Honglak Lee1\\n\\n1Computer Science and Engineering,\\n\\nUniversity of Michigan\\n\\n2Adobe Research, 3NEC Labs\\n\\nxcyan@umich.edu, jimyang@adobe.com, ksohn@nec-labs.com, honglak@umich.edu\\n\\n6\\n1\\n0\\n2\\n\\n \\nt\\nc\\nO\\n8\\n\\n \\n\\n \\n \\n]\\n\\nG\\nL\\n.\\ns\\nc\\n[\\n \\n \\n\\n2\\nv\\n0\\n7\\n5\\n0\\n0\\n\\n.\\n\\n2\\n1\\n5\\n1\\n:\\nv\\ni\\nX\\nr\\na\\n\\nAbstract. This paper investigates a novel problem of generating images\\nfrom visual attributes. We model the image as a composite of foreground\\nand background and develop a layered generative model with disentan-\\ngled latent variables that can be learned end-to-end using a variational\\nauto-encoder. We experiment with natural images of faces and birds and\\ndemonstrate that the proposed models are capable of generating realistic\\nand diverse samples with disentangled latent representations. We use a\\ngeneral energy minimization algorithm for posterior inference of latent\\nvariables given novel images. Therefore, the learned generative models\\nshow excellent quantitative and visual results in the tasks of attribute-\\nconditioned image reconstruction and completion.\\n\\n1 Introduction\\n\\nGenerative image modeling is of fundamental interest in computer vision and ma-\\nchine learning. Early works [30,32,36,21,26,20] studied statistical and physical\\nprinciples of building generative models, but due to the lack of e\\ufb00ective feature\\nrepresentations, their results are limited to textures or particular patterns such\\nas well-aligned faces. Recent advances on representation learning using deep\\nneural networks [16,29] nourish a series of deep generative models that enjoy\\njoint generative modeling and representation learning through Bayesian infer-\\nence [34,1,28,15,14,9] or adversarial training [8,3]. Those works show promising\\nresults of generating natural images, but the generated samples are still in low\\nresolution and far from being perfect because of the fundamental challenges of\\nlearning unconditioned generative models of images.\\n\\nIn this paper, we are interested in generating object images from high-level\\ndescription. For example, we would like to generate portrait images that all\\nmatch the description \\u201ca young girl with brown hair is smiling\\u201d (Figure 1). This\\nconditioned treatment reduces sampling uncertainties and helps generating more\\nrealistic images, and thus has potential real-world applications such as forensic\\nart and semantic photo editing [19,40,12]. The high-level descriptions are usually\\nnatural languages, but what underlies its corresponding images are essentially a\\ngroup of facts or visual attributes that are extracted from the sentence. In the\\nexample above, the attributes are (hair color: brown), (gender: female), (age:\\n\\n\\x0c2\\n\\nXinchen Yan, Jimei Yang, Kihyuk Sohn and Honglak Lee\\n\\nFig. 1. An example that demonstrates the problem of conditioned image generation\\nfrom visual attributes. We assume a vector of visual attributes is extracted from a\\nnatural language description, and then this attribute vector is combined with learned\\nlatent factors to generate diverse image samples.\\n\\nyoung) and (expression: smile). Based on this assumption, we propose to learn\\nan attribute-conditioned generative model.\\n\\nIndeed, image generation is a complex process that involves many factors.\\nOther than enlisted attributes, there are many unknown or latent factors. It has\\nbeen shown that those latent factors are supposed to be interpretable accord-\\ning to their semantic or physical meanings [17,4,27]. Inspired by layered image\\nmodels [38,23], we disentangle the latent factors into two groups: one related\\nto uncertain properties of foreground object and the other related to the back-\\nground, and model the generation process as layered composition. In particular,\\nthe foreground is overlaid on the background so that the background visibility\\ndepends on the foreground shape and position. Therefore, we propose a novel\\nlayered image generative model with disentangled foreground and background\\nlatent variables. The entire background is \\ufb01rst generated from background vari-\\nables, then the foreground variables are combined with given attributes to gener-\\nate object layer and its shape map determining the visibility of background and\\n\\ufb01nally the image is composed by the summation of object layer and the back-\\nground layer gated by its visibility map. We learn this layered generative model\\nin an end-to-end deep neural network using a variational auto-encoder [15] (Sec-\\ntion 3). Our variational auto-encoder includes two encoders or recognition models\\nfor approximating the posterior distributions of foreground and background la-\\ntent variables respectively, and two decoders for generating a foreground image\\nand a full image by composition. Assuming the latent variables are Gaussian,\\nthe whole network can be trained end-to-end by back-propagation using the\\nreparametrization trick.\\n\\nGenerating realistic samples is certainly an important goal of deep generative\\nmodels. Moreover, generative models can be also used to perform Bayesian in-\\nference on novel images. Since the true posterior distribution of latent variables\\n\\nage: younggender: femalehair color: brownexpression: smile0.91.3-0.40.8????viewpointbackgroundlighting\\u2026a younggirlwith brown hair is smiling. Attribute-conditioned Image Generation?\\x0cAttribute2Image: Conditional Image Generation from Visual Attributes\\n\\n3\\n\\nis unknown, we propose a general optimization-based approach for posterior\\ninference using image generation models and latent priors (Section 4).\\n\\nWe evaluate the proposed model on two datasets, the Labeled Faces in\\nthe Wild (LFW) dataset [10] and the Caltech-UCSD Birds-200-2011 (CUB)\\ndataset [37]. In the LFW dataset, the attributes are 73-dimensional vectors de-\\nscribing age, gender, expressions, hair and many others [18]. In the CUB dataset,\\nthe 312-dimensional binary attribute vectors are converted from descriptions\\nabout bird parts and colors. We organize our experiments in the following two\\ntasks. First, we demonstrate the quality of attribute-conditioned image genera-\\ntion with comparisons to nearest-neighbor search, and analyze the disentangling\\nperformance of latent space and corresponding foreground-background layers.\\nSecond, we perform image reconstruction and completion on a set of novel test\\nimages by posterior inference with quantitative evaluation. Results from those\\nexperiments show the superior performance of the proposed model over previous\\nart. The contributions of this paper are summarized as follows:\\n\\n\\u2013 We propose a novel problem of conditioned image generation from visual\\n\\nattributes.\\n\\n\\u2013 We tackle this problem by learning conditional variational auto-encoders\\nand propose a novel layered foreground-background generative model that\\nsigni\\ufb01cantly improves the generation quality of complex images.\\n\\n\\u2013 We propose a general optimization-based method for posterior inference on\\nnovel images and use it to evaluate generative models in the context of image\\nreconstruction and completion.\\n\\n2 Related Work\\n\\nImage generation. In terms of generating realistic and novel images, there are\\nseveral recent work [4,9,17,8,3,25] that are relevant to ours. Dosovitskiy et al. [4]\\nproposed to generate 3D chairs given graphics code using deep convolutional\\nneural networks, and Kulkarni et al. [17] used variational auto-encoders [15] to\\nmodel the rendering process of 3D objects. Both of these models [17,4] assume\\nthe existence of a graphics engine during training, from which they have 1)\\nvirtually in\\ufb01nite amount of training data and/or 2) pairs of rendered images\\nthat di\\ufb00er only in one factor of variation. Therefore, they are not directly ap-\\nplicable to natural image generation. While both work [17,4] studied generation\\nof rendered images from complete description (e.g., object identity, view-point,\\ncolor) trained from synthetic images (via graphics engine), generation of im-\\nages from an incomplete description (e.g., class labels, visual attributes) is still\\nunder-explored. In fact, image generation from incomplete description is a more\\nchallenging task and the one-to-one mapping formulation of [4] is inherently lim-\\nited. Gregor et al. [9] developed recurrent variational auto-encoders with spatial\\nattention mechanism that allows iterative image generation by patches. This ele-\\ngant algorithm mimics the process of human drawing but at the same time faces\\nchallenges when scaling up to large complex images. Recently, generative adver-\\nsarial networks (GANs)\\n[8,7,3,25] have been developed for image generation.\\n\\n\\x0c4\\n\\nXinchen Yan, Jimei Yang, Kihyuk Sohn and Honglak Lee\\n\\nIn the GAN, two models are trained to against each other: a generative model\\naims to capture the data distribution, while a discriminative model attempts to\\ndistinguish between generated samples and training data. The GAN training is\\nbased on a min-max objective, which is known to be challenging to optimize.\\n\\nLayered modeling of images. Layered models or 2.1D representations of im-\\nages have been studied in the context of moving or still object segmentation\\n[38,23,39,41,11]. The layered structure is introduced into generative image mod-\\neling [20,35]. Tang et al. [35] modeled the occluded images with gated restricted\\nBoltzmann machines and achieved good inpainting and denoising results on well\\ncropped face images. Le Roux et al. [20] explicitly modeled the occlusion layer\\nin a masked restricted Boltzmann machine for separating foreground and back-\\nground and demonstrated promising results on small patches. Though similar to\\nour proposed gating in the form, these models face challenges when applied to\\nmodel large natural images due to its di\\ufb03culty in learning hierarchical represen-\\ntation based on restricted Boltzmann machine.\\n\\nMultimodal Learning. Generative models of image and text have been stud-\\nied in multimodal learning to model joint distribution of multiple data modali-\\nties [22,33,31]. For example, Srivastava and Salakhutdinov [33] developed a mul-\\ntimodal deep Boltzmann machine that models joint distribution of image and\\ntext (e.g., image tag). Sohn et al. [31] proposed improved shared representation\\nlearning of multimodal data through bi-directional conditional prediction by de-\\nriving a conditional prediction model of one data modality given the other and\\nvice versa. Both of these works focused more on shared representation learning\\nusing hand-crafted low-level image features and therefore have limited appli-\\ncations such as conditional image or text retrieval than actual generation of\\nimages.\\n\\n3 Attribute-conditioned Generative Modeling of Images\\n\\nIn this section, we describe our proposed method for attribute-conditioned gen-\\nerative modeling of images. We \\ufb01rst describe a conditional variational auto-\\nencoder, followed by the formulation of layered generative model and its varia-\\ntional learning.\\n\\n3.1 Base Model: Conditional Variational Auto-Encoder (CVAE)\\nGiven the attribute y \\u2208 RNy and latent variable z \\u2208 RNz , our goal is to build\\na model p\\u03b8(x|y, z) that generates realistic image x \\u2208 RNx conditioned on y\\nand z. Here, we refer p\\u03b8 a generator (or generation model), parametrized by \\u03b8.\\nConditioned image generation is simply a two-step process in the following:\\n\\n1. Randomly sample latent variable z from prior distribution p(z);\\n2. Given y and z as conditioning variable, generate image x from p\\u03b8(x|y, z).\\n\\n\\x0cAttribute2Image: Conditional Image Generation from Visual Attributes\\n\\n5\\n\\nHere, the purpose of learning is to \\ufb01nd the best parameter \\u03b8 that maximizes\\nthe log-likelihood log p\\u03b8(x|y). As proposed in [28,15], variational auto-encoders\\ntry to maximize the variational lower bound of the log-likelihood log p\\u03b8(x|y).\\nSpeci\\ufb01cally, an auxiliary distribution q\\u03c6(z|x, y) is introduced to approximate\\nthe true posterior p\\u03b8(z|x, y). We refer the base model a conditional variational\\nauto-encoder (CVAE) with the conditional log-likelihood\\n\\nlog p\\u03b8(x|y) = KL(q\\u03c6(z|x, y)||p\\u03b8(z|x, y)) + LCVAE(x, y; \\u03b8, \\u03c6),\\n\\nwhere the variational lower bound\\n\\nLCVAE(x, y; \\u03b8, \\u03c6) = \\u2212KL(q\\u03c6(z|x, y)||p\\u03b8(z)) + Eq\\u03c6(z|x,y)\\n\\n(cid:2) log p\\u03b8(x|y, z)(cid:3)\\n\\n(1)\\n\\n(cid:17)\\n\\n\\xb5\\u03c6(x, y), diag(\\u03c32\\n\\nN(cid:0)\\xb5\\u03b8(z, y), diag(\\u03c32\\n\\n\\u03b8 (z, y))(cid:1) and N(cid:16)\\n\\nis maximized for learning the model parameters.\\nHere, the prior p\\u03b8(z) is assumed to follow isotropic multivariate Gaussian dis-\\ntribution, while two conditional distributions p\\u03b8(x|y, z) and q\\u03c6(z|x, y) are multi-\\nvariate Gaussian distributions whose mean and covariance are parametrized by\\n, respectively. We\\n\\u03c6(x, y))\\nrefer the auxiliary proposal distribution q\\u03c6(z|x, y) a recognition model and the\\nconditional data distribution p\\u03b8(x|y, z) a generation model.\\nThe \\ufb01rst term KL(q\\u03c6(z|x, y)||p\\u03b8(z)) is a regularization term that reduces the\\ngap between the prior p(z) and the proposal distribution q\\u03c6(z|x, y), while the\\nsecond term log p\\u03b8(x|y, z) is the log likelihood of samples. In practice, we usually\\ntake as a deterministic generation function the mean x = \\xb5\\u03b8(z, y) of conditional\\ndistribution p\\u03b8(x|z, y) given z and y, so it is convenient to assume the standard\\ndeviation function \\u03c3\\u03b8(z, y) is a constant shared by all the pixels as the latent\\nfactors capture all the data variations. We will keep this assumption for the rest\\nof the paper if not particularly mentioned. Thus, we can rewrite the second term\\nin the variational lower bound as reconstruction loss L(\\xb7,\\xb7) (e.g., (cid:96)2 loss):\\n\\nLCVAE = \\u2212 KL(q\\u03c6(z|x, y)||p\\u03b8(z)) \\u2212 Eq\\u03c6(z|x,y)L(\\xb5\\u03b8(y, z), x)\\n\\n(2)\\nNote that the discriminator of GANs [8] can be used as the loss function L(\\xb7,\\xb7)\\nas well, especially when (cid:96)2 (or (cid:96)1) reconstruction loss may not capture the true\\nimage similarities. We leave it for future study.\\n\\n3.2 Disentangling CVAE with a Layered Representation\\n\\nAn image x can be interpreted as a composite of a foreground layer (or a fore-\\nground image xF ) and a background layer (or a background image xB) via a\\nmatting equation [24]:\\n\\nx = xF (cid:12) (1 \\u2212 g) + xB (cid:12) g,\\n\\n(3)\\nwhere (cid:12) denotes the element-wise product. g \\u2208 [0, 1]Nx is an occlusion layer\\nor a gating function that determines the visibility of background pixels while\\n1 \\u2212 g de\\ufb01nes the visibility of foreground pixels. However, the model based on\\n\\n\\x0c6\\n\\nXinchen Yan, Jimei Yang, Kihyuk Sohn and Honglak Lee\\n\\n(a) CVAE: p\\u03b8(x|y, z)\\n\\n(b) disCVAE: p\\u03b8(x, xF , g|y, zF , zB)\\n\\nFig. 2. Graphical model representations of attribute-conditioned image generation\\nmodels (a) without (CVAE) and (b) with (disCVAE) disentangled latent space.\\n\\nEquation (3) may su\\ufb00er from the incorrectly estimated mask as it gates the\\nforeground region with imperfect mask estimation. Instead, we approximate the\\nfollowing formulation that is more robust to estimation error on mask:\\n\\nx = xF + xB (cid:12) g.\\n\\n(4)\\n\\nWhen lighting condition is stable and background is at a distance, we can safely\\nassume foreground and background pixels are generated from independent latent\\nfactors. To this end, we propose a disentangled representation z = [zF , zB] in\\nthe latent space, where zF together with attribute y captures the foreground\\nfactors while zB the background factors. As a result, the foreground layer xF\\nis generated from \\xb5\\u03b8F (y, zF ) and the background layer xB from \\xb5\\u03b8B (zB). The\\nforeground shape and position determine the background occlusion so the gating\\nlayer g is generated from s\\u03b8g (y, zF ) where the last layer of s(\\xb7) is sigmoid function.\\nIn summary, we approximate the layered generation process as follows:\\n1. Sample foreground and background latent variables zF \\u223c p(zF ), zB \\u223c p(zB);\\n\\n(cid:1) and\\n2. Given y and zF , generate foreground layer xF \\u223c N(cid:0)\\xb5\\u03b8F (y, zF ), \\u03c32\\ngating layer g \\u223c Bernoulli(cid:0)s\\u03b8g (y, zF )(cid:1); here, \\u03c30 is a constant. The back-\\n(cid:1) where \\xb5\\u03b8(y, zF , zB) =\\n3. Synthesize an image x \\u223c N(cid:0)\\xb5\\u03b8(y, zF , zB), \\u03c32\\n\\n0INx\\n\\nground layer (which correspond to xB) is implicitly computed as \\xb5\\u03b8B (zB).\\n\\xb5\\u03b8F (y, zF ) + s\\u03b8g (y, zF ) (cid:12) \\xb5\\u03b8B (zB).\\n\\n0INx\\n\\nLearning. It is very challenging to learn our layered generative model in a fully-\\nunsupervised manner since we need to infer about xF , xB, and g from the image\\nx only. In this paper, we further assume the foreground layer xF (as well as\\ngating variable g) is observable during the training and we train the model to\\nmaximize the joint log-likelihood log p\\u03b8(x, xF , g|y) instead of log p\\u03b8(x|y). With\\ndisentangled latent variables zF and zB, we refer our layered model a disentan-\\ngling conditional variational auto-encoder (disCVAE). We compare the graphical\\nmodels of disCVAE with vanilla CVAE in Figure 2. Based on the layered gener-\\nation process, we write the generation model by\\n\\np\\u03b8(xF , g, x, zF , zB|y) = p\\u03b8(x|zF , zB, y)p\\u03b8(xF , g|zF , y)p\\u03b8(zF )p\\u03b8(zB),\\n\\n(5)\\n\\nzyxzBzFyxF,gx\\x0cAttribute2Image: Conditional Image Generation from Visual Attributes\\n\\n7\\n\\nthe recognition model by\\n\\n(6)\\n\\n(cid:2)KL(q\\u03c6(zB|zF , xF , g, x, y)||p\\u03b8(zB))(cid:3)\\n\\n(cid:2)L(\\xb5\\u03b8F (y, zF ), xF ) + \\u03bbgL(s\\u03b8g (y, zF ), g)(cid:3)\\n\\nq\\u03c6(zF , zB|xF , g, x, y) = q\\u03c6(zB|zF , xF , g, x, y)q\\u03c6(zF|xF , g, y)\\nand the variational lower bound LdisCVAE(xF , g, x, y; \\u03b8, \\u03c6) is given by\\nLdisCVAE(xF , g, x, y; \\u03b8, \\u03c6) =\\n\\u2212 KL(q\\u03c6(zF|xF , g, y)||p\\u03b8(zF )) \\u2212 Eq\\u03c6(zF |xF ,g,y)\\n\\u2212 Eq\\u03c6(zF |xF ,g,y)\\n\\u2212 Eq\\u03c6(zF ,zB|xF ,g,x,y)L(\\xb5\\u03b8(y, zF , zB), x)\\n(7)\\nwhere \\xb5\\u03b8(y, zF , zB) = \\xb5\\u03b8F (y, zF ) + s\\u03b8g (y, zF ) (cid:12) \\xb5\\u03b8B (zB) as in Equation (4). We\\nfurther assume that log p\\u03b8(xF , g|zF , y) = log p\\u03b8(xF|zF , y) + \\u03bbg log p\\u03b8(g|zF , y),\\nwhere we introduce \\u03bbg as additional hyperparameter when decomposing the\\nprobablity p\\u03b8(xF , g|zF , y). For the loss function L(\\xb7,\\xb7), we used reconstruction\\nerror for predicting x or xF and cross entropy for predicting the binary mask g.\\nSee the supplementary material A for details of the derivation. All the genera-\\ntion and recognition models are parameterized by convolutional neural networks\\nand trained end-to-end in a single architecture with back-propagation. We will\\nintroduce the exact network architecture in the experiment section.\\n\\n4 Posterior Inference via Optimization\\n\\nOnce the attribute-conditioned generative model is trained, the inference or gen-\\neration of image x given attribute y and latent variable z is straight-forward.\\nHowever, the inference of latent variable z given an image x and its correspond-\\ning attribute y is unknown. In fact, the latent variable inference is quite useful\\nas it enables model evaluation on novel images. For simplicity, we introduce our\\ninference algorithm based on the vanilla CVAE and the same algorithm can be\\ndirectly applied to the proposed disCVAE and the other generative models such\\nas GANs [7,3]. Firstly we notice that the recognition model q\\u03c6(z|y, x) may not\\nbe directly used to infer z. On one hand, as an approximate, we don\\u2019t know how\\nfar it is from the true posterior p\\u03b8(z|x, y) because the KL divergence between\\nthem is thrown away in the variational learning objective; on the other hand,\\nthis approximation does not even exist in the models such as GANs. We propose\\na general approach for posterior inference via optimization in the latent space.\\nUsing Bayes\\u2019 rule, we can formulate the posterior inference by\\n\\nmax\\n\\nz\\n\\n(8)\\nNote that the generation models or likelihood terms p\\u03b8(x|z, y) could be non-\\nGaussian or even a deterministic function (e.g. in GANs) with no proper proba-\\nbilistic de\\ufb01nition. Thus, to make our algorithm general enough, we reformulate\\n\\nz\\n\\nlog p\\u03b8(z|x, y) = max\\n= max\\n\\nz\\n\\n(cid:2) log p\\u03b8(x|z, y) + log p\\u03b8(z|y)(cid:3)\\n(cid:2) log p\\u03b8(x|z, y) + log p\\u03b8(z)(cid:3)\\n\\n\\x0c8\\n\\nXinchen Yan, Jimei Yang, Kihyuk Sohn and Honglak Lee\\n\\n(cid:2)L(\\xb5(z, y), x) + \\u03bbR(z)(cid:3)\\n(cid:2)(cid:107)\\xb5(z, y) \\u2212 x(cid:107)2 + \\u03bb(cid:107)z(cid:107)2)(cid:3)\\n\\nthe inference in (8) as an energy minimization problem,\\n\\nmin\\n\\nz\\n\\nE(z, x, y) = min\\n\\n(9)\\nwhere L(\\xb7,\\xb7) is the image reconstruction loss and R(\\xb7) is a prior regularization\\nterm. Taking the simple Gaussian model as an example, the posterior inference\\ncan be re-written as,\\n\\nz\\n\\nmin\\n\\nz\\n\\nE(z, x, y) = min\\n\\nz\\n\\n(10)\\n\\nNote that we abuse the mean function \\xb5(z, y) as a general image generation\\nfunction. Since \\xb5(z, y) is a complex neural network, optimizing (9) is essentially\\nerror back-propagation from the energy function to the variable z, which we\\nsolve by the ADAM method [13]. Our algorithm actually shares a similar spirit\\nwith recently proposed neural network visualization [42] and texture synthesis\\nalgorithms [6]. The di\\ufb00erence is that we use generation models for recognition\\nwhile their algorithms use recognition models for generation. Compared to the\\nconventional way of inferring z from recognition model q\\u03c6(z|x, y), the proposed\\noptimization contributed to an empirically more accurate latent variable z and\\nhence was useful for reconstruction, completion, and editing.\\n\\n5 Experiments\\n\\nDatasets. We evaluated our model on two datasets: Labeled Faces in the Wild\\n(LFW) [10] and Caltech-UCSD Birds-200-2011 (CUB) [37]. For experiments on\\nLFW, we aligned the face images using \\ufb01ve landmarks [43] and rescaled the\\ncenter region to 64\\xd7 64. We used 73 dimensional attribute score vector provided\\nby [18] that describes di\\ufb00erent aspects of facial appearance such as age, gender,\\nor facial expression. We trained our model using 70% of the data (9,000 out\\nof 13,000 face images) following the training-testing split (View 1) [10], where\\nthe face identities are distinct between train and test sets. For experiments on\\nCUB, we cropped the bird region using the tight bounding box computed from\\nthe foreground mask and rescaled to 64 \\xd7 64. We used 312 dimensional binary\\nattribute vector that describes bird parts and colors. We trained our model using\\n50% of the data (6,000 out of 12,000 bird images) following the training-testing\\nsplit [37]. For model training, we held-out 10% of training data for validation.\\n\\nData preprocessing and augmentation. To make the learning easier, we pre-\\nprocessed the data by normalizing the pixel values to the range [\\u22121, 1]. We\\naugmented the training data with the following image transformations [16,5]:\\n1) \\ufb02ipping images horizontally with probability 0.5, 2) multiplying pixel values\\nof each color channel with a random value c \\u2208 [0.97, 1.03], and 3) augment-\\ning the image with its residual with a random tradeo\\ufb00 parameter s \\u2208 [0, 1.5].\\nSpeci\\ufb01cally, for CUB experiments, we performed two extra transformations: 4)\\nrotating images around the centering point by a random angle \\u03b8r \\u2208 [\\u22120.08, 0.08],\\n5) rescaling images to the scale of 72 \\xd7 72 and performing random cropping of\\n64 \\xd7 64 regions. Note that these methods are designed to be invariant to the\\nattribute description.\\n\\n\\x0cAttribute2Image: Conditional Image Generation from Visual Attributes\\n\\n9\\n\\nArchitecture design. For disCVAE, we build four convolutional neural networks\\n(one for foreground and the other for background for both recognition and gen-\\neration networks) for auto-encoding style training. The foreground encoder net-\\nwork consists of 5 convolution layers, followed by 2 fully-connected layers (con-\\nvolution layers have 64, 128, 256, 256 and 1024 channels with \\ufb01lter size of 5\\xd7 5,\\n5 \\xd7 5, 3 \\xd7 3, 3 \\xd7 3 and 4 \\xd7 4, respectively; the two fully-connected layers have\\n1024 and 192 neurons). The attribute stream is merged with image stream at the\\nend of the recognition network. The foreground decoder network consists of 2\\nfully-connected layers, followed by 5 convolution layers with 2-by-2 upsampling\\n(fully-connected layers have 256 and 8 \\xd7 8 \\xd7 256 neurons; the convolution layers\\nhave 256, 256, 128, 64 and 3 channels with \\ufb01lter size of 3 \\xd7 3, 5 \\xd7 5, 5 \\xd7 5,\\n5 \\xd7 5 and 5 \\xd7 5. The foreground prediction stream and gating prediction stream\\nare separated at the last convolution layer. We adopt the same encoder/decoder\\narchitecture for background networks but with fewer number of channels. See\\nthe supplementary material B for more details.\\nFor all the models, we \\ufb01xed the latent dimension to be 256 and found this con-\\n\\ufb01guration is su\\ufb03cient to generate 64\\xd764 images in our setting. We adopt slightly\\ndi\\ufb00erent architectures for di\\ufb00erent datasets: we use 192 dimensions to foreground\\nlatent space and 64 dimensions to background latent space for experiments on\\nLFW dataset; we use 128 dimensions for both foreground and background latent\\nspaces on CUB dataset. Compared to vanilla CVAE, the proposed disCVAE has\\nmore parameters because of the additional convolutions introduced by the two-\\nstream architecture. However, we found that adding more parameters to vanilla\\nCVAE does not lead to much improvement in terms of image quality. Although\\nboth [4] and the proposed method use segmentation masks as supervision, naive\\nmask prediction was not comparable to the proposed model in our setting based\\non the preliminary results. In fact, the proposed disCVAE architecture assigns\\nforeground/background generation to individual networks and composite with\\ngated interaction, which we found very e\\ufb00ective in practice.\\n\\nImplementation details. We used ADAM [13] for stochastic optimization in all\\nexperiments. For training, we used mini-batch of size 32 and the learning rate\\n0.0003. We also added dropout layer of ratio 0.5 for the image stream of the\\nencoder network before merging with attribute stream. For posterior inference,\\nwe used the learning rate 0.3 with 1000 iterations. The models are implemented\\nusing deep learning toolbox Torch7 [2].\\n\\nBaselines. For the vanilla CVAE model, we used the same convolution archi-\\ntecture from foreground encoder network and foreground decoder network. To\\ndemonstrate the signi\\ufb01cance of attribute-conditioned modeling, we trained an\\nunconditional variational auto-encoders (VAE) with almost the same convolu-\\ntional architecture as our CVAE.\\n\\n5.1 Attribute-conditioned Image Generation\\n\\nTo examine whether the model has the capacity to generate diverse and realistic\\nimages from given attribute description, we performed the task of attribute-\\n\\n\\x0c10\\n\\nXinchen Yan, Jimei Yang, Kihyuk Sohn and Honglak Lee\\n\\nFig. 3. Attribute-conditioned image generation.\\n\\nconditioned image generation. For each attribute description from testing set,\\nwe generated 5 samples by the proposed generation process: x \\u223c p\\u03b8(x|y, z),\\nwhere z is sampled from isotropic Gaussian distribution. For vanilla CVAE, x is\\nthe only output of the generation. In comparison, for disCVAE, the foreground\\nimage xF can be considered a by-product of the layered generation process.\\nFor evaluation, we visualized the samples generated from the model in Figure 3\\nand compared them with the corresponding image in the testing set, which we\\nname as \\u201creference\\u201d image. To demonstrate that model did not exploit the trivial\\nsolution of attribute-conditioned generation by memorizing the training data, we\\nadded a simple baseline as experimental comparison. Basically, for each given\\nattribute description in the testing set, we conducted the nearest neighbor search\\nin the training set. We used the mean squared error as the distance metric for\\nthe nearest neighbor search (in the attribute space). For more visual results and\\ncode, please refer to the project website: https://sites.google.com/site/\\nattribute2image/.\\n\\nAttribute-conditioned face image generation. As we can see in Figure 3, face im-\\nages generated by the proposed models look realistic and non-trivially di\\ufb00erent\\nfrom each other, especially for view-point and background color. Moreover, it\\nis clear that images generated by disCVAE have clear boundaries against the\\nbackground. In comparison, the boundary regions between the hair area and\\nbackground are quite blurry for samples generated by vanilla CVAE. This ob-\\nservation suggests the limitation of vanilla CVAE in modeling hair pattern for\\nface images. This also justi\\ufb01es the signi\\ufb01cance of layered modeling and latent\\n\\nMale, No eyewear, Frowning, Receding hairline, Bushy eyebrow, Eyes open, Pointy nose, Teeth not visible, Rosy cheeks, Flushed faceFemale, Asian, Youth, No eyewear, Smiling, Straight hair, Fully visible forehead, Arched eyebrows, eyes open, mouth slightly open, round jaw, oval face, heavy makeup, Shiny skin, High cheekbonesWing_color:brown, Breast_color:yellow, Primary_color:black, Primary_color:red, Wing_pattern:stripedWing_color:black, Primary_color:yellow, Breast_color:yellow, Primary_color:black, Wing_pattern:solidAttributesNearest Neighbor Vanilla CVAEdisCVAE(foreground)disCVAE(full)Reference\\x0cAttribute2Image: Conditional Image Generation from Visual Attributes\\n\\n11\\n\\nFig. 4. Attribute-conditioned image progression. The visualization is organized into six\\nattribute groups (e.g., \\u201cgender\\u201d, \\u201cage\\u201d, \\u201cfacial expression\\u201d, \\u201ceyewear\\u201d, \\u201chair color\\u201d and\\n\\u201cprimary color (blue vs. yellow)\\u201d). Within each group, the images are generated from\\np\\u03b8(x|y, z) with z \\u223c N (0, I) and y = [y\\u03b1, yrest], where y\\u03b1 = (1\\u2212\\u03b1)\\xb7ymin +\\u03b1\\xb7ymax. Here,\\nymin and ymax stands for the minimum and maximum attribute value respectively in\\nthe dataset along the corresponding dimension.\\n\\nspace disentangling in our attribute-conditioned generation process. Compared\\nto the nearest neighbors in the training set, the generated samples can better\\nre\\ufb02ect the input attribute description. For quantitative evaluations, please refer\\nto supplementary material C for details.\\n\\nAttribute-conditioned bird image generation. Compared to the experiments on\\nLFW database, the bird image modeling is more challenging because the bird\\nimages have more diverse shapes and color patterns and the binary-valued at-\\ntributes are more sparse and higher dimensional. As we can see in Figure 3,\\nthere is a big di\\ufb00erence between two versions of the proposed CVAE model. Ba-\\nsically, the samples generated by vanilla CVAE are blurry and sometimes blended\\nwith the background area. However, samples generated by disCVAE have clear\\nbird shapes and re\\ufb02ect the input attribute description well. This con\\ufb01rms the\\nstrengths of the proposed layered modeling of images.\\n\\nAttribute-conditioned Image Progression. To better analyze the proposed model,\\nwe generate images with interpolated attributes by gradually increasing or de-\\ncreasing the values along each attribute dimension. We regard this process as\\nattribute-conditioned image progression. Speci\\ufb01cally, for each attribute vector,\\nwe modify the value of one attribute dimension by interpolating between the\\nminimum and maximum attribute value. Then, we generate images by inter-\\npolating the value of y between the two attribute vectors while keeping latent\\nvariable z \\ufb01xed. For visualization, we use the attribute vector from testing set.\\nAs we can see in Figure 4, samples generated by progression are visually\\nconsistent with attribute description. For face images, by changing attributes\\nlike \\u201cgender\\u201d and \\u201cage\\u201d, the identity-related visual appearance is changed ac-\\n\\nMaleFemaleSmilingFrowningYoungSeniorBlueYellowNo eyewearEyewearBlack hairBlonde hair(a)progression on gender(b)progression on age(c)progression on expression(e)progression on hair color(f)progression on primary color(d)progression on eyewear\\x0c12\\n\\nXinchen Yan, Jimei Yang, Kihyuk Sohn and Honglak Lee\\n\\nFig. 5. Analysis: Latent Space Disentangling.\\n\\ncordingly but the viewpoint, background color, and facial expression are well pre-\\nserved; on the other hand, by changing attributes like \\u201cfacial expression\\u201d,\\u201ceyewear\\u201d\\nand \\u201chair color\\u201d, the global appearance is well preserved but the di\\ufb00erence ap-\\npears in the local region. For bird images, by changing the primary color from\\none to the other, the global shape and background color are well preserved.\\nThese observations demonstrated that the generation process of our model is\\nwell controlled by the input attributes.\\n\\nAnalysis: Latent Space Disentangling. To better analyze the disCVAE, we per-\\nformed the following experiments on the latent space. In this model, the image\\ngeneration process is driven by three factors: attribute y, foreground latent vari-\\nable zF and background latent variable zB. By changing one variable while \\ufb01xing\\n\\nBackground generationGating generationImage generationlatent space (zB)attribute space (y)(a)Variation in latent (zB) -- attribute (y) space while fixing latent (zF)(b)Variation in latent (zF) -- attribute (y) space while fixing latent (zB)(c)Variation in latent (zF) -- latent (zB) space while fixing attribute (y)latent space (zF)attribute space (y)latent space (zF)latent space (zB)\\x0cAttribute2Image: Conditional Image Generation from Visual Attributes\\n\\n13\\n\\nthe other two, we can analyze how each variable contributes to the \\ufb01nal gen-\\neration results. We visualize the samples x, the generated background xB and\\nthe gating variables g in Figure 5. We summarized the observations as follows:\\n1) The background of the generated samples look di\\ufb00erent but with identical\\nforeground region when we change background latent variable zB only; 2) the\\nforeground region of the generated samples look diverse in terms of viewpoints\\nbut still look similar in terms of appearance and the samples have uniform back-\\nground pattern when we change foreground latent variable zF only. Interestingly,\\nfor face images, one can identify a \\u201chole\\u201d in the background generation. This\\ncan be considered as the location prior of the face images, since the images are\\nrelatively aligned. Meanwhile, the generated background for birds are relatively\\nuniform, which demonstrates our model learned to recover missing background\\nin the training set and also suggests that foreground and background have been\\ndisentangled in the latent space.\\n\\n5.2 Attribute-conditioned Image Reconstruction and Completion\\n\\nImage reconstruction. Given a test image x and its attribute vector y, we \\ufb01nd z\\nthat maximizes the posterior p\\u03b8(z|x, y) following Equation (9).\\n\\nImage completion. Given a test image with synthetic occlusion, we evaluate\\nwhether the model has the capacity to \\ufb01ll in the occluded region by recognizing\\nthe observed region. We denote the occluded (unobserved) region and observed\\nregion as xu and xo, respectively. For completion, we \\ufb01rst \\ufb01nd z that maximizes\\nthe posterior p\\u03b8(z|xo, y) by optimization (9). Then, we \\ufb01ll in the unobserved\\nregion xu by generation using p\\u03b8(xu|z, y). For each face image, we consider four\\ntypes of occlusions: occlusion on the eye region, occlusion on the mouth region,\\nocclusion on the face region and occlusion on right half of the image. For occluded\\nregions, we set the pixel value to 0. For each bird image, we consider blocks of\\nocclusion of size 8 \\xd7 8 and 16 \\xd7 16 at random locations.\\n\\nIn Figure 6, we visualize the results of image reconstruction (a,b) and image\\ncompletion (c-h). As we can see, for face images, our proposed CVAE models are\\n\\nFig. 6. Attribute-conditioned image reconstruction and completion.\\n\\n(b) Bird Reconstruction(d) Face Completion (mouth)(f) Face Completion (half)(h) Bird Completion (16x16 patch)(c) Face Completion (eyes)(e) Face Completion (face)(g) Bird Completion (8x8 patch)(a) Face ReconstructionVAEdisCVAEGTCVAEInputInputdisCVAEGTCVAEVAEVAEdisCVAEGTCVAEInputdisCVAEGTCVAEVAE\\x0c14\\n\\nXinchen Yan, Jimei Yang, Kihyuk Sohn and Honglak Lee\\n\\nTable 1. Quantitative comparisons on face reconstruction and completion tasks.\\n\\nFace Recon: full Recon: fg Comp: eye Comp: mouth Comp: face Comp: half\\n13.1 \\xb1 0.1 21.3 \\xb1 0.2\\n11.8 \\xb1 0.1 9.4 \\xb1 0.1\\nVAE\\nCVAE 11.8 \\xb1 0.1 9.3 \\xb1 0.1\\n12.3 \\xb1 0.1 20.3 \\xb1 0.2\\n10.9 \\xb1 0.1 18.8 \\xb1 0.2\\ndisCVAE 10.0 \\xb1 0.1 7.9 \\xb1 0.1\\n\\n13.0 \\xb1 0.1\\n12.0 \\xb1 0.1\\n10.3 \\xb1 0.1\\n\\n12.1 \\xb1 0.1\\n12.0 \\xb1 0.1\\n10.3 \\xb1 0.1\\n\\nBird Recon: full Recon: fg Comp: 8 \\xd7 8 Comp: 16 \\xd7 16\\n14.5 \\xb1 0.1 11.7 \\xb1 0.1\\nVAE\\nCVAE 14.3 \\xb1 0.1 11.5 \\xb1 0.1\\ndisCVAE 12.9 \\xb1 0.1 10.2 \\xb1 0.1\\n\\n4.6 \\xb1 0.1\\n4.4 \\xb1 0.1\\n4.4 \\xb1 0.1\\n\\n1.8 \\xb1 0.1\\n1.8 \\xb1 0.1\\n1.8 \\xb1 0.1\\n\\nin general good at reconstructing and predicting the occluded region in unseen\\nimages (from testing set). However, for bird images, vanilla CVAE model had\\nsigni\\ufb01cant failures in general. This agreed with the previous results in attribute-\\nconditioned image generation.\\n\\nIn addition, to demonstrate the signi\\ufb01cance of attribute-conditioned mod-\\neling, we compared our vanilla CVAE and disCVAE with unconditional VAE\\n(attribute is not given) for image reconstruction and completion. It can be seen\\nin Fig. 6(c)(d), the generated images using attributes actually perform better in\\nterms of expression and eyewear (\\u201csmiling\\u201d and \\u201csunglasses\\u201d).\\n\\nFor quantitative comparisons, we measured the pixel-level mean squared er-\\nror on the entire image and occluded region for reconstruction and completion,\\nrespectively. We summarized the results in Table 1 (mean squared error and\\nstandard error). The quantitative analysis highlighted the bene\\ufb01ts of attribute-\\nconditioned modeling and the importance of layered modeling.\\n\\n6 Conclusion\\n\\nTo conclude, this paper studied a novel problem of attribute-conditioned im-\\nage generation and proposed a solution with CVAEs. Considering the composi-\\ntional structure of images, we proposed a novel disentangling CVAE (disCVAE)\\nwith a layered representation. Results on faces and birds demonstrate that our\\nmodels can generate realistic samples with diverse appearance and especially\\ndisCVAE signi\\ufb01cantly improved the generation quality on bird images. To eval-\\nuate the learned generation models on the novel images, we also developed an\\noptimization-based approach to posterior inference and applied it to the tasks\\nof image reconstruction and completion with quantitative evaluation.\\n\\nAcknowledgement. This work was supported in part by NSF CAREER\\nIIS-1453651, ONR N00014-13-1-0762, Sloan Research Fellowship, and a gift from\\nAdobe. We acknowledge NVIDIA for the donation of GPUs. We also thank Yut-\\ning Zhang, Scott Reed, Junhyuk Oh, Ruben Villegas, Seunghoon Hong, Wenling\\nShang, Ye Liu, Kibok Lee, Lajanugen Logeswaran, Rui Zhang, Changhan Wang\\nand Yi Zhang for helpful comments and discussions.\\n\\n\\x0cAttribute2Image: Conditional Image Generation from Visual Attributes\\n\\n15\\n\\nA Derivation of disCVAE objective\\n\\nWe provide a detailed derivation of the objective function for disentangling\\nCVAE (disCVAE). Similarly to the vanilla CVAE, we have x and xF as in-\\nput image (full, foreground), g as foreground mask, y as attribute labels, and\\nz = [zF , zB] as latent variables (zF for foreground and zB for background).\\n\\nThe joint conditional log-likelihood of x, xF and g given y can be written as\\n\\nfollows:\\n\\nlog p\\u03b8(xF , g, x|y)\\n= Eq\\u03c6(zF ,zB|xF ,g,x,y)\\n= Eq\\u03c6(zF ,zB|xF ,g,x,y)\\n= KL(q\\u03c6(zF , zB|xF , g, x, y)||p\\u03b8(zF , zB|xF , g, x, y))\\n\\n(cid:2) log p\\u03b8(xF , g, x|y)(cid:3)\\n(cid:2) log p\\u03b8(xF , g, x, zF , zB|y) \\u2212 log p\\u03b8(zF , zB|xF , g, x, y)(cid:3)\\n(cid:2) log p\\u03b8(xF , g, x, zF , zB|y) \\u2212 log q\\u03c6(zF , zB|xF , g, x, y)(cid:3)\\n(cid:125)\\n\\n+ Eq\\u03c6(zF ,zB|xF ,g,x,y)\\n\\n(cid:123)(cid:122)\\n\\n(cid:124)\\n\\n,\\n\\n(11)\\n\\n(cid:44)LdisCVAE(xF ,g,x,y;\\u03b8,\\u03c6)\\n\\nBased on the disentangling assumptions, we write the generation model by\\np\\u03b8(xF , g, x, zF , zB|y) = p\\u03b8(x|zF , zB, y)p\\u03b8(xF , g|zF , y)p\\u03b8(zF )p\\u03b8(zB),\\n\\n(12)\\n\\nthe recognition model by\\n\\nq\\u03c6(zF , zB|xF , g, x, y) = q\\u03c6(zB|zF , xF , g, x, y)q\\u03c6(zF|xF , g, y)\\n\\n(13)\\n\\nand thus the variational lower bound LdisCVAE(xF , g, x, y; \\u03b8, \\u03c6) is given by\\n\\nLdisCVAE(xF , g, x, y; \\u03b8, \\u03c6)\\n\\n+ Eq\\u03c6(zF |xF ,g,y)\\n\\n= \\u2212 KL(q\\u03c6(zF|xF , g, y)||p\\u03b8(zF )) \\u2212 Eq\\u03c6(zF |xF ,g,y)\\n\\n(cid:2) log p\\u03b8(xF , g|zF , y)(cid:3) + Eq\\u03c6(zF ,zB|xF ,g,x,y)\\n(cid:2)L(\\xb5\\u03b8F (y, zF ), xF ) + \\u03bbgL(s\\u03b8g (y, zf ), g)(cid:3)\\n\\n= \\u2212 KL(q\\u03c6(zF|xF , g, y)||p\\u03b8(zF )) \\u2212 Eq\\u03c6(zF |xF ,g,y)\\n\\n\\u2212 Eq\\u03c6(zF |xF ,g,y)\\n\\u2212 Eq\\u03c6(zF ,zB|xF ,g,x,y)L(\\xb5\\u03b8(y, zF , zB), x)\\n\\n(cid:2)KL(q\\u03c6(zB|zF , xF , g, x, y)||p\\u03b8(zB))(cid:3)\\n(cid:2)KL(q\\u03c6(zB|zF , xF , g, x, y)||p\\u03b8(zB))(cid:3)\\n\\n(cid:2) log p\\u03b8(x|zF , zB, y)(cid:3)\\n\\n(14)\\nIn the last step, we assumed that log p\\u03b8(xF , g|zF , y) = log p\\u03b8(xF|zF , y) +\\n\\u03bbg log p\\u03b8(g|zF , y), where \\u03bbg is a hyperparameter when decomposing the proba-\\nblity p\\u03b8(xF , g|zF , y). Here, the third and fourth terms are rewritten as expecta-\\ntions involving reconstruction loss (e.g., (cid:96)2 loss) or cross entropy.\\n\\nB Network Architecture for disCVAE\\n\\nAs we visualize in Figure 7, disCVAE consists of four convolutional neural net-\\nworks (one for foreground and the other for background for both recognition and\\ngeneration networks).\\n\\n\\x0c16\\n\\nXinchen Yan, Jimei Yang, Kihyuk Sohn and Honglak Lee\\n\\nFig. 7. Network Architecture for disentangling CVAE\\n\\nThe foreground encoder network consists of 5 convolution layers, followed\\nby 2 fully-connected layers (convolution layers have 64, 128, 256, 256 and 1024\\nchannels with \\ufb01lter size of 5 \\xd7 5, 5 \\xd7 5, 3 \\xd7 3, 3 \\xd7 3 and 4 \\xd7 4, respectively; the\\ntwo fully-connected layers have 1024 and 192 neurons). The attribute stream is\\nmerged with image stream at the end of the recognition network. The foreground\\ndecoder network consists of 2 fully-connected layers, followed by 5 convolution\\nlayers with 2-by-2 upsampling (fully-connected layers have 256 and 8 \\xd7 8 \\xd7 256\\nneurons; the convolution layers have 256, 256, 128, 64 and 3 channels with \\ufb01lter\\nsize of 3\\xd7 3, 5\\xd7 5, 5\\xd7 5, 5\\xd7 5 and 5\\xd7 5. The foreground prediction stream and\\ngating prediction stream are separated at the last convolution layer.\\n\\nWe adopt the same encoder/decoder architecture for background networks\\nbut with fewer number of channels. For better modeling on the background latent\\nvariable zB, we introduce attribute y and foreground latent variable zF into the\\nbackground encoder network, which also agrees with the assumption made in the\\nderivation (q\\u03c6(zB|zF , xF , g, x, y)). Here, the connection from foreground latent\\nvariable zF to background latent variable zB only exists in the recognition model.\\nNote that encoder networks are only used during the training stage. Once\\n\\ntrained, we can generate images using decoder networks only.\\n\\nC Quantitative Analysis: Attribute Similarity, Labeled\\n\\nFaces in the Wild\\n\\nIn order to measure the performance quantitatively in the attribute space, we\\npropose to evaluate whether the generated samples exactly capture the condi-\\ntion (attributes). Therefore, we trained a separate convolutional neural network\\nfrom scratch as attribute regressor using image-attribute pairs in the training\\nset. The attribute regressor shares almost the same architecture with the auxil-\\niary recognition model used in generative training. As a reference, the attribute\\nregressor achieves 14.51 mean squared error (MSE) and 0.98 cosine similarity on\\nthe test set.\\n\\nlatent variable zF5x5 conv5x5 conv5x5 conv3x3 conv4x4 conv64x64x332x32x6416x16x1288x8x2564x4x2561x1x10241x1x10241x1x1921x1x731x1x1921x1x731x1x2568x8x2568x8x25616x16x25632x32x12864x64x6464x64x33x3 conv5x5 conv5x5 conv5x5 conv5x5 conv5x5 conv5x5 conv3x3 conv3x3 conv4x4 conv64x64x332x32x6416x16x648x8x644x4x641x1x2561x1x2561x1x641x1x738x8x12816x16x6416x16x6432x32x6464x64x323x3 conv3x3 conv3x3 conv5x5 conv5x5 conv1x1x25632x32x643x3 conv1x1x3842x2x2564x4x1283x3 conv3x3 convlatent variable zBattribute yattribute yattribute yinput image xFinput image xtarget image xFgating gimage xBtarget image xRecognition modelGeneration modelGated Interaction:x=xF+g\\u2299xB\\x0cAttribute2Image: Conditional Image Generation from Visual Attributes\\n\\n17\\n\\nFor each attribute vector in the test set (reference attribute), we randomly\\ngenerate 10 samples and feed them into the attribute regressor. We then com-\\npute the cosine similarity and mean squared error between the reference attribute\\nand the predicted attributes from generated samples. As a baseline method, we\\ncompute the cosine similarity between the reference attribute and the predicted\\nattributes from nearest neighbor samples (NN). Furthermore, to verify that our\\nproposed method does not take unfair advantage in evaluation due to its some-\\nwhat blurred image generation, we add another baseline method (NNblur) by\\nblurring the images by a 5 x 5 average \\ufb01lter.\\n\\nAs we can see in Table 2, the generated samples are quantitatively closer to\\nreference attribute in the testing set than nearest attributes in the training set.\\nIn addition, explicit foreground-background modeling produces more accurate\\nsamples in the attribute space.\\n\\nTable 2. Quantitative comparisons on attribute-conditional image generation. The\\nbest out of 10 samples are evaluated by the cosine similarity and mean squared error\\nin the attribute space. We use a pre-trained convolutional neural network to predict\\nattributes of generated samples.\\n\\nModel Cosine Similarity Mean Squared Error\\n\\nNN\\n\\nNNblur\\nCVAE\\n\\ndisCVAE\\n\\n0.8719\\n0.8291\\n0.9054\\n0.9057\\n\\n21.88\\n28.24\\n17.20\\n16.71\\n\\n\\x0c18\\n\\nXinchen Yan, Jimei Yang, Kihyuk Sohn and Honglak Lee\\n\\nReferences\\n\\n1. Bengio, Y., Thibodeau-Laufer, E., Alain, G., Yosinski, J.: Deep generative stochas-\\n\\ntic networks trainable by backprop. arXiv preprint arXiv:1306.1091 (2013)\\n\\n2. Collobert, R., Kavukcuoglu, K., Farabet, C.: Torch7: A matlab-like environment\\n\\nfor machine learning. In: BigLearn, NIPS Workshop (2011)\\n\\n3. Denton, E., Chintala, S., Szlam, A., Fergus, R.: Deep generative image models\\n\\nusing a Laplacian pyramid of adversarial networks. In: NIPS (2015)\\n\\n4. Dosovitskiy, A., Springenberg, J.T., Brox, T.: Learning to generate chairs with\\n\\nconvolutional neural networks. In: CVPR (2015)\\n\\n5. Eigen, D., Puhrsch, C., Fergus, R.: Depth map prediction from a single image using\\n\\na multi-scale deep network. In: NIPS (2014)\\n\\n6. Gatys, L.A., Ecker, A.S., Bethge, M.: Texture synthesis using convolutional neural\\n\\nnetworks. In: NIPS (2015)\\n\\n7. Gauthier, J.: Conditional generative adversarial nets for convolutional face gener-\\n\\nation. Tech. rep. (2015)\\n\\n8. Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S.,\\nCourville, A., Bengio, Y.: Generative adversarial nets. In: NIPS. pp. 2672\\u20132680\\n(2014)\\n\\n9. Gregor, K., Danihelka, I., Graves, A., Wierstra, D.: DRAW: A recurrent neural\\n\\nnetwork for image generation. In: ICML (2015)\\n\\n10. Huang, G.B., Ramesh, M., Berg, T.: Labeled faces in the wild: A database for\\n\\nstudying. month (07-49) (2007)\\n\\n11. Isola, P., Liu, C.: Scene collaging: Analysis and synthesis of natural images with\\n\\nsemantic layers. In: ICCV (2013)\\n\\n12. Kemelmacher-Shlizerman, I., Suwajanakorn, S., Seitz, S.M.: Illumination-aware age\\n\\nprogression. In: CVPR (2014)\\n\\n13. Kingma, D., Ba, J.: ADAM: A method for stochastic optimization. In: ICLR (2015)\\n14. Kingma, D.P., Mohamed, S., Rezende, D.J., Welling, M.: Semi-supervised learning\\n\\nwith deep generative models. In: NIPS (2014)\\n\\n15. Kingma, D.P., Welling, M.: Auto-encoding variational Bayes. In: ICLR (2014)\\n16. Krizhevsky, A., Sutskever, I., Hinton, G.E.: Imagenet classi\\ufb01cation with deep con-\\n\\nvolutional neural networks. In: NIPS (2012)\\n\\n17. Kulkarni, T.D., Whitney, W., Kohli, P., Tenenbaum, J.B.: Deep convolutional in-\\n\\nverse graphics network. In: NIPS (2015)\\n\\n18. Kumar, N., Berg, A.C., Belhumeur, P.N., Nayar, S.K.: Attribute and simile clas-\\n\\nsi\\ufb01ers for face veri\\ufb01cation. In: ICCV (2009)\\n\\n19. Laput, G.P., Dontcheva, M., Wilensky, G., Chang, W., Agarwala, A., Linder, J.,\\nAdar, E.: Pixeltone: a multimodal interface for image editing. In: Proceedings of\\nthe SIGCHI Conference on Human Factors in Computing Systems (2013)\\n\\n20. Le Roux, N., Heess, N., Shotton, J., Winn, J.: Learning a generative model of\\nimages by factoring appearance and shape. Neural Computation 23(3), 593\\u2013650\\n(2011)\\n\\n21. Lee, H., Grosse, R., Ranganath, R., Ng, A.Y.: Convolutional deep belief networks\\nfor scalable unsupervised learning of hierarchical representations. In: ICML (2009)\\n22. Ngiam, J., Khosla, A., Kim, M., Nam, J., Lee, H., Ng, A.Y.: Multimodal deep\\n\\nlearning. In: ICML (2011)\\n\\n23. Nitzberg, M., Mumford, D.: The 2.1-d sketch. In: ICCV (1990)\\n24. Porter, T., Du\\ufb00, T.: Compositing digital images. In: ACM Siggraph Computer\\n\\nGraphics. vol. 18, pp. 253\\u2013259. ACM (1984)\\n\\n\\x0cAttribute2Image: Conditional Image Generation from Visual Attributes\\n\\n19\\n\\n25. Radford, A., Metz, L., Chintala, S.: Unsupervised representation learning with deep\\nconvolutional generative adversarial networks. arXiv preprint arXiv:1511.06434\\n(2015)\\n\\n26. Ranzato, M., Mnih, V., Hinton, G.E.: Generating more realistic images using gated\\n\\nmrfs. In: NIPS (2010)\\n\\n27. Reed, S., Sohn, K., Zhang, Y., Lee, H.: Learning to disentangle factors of variation\\n\\nwith manifold interaction. In: ICML (2014)\\n\\n28. Rezende, D.J., Mohamed, S., Wierstra, D.: Stochastic backpropagation and ap-\\n\\nproximate inference in deep generative models. In: ICML (2014)\\n\\n29. Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale\\n\\nimage recognition. arXiv preprint arXiv:1409.1556 (2014)\\n\\n30. Smolensky, P.: Information processing in dynamical systems: Foundations of har-\\nmony theory. In: Rumelhart, D.E., McClelland, J.L. (eds.) Parallel Distributed\\nProcessing: Volume 1 (1986)\\n\\n31. Sohn, K., Shang, W., Lee, H.: Improved multimodal deep learning with variation\\n\\nof information. In: NIPS (2014)\\n\\n32. Srivastava, A., Lee, A.B., Simoncelli, E.P., Zhu, S.C.: On advances in statistical\\nmodeling of natural images. Journal of mathematical imaging and vision 18(1),\\n17\\u201333 (2003)\\n\\n33. Srivastava, N., Salakhutdinov, R.R.: Multimodal learning with deep Boltzmann\\n\\nmachines. In: NIPS (2012)\\n\\n34. Tang, Y., Salakhutdinov, R.: Learning stochastic feedforward neural networks. In:\\n\\nNIPS (2013)\\n\\n35. Tang, Y., Salakhutdinov, R., Hinton, G.: Robust boltzmann machines for recogni-\\n\\ntion and denoising. In: CVPR (2012)\\n\\n36. Tu, Z.: Learning generative models via discriminative approaches. In: CVPR (2007)\\n37. Wah, C., Branson, S., Welinder, P., Perona, P., Belongie, S.: The Caltech-UCSD\\n\\nbirds-200-2011 dataset (2011)\\n\\n38. Wang, J.Y., Adelson, E.H.: Representing moving images with layers. Image Pro-\\n\\ncessing, IEEE Transactions on 3(5), 625\\u2013638 (1994)\\n\\n39. Williams, C.K., Titsias, M.K.: Greedy learning of multiple objects in images us-\\ning robust statistics and factorial learning. Neural Computation 16(5), 1039\\u20131062\\n(2004)\\n\\n40. Yang, F., Wang, J., Shechtman, E., Bourdev, L., Metaxas, D.: Expression \\ufb02ow for\\n\\n3D-aware face component transfer. In: SIGGRAPH (2011)\\n\\n41. Yang, Y., Hallman, S., Ramanan, D., Fowlkes, C.C.: Layered object models for\\n\\nimage segmentation. PAMI 34(9), 1731\\u20131743 (2012)\\n\\n42. Yosinski, J., Clune, J., Nguyen, A., Fuchs, T., Lipson, H.: Understanding neural\\n\\nnetworks through deep visualization. arXiv preprint arXiv:1506.06579 (2015)\\n\\n43. Zhu, S., Li, C., Loy, C.C., Tang, X.: Transferring landmark annotations for cross-\\n\\ndataset face alignment. arXiv preprint arXiv:1409.0602 (2014)\\n\\n\\x0c', u'Multimodal Learning with Deep Boltzmann Machines\\n\\nNitish Srivastava\\n\\nRuslan Salakhutdinov\\n\\nDepartment of Computer Science\\n\\nDepartment of Statistics and Computer Science\\n\\nUniversity of Toronto\\n\\nnitish@cs.toronto.edu\\n\\nUniversity of Toronto\\n\\nrsalakhu@cs.toronto.edu\\n\\nAbstract\\n\\nA Deep Boltzmann Machine is described for learning a generative model of data\\nthat consists of multiple and diverse input modalities. The model can be used\\nto extract a uni\\ufb01ed representation that fuses modalities together. We \\ufb01nd that\\nthis representation is useful for classi\\ufb01cation and information retrieval tasks. The\\nmodel works by learning a probability density over the space of multimodal inputs.\\nIt uses states of latent variables as representations of the input. The model can\\nextract this representation even when some modalities are absent by sampling\\nfrom the conditional distribution over them and \\ufb01lling them in. Our experimental\\nresults on bi-modal data consisting of images and text show that the Multimodal\\nDBM can learn a good generative model of the joint space of image and text\\ninputs that is useful for information retrieval from both unimodal and multimodal\\nqueries. We further demonstrate that this model signi\\ufb01cantly outperforms SVMs\\nand LDA on discriminative tasks. Finally, we compare our model to other deep\\nlearning methods, including autoencoders and deep belief networks, and show that\\nit achieves noticeable gains.\\n\\n1\\n\\nIntroduction\\n\\nInformation in the real world comes through multiple input channels. Images are associated with\\ncaptions and tags, videos contain visual and audio signals, sensory perception includes simultaneous\\ninputs from visual, auditory, motor and haptic pathways. Each modality is characterized by very\\ndistinct statistical properties which make it dif\\ufb01cult to ignore the fact that they come from different\\ninput channels. Useful representations can be learned about such data by fusing the modalities into a\\njoint representation that captures the real-world \\u2018concept\\u2019 that the data corresponds to. For example,\\nwe would like a probabilistic model to correlate the occurrence of the words \\u2018beautiful sunset\\u2019 and\\nthe visual properties of an image of a beautiful sunset and represent them jointly, so that the model\\nassigns high probability to one conditioned on the other.\\nBefore we describe our model in detail, it is useful to note why such a model is required. Different\\nmodalities typically carry different kinds of information. For example, people often caption an image\\nto say things that may not be obvious from the image itself, such as the name of the person, place,\\nor a particular object in the picture. Unless we do multimodal learning, it would not be possible to\\ndiscover a lot of useful information about the world (for example, \\u2018what do beautiful sunsets look\\nlike?\\u2019). We cannot afford to have discriminative models for each such concept and must extract this\\ninformation from unlabeled data.\\nIn a multimodal setting, data consists of multiple input modalities, each modality having a different\\nkind of representation and correlational structure. For example, text is usually represented as dis-\\ncrete sparse word count vectors, whereas an image is represented using pixel intensities or outputs\\nof feature extractors which are real-valued and dense. This makes it much harder to discover rela-\\ntionships across modalities than relationships among features in the same modality. There is a lot\\nof structure in the input but it is dif\\ufb01cult to discover the highly non-linear relationships that exist\\n\\n1\\n\\n\\x0cFigure 1: Left: Examples of text generated from a DBM by sampling from P (vtxt|vimg, \\u03b8). Right: Exam-\\nples of images retrieved using features generated from a DBM by sampling from P (vimg|vtxt, \\u03b8).\\n\\nbetween low-level features across different modalities. Moreover, these observations are typically\\nvery noisy and may have missing values.\\nA good multimodal learning model must satisfy certain properties. The joint representation must be\\nsuch that similarity in the representation space implies similarity of the corresponding \\u2018concepts\\u2019. It\\nis also desirable that the joint representation be easy to obtain even in the absence of some modalities.\\nIt should also be possible to \\ufb01ll-in missing modalities given the observed ones. In addition, it is also\\ndesirable that extracted representation be useful for discriminative tasks.\\nOur proposed multimodal Deep Boltzmann Machine (DBM) model satis\\ufb01es the above desiderata.\\nDBMs are undirected graphical models with bipartite connections between adjacent layers of hid-\\nden units [1]. The key idea is to learn a joint density model over the space of multimodal inputs.\\nMissing modalities can then be \\ufb01lled-in by sampling from the conditional distributions over them\\ngiven the observed ones. For example, we use a large collection of user-tagged images to learn a\\njoint distribution over images and text P (vimg, vtxt|\\u03b8). By drawing samples from P (vtxt|vimg, \\u03b8)\\nand from P (vimg|vtxt, \\u03b8) we can \\ufb01ll-in missing data, thereby doing image annotation and image\\nretrieval respectively, as shown in Fig. 1.\\nThere have been several approaches to learning from multimodal data. In particular, Huiskes et\\nal. [2] showed that using captions, or tags, in addition to standard low-level image features signi\\ufb01-\\ncantly improves classi\\ufb01cation accuracy of Support Vector Machines (SVM) and Linear Discriminant\\nAnalysis (LDA) models. A similar approach of Guillaumin et al. [3], based on multiple kernel learn-\\ning framework, further demonstrated that an additional text modality can improve the accuracy of\\nSVMs on various object recognition tasks. However, all of these approaches are discriminative by\\nnature and cannot make use of large amounts of unlabeled data or deal easily with missing input\\nmodalities.\\nOn the generative side, Xing et al. [4] used dual-wing harmoniums to build a joint model of im-\\nages and text, which can be viewed as a linear RBM model with Gaussian hidden units together\\nwith Gaussian and Poisson visible units. However, various data modalities will typically have very\\ndifferent statistical properties which makes it dif\\ufb01cult to model them using shallow models. Most\\nsimilar to our work is the recent approach of Ngiam et al. [5] that used a deep autoencoder for speech\\nand vision fusion. There are, however, several crucial differences. First, in this work we focus on\\nintegrating together very different data modalities: sparse word count vectors and real-valued dense\\nimage features. Second, we develop a Deep Boltzmann Machine as a generative model as opposed\\nto unrolling the network and \\ufb01ne-tuning it as an autoencoder. While both approaches have lead\\nto interesting results in several domains, using a generative model is important for applications we\\nconsider in this paper, as it allows our model to naturally handle missing data modalities.\\n\\n2\\n\\n\\x0c2 Background: RBMs and Their Generalizations\\nRestricted Boltzmann Machines (RBMs) have been used effectively in modeling distributions over\\nbinary-valued data. Recent work on Boltzmann machine models and their generalizations to expo-\\nnential family distributions have allowed these models to be successfully used in many application\\ndomains. In particular, the Replicated Softmax model [6] has been shown to be effective in model-\\ning sparse word count vectors, whereas Gaussian RBMs have been used for modeling real-valued\\ninputs for speech and vision tasks. In this section we brie\\ufb02y review these models, as they will serve\\nas our building blocks for the multimodal model.\\n2.1 Restricted Boltzmann Machines\\nA Restricted Boltzmann Machine is an undirected graphical model with stochastic visible units\\nv \\u2208 {0, 1}D and stochastic hidden units h \\u2208 {0, 1}F , with each visible unit connected to each\\nhidden unit. The model de\\ufb01nes the following energy function E : {0, 1}D+F \\u2192 R:\\n\\nE(v, h; \\u03b8) = \\u2212 D(cid:88)\\n\\nF(cid:88)\\n\\nviWijhj \\u2212 D(cid:88)\\n\\nbivi \\u2212 F(cid:88)\\n\\najhj\\n\\nwhere \\u03b8 = {a, b, W} are the model parameters. The joint distribution over the visible and hidden\\nunits is de\\ufb01ned by:\\n(1)\\n\\nexp (\\u2212E(v, h; \\u03b8)).\\n\\nP (v, h; \\u03b8) =\\n\\ni=1\\n\\nj=1\\n\\ni=1\\n\\nj=1\\n\\n1\\nZ(\\u03b8)\\n\\n2.2 Gaussian RBM\\nConsider modeling visible real-valued units v \\u2208 RD, and let h \\u2208 {0, 1}F be binary stochastic\\nhidden units. The energy of the state {v, h} of the Gaussian RBM is de\\ufb01ned as follows:\\n\\nD(cid:88)\\n\\ni=1\\n\\n(vi \\u2212 bi)2\\n\\n2\\u03c32\\ni\\n\\n\\u2212 D(cid:88)\\n\\nF(cid:88)\\n\\ni=1\\n\\nj=1\\n\\nvi\\n\\u03c3i\\n\\nWijhj \\u2212 F(cid:88)\\n\\nj=1\\n\\nE(v, h; \\u03b8) =\\n\\najhj,\\n\\n(2)\\n\\nwhere \\u03b8 = {a, b, W, \\u03c3} are the model parameters.\\n\\n2.3 Replicated Softmax Model\\nThe Replicated Softmax Model is useful for modeling sparse count data, such as word count vectors\\nin a document. Let v \\u2208 NK be a vector of visible units where vk is the number of times word k\\noccurs in the document with the vocabulary of size K. Let h \\u2208 {0, 1}F be binary stochastic hidden\\ntopic features. The energy of the state {v, h} is de\\ufb01ned as follows\\n\\nE(v, h; \\u03b8) = \\u2212 K(cid:88)\\n\\nvkWkjhj \\u2212 K(cid:88)\\nwhere \\u03b8 = {a, b, W} are the model parameters and M =(cid:80)\\n\\nF(cid:88)\\n\\nk=1\\n\\nk=1\\n\\nj=1\\n\\nk vk is the total number of words in\\na document. We note that this replicated softmax model can also be interpreted as an RBM model\\nthat uses a single visible multinomial unit with support {1, ..., K} which is sampled M times.\\nFor all of the above models, exact maximum likelihood learning is intractable. In practice, ef\\ufb01cient\\nlearning is performed using Contrastive Divergence (CD) [7].\\n\\nbkvk \\u2212 M\\n\\najhj\\n\\n(3)\\n\\nF(cid:88)\\n\\nj=1\\n\\n3 Multimodal Deep Boltzmann Machine\\n\\nA Deep Boltzmann Machine (DBM) is a network of symmetrically coupled stochastic binary units.\\nIt contains a set of visible units v \\u2208 {0, 1}D, and a sequence of layers of hidden units h(1) \\u2208\\n{0, 1}F1, h(2) \\u2208 {0, 1}F2,..., h(L) \\u2208 {0, 1}FL. There are connections only between hidden units\\nin adjacent layers. Let us \\ufb01rst consider a DBM with two hidden layers. The energy of the joint\\ncon\\ufb01guration {v, h} is de\\ufb01ned as (ignoring bias terms):\\n\\nE(v, h; \\u03b8) = \\u2212v(cid:62)W(1)h(1) \\u2212 h(1)(cid:62)W(2)h(2),\\n\\nwhere h = {h(1), h(2)} represent the set of hidden units, and \\u03b8 = {W(1), W(2)} are the model pa-\\nrameters, representing visible-to-hidden and hidden-to-hidden symmetric interaction terms. Similar\\nto RBMs, this binary-binary DBM can be easily extended to modeling dense real-valued or sparse\\ncount data, which we discuss next.\\n\\n3\\n\\n\\x0cImage-speci\\ufb01c DBM Text-speci\\ufb01c DBM\\n\\nMultimodal DBM\\n\\nFigure 2: Left: Image-speci\\ufb01c two-layer DBM that uses a Gaussian model to model the distribution over real-\\nvalued image features. Middle: Text-speci\\ufb01c two-layer DBM that uses a Replicated Softmax model to model\\nits distribution over the word count vectors. Right: A Multimodal DBM that models the joint distribution over\\nimage and text inputs.\\nWe illustrate the construction of a multimodal DBM using an image-text bi-modal DBM as our\\nrunning example. Let vm \\u2208 RD denote an image input and vt \\u2208 NK denote a text input. Consider\\nmodeling each data modality using separate two-layer DBMs (Fig. 2). The image-speci\\ufb01c two-layer\\nDBM assigns probability to vm that is given by (ignoring bias terms on the hidden units for clarity):\\n(4)\\n\\nP (vm, h(1), h(2); \\u03b8) =\\n\\n(cid:88)\\n\\nP (vm; \\u03b8) =\\n\\nh(1),h(2)\\n\\n(cid:88)\\n\\n\\uf8eb\\uf8ed\\u2212 D(cid:88)\\n\\nexp\\n\\nh(1),h(2)\\n\\ni=1\\n\\n(vmi \\u2212 bi)2\\n\\n2\\u03c32\\ni\\n\\n+\\n\\nD(cid:88)\\n\\nF1(cid:88)\\n\\ni=1\\n\\nj=1\\n\\n=\\n\\n1\\nZ(\\u03b8)\\n\\nF1(cid:88)\\n\\nF2(cid:88)\\n\\nj=1\\n\\nl=1\\n\\nvmi\\n\\u03c3i\\n\\nW (1)\\n\\nij h(1)\\n\\nj +\\n\\nh(1)\\nj W (2)\\n\\njl h(2)\\n\\nl\\n\\n\\uf8f6\\uf8f8 .\\n\\nNote that we borrow the visible-hidden interaction term from the Gaussian RBM (Eq. 2) and the\\nhidden-hidden one from the Binary RBM (Eq. 1). Similarly, the text-speci\\ufb01c DBM will use terms\\nfrom the Replicated Softmax model for the visible-hidden interactions (Eq. 3) and the hidden-hidden\\nones from the Binary RBM (Eq. 1).\\nTo form a multimodal DBM, we combine the two models by adding an additional layer of binary\\nhidden units on top of them. The resulting graphical model is shown in Fig. 2, right panel. The joint\\ndistribution over the multi-modal input can be written as:\\n\\n(cid:19)(cid:18)(cid:88)\\n\\n(cid:19)\\n\\nP (vm, vt; \\u03b8)=\\n\\nP (h(2)\\n\\nm , h(2)\\n\\nt\\n\\n, h(3))\\n\\nP (vm, h(1)\\n\\nm , h(2)\\nm )\\n\\nP (vt, h(1)\\n\\nt\\n\\n, h(2)\\nt )\\n\\n.\\n\\nh(2)\\n\\nm ,h(2)\\n\\nt\\n\\n,h(3)\\n\\nh(1)\\nm\\n\\nh(1)\\n\\nt\\n\\n(cid:88)\\n\\n(cid:18)(cid:88)\\n\\nt\\n\\n(cid:18) F1(cid:89)\\n\\nt\\n\\n, h(3)}:\\nmj|v)\\n\\nF2(cid:89)\\n\\nq(h(1)\\n\\n3.1 Approximate Learning and Inference\\nExact maximum likelihood learning in this model is intractable, but ef\\ufb01cient approximate learning\\ncan be carried out by using mean-\\ufb01eld inference to estimate data-dependent expectations, and an\\nMCMC based stochastic approximation procedure to approximate the model\\u2019s expected suf\\ufb01cient\\nstatistics [1]. In particular, during the inference step, we approximate the true posterior P (h|v; \\u03b8),\\nwhere v = {vm, vt}, with a fully factorized approximating distribution over the \\ufb01ve sets of hidden\\nunits {h(1)\\n\\nm , h(1)\\n\\nm , h(2)\\n\\n, h(2)\\n\\n(cid:19)(cid:18) F1(cid:89)\\n\\nF2(cid:89)\\n\\n(cid:19) F3(cid:89)\\n\\nQ(h|v; \\xb5) =\\n\\nq(h(2)\\n\\nml|v)\\n\\nq(h(1)\\n\\ntj |v)\\n\\nq(h(2)\\n\\ntl |v)\\n\\nq(h(3)\\n\\nk |v),\\n\\n(5)\\n\\nj=1\\n\\nl=1\\n\\nj=1\\n\\nl=1\\n\\nk=1\\n\\nt\\n\\nt\\n\\n, \\xb5(2)\\n\\nm , \\xb5(1)\\n\\nm , \\xb5(2)\\n\\n, \\xb5(3)} are the mean-\\ufb01eld parameters with q(h(l)\\n\\nwhere \\xb5 = {\\xb5(1)\\nl = 1, 2, 3.\\nLearning proceeds by \\ufb01nding the value of \\xb5 that maximizes the variational lower bound for the\\ncurrent value of model parameters \\u03b8, which results in a set of the mean-\\ufb01eld \\ufb01xed-point equations.\\nGiven the variational parameters \\xb5, the model parameters \\u03b8 are then updated to maximize the vari-\\national bound using an MCMC-based stochastic approximation [1, 8, 9].\\nTo initialize the model parameters to good values, we use a greedy layer-wise pretraining strategy\\nby learning a stack of modi\\ufb01ed RBMs (for details see [1]).\\n\\ni = 1) = \\xb5(l)\\n\\nfor\\n\\ni\\n\\n4\\n\\n\\x0c(a) RBM\\n\\n(b) Multimodal DBN\\n\\n(c) Multimodal DBM\\n\\nFigure 3: Different ways of combining multimodal inputs\\n\\n3.2 Salient Features\\nA Multimodal DBM can be viewed as a composition of unimodal undirected pathways. Each path-\\nway can be pretrained separately in a completely unsupervised fashion, which allows us to leverage\\na large supply of unlabeled data. Any number of pathways each with any number of layers could\\npotentially be used. The type of the lower-level RBMs in each pathway could be different, account-\\ning for different input distributions, as long as the \\ufb01nal hidden representations at the end of each\\npathway are of the same type.\\nThe intuition behind our model is as follows. Each data modality has very different statistical prop-\\nerties which make it dif\\ufb01cult for a single hidden layer model (such as Fig. 3a) to directly \\ufb01nd corre-\\nlations across modalities. In our model, this difference is bridged by putting layers of hidden units\\nbetween the modalities. The idea is illustrated in Fig. 3c, which is just a different way of display-\\ning Fig. 2. Compared to the simple RBM (Fig. 3a), where the hidden layer h directly models the\\ndistribution over vt and vm, the \\ufb01rst layer of hidden units h(1)\\nm in a DBM has an easier task to\\nperform - that of modeling the distribution over vm and h(2)\\nm . Each layer of hidden units in the\\nDBM contributes a small part to the overall task of modeling the distribution over vm and vt. In the\\nprocess, each layer learns successively higher-level representations and removes modality-speci\\ufb01c\\ncorrelations. Therefore, the middle layer in the network can be seen as a (relatively) \\u201cmodality-free\\u201d\\nrepresentation of the input as opposed to the input layers which were \\u201cmodality-full\\u201d.\\nAnother way of using a deep model to combine multimodal inputs is to use a Multimodal Deep\\nBelief Network (DBN) (Fig. 3b) which consists of an RBM followed by directed belief networks\\nleading out to each modality. We emphasize that there is an important distinction between this model\\nand the DBM model of Fig. 3c. In a DBN model the responsibility of the multimodal modeling falls\\nentirely on the joint layer. In the DBM, on the other hand, this responsibility is spread out over the\\nentire network. The modality fusion process is distributed across all hidden units in all layers. From\\nthe generative perspective, states of low-level hidden units in one pathway can in\\ufb02uence the states\\nof hidden units in other pathways through the higher-level layers, which is not the case for DBNs.\\n\\n3.3 Modeling Tasks\\nGenerating Missing Modalities: As argued in the introduction, many real-world applications will\\noften have one or more modalities missing. The Multimodal DBM can be used to generate such\\nmissing data modalities by clamping the observed modalities at the inputs and sampling the hidden\\nmodalities from the conditional distribution by running the standard alternating Gibbs sampler [1].\\nFor example, consider generating text conditioned on a given image1 vm. The observed modality\\nvm is clamped at the inputs and all hidden units are initialized randomly. P (vt|vm) is a multi-\\nnomial distribution over the vocabulary. Alternating Gibbs sampling can be used to sample words\\nfrom P (vt|vm). Fig. 1 shows examples of words that have high probability under the conditional\\ndistributions.\\nInferring Joint Representations: The model can also be used to generate a fused representation\\nthat multiple data modalities. This fused representation is inferred by clamping the observed modal-\\nities and doing alternating Gibbs sampling to sample from P (h(3)|vm, vt) (if both modalities are\\npresent) or from P (h(3)|vm) (if text is missing). A faster alternative, which we adopt in our experi-\\nmental results, is to use variational inference (see Sec. 3.1) to approximate posterior Q(h(3)|vm, vt)\\nor Q(h(3)|vm). The activation probabilities of hidden units h(3) constitute the joint representation\\nof the inputs.\\n\\n1Generating image features conditioned on text can be done in a similar way.\\n\\n5\\n\\n\\x0cThis representation can then be used to do information retrieval for multimodal or unimodal queries.\\nEach data point in the database (whether missing some modalities or not) can be mapped to this\\nlatent space. Queries can also be mapped to this space and an appropriate distance metric can be\\nused to retrieve results that are close to the query.\\nDiscriminative Tasks: Classi\\ufb01ers such as SVMs can be trained with these fused representations as\\ninputs. Alternatively, the model can be used to initialize a feed forward network which can then be\\n\\ufb01netuned [1]. In our experiments, logistic regression was used to classify the fused representations.\\nUnlike \\ufb01netuning, this ensures that all learned representations that we compare (DBNs, DBMs and\\nDeep Autoencoders) use the same discriminative model.\\n4 Experiments\\n4.1 Dataset and Feature Extraction\\nThe MIR Flickr Data set [10] was used in our experiments. The data set consists of 1 million images\\nretrieved from the social photography website Flickr along with their user assigned tags. Among the\\n1 million images, 25,000 have been annotated for 24 topics including object categories such as, bird,\\ntree, people and scene categories, such as indoor, sky and night. A stricter labeling was done for\\n14 of these classes where an image was annotated with a category only if that category was salient.\\nThis leads to a total of 38 classes where each image may belong to several classes. The unlabeled\\n975,000 images were used only for pretraining. We use 15,000 images for training and 10,000 for\\ntesting, following Huiskes et al. [2]. Mean Average Precision (MAP) is used as the performance\\nmetric. Results are averaged over 5 random splits of training and test sets.\\nEach text input was represented using a vocabulary of the 2000 most frequent tags. The average\\nnumber of tags associated with an image is 5.15 with a standard deviation of 5.13. There are 128,501\\nimages which do not have any tags, out of which 4,551 are in the labeled set. Hence about 18% of the\\nlabeled data has images but is missing text. Images were represented by 3857-dimensional features,\\nthat were extracted by concatenating Pyramid Histogram of Words (PHOW) features [11], Gist [12]\\nand MPEG-7 descriptors [13] (EHD, HTD, CSD, CLD, SCD). Each dimension was mean-centered\\nand normalized to unit variance. PHOW features are bags of image words obtained by extracting\\ndense SIFT features over multiple scales and clustering them. We used publicly available code\\n( [14, 15]) for extracting these features.\\n\\n4.2 Model Architecture and Learning\\nThe image pathway consists of a Gaussian RBM with 3857 visible units followed by 2 layers of\\n1024 hidden units. The text pathway consists of a Replicated Softmax Model with 2000 visible\\nunits followed by 2 layers of 1024 hidden units. The joint layer contains 2048 hidden units. Each\\nlayer of weights was pretrained using PCD for initializing the DBM model. When learning the\\nDBM model, all word count vectors were scaled so that they sum to 5. This avoids running separate\\nMarkov chains for each word count to get the model distribution\\u2019s suf\\ufb01cient statistics.\\nEach pathway was pretrained using a stack of modi\\ufb01ed RBMs. Each Gaussian unit has unit vari-\\nance that was kept \\ufb01xed. For discriminative tasks, we perform 1-vs-all classi\\ufb01cation using logistic\\nregression on the joint hidden layer representation. We further split the 15K training set into 10K\\nfor training and 5K for validation.\\n\\n4.3 Classi\\ufb01cation Tasks\\nMultimodal Inputs: Our \\ufb01rst set of experiments, evaluate the DBM as a discriminative model\\nfor multimodal data. For each model that we trained, the fused representation of the data was\\nextracted and feed to a separate logistic regression for each of the 38 topics. The text input layer\\nin the DBM was left unclamped when the text was missing. Fig. 4 summarizes the Mean Average\\nPrecision (MAP) and precision@50 (precision at top 50 predictions) obtained by different models.\\nLinear Discriminant Analysis (LDA) and Support Vector Machines (SVMs) [2] were trained using\\nthe labeled data on concatenated image and text features that did not include SIFT-based features.\\nHence, to make a fair comparison, our model was \\ufb01rst trained using only labeled data with a similar\\nset of features (i.e., excluding our SIFT-based features). We call this model DBM-Lab. Fig. 4\\nshows that the DBM-Lab model already outperforms its competitor SVM and LDA models. DBM-\\nLab achieves a MAP of 0.526, compared to 0.475 and 0.492, achieved by SVM and LDA models.\\n\\n6\\n\\n\\x0cMultimodal Inputs\\n\\nModel\\n\\nRandom\\nLDA [2]\\nSVM [2]\\nDBM-Lab\\nDBM-Unlab\\nDBN\\nAutoencoder (based on [5])\\nDBM\\n\\nMAP\\n\\n0.124\\n0.492\\n0.475\\n0.526\\n0.585\\n0.599\\n0.600\\n0.609\\n\\nUnimodal Inputs\\n\\nPrec@50\\n\\n0.124\\n0.754\\n0.758\\n0.791\\n0.836\\n0.867\\n0.875\\n0.873\\n\\nModel\\n\\nImage-SVM [2]\\nImage-DBN\\nImage-DBM\\nDBM-ZeroText\\nDBM-GenText\\n\\nMAP\\n\\n0.375\\n0.463\\n0.469\\n0.522\\n0.531\\n\\nPrec@50\\n\\n-\\n\\n0.801\\n0.803\\n0.827\\n0.832\\n\\nFigure 4: Classi\\ufb01cation Results. Left: Mean Average Precision (MAP) and precision@50 obtained by differ-\\nent models. Right: MAP using representations from different layers of multimodal DBMs and DBNs.\\nTo measure the effect of using unlabeled data, a DBM was trained using all the unlabeled examples\\nthat had both modalities present. We call this model DBM-Unlab. The only difference between the\\nDBM-Unlab and DBM-Lab models is that DBM-Unlab used unlabeled data during its pretraining\\nstage. The input features for both models remained the same. Not surprisingly, the DBM-Unlab\\nmodel signi\\ufb01cantly improved upon DBM-Lab achieving a MAP of 0.585. Our third model, DBM,\\nwas trained using additional SIFT-based features. Adding these features improves the MAP to 0.609.\\nWe compared our model to two other deep learning models: Multimodal Deep Belief Network\\n(DBN) and a deep Autoencoder model [5]. These models were trained with the same number of\\nlayers and hidden units as the DBM. The DBN achieves a MAP of 0.599 and the autoencoder\\ngets 0.600. Their performance was comparable but slightly worse than that of the DBM. In terms\\nof precision@50, the autoencoder performs marginally better than the rest. We also note that the\\nMultiple Kernel Learning approach proposed in Guillaumin et. al. [3] achieves a MAP of 0.623 on\\nthe same dataset. However, they used a much larger set of image features (37,152 dimensions).\\nUnimodal Inputs: Next, we evaluate the ability of the model to improve classi\\ufb01cation of unimodal\\ninputs by \\ufb01lling in other modalities. For multimodal models, the text input was only used during\\ntraining. At test time, all models were given only image inputs.\\nFig. 4 compares the Multimodal DBM model with an SVM over image features alone (Image-\\nSVM) [2], a DBN over image features (Image-DBN) and a DBM over image features (Image-\\nDBM). All deep models had the same depth and same number of hidden units in each layer. Results\\nare reported for two different settings for the Multimodal DBM at test time. In one case (DBM-\\nZeroText), the state of the joint hidden layer was inferred keeping the missing text input clamped at\\nzero. In the other case (DBM-GenText), the text input was not clamped and the model was allowed\\nto update the state of the text input layer when performing mean-\\ufb01eld updates. In doing so, the\\nmodel effectively \\ufb01lled-in the missing text modality (some examples of which are shown in Fig. 1).\\nThese two settings helped to ascertain the contribution to the improvement that comes from \\ufb01lling\\nin the missing modality.\\nThe DBM-GenText model performs better than all other models, showing that the DBM is able to\\ngenerate meaningful text that serves as a plausible proxy for missing data. Interestingly, the DBM-\\nZeroText model does better than any unimodal model. This suggests that learning multimodal fea-\\ntures helps even when some modalities are absent at test time. Having multiple modalities probably\\nregularizes the model and makes it learn much better features. Moreover, this means that we do not\\nneed to learn separate models to handle each possible combination of missing data modalities. One\\njoint model can be deployed at test time and used for any situation that may arise.\\nEach layer of the DBM provides a different representation of the input. Fig. 4, right panel, shows\\nthe MAP obtained by using each of these representations for classi\\ufb01cation using logistic regression.\\nThe input layers, shown at the extreme ends, are not very good at representing useful features. As\\nwe go deeper into the model from either input layer towards the middle, the internal representations\\nget better. The joint layer in the middle serves as the most useful feature representation. Observe\\nthat the performance of any DBM layer is always better than the corresponding DBN layer, though\\nthey get close at the joint layer.\\n\\n7\\n\\n\\x0c(a) Multimodal Queries\\n\\n(b) Unimodal Queries\\n\\nFigure 5: Precision-Recall curves for Retrieval Tasks\\n\\nFigure 6: Retrieval Results for Multimodal Queries from the DBM model\\n\\n4.4 Retrieval Tasks\\nMultimodal Queries: The next set of experiments was designed to evaluate the quality of the\\nlearned joint representations. A database of images was created by randomly selecting 5000 image-\\ntext pairs from the test set. We also randomly selected a disjoint set of 1000 images to be used as\\nqueries. Each query contained both image and text modalities. Binary relevance labels were created\\nby assuming that if any of the 38 class labels overlapped between a query and a data point, then that\\ndata point is relevant to the query.\\nFig. 5a shows the precision-recall curves for the DBM, DBN, and Autoencoder models (averaged\\nover all queries). For each model, all queries and all points in the database were mapped to the joint\\nhidden representation under that model. Cosine similarity function was used to match queries to data\\npoints. The DBM model performs the best among the compared models achieving a MAP of 0.622.\\nThe autoencoder and DBN models perform worse with a MAP of 0.612 and 0.609 respectively.\\nFig. 6 shows some examples of multimodal queries and the top 4 retrieved results. Note that even\\nthough there is little overlap in terms of text, the model is able to perform well.\\nUnimodal Queries: The DBM model can also be used to query for unimodal inputs by \\ufb01lling in\\nthe missing modality. Fig. 5b shows the precision-recall curves for the DBM model along with\\nother unimodal models, where each model received the same image queries as input. By effectively\\ninferring the missing text, the DBM model was able to achieve far better results than any unimodal\\nmethod (MAP of 0.614 as compared to 0.587 for an Image-DBM and 0.578 for an Image-DBN).\\n\\n5 Conclusion\\nWe proposed a Deep Boltzmann Machine model for learning multimodal data representations. Large\\namounts of unlabeled data can be effectively utilized by the model. Pathways for each modality can\\nbe pretrained independently and \\u201cplugged in\\u201d together for doing joint training. The model fuses\\nmultiple data modalities into a uni\\ufb01ed representation. This representation captures features that are\\nuseful for classi\\ufb01cation and retrieval. It also works nicely when some modalities are absent and\\nimproves upon models trained on only the observed modalities.\\nAcknowledgments: This research was supported by OGS, NSERC and by Early Researcher Award.\\n\\n8\\n\\n\\x0cReferences\\n[1] R. R. Salakhutdinov and G. E. Hinton. Deep Boltzmann machines.\\n\\nInternational Conference on Arti\\ufb01cial Intelligence and Statistics, volume 12, 2009.\\n\\nIn Proceedings of the\\n\\n[2] Mark J. Huiskes, Bart Thomee, and Michael S. Lew. New trends and ideas in visual concept\\ndetection: the MIR \\ufb02ickr retrieval evaluation initiative. In Multimedia Information Retrieval,\\npages 527\\u2013536, 2010.\\n\\n[3] M. Guillaumin, J. Verbeek, and C. Schmid. Multimodal semi-supervised learning for image\\nclassi\\ufb01cation. In Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference\\non, pages 902 \\u2013909, june 2010.\\n\\n[4] Eric P. Xing, Rong Yan, and Alexander G. Hauptmann. Mining associated text and images\\n\\nwith dual-wing harmoniums. In UAI, pages 633\\u2013641. AUAI Press, 2005.\\n\\n[5] Jiquan Ngiam, Aditya Khosla, Mingyu Kim, Juhan Nam, Honglak Lee, and Andrew Y. Ng.\\nMultimodal deep learning. In International Conference on Machine Learning (ICML), Belle-\\nvue, USA, June 2011.\\n\\n[6] Ruslan Salakhutdinov and Geoffrey E. Hinton. Replicated softmax: an undirected topic model.\\n\\nIn NIPS, pages 1607\\u20131614. Curran Associates, Inc., 2009.\\n\\n[7] Geoffrey E. Hinton. Training products of experts by minimizing contrastive divergence. Neural\\n\\nComputation, 14(8):1711\\u20131800, 2002.\\n\\n[8] T. Tieleman. Training restricted Boltzmann machines using approximations to the likelihood\\n\\ngradient. In ICML. ACM, 2008.\\n\\n[9] L. Younes. On the convergence of Markovian stochastic algorithms with rapidly decreasing\\n\\nergodicity rates, March 17 2000.\\n\\n[10] Mark J. Huiskes and Michael S. Lew. The MIR Flickr retrieval evaluation.\\n\\nIn MIR \\u201908:\\nProceedings of the 2008 ACM International Conference on Multimedia Information Retrieval,\\nNew York, NY, USA, 2008. ACM.\\n\\n[11] A Bosch, Andrew Zisserman, and X Munoz. Image classi\\ufb01cation using random forests and\\n\\nferns. IEEE 11th International Conference on Computer Vision (2007), 23:1\\u20138, 2007.\\n\\n[12] Aude Oliva and Antonio Torralba. Modeling the shape of the scene: A holistic representation\\n\\nof the spatial envelope. International Journal of Computer Vision, 42:145\\u2013175, 2001.\\n\\n[13] B.S. Manjunath, J.-R. Ohm, V.V. Vasudevan, and A. Yamada. Color and texture descriptors.\\n\\nCircuits and Systems for Video Technology, IEEE Transactions on, 11(6):703 \\u2013715, 2001.\\n\\n[14] A. Vedaldi and B. Fulkerson. VLFeat: An open and portable library of computer vision algo-\\n\\nrithms, 2008.\\n\\n[15] Muhammet Bastan, Hayati Cam, Ugur Gudukbay, and Ozgur Ulusoy. Bilvideo-7: An mpeg-7-\\n\\ncompatible video indexing and retrieval system. IEEE Multimedia, 17:62\\u201373, 2010.\\n\\n9\\n\\n\\x0c', u'Deep Canonical Correlation Analysis\\n\\nGalen Andrew\\nUniversity of Washington\\n\\nRaman Arora\\nToyota Technological Institute at Chicago\\n\\nJe\\ufb00 Bilmes\\nUniversity of Washington\\n\\nKaren Livescu\\nToyota Technological Institute at Chicago\\n\\ngalen@cs.washington.edu\\n\\narora@ttic.edu\\n\\nbilmes@ee.washington.edu\\n\\nklivescu@ttic.edu\\n\\nAbstract\\n\\n1. Introduction\\n\\nWe introduce Deep Canonical Correlation\\nAnalysis (DCCA), a method to learn com-\\nplex nonlinear transformations of two views\\nof data such that the resulting representations\\nare highly linearly correlated. Parameters of\\nboth transformations are jointly learned to\\nmaximize the (regularized) total correlation.\\nIt can be viewed as a nonlinear extension of\\nthe linear method canonical correlation analy-\\nsis (CCA). It is an alternative to the nonpara-\\nmetric method kernel canonical correlation\\nanalysis (KCCA) for learning correlated non-\\nlinear transformations. Unlike KCCA, DCCA\\ndoes not require an inner product, and has\\nthe advantages of a parametric method: train-\\ning time scales well with data size and the\\ntraining data need not be referenced when\\ncomputing the representations of unseen in-\\nstances.\\nIn experiments on two real-world\\ndatasets, we \\ufb01nd that DCCA learns represen-\\ntations with signi\\ufb01cantly higher correlation\\nthan those learned by CCA and KCCA. We\\nalso introduce a novel non-saturating sigmoid\\nfunction based on the cube root that may be\\nuseful more generally in feedforward neural\\nnetworks.\\n\\nProceedings of the 30 th International Conference on Ma-\\nchine Learning, Atlanta, Georgia, USA, 2013.\\nJMLR:\\nW&CP volume 28. Copyright 2013 by the author(s).\\n\\nCanonical correlation analysis (CCA) (Hotelling, 1936;\\nAnderson, 1984) is a standard statistical technique\\nfor \\ufb01nding linear projections of two random vectors\\nthat are maximally correlated. Kernel canonical cor-\\nrelation analysis (KCCA) (Akaho, 2001; Melzer et al.,\\n2001; Bach & Jordan, 2002; Hardoon et al., 2004) is\\nan extension of CCA in which maximally correlated\\nnonlinear projections, restricted to reproducing kernel\\nHilbert spaces with corresponding kernels, are found.\\nBoth CCA and KCCA are techniques for learning rep-\\nresentations of two data views, such that each view\\u2019s\\nrepresentation is simultaneously the most predictive of,\\nand the most predictable by, the other.\\n\\nCCA and KCCA have been used for unsupervised data\\nanalysis when multiple views are available (Hardoon\\net al., 2007; Vinokourov et al., 2003; Dhillon et al.,\\n2011); learning features for multiple modalities that are\\nthen fused for prediction (Sargin et al., 2007); learn-\\ning features for a single view when another view is\\navailable for representation learning but not at pre-\\ndiction time (Blaschko & Lampert, 2008; Chaudhuri\\net al., 2009; Arora & Livescu, 2012); and reducing sam-\\nple complexity of prediction problems using unlabeled\\ndata (Kakade & Foster, 2007). The applications range\\nbroadly across a number of \\ufb01elds, including medicine,\\nmeteorology (Anderson, 1984), chemometrics (Mon-\\ntanarella et al., 1995), biology and neurology (Vert &\\nKanehisa, 2002; Hardoon et al., 2007), natural language\\nprocessing (Vinokourov et al., 2003; Haghighi et al.,\\n2008; Dhillon et al., 2011), speech processing (Choukri\\n& Chollet, 1986; Rudzicz, 2010; Arora & Livescu, 2013),\\ncomputer vision (Kim et al., 2007), and multimodal\\nsignal processing (Sargin et al., 2007; Slaney & Covell,\\n\\n\\x0cDeep Canonical Correlation Analysis\\n\\n2000). An appealing property of CCA for prediction\\ntasks is that, if there is noise in either view that is\\nuncorrelated with the other view, the learned represen-\\ntations should not contain the noise in the uncorrelated\\ndimensions.\\n\\nwe could evaluate the learned representations on any\\ntask in which CCA or KCCA have been used. However,\\nin this paper we focus on the most direct measure of\\nperformance, namely correlation between the learned\\nrepresentations on unseen test data.\\n\\nWhile kernel CCA allows learning of nonlinear repre-\\nsentations, it has the drawback that the representation\\nis limited by the \\ufb01xed kernel. Also, as it is a nonpara-\\nmetric method, the time required to train KCCA or\\ncompute the representations of new datapoints scales\\npoorly with the size of the training set. In this paper,\\nwe consider learning \\ufb02exible nonlinear representations\\nvia deep networks. Deep networks do not su\\ufb00er from\\nthe aforementioned drawbacks of nonparametric mod-\\nels, and given the empirical success of deep models on\\na wide variety of tasks, we may expect to be able to\\nlearn more highly correlated representations. Deep net-\\nworks have been used widely to learn representations,\\nfor example using deep Boltzmann machines (Salakhut-\\ndinov & Hinton, 2009), deep autoencoders (Hinton &\\nSalakhutdinov, 2006), and deep nonlinear feedforward\\nnetworks (Hinton et al., 2006). These have been very\\nsuccessful for learning representations of a single data\\nview. In this work we introduce deep CCA (DCCA),\\nwhich simultaneously learns two deep nonlinear map-\\npings of two views that are maximally correlated. This\\ncan be loosely thought of as learning a kernel for KCCA,\\nbut the mapping function is not restricted to live in a\\nreproducing kernel Hilbert space.\\n\\nThe most closely related work is that of Ngiam et\\nal. on multimodal autoencoders (Ngiam et al., 2011)\\nand of Srivastava and Salakhutdinov on multimodal\\nrestricted Boltzmann machines (Srivastava & Salakhut-\\ndinov, 2012). In these approaches, there is a single\\nnetwork being learned with one or more layers con-\\nnected to both views (modalities); in the absence of\\none of the views, it can be predicted from the other view\\nusing the learned network. The key di\\ufb00erence is that\\nin our approach we learn two separate deep encodings,\\nwith the objective that the learned encodings are as\\ncorrelated as possible. These di\\ufb00erent objectives may\\nhave advantages in di\\ufb00erent settings. In the current\\nwork, we are interested speci\\ufb01cally in the correlation\\nobjective, that is in extending CCA with learned non-\\nlinear mappings. Our approach is therefore directly\\napplicable in all of the settings where CCA and KCCA\\nare used, and we compare its ability relative to CCA\\nand KCCA to generalize the correlation objective to\\nnew data, showing that DCCA achieves much better\\nresults.\\n\\nIn the following sections, we review CCA and KCCA,\\nintroduce deep CCA, and describe experiments on two\\ndata sets comparing the three methods. In principle\\n\\n2. Background: CCA, KCCA, and deep\\n\\nrepresentations\\n\\nLet (X1, X2) \\u2208 Rn1 \\xd7 Rn2 denote random vectors\\n\\nwith covariances (\\u03a311, \\u03a322) and cross-covariance \\u03a312.\\nCCA \\ufb01nds pairs of linear projections of the two views,\\n(w(cid:48)\\n\\n2X2) that are maximally correlated:\\n1, w\\u2217\\n\\n2X2)\\n\\n1X1, w(cid:48)\\n(w\\u2217\\n\\n2) = argmax\\nw1,w2\\n\\ncorr(w(cid:48)\\n1X1, w(cid:48)\\nw(cid:48)\\n1\\u03a312w2\\n1\\u03a311w1w(cid:48)\\n\\n(cid:112)w(cid:48)\\n\\n2\\u03a322w2\\n\\n(1)\\n\\n.\\n\\n(2)\\n\\n= argmax\\n\\nw1,w2\\n\\nSince the objective is invariant to scaling of w1 and w2,\\nthe projections are constrained to have unit variance:\\n\\n(w\\u2217\\n\\n1, w\\u2217\\n\\n2) =\\n\\nargmax\\n\\nw(cid:48)\\n1\\u03a311w1=w(cid:48)\\n\\n2\\u03a322w2=1\\n\\nw(cid:48)\\n1\\u03a312w2\\n\\n(3)\\n\\n1, wi\\n\\nWhen \\ufb01nding multiple pairs of vectors (wi\\n2), sub-\\nsequent projections are also constrained to be un-\\ncorrelated with previous ones, that is wi\\n1 =\\nwi\\n2 = 0 for i < j. Assembling the top k\\nprojection vectors wi\\n1 into the columns of a matrix\\n2 into A2 \\u2208 Rn2\\xd7k,\\n\\n2\\u03a322wj\\nA1 \\u2208 Rn1\\xd7k, and similarly placing wi\\nwe obtain the following formulation to identify the top\\nk \\u2264 min(n1, n2) projections:\\n\\n1\\u03a311wj\\n\\nmaximize:\\nsubject to: A(cid:48)\\n\\ntr(A(cid:48)\\n1\\u03a312A2)\\n1\\u03a311A1 = A(cid:48)\\n\\n2\\u03a322A2 = I.\\n\\n(4)\\n\\n\\u22121/2\\n22\\n\\n\\u22121/2\\n11 \\u03a312\\u03a3\\n\\nThere are several ways to express the solution to this\\nobjective; we follow the one in (Mardia et al., 1979).\\nDe\\ufb01ne T (cid:44) \\u03a3\\n, and let Uk and Vk be\\nthe matrices of the \\ufb01rst k left- and right- singular\\nvectors of T . Then the optimal objective value is\\nthe sum of the top k singular values of T (the Ky\\nFan k-norm of T ) and the optimum is attained at\\n\\u22121/2\\n(A\\u2217\\n22 Vk). Note that this solu-\\ntion assumes that the covariance matrices \\u03a311 and \\u03a322\\nare nonsingular, which is satis\\ufb01ed in practice because\\nthey are estimated from data with regularization: given\\n\\ncentered data matrices \\xafH1 \\u2208 Rn1\\xd7m, \\xafH2 \\u2208 Rn2\\xd7m, one\\n\\n\\u22121/2\\n11 Uk, \\u03a3\\n\\n2) = (\\u03a3\\n\\n1, A\\u2217\\n\\ncan estimate, e.g.\\n\\n\\u02c6\\u03a311 =\\n\\n1\\n\\n\\xafH1 \\xafH(cid:48)\\n\\n1 + r1I,\\n\\n(5)\\n\\nm \\u2212 1\\n\\nwhere r1 > 0 is a regularization parameter. Estimating\\nthe covariance matrices with regularization also reduces\\nthe detection of spurious correlations in the training\\ndata, a.k.a. \\u201cover\\ufb01tting\\u201d (De Bie & De Moor, 2003).\\n\\n\\x0cDeep Canonical Correlation Analysis\\n\\n2.1. Kernel CCA\\n\\nKernel CCA \\ufb01nds pairs of nonlinear projections of the\\ntwo views (Hardoon et al., 2004). The Reproducing\\nKernel Hilbert Spaces (RKHS) of functions on Rn1, Rn2\\nare denoted H1, H2 and the associated positive de\\ufb01nite\\nkernels are denoted \\u03ba1, \\u03ba2. The optimal projections\\nare those functions f\\u2217\\n2 \\u2208 H2 that maximize\\nthe correlation between f\\u2217\\n(f\\u2217\\n1 , f\\u2217\\n\\n1 (X1) and f\\u2217\\ncorr (f1(X1), f2(X2))\\n\\n1 \\u2208 H1, f\\u2217\\n\\n2 ) = argmax\\n\\n2 (X2):\\n\\n(6)\\n\\nf1\\u2208H1,f2\\u2208H2\\n\\n= argmax\\n\\nf1\\u2208H1,f2\\u2208H2\\n\\ncov (f1(X1), f2(X2))\\nvar (f1(X1)) var (f2(X2))\\n\\n,\\n\\ndata sets of interest, and iterative SVD algorithms for\\nthe initial dimensionality reduction can be used (Arora\\n& Livescu, 2012).\\n\\n2.2. Deep learning\\n\\n\\u201cDeep\\u201d networks, having more than two layers, are\\ncapable of representing nonlinear functions involving\\nmultiply nested high-level abstractions of the kind that\\nmay be necessary to accurately model complex real-\\nworld data. There has been a resurgence of interest in\\nsuch models following the advent of various successful\\nunsupervised methods for initializing the parameters\\n(\\u201cpretraining\\u201d) in such a way that a useful solution can\\nbe found (Hinton et al., 2006; Hinton & Salakhutdinov,\\n2006). Contrastive divergence (Bengio & Delalleau,\\n2009) has had great success as a pretraining technique,\\nas have many variants of autoencoder networks, includ-\\ning the denoising autoencoder (Vincent et al., 2008)\\nused in the present work. The growing availability of\\nboth data and compute resources also contributes to\\nthe resurgence, because empirically the performance of\\ndeep networks seems to scale very well with data size\\nand complexity.\\n\\nWhile deep networks are more commonly used for learn-\\ning classi\\ufb01cation labels or mapping to another vector\\nspace with supervision, here we use them to learn non-\\nlinear transformations of two datasets to a space in\\nwhich the data is highly correlated, just as KCCA does.\\nThe same properties that may account for deep net-\\nworks\\u2019 success in other tasks\\u2014high model complexity,\\nthe ability to concisely represent a hierarchy of features\\nfor modeling real-world data distributions\\u2014could be\\nparticularly useful in a setting where the output space\\nis signi\\ufb01cantly more complex than a single label.\\n\\n3. Deep Canonical Correlation Analysis\\n\\nDeep CCA computes representations of the two views\\nby passing them through multiple stacked layers of\\nnonlinear transformation (see Figure 1). Assume for\\nsimplicity that each intermediate layer in the network\\nfor the \\ufb01rst view has c1 units, and the \\ufb01nal (output)\\n\\nlayer has o units. Let x1 \\u2208 Rn1 be an instance of\\nthe \\ufb01rst view. The outputs of the \\ufb01rst layer for the\\n1) \\u2208 Rc1, where W 1\\n1 x1 + b1\\ninstance x1 are h1 = s(W 1\\n1 \\u2208\\nRc1\\xd7n1 is a matrix of weights, b1\\n1 \\u2208 Rc1 is a vector of\\nbiases, and s : R (cid:55)\\u2192 R is a nonlinear function applied\\ncomponentwise. The outputs h1 may then be used to\\ncompute the outputs of the next layer as h2 = s(W 1\\n2 h1+\\n2) \\u2208 Rc1 , and so on until the \\ufb01nal representation\\nb1\\nd) \\u2208 Ro is computed, for a\\nf1(x1) = s(W 1\\nnetwork with d layers. Given an instance x2 of the\\nsecond view, the representation f2(x2) is computed the\\n\\nd hd\\u22121 + b1\\n\\n(cid:112)\\n\\n(cid:112)\\n\\nTo solve the nonlinear KCCA problem, the \\u201ckernel\\ntrick\\u201d is used: Since the nonlinear maps f1 \\u2208 H1,\\nf2 \\u2208 H2 are in RKHS, the solutions can be expressed\\nas linear combinations of the kernels evaluated at the\\ndata: f1(x) = \\u03b1(cid:48)\\n1\\u03ba1(x,\\xb7), where \\u03ba1(x,\\xb7) is a vector\\nwhose ith element is \\u03ba1(x, xi) (resp. for f2(x)). KCCA\\ncan then be written as \\ufb01nding vectors \\u03b11, \\u03b12 \\u2208 Rm\\n\\nthat solve the optimization problem\\n\\n(\\u03b1\\u2217\\n\\n1, \\u03b1\\u2217\\n\\n2) = argmax\\n\\n\\u03b11,\\u03b12\\n\\n(\\u03b1(cid:48)\\nargmax\\n1 \\u03b11=\\u03b1(cid:48)\\n2K2\\n\\n=\\n\\n\\u03b1(cid:48)\\n1K2\\n\\n\\u03b1(cid:48)\\n1K1K2\\u03b12\\n1 \\u03b12) (\\u03b1(cid:48)\\n1K 2\\n\\u03b1(cid:48)\\n1K1K2\\u03b12,\\n\\n1K 2\\n\\n2 \\u03b12)\\n\\n2 \\u03b12=1\\n\\n(7)\\n\\nwhere K1 \\u2208 Rm\\xd7m is the centered Gram matrix\\nK1 = K \\u2212 K1 \\u2212 1K + 1K1, Kij = \\u03ba1(xi, xj) and\\n1 \\u2208 Rm\\xd7m is an all-1s matrix, and similarly for K2.\\nSubsequent vectors (\\u03b1j\\n2) are solutions of (7) with\\nthe constraints that (f j\\n2 (X2)) are uncorrelated\\nwith the previous ones.\\n\\n1, \\u03b1j\\n1 (X1), f j\\n\\nProper regularization may be critical to the perfor-\\nmance of KCCA, since the spaces H1, H2 could have\\nhigh complexity. Since \\u03b1(cid:48)\\n1f1(\\xb7) plays the role of w1 in\\nKCCA, the generalization of w(cid:48)\\n1K1\\u03b1.\\nTherefore the correct generalization of (5) is to use\\nK 2\\n1 in the constraints of (7), for\\nregularization parameter r1 > 0 (resp. for K 2\\n\\n1 + r1K1 in place of K 2\\n\\n1w1 would be \\u03b1(cid:48)\\n\\n2 ).\\n\\nThe optimization is in principle simple: The objective\\nis maximized by the top eigenvectors of the matrix\\n\\n(K1 + r1I)\\n\\n\\u22121 K2 (K2 + r2I)\\n\\n\\u22121 K1.\\n\\n(8)\\n\\nThe regularization coe\\ufb03cients r1 and r2, as well as\\nany parameters of the kernel in KCCA, can be tuned\\nusing held-out data. Often a further regularization is\\ndone by \\ufb01rst projecting the data onto an intermediate-\\ndimensionality space, between the target and original\\ndimensionality (Ek et al., 2008; Arora & Livescu, 2012).\\nIn practice solving KCCA may not be straightforward,\\nas the kernel matrices become very large for real-world\\n\\n\\x0cDeep Canonical Correlation Analysis\\n\\nl and bv\\n\\noptimize this quantity using gradient-based optimiza-\\ntion. To compute the gradient of corr(H1, H2) with\\nrespect to all parameters W v\\nl , we can compute\\nits gradient with respect to H1 and H2 and then use\\nbackpropagation. If the singular value decomposition\\nof T is T = U DV (cid:48), then\\n1\\n\\n(cid:0)2\\u220711 \\xafH1 + \\u220712 \\xafH2\\n\\n\\u2202corr(H1, H2)\\n\\n(cid:1) .\\n\\n(11)\\n\\n=\\n\\n\\u2202H1\\n\\nwhere\\n\\nand\\n\\nm \\u2212 1\\n\\u22121/2\\n11 U V (cid:48) \\u02c6\\u03a3\\n\\u220712 = \\u02c6\\u03a3\\n1\\n2\\n\\n\\u22121/2\\n11 U DU(cid:48) \\u02c6\\u03a3\\n\\n\\u02c6\\u03a3\\n\\n\\u22121/2\\n22\\n\\n\\u220711 = \\u2212\\n\\n(12)\\n\\n(13)\\n\\n\\u22121/2\\n11\\n\\n,\\n\\nFigure 1. A schematic of deep CCA, consisting of two deep\\nnetworks learned so that the output layers (topmost layer\\nof each network) are maximally correlated. Blue nodes\\ncorrespond to input features (n1 = n2 = 3), grey nodes\\nare hidden units (c1 = c2 = 4), and the output layer is red\\n(o = 2). Both networks have d = 4 layers.\\n\\nl and b2\\n\\nsame way, with di\\ufb00erent parameters W 2\\nl (and\\npotentially di\\ufb00erent architectural parameters c2 and d).\\nThe goal is to jointly learn parameters for both views\\nW v\\nl such that corr(f1(X1), f2(X2)) is as high\\nas possible. If \\u03b81 is the vector of all parameters W 1\\nl\\nand b1\\nl of the \\ufb01rst view for l = 1, . . . , d, and similarly\\nfor \\u03b82, then\\n\\nl and bv\\n\\n(9)\\n\\ncorr(f1(X1; \\u03b81), f2(X2; \\u03b82)).\\n\\n2) = argmax\\n(\\u03b81,\\u03b82)\\n1, \\u03b8\\u2217\\n\\n(\\u03b8\\u2217\\n1, \\u03b8\\u2217\\nTo \\ufb01nd (\\u03b8\\u2217\\n2), we follow the gradient of the correlation\\nobjective as estimated on the training data. Let H1 \\u2208\\nRo\\xd7m, H2 \\u2208 Ro\\xd7m be matrices whose columns are the\\ntop-level representations produced by the deep models\\non the two views, for a training set of size m. Let \\xafH1 =\\nm H11 be the centered data matrix (resp. \\xafH2), and\\nH1\\u2212 1\\nde\\ufb01ne \\u02c6\\u03a312 = 1\\n1 + r1I\\nfor regularization constant r1 (resp. \\u02c6\\u03a322). Assume that\\nr1 > 0 so that \\u02c6\\u03a311 is positive de\\ufb01nite.\\n\\n2, and \\u02c6\\u03a311 = 1\\nm\\u22121\\n\\n\\xafH1 \\xafH(cid:48)\\n\\n\\xafH1 \\xafH(cid:48)\\n\\nm\\u22121\\n\\nAs discussed in section 2 for CCA, the total correlation\\nof the top k components of H1 and H2 is the sum of the\\n\\u22121/2\\ntop k singular values of the matrix T = \\u02c6\\u03a3\\n.\\n22\\nIf we take k = o, then this is exactly the matrix trace\\nnorm of T , or1\\n\\n\\u22121/2\\n11\\n\\n\\u02c6\\u03a312 \\u02c6\\u03a3\\n\\ncorr(H1, H2) = ||T||tr = tr(T (cid:48)T )1/2.\\n\\n(10)\\n\\nThe parameters W v\\n\\nl and bv\\n\\nl of DCCA are trained to\\n\\n1Here we abuse notation slightly, writing corr(H1, H2)\\nas the empirical correlation of the data represented by the\\nmatrices H1 and H2.\\n\\nand \\u2202corr(H1, H2)/\\u2202H2 has a symmetric expression.\\nThe derivation of the gradient is not entirely straight-\\nforward (involving, for example, the gradient of the\\ntrace of the matrix square-root, which we could not \\ufb01nd\\nin standard references such as (Petersen & Pedersen,\\n2012)) and is given in the appendix. We also regularize\\n(10) by adding to it a quadratic penalty with weight\\n\\u03bbb > 0 for all parameters.\\n\\nBecause the correlation objective is a function of the\\nentire training set that does not decompose into a sum\\nover data points, it is not clear how to use a stochastic\\noptimization procedure that operates on data points\\none at a time. We experimented with a stochastic\\nmethod based on mini-batches, but obtained much\\nbetter results with full-batch optimization using the\\nL-BFGS second-order optimization method (Nocedal\\n& Wright, 2006) which has been found to be useful for\\ndeep learning in other contexts (Le et al., 2011).\\n\\nAs discussed in section 2.2 for deep models in general,\\nthe best results will in general not be obtained if param-\\neter optimization is started from random initialization\\u2014\\nsome form of pretraining is necessary. In our experi-\\nments, we initialize the parameters of each layer with\\na denoising autoencoder (Vincent et al., 2008). Given\\ncentered input training data assembled into a matrix\\n\\nX \\u2208 Rn\\xd7m, a distorted matrix \\u02dcX is created by adding\\ni.i.d. zero-mean Gaussian noise with variance \\u03c32\\na. For\\nparameters W \\u2208 Rc\\xd7n and b \\u2208 Rc, the reconstructed\\ndata \\u02c6X = W (cid:48)s(W \\u02dcX + b\\xaf1(cid:48)) is formed. Then we use\\nL-BFGS to \\ufb01nd a local minimum of the total squared\\nerror from the reconstruction to the original data, plus\\na quadratic penalty:\\n\\nF + ||b||2\\n2),\\n\\nF + \\u03bba(||W||2\\n\\nla(W, b) = || \\u02c6X \\u2212 X||2\\n\\n(14)\\nwhere || \\xb7 ||F is the matrix Frobenius norm. The min-\\nimizing values W \\u2217 and b\\u2217 are used to initialize opti-\\nmization of the DCCA objective, and to produce the\\nrepresentation for pretraining the next layer. \\u03c32\\na and\\n\\u03bba are treated as hyperparameters, and optimized on\\na development set, as described in section 4.1.\\n\\n000001002003004005006007008009010011012013014015016017018019020021022023024025026027028029030031032033034035036037038039040041042043044045046047048049050051052053054055056057058059060061062063064065066067068069070071072073074075076077078079080081082083084085086087088089090091092093094095096097098099100101102103104105106107108109\\x07\\x06\\x04\\x05CanonicalCorrelationAnalysismView1mView2\\x0cDeep Canonical Correlation Analysis\\n\\n3.1. Non-saturating nonlinearity\\n\\nfor g(y) \\u2212 x = 0, iterate\\n\\nAny form of sigmoid nonlinearity could be used to deter-\\nmine the output of the nodes in a DCCA network, but\\nin our experiments we obtained the best results using\\na novel non-saturating sigmoid function based on the\\ncube root. If g : R (cid:55)\\u2192 R is the function g(y) = y3/3 + y,\\nthen our function is s(x) = g\\u22121(x). Like the more pop-\\nular logistic (\\u03c3) and tanh nonlinearities, s has sigmoid\\nshape and has unit slope at x = 0. Like tanh, it is\\nan odd function. However, logistic and tanh approach\\ntheir asymptotic value very quickly, at which point\\nthe derivative drops to essentially zero (i.e., they sat-\\nurate). On the other hand, s is not bounded, and its\\nderivative falls o\\ufb00 much more gradually with x. We hy-\\npothesize that these properties make s better-suited for\\nbatch optimization with second-order methods which\\nmight otherwise get stuck on a plateau early during\\noptimization. In \\ufb01gure 2 we plot s alongside tanh for\\ncomparison.\\n\\nFigure 2. Comparison of our modi\\ufb01ed cube-root sigmoid\\nfunction (red) with the more standard tanh (blue).\\n\\n(cid:48)\\n\\u03c3(cid:48)(x) = \\u03c3(x)(1 \\u2212 \\u03c3(x)), and tanh\\n\\nAnother property that our nonsaturating sigmoid func-\\ntion shares with logistic and tanh is that its deriva-\\ntive is a simple function of its value. For example,\\n(x) = 1 \\u2212 tanh2(x).\\nThis property is convenient in implementations, be-\\ncause it means the input to a unit can be overwritten\\nby its output. Also, as it turns out, it is more e\\ufb03cient\\nto compute the derivatives as a function of the value in\\nall of these cases (e.g., given y = tanh(x), 1 \\u2212 y2 can\\nbe computed more e\\ufb03ciently than 1 \\u2212 tanh2(x)). In\\nthe case of s, we have s(cid:48)(x) = (s2(x) + 1)\\u22121 as is easily\\nshown with implicit di\\ufb00erentiation. If y = s(x), then\\n\\nx = y3/3 + y,\\n\\ndx\\ndy\\n\\n= y2 + 1,\\n\\nand\\n\\ndy\\ndx\\n\\n=\\n\\n1\\n\\ny2 + 1\\n\\n.\\n\\nTo compute s(x), we use Newton\\u2019s method. To solve\\n\\nyn+1 = yn \\u2212\\n\\n= yn \\u2212\\n\\ng(yn) \\u2212 x\\ng(cid:48)(yn)\\ny3\\nn/3 + yn \\u2212 x\\n\\ny2\\nn + 1\\n\\n=\\n\\n2y3\\nn/3 + x\\ny2\\nn + 1\\n\\n.\\n\\nFor positive x, initializing y0 = x, the iteration de-\\ncreases monotonically, so convergence is guaranteed.\\nIn the range of values in our experiments, it converges\\nto machine precision in just a few iterations. When\\nx is negative, we use the property that s is odd, so\\ns(x) = \\u2212s(\\u2212x). As a further optimization, we wrote a\\nvectorized implementation.\\n\\n4. Experiments\\n\\nWe perform experiments on two datasets to demon-\\nstrate that DCCA learns transformations that are not\\nonly dramatically more correlated than a linear CCA\\nbaseline, but also signi\\ufb01cantly more correlated than\\nwell-tuned KCCA representations. We refer to a DCCA\\nmodel with an output size of o and d layers (including\\nthe output) as DCCA-o-d.\\n\\nBecause the total correlation of two transformed views\\ngrows with dimensionality, it is important to compare\\nonly equal-dimensionality representations. In addition,\\nin order to compare the test correlation of the top k\\ncomponents of two representations of dimensionality\\no1, o2 \\u2265 k, the components must be ordered by their\\ncorrelation on the training data. In the case of CCA\\nand KCCA, the dimensions are always ordered in this\\nway; but in DCCA, there is no ordering to the output\\nnodes. Therefore, we derive such an ordering by per-\\nforming a \\ufb01nal (linear) CCA on the output layers of\\nthe two views on the training data. This \\ufb01nal CCA\\nproduces two projection matrices A1, A2, which are\\napplied to the DCCA test output before computing\\ntest set correlation. Another way would be to compute\\na new DCCA representation at each target dimension-\\nality; this is not done here for expediency but should,\\nif anything, improve performance.\\n\\n4.1. Hyperparameter optimization\\n\\nEach of the DCCA models we tested has a \\ufb01xed num-\\nber of layers and output size, and the parameters W v\\nl\\nand bv\\nl are trained as discussed in section 3. Several\\nother values are treated as hyperparameters. Speci\\ufb01-\\ncally, for each view, we have \\u03c32\\na and \\u03bba for autoencoder\\npretraining, c, the width of all hidden layers (a large\\ninteger parameter treated as a real value) and r, the\\nCCA regularization hyperparameter. Finally there is a\\nsingle hyperparameter \\u03bbb, the \\ufb01ne-tuning regulariza-\\ntion weight. These values were chosen to optimize total\\n\\n\\x0cDeep Canonical Correlation Analysis\\n\\ncorrelation on a development set using a derivative-free\\noptimization method.\\n\\n4.2. MNIST handwritten digits\\n\\nFor our \\ufb01rst experiments, we learn correlated repre-\\nsentations of the left and right halves of handwritten\\ndigit images. We use the MNIST handwritten image\\ndataset (LeCun & Cortes, 1998), which consists of\\n60,000 train images and 10,000 test images. We ran-\\ndomly selected 10% (6,000) images from the training\\nset to use for hyperparameter tuning. Each image is\\na 28x28 matrix of pixels, each representing one of 256\\ngrayscale values. The left and right 14 columns are sepa-\\nrated to form the two views, making 392 features in each\\nview. For KCCA, we use a radial basis function (RBF)\\nkernel for both views: k1(xi, xj) = e\\u2212(cid:107)xi\\u2212xj(cid:107)2/2\\u03c32\\n1 and\\nsimilarly for k2. The bandwidth parameters \\u03c31, \\u03c32 are\\ntuned over the range [0.25, 64]. Regularization parame-\\nters r1, r2 for CCA and KCCA are tuned over the range\\n[10\\u22128, 10]. The four parameters were jointly tuned to\\nmaximize correlation at k = 50 on the development set.\\nWe use a scalable KCCA algorithm based on incremen-\\ntal SVD (Arora & Livescu, 2012). The selected widths\\nof the hidden layers for the DCCA-50-2 model were\\n2038 (left half-images) and 1608 (right half-images).\\nTable 1 compares the total correlation on the develop-\\nment and test sets obtained for the 50 most correlated\\ndimensions with linear CCA, KCCA, and DCCA.\\n\\n(RBF)\\n\\nCCA KCCA DCCA\\n(50-2)\\n39.4\\n39.7\\n\\n28.1\\n28.0\\n\\n33.5\\n33.0\\n\\nDev\\nTest\\n\\nTable 1. Correlation captured in the 50 most correlated\\ndimensions on the split MNIST dataset.\\n\\n4.3. Articulatory speech data\\n\\nThe second set of experiments uses speech data from the\\nWisconsin X-ray Microbeam Database (XRMB) (West-\\nbury, 1994) of simultaneous acoustic and articulatory\\nrecordings. The articulatory data consist of hori-\\nzontal and vertical displacements of eight pellets on\\nthe speaker\\u2019s lips, tongue, and jaws, yielding a 16-\\ndimensional vector at each time point. The baseline\\nacoustic features consist of standard 13-dimensional\\nmel-frequency cepstral coe\\ufb03cients (MFCCs) (Davis &\\nMermelstein, 1980) and their \\ufb01rst and second deriva-\\ntives computed every 10ms over a 25ms window. The\\narticulatory measurements are downsampled to match\\nthe MFCC frame rate.\\n\\nThe input features X1 and X2 to CCA/KCCA/DCCA\\n\\nare the acoustic and articulatory features concatenated\\nover a 7-frame window around each frame, giving\\n\\nacoustic vectors X1 \\u2208 R273 and articulatory vectors\\nX2 \\u2208 R112. We discard frames that are missing any of\\nthe articulatory data (e.g., due to mistracked pellets),\\nresulting in m \\u2248 50, 000 frames for each speaker. For\\nd, with k1(xi, xj) =(cid:0)xT\\nKCCA, besides an RBF kernel (described in the previ-\\nous section) we also use a polynomial kernel of degree\\nand similarly for k2.\\n\\ni xj + c(cid:1)d\\n\\nWe run \\ufb01ve independent experiments, each using 60%\\nof the utterances for learning projections, 20% for tun-\\ning hyperparameters (regularization parameters and\\nkernel bandwidths), and 20% for \\ufb01nal testing. For this\\nset of experiments, kernel bandwidths for the RBF\\nkernel were \\ufb01xed at \\u03c31 = 4 \\xd7 106, \\u03c32 = 2 \\xd7 104 to\\nmatch the variance in the un-normalized data. For the\\npolynomial kernel we tuned the degree d over the set\\n{2, 3} and the o\\ufb00set parameter c over the range [0.25, 2]\\nto optimize development set correlation at k = 110.\\nHyperparameter optimization selected the number of\\nhidden units per layer in the DCCA-50-2 model as\\n1641 and 1769 for the MFCC and XRMB views respec-\\ntively. In the DCCA-112-3 model, 1811 and 1280 units\\nper layer, respectively, were chosen. The widths for\\nthe DCCA-112-8 model were \\ufb01xed at 781 and 552 as\\ndiscussed in the last paragraph of this section.\\n\\nTable 2 compares total correlation captured in the top\\n50 dimensions on the test data for all \\ufb01ve folds with\\nCCA, KCCA with both kernels, and DCCA-50-2. The\\npattern of performance across the folds is similar for\\nall four models, and DCCA consistently \\ufb01nds more\\ncorrelation.\\n\\n(RBF)\\n\\nCCA KCCA KCCA DCCA\\n(50-2)\\n38.2\\n34.1\\n39.4\\n37.1\\n34.0\\n\\n(Poly)\\n32.3\\n29.1\\n34.0\\n32.4\\n29.9\\n\\n16.8\\n15.8\\n16.9\\n16.6\\n16.2\\n\\n29.2\\n25.3\\n30.8\\n28.6\\n26.2\\n\\nFold 1\\nFold 2\\nFold 3\\nFold 4\\nFold 5\\n\\nTable 2. Correlation captured in the 50 most correlated\\ndimensions on the articulatory dataset.\\n\\nFigure 3 shows correlation obtained using linear CCA,\\nKCCA with RBF kernel, KCCA with a polynomial\\nkernel (d = 2, c = 1), and various topologies of Deep\\nCCA, on the test set of one of the folds as a function\\nof number of dimensions. The KCCA models tend to\\ndetect slightly more correlation in the \\ufb01rst few compo-\\nnents, after which the Deep CCA models outperform\\nthem by a large margin. We note that DCCA may\\nparticularly have an advantage when k is equal to the\\n\\n\\x0cDeep Canonical Correlation Analysis\\n\\nnumber of output units o. We found that DCCA mod-\\nels with only two or three output units can indeed \\ufb01nd\\nmore correlation than the top two or three components\\nof KCCA (results not shown). This is also consistent\\nwith the observation that DCCA-50-2 has the highest\\nperformance of any model at k = 50.\\n\\nFigure 3. Correlation as a function of number of dimensions.\\nNote that DCCA-50-2 is truncated at k = o = 50.\\n\\nTo determine the impact of model depth (number of\\nlayers) on performance, we conducted an experiment in\\nwhich we increased the number of layers from three to\\neight, while reducing the number of hidden units in each\\nlayer in order to keep the total number of parameters\\napproximately constant. The output width was \\ufb01xed\\nat 112, and all hyperparameters other than the number\\nof hidden units were kept \\ufb01xed at the values chosen for\\nDCCA-112-3. Table 3 gives the total correlation on the\\n\\ufb01rst fold as a function of the number of layers. Note\\nthat the total correlation of both datasets increases\\nmonotonically with the depth of DCCA and even with\\neight layers we have not reached saturation.\\n\\nlayers (d)\\nDev set\\nTest set\\n\\n3\\n\\n66.7\\n80.4\\n\\n4\\n\\n68.1\\n81.9\\n\\n5\\n\\n70.1\\n84.0\\n\\n6\\n\\n72.5\\n86.1\\n\\n7\\n\\n76.0\\n88.5\\n\\n8\\n\\n79.1\\n88.6\\n\\nTable 3. Total correlation captured, on one of the folds, by\\nDCCA-112-d, for d ranging from three to eight.\\n\\n5. Discussion\\n\\nWe have shown that deep CCA can obtain improved\\nrepresentations with respect to the correlation objective\\nmeasured on unseen data. DCCA provides a \\ufb02exible\\nnonlinear alternative to KCCA. Another appealing fea-\\nture of DCCA is that, like CCA, it does not require\\n\\nan inner product. As a parametric model, representa-\\ntions of unseen datapoints can be computed without\\nreference to the training set.\\n\\nIn many applications of CCA, such as classi\\ufb01cation and\\nregression, maximizing the correlation is not the \\ufb01nal\\ngoal and the correlated representations are used in the\\nservice of another task. A natural next step is therefore\\nto test the representations produced by deep CCA in\\nthe context of prediction tasks and to compare against\\nother nonlinear multi-view representation learning ap-\\nproaches that optimize other objectives, e.g., (Ngiam\\net al., 2011; Srivastava & Salakhutdinov, 2012).\\n\\n6. Acknowledgments\\n\\nThis research was supported by NSF grant IIS-0905633\\nand by the Intel/UW ISTC. The opinions expressed in\\nthis work are those of the authors and do not necessarily\\nre\\ufb02ect the views of the funders.\\n\\n7. Appendix: Derivation of DCCA\\n\\nGradient\\n\\nTo perform backpropagation, we must be able to com-\\npute the gradient of f = corr(H1, H2) de\\ufb01ned in Equa-\\ntion (10). Denote by \\u2207ij the matrix of partial deriva-\\ntives of f with respect to the entries of \\u02c6\\u03a3ij. Let the\\n\\u22121/2\\n\\u02c6\\u03a312 \\u02c6\\u03a3\\nsingular value decomposition of T = \\u02c6\\u03a3\\n22\\nbe given as T = U DV (cid:48). First we will show that\\n\\n\\u22121/2\\n11\\n\\nand\\n\\n\\u220712 = \\u02c6\\u03a3\\n1\\n2\\n\\n\\u220711 = \\u2212\\n\\n\\u22121/2\\n11 U V (cid:48) \\u02c6\\u03a3\\n\\n\\u22121/2\\n22\\n\\n\\u22121/2\\n11 U DU(cid:48) \\u02c6\\u03a3\\n\\n\\u22121/2\\n11\\n\\n\\u02c6\\u03a3\\n\\n(15)\\n\\n(16)\\n\\n(resp. \\u220722). To prove (15), we use the fact that for a\\nmatrix X, \\u2207||X||tr = U V (cid:48), where X = U DV (cid:48) is the\\nsingular value decomposition of X (Bach, 2008). Using\\nthe chain rule:\\n\\n(\\u220712)ab =\\n\\n=\\n\\n=\\n\\ncd\\n= ( \\u02c6\\u03a3\\n\\n\\u2202Tcd\\n\\n\\u2202f\\n\\n\\u2202( \\u02c6\\u03a312)ab\\n\\u2202f\\n\\u2202Tcd \\xb7\\n(U V (cid:48)\\n\\n(cid:88)\\n(cid:88)\\n\\u22121/2\\n11 U V (cid:48) \\u02c6\\u03a3\\n\\ncd\\n\\n\\u2202( \\u02c6\\u03a312)ab\\n\\u22121/2\\n)cd \\xb7 ( \\u02c6\\u03a3\\n11\\n\\u22121/2\\n22\\n\\n)ab\\n\\n)ca( \\u02c6\\u03a3\\n\\n\\u22121/2\\n22\\n\\n)bd\\n\\nFor (16) we use the identity \\u2207 tr X 1/2 = 1\\nis easily derived from Theorem 1 of (Lewis, 1996):\\nTheorem 1. A matrix function f is called a spectral\\nfunction if it depends only on the set of eigenvalues of\\nits argument. That is, for any positive de\\ufb01nite matrix\\n\\n2 X\\u22121/2. This\\n\\n1102030405060708090100110020406080100Number of dimensionsSum Correlation  DCCA\\u221250\\u22122DCCA\\u2212112\\u22128DCCA\\u2212112\\u22123KCCA\\u2212POLYKCCA\\u2212RBFCCA\\x0cDeep Canonical Correlation Analysis\\n\\nX and any unitary matrix V , f (X) = f (V XV (cid:48)). If f\\nis a spectral function, and X is positive de\\ufb01nite with\\neigendecomposition X = U DU(cid:48), then\\n\\nAlso,\\n\\n\\u2202f (X)\\n\\n\\u2202X\\n\\n= U diag\\n\\n\\u2202f (D)\\n\\n\\u2202D\\n\\nU(cid:48)\\n\\n(17)\\n\\n\\u2202 \\u02c6\\u03a312\\nab\\n\\u2202H 1\\nij\\n\\n(cid:32)\\n\\n1{a=i}\\n\\nH 2\\nbj \\u2212\\n\\n1{a=i} \\xafH 2\\nbj.\\n\\n(cid:33)\\n\\n(cid:88)\\n\\nk\\n\\n1\\nm\\n\\nH 2\\nbk\\n\\nNow we can proceed\\n\\nPutting this together, we obtain\\n\\n=\\n\\n=\\n\\n1\\n\\nm \\u2212 1\\n\\n1\\n\\nm \\u2212 1\\n\\n(cid:88)\\n\\n+\\n\\n(cid:88)\\n\\nab\\n\\n\\u220711\\n\\nab\\n\\n1\\n\\nm \\u2212 1\\n\\n1\\n\\nm \\u2212 1\\n\\n\\u2202 \\u02c6\\u03a311\\nab\\n\\u2202H 1\\nij\\n\\n(cid:32)(cid:88)\\n(cid:16)(cid:0)\\n\\nb\\n\\n\\xafH 1\\n\\n\\u220711\\n\\nib\\n\\n\\u220711 \\xafH1\\n\\n(cid:33)\\n(cid:17)\\n\\n.\\n\\n\\xafH 2\\nbj\\n\\n\\u220712\\n\\nib\\n\\n(cid:1)\\n\\n\\u220712 \\xafH2\\n\\nij\\n\\n(cid:1)\\n\\n\\u2202 \\u02c6\\u03a312\\nab\\n\\u2202H 1\\nij\\n\\nab\\n\\nb\\n\\nai\\n\\nab\\n\\n\\xafH 1\\n\\n(cid:1)\\n\\nbj +\\n\\naj +\\n\\n\\u220712\\n(cid:88)\\n(cid:88)\\na \\u220711\\nij +(cid:0)\\nij +(cid:0)\\n(cid:48)\\n\\xafH1\\n\\u2207\\n11\\n(cid:0)2\\u220711 \\xafH1 + \\u220712 \\xafH2\\n(cid:1) .\\n\\n(\\u220711)ab =\\n\\n=\\n\\n=\\n\\n\\u2202f\\n\\n\\u2202( \\u02c6\\u03a311)ab\\n\\u2202f\\n\\n(cid:88)\\n(cid:88)\\n\\ncd\\n\\n(cid:18) 1\\n\\n\\u2202(T (cid:48)T )cd\\n(T (cid:48)T )\\n\\n2\\n\\n\\u2202(T (cid:48)T )cd\\n\\u2202( \\u02c6\\u03a311)ab\\n\\u22121/2\\n\\n(cid:19)\\n\\n\\u2202(T (cid:48)T )cd\\n\\u2202( \\u02c6\\u03a311)ab\\n\\ncd\\n\\n\\u2202f\\n\\u2202H 1\\nij\\n\\n(18)\\n\\n=\\n\\n=\\n\\n=\\n\\nSince T (cid:48)T = \\u02c6\\u03a3\\n, and using Eq. 60\\nfrom (Petersen & Pedersen, 2012) for the derivative of\\nan inverse,\\n\\n\\u02c6\\u03a321 \\u02c6\\u03a3\\n\\n\\u02c6\\u03a312 \\u02c6\\u03a3\\n\\n\\u22121\\n11\\n\\n\\u22121/2\\n22\\n\\n\\u2202(T (cid:48)T )cd\\n\\u2202( \\u02c6\\u03a311)ab\\n\\n=\\n\\n\\u22121\\n\\u2202( \\u02c6\\u03a3\\n11 )ij\\n\\u2202( \\u02c6\\u03a311)ab\\n\\ncd\\n\\u22121/2\\n22\\n\\n(cid:88)\\n(cid:88)\\n\\nij\\n\\n\\u2202(T (cid:48)T )cd\\n\\u22121\\n\\u2202( \\u02c6\\u03a3\\n11 )ij\\n\\u22121/2\\n22\\n\\n( \\u02c6\\u03a3\\n\\n\\u02c6\\u03a321)ci( \\u02c6\\u03a312 \\u02c6\\u03a3\\n\\n\\u22121/2\\n22\\n\\n)jd( \\u02c6\\u03a3\\n\\n\\u22121\\n11 )ia( \\u02c6\\u03a3\\n\\n\\u22121\\n11 )bj\\n\\n\\u22121/2\\n22\\n\\n\\u02c6\\u03a321 \\u02c6\\u03a3\\n\\n\\u22121\\n\\u22121\\n11 )ca( \\u02c6\\u03a3\\n11\\n\\u22121/2\\n11 T )bd\\n\\n)ca( \\u02c6\\u03a3\\n\\n\\u22121/2\\n11\\n\\n\\u02c6\\u03a312 \\u02c6\\u03a3\\n\\n\\u22121/2\\n22\\n\\n)bd\\n\\nij\\n\\n1\\n2\\n\\n= \\u2212\\n= \\u2212( \\u02c6\\u03a3\\n= \\u2212(T (cid:48) \\u02c6\\u03a3\\n(cid:88)\\n(cid:88)\\n(cid:0) \\u02c6\\u03a3\\n(cid:0) \\u02c6\\u03a3\\n(cid:0) \\u02c6\\u03a3\\n\\n= \\u2212\\n= \\u2212\\n= \\u2212\\n\\n1\\n2\\n1\\n2\\n1\\n2\\n\\n= \\u2212\\n\\n1\\n2\\n\\ncd\\n\\ncd\\n\\nSo continuing from (18),\\n\\n(\\u220711)ab = \\u2212\\n\\n(T (cid:48) \\u02c6\\u03a3\\n\\n\\u22121/2\\n11\\n\\n)ca(T (cid:48)T )\\n\\n\\u22121/2\\ncd\\n\\n\\u22121/2\\n11 T )bd\\n\\n( \\u02c6\\u03a3\\n\\n\\u22121/2\\n11 T )ac(T (cid:48)T )\\n\\n\\u22121/2\\ncd\\n\\n( \\u02c6\\u03a3\\n\\n(T (cid:48) \\u02c6\\u03a3\\n\\n\\u22121/2\\n11\\n\\n)db\\n\\n(cid:1)\\n\\n\\u22121/2\\n11\\n\\nab\\n\\n)V DU(cid:48) \\u02c6\\u03a3\\n\\n\\u22121/2\\n11\\n\\n(cid:1)\\n\\nab\\n\\n\\u22121/2\\n11 T (T (cid:48)T )\\n\\u22121/2\\n11 U DV (cid:48)\\n\\u22121/2\\n11 U DU(cid:48) \\u02c6\\u03a3\\n\\n\\u22121/2T (cid:48) \\u02c6\\u03a3\\n(V D\\u22121V (cid:48)\\n\\u22121/2\\n11\\n\\n(cid:1)\\n\\nab\\n\\nUsing \\u220712 and \\u220711, we are ready to compute \\u2202f /\\u2202H1.\\nFirst (temporarily moving subscripts on H1 and \\u02c6\\u03a311\\nto superscripts so subscripts can index into matrices)\\n\\n(cid:1)\\n(cid:1)\\n(cid:1)\\n\\nk H 1\\nik\\nk H 1\\nbk\\nk H 1\\nak\\n\\nm\\n\\n(cid:0)H 1\\n(cid:80)\\n(cid:0)H 1\\n(cid:80)\\n(cid:0)H 1\\n(cid:80)\\n(cid:0)1{a=i} \\xafH 1\\n\\nij \\u2212 1\\nbj \\u2212 1\\naj \\u2212 1\\n\\nm\\n\\nm\\n\\n\\uf8f1\\uf8f4\\uf8f4\\uf8f4\\uf8f2\\uf8f4\\uf8f4\\uf8f4\\uf8f3\\n\\n2\\n\\n1\\n\\n1\\n\\nm\\u22121\\nm\\u22121\\nm\\u22121\\n0\\n1\\n\\nm \\u2212 1\\n\\nbj + 1{b=i} \\xafH 1\\naj\\n\\nif a = i, b = i\\nif a = i, b (cid:54)= i\\nif a (cid:54)= i, b = i\\nif a (cid:54)= i, b (cid:54)= i\\n\\n(cid:1).\\n\\n\\u2202 \\u02c6\\u03a311\\nab\\n\\u2202H 1\\nij\\n\\n=\\n\\n=\\n\\nUsing the fact that \\u220711 is symmetric, this can be\\nwritten more compactly as\\n\\n\\u2202f\\n\\u2202H1\\n\\n=\\n\\n1\\n\\nm \\u2212 1\\n\\nReferences\\n\\nAkaho, S. A kernel method for canonical correlation analysis.\\n\\nIn Proc. Int\\u2019l Meeting on Psychometric Society, 2001.\\n\\nAnderson, T. W. An Introduction to Multivariate Statistical\\n\\nAnalysis (2nd edition). John Wiley and Sons, 1984.\\n\\nArora, R. and Livescu, K. Kernel CCA for multi-view learn-\\ning of acoustic features using articulatory measurements.\\nIn Symp. on Machine Learning in Speech and Language\\nProcessing, 2012.\\n\\nArora, R. and Livescu, K. Multi-view CCA-based acoustic\\nfeatures for phonetic recognition across speakers and\\ndomains. In Int. Conf. on Acoustics, Speech, and Signal\\nProcessing, 2013.\\n\\nBach, F. R. Consistency of trace norm minimization. J.\\n\\nMach. Learn. Res., 9:1019\\u20131048, June 2008.\\n\\nBach, F. R. and Jordan, M. I. Kernel independent compo-\\n\\nnent analysis. J. Mach. Learn. Res., 3:1\\u201348, 2002.\\n\\nBengio, Y. and Delalleau, O. Justifying and generalizing\\ncontrastive divergence. Neural Computation, 21(6):1601\\u2013\\n1621, 2009.\\n\\nBlaschko, M. B. and Lampert, C. H. Correlational spectral\\n\\nclustering. In CVPR, 2008.\\n\\nChaudhuri, K., Kakade, S. M., Livescu, K., and Sridha-\\nran, K. Multi-view clustering via canonical correlation\\nanalysis. In ICML, 2009.\\n\\nChoukri, K. and Chollet, G. Adaptation of automatic speech\\nrecognizers to new speakers using canonical correlation\\nanalysis techniques. Speech Comm., 1:95\\u2013107, 1986.\\n\\n\\x0cDeep Canonical Correlation Analysis\\n\\nDavis, S. B. and Mermelstein, P. Comparison of paramet-\\nric representations for monosyllabic word recognition in\\ncontinuously spoken sentences. IEEE Trans. Acoustics,\\nSpeech, and Signal Proc., 28(4):357\\u2013366, 1980.\\n\\nDe Bie, T. and De Moor, B. On the regularization of\\ncanonical correlation analysis. In Proc. Int\\u2019l Conf. on\\nIndependent Component Analysis and Blind Source Sep-\\naration, 2003.\\n\\nDhillon, P., Foster, D., and Ungar, L. Multi-view learning\\n\\nof word embeddings via CCA. In NIPS, 2011.\\n\\nEk, C. H., Torr, P. H., , and Lawrence, N. D. Ambiguity\\n\\nmodelling in latent spaces. In MLMI, 2008.\\n\\nHaghighi, A., Liang, P., Berg-Kirkpatrick, T., and Klein,\\nD. Learning bilingual lexicons from monolingual corpora.\\nIn ACL-HLT, 2008.\\n\\nHardoon, D. R., Szedm\\xb4ak, S., and Shawe-Taylor, J. Canon-\\nical correlation analysis: An overview with application\\nto learning methods. Neural Computation, 16(12):2639\\u2013\\n2664, 2004.\\n\\nHardoon, D. R., Mourao-Miranda, J., Brammer, M., and\\nShawe-Taylor, J. Unsupervised analysis of fMRI data\\nusing kernel canonical correlation. NeuroImage, 37(4):\\n1250\\u20131259, 2007.\\n\\nHinton, G. E. and Salakhutdinov, R. R. Reducing the\\ndimensionality of data with neural networks. Science,\\n313(5786):504\\u2013507, 2006.\\n\\nHinton, G. E., Osindero, S., and Teh, Y.-W. A fast learning\\nalgorithm for deep belief nets. Neural computation, 18\\n(7):1527\\u20131554, 2006.\\n\\nHotelling, H. Relations between two sets of variates.\\n\\nBiometrika, 28(3/4):321\\u2013377, 1936.\\n\\nKakade, S. M. and Foster, D. P. Multi-view regression via\\n\\ncanonical correlation analysis. In COLT, 2007.\\n\\nKim, T. K., Wong, S. F., and Cipolla, R. Tensor canonical\\ncorrelation analysis for action classi\\ufb01cation. In CVPR,\\n2007.\\n\\nLe, Q. V., Ngiam, J., Coates, A., Lahiri, A., Prochnow,\\nB., and Ng, A. Y. On optimization methods for deep\\nlearning. In ICML, 2011.\\n\\nLeCun, Y. and Cortes, C. The MNIST database of hand-\\n\\nwritten digits, 1998.\\n\\nLewis, A. S. Derivatives of spectral functions. Mathematics\\n\\nof Operations Research, 21(3):576\\u2013588, 1996.\\n\\nMardia, K. V., Kent, J. T., and Bibby, J. M. Multivariate\\n\\nAnalysis. Academic Press, 1979.\\n\\nMelzer, T., Reiter, M., and Bischof, H. Nonlinear feature ex-\\ntraction using generalized canonical correlation analysis.\\nIn ICANN, 2001.\\n\\nMontanarella, L., Bassami, M., and Breas, O. Chemometric\\nclassi\\ufb01cation of some European wines using pyrolysis\\nmass spectrometry. Rapid Communications in Mass\\nSpectrometry, 9(15):1589\\u20131593, 1995.\\n\\nNgiam, J., Khosla, A., Kim, M., Nam, J., Lee, H., and Ng,\\n\\nA. Y. Multimodal deep learning. In ICML, 2011.\\n\\nNocedal, J. and Wright, S. J. Numerical Optimization.\\n\\nSpringer, New York, 2nd edition, 2006.\\n\\nPetersen, K. B. and Pedersen, M. S. The matrix cook-\\nbook, Nov 2012. URL http://www2.imm.dtu.dk/pubdb/\\np.php?3274.\\n\\nRudzicz, F. Adaptive kernel canonical correlation analy-\\nsis for estimation of task dynamics from acoustics. In\\nICASSP, 2010.\\n\\nSalakhutdinov, R. and Hinton, G. E. Deep Boltzmann\\n\\nmachines. In AISTATS, 2009.\\n\\nSargin, M. E., Yemez, Y., and Tekalp, A. M. Audiovisual\\nsynchronization and fusion using canonical correlation\\nanalysis. IEEE. Trans. Multimedia, 9(7):1396\\u20131403, 2007.\\n\\nSlaney, M. and Covell, M. FaceSync: A linear operator\\nfor measuring synchronization of video facial images and\\naudio tracks. In NIPS, 2000.\\n\\nSrivastava, N. and Salakhutdinov, R. Multimodal learning\\n\\nwith deep Boltzmann machines. In NIPS, 2012.\\n\\nVert, J.-P. and Kanehisa, M. Graph-driven features extrac-\\ntion from microarray data using di\\ufb00usion kernels and\\nkernel CCA. In NIPS, 2002.\\n\\nVincent, P., Larochelle, H., Bengio, Y., and Manzagol, P.-A.\\nExtracting and composing robust features with denoising\\nautoencoders. In ICML. ACM, 2008.\\n\\nVinokourov, A., Shawe-Taylor, J., and Cristianini, N. Infer-\\nring a semantic representation of text via cross-language\\ncorrelation analysis. In NIPS, 2003.\\n\\nWestbury, J. R. X-ray microbeam speech production\\ndatabase user\\u2019s handbook. Waisman Center on Men-\\ntal Retardation & Human Development, U. Wisconsin,\\nMadison, WI, version 1.0 edition, June 1994.\\n\\n\\x0c', u'Colorful Image Colorization\\n\\nRichard Zhang, Phillip Isola, Alexei A. Efros\\n\\n{rich.zhang,isola,efros}@eecs.berkeley.edu\\n\\nUniversity of California, Berkeley\\n\\nAbstract. Given a grayscale photograph as input, this paper attacks\\nthe problem of hallucinating a plausible color version of the photograph.\\nThis problem is clearly underconstrained, so previous approaches have\\neither relied on signi\\ufb01cant user interaction or resulted in desaturated col-\\norizations. We propose a fully automatic approach that produces vibrant\\nand realistic colorizations. We embrace the underlying uncertainty of the\\nproblem by posing it as a classi\\ufb01cation task and use class-rebalancing at\\ntraining time to increase the diversity of colors in the result. The sys-\\ntem is implemented as a feed-forward pass in a CNN at test time and is\\ntrained on over a million color images. We evaluate our algorithm using a\\n\\u201ccolorization Turing test,\\u201d asking human participants to choose between\\na generated and ground truth color image. Our method successfully fools\\nhumans on 32% of the trials, signi\\ufb01cantly higher than previous methods.\\nMoreover, we show that colorization can be a powerful pretext task for\\nself-supervised feature learning, acting as a cross-channel encoder. This\\napproach results in state-of-the-art performance on several feature learn-\\ning benchmarks.\\n\\nKeywords: Colorization, Vision for Graphics, CNNs, Self-supervised\\nlearning\\n\\n6\\n1\\n0\\n2\\n\\n \\nt\\nc\\nO\\n5\\n\\n \\n\\n \\n \\n]\\n\\nV\\nC\\n.\\ns\\nc\\n[\\n \\n \\n\\n5\\nv\\n1\\n1\\n5\\n8\\n0\\n\\n.\\n\\n3\\n0\\n6\\n1\\n:\\nv\\ni\\nX\\nr\\na\\n\\n1 Introduction\\n\\nConsider the grayscale photographs in Figure 1. At \\ufb01rst glance, hallucinating\\ntheir colors seems daunting, since so much of the information (two out of the\\nthree dimensions) has been lost. Looking more closely, however, one notices that\\nin many cases, the semantics of the scene and its surface texture provide ample\\ncues for many regions in each image: the grass is typically green, the sky is\\ntypically blue, and the ladybug is most de\\ufb01nitely red. Of course, these kinds of\\nsemantic priors do not work for everything, e.g., the croquet balls on the grass\\nmight not, in reality, be red, yellow, and purple (though it\\u2019s a pretty good guess).\\nHowever, for this paper, our goal is not necessarily to recover the actual ground\\ntruth color, but rather to produce a plausible colorization that could potentially\\nfool a human observer. Therefore, our task becomes much more achievable: to\\nmodel enough of the statistical dependencies between the semantics and the\\ntextures of grayscale images and their color versions in order to produce visually\\ncompelling results.\\n\\nGiven the lightness channel L, our system predicts the corresponding a and\\nb color channels of the image in the CIE Lab colorspace. To solve this problem,\\n\\n\\x0c2\\n\\nZhang, Isola, Efros\\n\\nFig. 1. Example input grayscale photos and output colorizations from our algo-\\nrithm. These examples are cases where our model works especially well. Please visit\\nhttp://richzhang.github.io/colorization/ to see the full range of results and to\\ntry our model and code. Best viewed in color (obviously).\\n\\nwe leverage large-scale data. Predicting color has the nice property that training\\ndata is practically free: any color photo can be used as a training example, simply\\nby taking the image\\u2019s L channel as input and its ab channels as the supervisory\\nsignal. Others have noted the easy availability of training data, and previous\\nworks have trained convolutional neural networks (CNNs) to predict color on\\nlarge datasets [1,2]. However, the results from these previous attempts tend to\\nlook desaturated. One explanation is that [1,2] use loss functions that encourage\\nconservative predictions. These losses are inherited from standard regression\\nproblems, where the goal is to minimize Euclidean error between an estimate\\nand the ground truth.\\n\\nWe instead utilize a loss tailored to the colorization problem. As pointed out\\nby [3], color prediction is inherently multimodal \\u2013 many objects can take on\\nseveral plausible colorizations. For example, an apple is typically red, green, or\\nyellow, but unlikely to be blue or orange. To appropriately model the multimodal\\nnature of the problem, we predict a distribution of possible colors for each pixel.\\nFurthermore, we re-weight the loss at training time to emphasize rare colors.\\nThis encourages our model to exploit the full diversity of the large-scale data on\\nwhich it is trained. Lastly, we produce a \\ufb01nal colorization by taking the annealed-\\nmean of the distribution. The end result is colorizations that are more vibrant\\nand perceptually realistic than those of previous approaches.\\n\\nEvaluating synthesized images is notoriously di\\ufb03cult [4]. Since our ultimate\\ngoal is to make results that are compelling to a human observer, we introduce\\na novel way of evaluating colorization results, directly testing their perceptual\\nrealism. We set up a \\u201ccolorization Turing test,\\u201d in which we show participants\\nreal and synthesized colors for an image, and ask them to identify the fake.\\nIn this quite di\\ufb03cult paradigm, we are able to fool participants on 32% of the\\ninstances (ground truth colorizations would achieve 50% on this metric), signif-\\nicantly higher than prior work [2]. This test demonstrates that in many cases,\\n\\n\\x0cColorful Image Colorization\\n\\n3\\n\\nour algorithm is producing nearly photorealistic results (see Figure 1 for selected\\nsuccessful examples from our algorithm). We also show that our system\\u2019s col-\\norizations are realistic enough to be useful for downstream tasks, in particular\\nobject classi\\ufb01cation, using an o\\ufb00-the-shelf VGG network [5].\\n\\nWe additionally explore colorization as a form of self-supervised representa-\\ntion learning, where raw data is used as its own source of supervision. The idea\\nof learning feature representations in this way goes back at least to autoencoders\\n[6]. More recent works have explored feature learning via data imputation, where\\na held-out subset of the complete data is predicted (e.g., [7,8,9,10,11,12,13]).\\nOur method follows in this line, and can be termed a cross-channel encoder.\\nWe test how well our model performs in generalization tasks, compared to pre-\\nvious [14,8,15,10] and concurrent [16] self-supervision algorithms, and \\ufb01nd that\\nour method performs surprisingly well, achieving state-of-the-art performance\\non several metrics.\\n\\nOur contributions in this paper are in two areas. First, we make progress\\non the graphics problem of automatic image colorization by (a) designing an\\nappropriate objective function that handles the multimodal uncertainty of the\\ncolorization problem and captures a wide diversity of colors, (b) introducing\\na novel framework for testing colorization algorithms, potentially applicable to\\nother image synthesis tasks, and (c) setting a new high-water mark on the task by\\ntraining on a million color photos. Secondly, we introduce the colorization task\\nas a competitive and straightforward method for self-supervised representation\\nlearning, achieving state-of-the-art results on several benchmarks.\\n\\nPrior work on colorization Colorization algorithms mostly di\\ufb00er in the\\nways they obtain and treat the data for modeling the correspondence between\\ngrayscale and color. Non-parametric methods, given an input grayscale image,\\n\\ufb01rst de\\ufb01ne one or more color reference images (provided by a user or retrieved\\nautomatically) to be used as source data. Then, following the Image Analogies\\nframework [17], color is transferred onto the input image from analogous regions\\nof the reference image(s) [18,19,20,21]. Parametric methods, on the other hand,\\nlearn prediction functions from large datasets of color images at training time,\\nposing the problem as either regression onto continuous color space [22,1,2] or\\nclassi\\ufb01cation of quantized color values [3]. Our method also learns to classify\\ncolors, but does so with a larger model, trained on more data, and with several\\ninnovations in the loss function and mapping to a \\ufb01nal continuous output.\\n\\nConcurrent work on colorization Concurrently with our paper, Larsson\\net al. [23] and Iizuka et al. [24] have developed similar systems, which leverage\\nlarge-scale data and CNNs. The methods di\\ufb00er in their CNN architectures and\\nloss functions. While we use a classi\\ufb01cation loss, with rebalanced rare classes,\\nLarsson et al. use an un-rebalanced classi\\ufb01cation loss, and Iizuka et al. use a\\nregression loss. In Section 3.1, we compare the e\\ufb00ect of each of these types\\nof loss function in conjunction with our architecture. The CNN architectures\\nare also somewhat di\\ufb00erent: Larsson et al. use hypercolumns [25] on a VGG\\nnetwork [5], Iizuka et al. use a two-stream architecture in which they fuse global\\nand local features, and we use a single-stream, VGG-styled network with added\\ndepth and dilated convolutions [26,27]. In addition, while we and Larsson et al.\\ntrain our models on ImageNet [28], Iizuka et al. train their model on Places\\n\\n\\x0c4\\n\\nZhang, Isola, Efros\\n\\nFig. 2. Our network architecture. Each conv layer refers to a block of 2 or 3 repeated\\nconv and ReLU layers, followed by a BatchNorm [30] layer. The net has no pool layers.\\nAll changes in resolution are achieved through spatial downsampling or upsampling\\nbetween conv blocks.\\n\\n[29]. In Section 3.1, we provide quantitative comparisons to Larsson et al., and\\nencourage interested readers to investigate both concurrent papers.\\n\\n2 Approach\\n\\nWe train a CNN to map from a grayscale input to a distribution over quantized\\ncolor value outputs using the architecture shown in Figure 2. Architectural de-\\ntails are described in the supplementary materials on our project webpage1, and\\nthe model is publicly available. In the following, we focus on the design of the\\nobjective function, and our technique for inferring point estimates of color from\\nthe predicted color distribution.\\n\\n2.1 Objective Function\\nGiven an input lightness channel X \\u2208 RH\\xd7W\\xd71, our objective is to learn a\\n\\nmapping (cid:98)Y = F(X) to the two associated color channels Y \\u2208 RH\\xd7W\\xd72, where\\n(We denote predictions with a(cid:98)\\xb7 symbol and ground truth without.) We per-\\n\\nH, W are image dimensions.\\n\\nform this task in CIE Lab color space. Because distances in this space model\\nperceptual distance, a natural objective function, as used in [1,2], is the Eu-\\nclidean loss L2(\\xb7,\\xb7) between predicted and ground truth colors:\\n\\n(1)\\n\\nL2((cid:98)Y, Y) =\\n\\n(cid:88)\\n(cid:107)Yh,w \\u2212 (cid:98)Yh,w(cid:107)2\\n\\n2\\n\\n1\\n2\\n\\nh,w\\n\\nHowever, this loss is not robust to the inherent ambiguity and multimodal\\nnature of the colorization problem. If an object can take on a set of distinct\\nab values, the optimal solution to the Euclidean loss will be the mean of the\\nset. In color prediction, this averaging e\\ufb00ect favors grayish, desaturated results.\\nAdditionally, if the set of plausible colorizations is non-convex, the solution will\\nin fact be out of the set, giving implausible results.\\n\\n1 http://richzhang.github.io/colorization/\\n\\n\\x0cColorful Image Colorization\\n\\n5\\n\\nFig. 3. (a) Quantized ab color space with a grid size of 10. A total of 313 ab pairs are\\nin gamut. (b) Empirical probability distribution of ab values, shown in log scale. (c)\\nEmpirical probability distribution of ab values, conditioned on L, shown in log scale.\\n\\nInstead, we treat the problem as multinomial classi\\ufb01cation. We quantize the\\nab output space into bins with grid size 10 and keep the Q = 313 values which\\nare in-gamut, as shown in Figure 3(a). For a given input X, we learn a mapping\\n\\n(cid:98)Z = G(X) to a probability distribution over possible colors (cid:98)Z \\u2208 [0, 1]H\\xd7W\\xd7Q,\\nTo compare predicted(cid:98)Z against ground truth, we de\\ufb01ne function Z = H\\u22121\\n\\ngt (Y),\\nwhich converts ground truth color Y to vector Z, using a soft-encoding scheme2.\\nWe then use multinomial cross entropy loss Lcl(\\xb7,\\xb7), de\\ufb01ned as:\\n\\nwhere Q is the number of quantized ab values.\\n\\nLcl((cid:98)Z, Z) = \\u2212(cid:88)\\n\\nv(Zh,w)\\n\\nh,w\\n\\n(cid:88)\\n\\nq\\n\\nZh,w,q log((cid:98)Zh,w,q)\\n\\n(2)\\n\\nwhere v(\\xb7) is a weighting term that can be used to rebalance the loss based\\non color-class rarity, as de\\ufb01ned in Section 2.2 below. Finally, we map probability\\n\\ndistribution (cid:98)Z to color values (cid:98)Y with function (cid:98)Y = H((cid:98)Z), which will be further\\n\\ndiscussed in Section 2.3.\\n\\n2.2 Class rebalancing\\n\\nThe distribution of ab values in natural images is strongly biased towards val-\\nues with low ab values, due to the appearance of backgrounds such as clouds,\\npavement, dirt, and walls. Figure 3(b) shows the empirical distribution of pix-\\nels in ab space, gathered from 1.3M training images in ImageNet [28]. Observe\\nthat the number of pixels in natural images at desaturated values are orders of\\nmagnitude higher than for saturated values. Without accounting for this, the\\n\\n2 Each ground truth value Yh,w can be encoded as a 1-hot vector Zh,w by searching for\\nthe nearest quantized ab bin. However, we found that soft-encoding worked well for\\ntraining, and allowed the network to quickly learn the relationship between elements\\nin the output space [31]. We \\ufb01nd the 5-nearest neighbors to Yh,w in the output\\nspace and weight them proportionally to their distance from the ground truth using\\na Gaussian kernel with \\u03c3 = 5.\\n\\n\\x0c6\\n\\nZhang, Isola, Efros\\n\\nloss function is dominated by desaturated ab values. We account for the class-\\nimbalance problem by reweighting the loss of each pixel at train time based on\\nthe pixel color rarity. This is asymptotically equivalent to the typical approach\\nof resampling the training space [32]. Each pixel is weighed by factor w \\u2208 RQ,\\nbased on its closest ab bin.\\n\\nv(Zh,w) = wq\\u2217 , where q\\u2217 = arg max\\n\\nw \\u221d(cid:16)\\n\\n(1 \\u2212 \\u03bb)(cid:101)p +\\n\\n\\u03bb\\nQ\\n\\n(cid:17)\\u22121\\n\\n(cid:88)\\n\\nq\\n\\nZh,w,q\\n\\n(cid:101)pqwq = 1\\n\\n, E[w] =\\n\\n(3)\\n\\n(4)\\n\\nq\\n\\nTo obtain smoothed empirical distribution(cid:101)p \\u2208 \\u2206Q, we estimate the empirical\\n\\nprobability of colors in the quantized ab space p \\u2208 \\u2206Q from the full ImageNet\\ntraining set and smooth the distribution with a Gaussian kernel G\\u03c3. We then\\nmix the distribution with a uniform distribution with weight \\u03bb \\u2208 [0, 1], take\\nthe reciprocal, and normalize so the weighting factor is 1 on expectation. We\\nfound that values of \\u03bb = 1\\n2 and \\u03c3 = 5 worked well. We compare results with and\\nwithout class rebalancing in Section 3.1.\\n\\n2.3 Class Probabilities to Point Estimates\\n\\nFinally, we de\\ufb01ne H, which maps the predicted distribution (cid:98)Z to point estimate\\n(cid:98)Y in ab space. One choice is to take the mode of the predicted distribution for\\n\\neach pixel, as shown in the right-most column of Figure 4 for two example im-\\nages. This provides a vibrant but sometimes spatially inconsistent result, e.g.,\\nthe red splotches on the bus. On the other hand, taking the mean of the predicted\\ndistribution produces spatially consistent but desaturated results (left-most col-\\numn of Figure 4), exhibiting an unnatural sepia tone. This is unsurprising, as\\ntaking the mean after performing classi\\ufb01cation su\\ufb00ers from some of the same\\nissues as optimizing for a Euclidean loss in a regression framework. To try to get\\nthe best of both worlds, we interpolate by re-adjusting the temperature T of the\\nsoftmax distribution, and taking the mean of the result. We draw inspiration\\nfrom the simulated annealing technique [33], and thus refer to the operation as\\ntaking the annealed-mean of the distribution:\\n\\nH(Zh,w) = E(cid:2)fT (Zh,w)(cid:3),\\n\\n(cid:80)\\n\\nfT (z) =\\n\\nexp(log(z)/T )\\nq exp(log(zq)/T )\\n\\n(5)\\n\\nSetting T = 1 leaves the distribution unchanged, lowering the temperature\\nT produces a more strongly peaked distribution, and setting T \\u2192 0 results in a\\n1-hot encoding at the distribution mode. We found that temperature T = 0.38,\\nshown in the middle column of Figure 4, captures the vibrancy of the mode while\\nmaintaining the spatial coherence of the mean.\\nOur \\ufb01nal system F is the composition of CNN G, which produces a predicted\\ndistribution over all pixels, and the annealed-mean operation H, which produces\\na \\ufb01nal prediction. The system is not quite end-to-end trainable, but note that\\nthe mapping H operates on each pixel independently, with a single parameter,\\nand can be implemented as part of a feed-forward pass of the CNN.\\n\\n\\x0cColorful Image Colorization\\n\\n7\\n\\nFig. 4. The e\\ufb00ect of temperature parameter T on the annealed-mean output (Equation\\n5). The left-most images show the means of the predicted color distributions and the\\nright-most show the modes. We use T = 0.38 in our system.\\n\\n3 Experiments\\n\\nIn Section 3.1, we assess the graphics aspect of our algorithm, evaluating the\\nperceptual realism of our colorizations, along with other measures of accuracy.\\nWe compare our full algorithm to several variants, along with recent [2] and\\nconcurrent work [23]. In Section 3.2, we test colorization as a method for self-\\nsupervised representation learning. Finally, in Section 10.1, we show qualitative\\nexamples on legacy black and white images.\\n\\n3.1 Evaluating colorization quality\\n\\nWe train our network on the 1.3M images from the ImageNet training set [28],\\nvalidate on the \\ufb01rst 10k images in the ImageNet validation set, and test on a\\nseparate 10k images in the validation set, same as in [23]. We show quantitative\\nresults in Table 1 on three metrics. A qualitative comparison for selected success\\nand failure cases is shown in Figure 5. For a comparison on a full selection of\\nrandom images, please see our project webpage.\\n\\nTo speci\\ufb01cally test the e\\ufb00ect of di\\ufb00erent loss functions, we train our CNN\\nwith various losses. We also compare to previous [2] and concurrent methods [23],\\nwhich both use CNNs trained on ImageNet, along with naive baselines:\\n\\n1. Ours (full) Our full method, with classi\\ufb01cation loss, de\\ufb01ned in Equation 2,\\nand class rebalancing, as described in Section 2.2. The network was trained\\nfrom scratch with k-means initialization [36], using the ADAM solver for\\napproximately 450k iterations3.\\n\\n2. Ours (class) Our network on classi\\ufb01cation loss but no class rebalancing\\n\\n(\\u03bb = 1 in Equation 4).\\n\\n3 \\u03b21 = .9, \\u03b22 = .99, and weight decay = 10\\u22123. Initial learning rate was 3 \\xd7 10\\u22125 and\\ndropped to 10\\u22125 and 3 \\xd7 10\\u22126 when loss plateaued, at 200k and 375k iterations,\\nrespectively. Other models trained from scratch followed similar training protocol.\\n\\n\\x0c8\\n\\nZhang, Isola, Efros\\n\\nFig. 5. Example results from our ImageNet test set. Our classi\\ufb01cation loss with re-\\nbalancing produces more accurate and vibrant results than a regression loss or a clas-\\nsi\\ufb01cation loss without rebalancing. Successful colorizations are above the dotted line.\\nCommon failures are below. These include failure to capture long-range consistency,\\nfrequent confusions between red and blue, and a default sepia tone on complex indoor\\nscenes. Please visit http://richzhang.github.io/colorization/ to see the full range\\nof results.\\n\\nGround truthRegressionClassi\\ufb01cation w/ rebalInputClassi\\ufb01cationSuccess casesFailure cases\\x0cColorful Image Colorization\\n\\n9\\n\\nColorization Results on ImageNet\\n\\nModel\\n\\nAuC\\n\\nVGG Top-1 AMT\\n\\nParams Feats Runtime non-rebal rebal Class Acc Labeled\\n(MB) (MB)\\nReal (%)\\n\\n(ms)\\n\\nMethod\\n\\nGround Truth\\nGray\\nRandom\\nDahl [2]\\nLarsson et al. [23]\\nOurs (L2)\\nOurs (L2, ft)\\nOurs (class)\\nOurs (full)\\n\\n\\u2013\\n\\u2013\\n\\u2013\\n\\u2013\\n\\n588\\n129\\n129\\n129\\n129\\n\\n\\u2013\\n\\u2013\\n\\u2013\\n\\u2013\\n\\n495\\n127\\n127\\n142\\n142\\n\\n\\u2013\\n\\u2013\\n\\u2013\\n\\u2013\\n\\n122.1\\n17.8\\n17.8\\n22.1\\n22.1\\n\\n(%)\\n100\\n89.1\\n84.2\\n90.4\\n91.7\\n91.2\\n91.5\\n91.6\\n89.5\\n\\n(%)\\n100\\n58.0\\n57.3\\n58.9\\n65.9\\n64.4\\n66.2\\n65.1\\n67.3\\n\\n(%)\\n68.3\\n52.7\\n41.0\\n48.7\\n59.4\\n54.9\\n56.5\\n56.6\\n56.0\\n\\n50\\n\\u2013\\n\\n13.0\\xb14.4\\n18.3\\xb12.8\\n27.2\\xb12.7\\n21.2\\xb12.5\\n23.9\\xb12.8\\n25.2\\xb12.7\\n32.3\\xb12.2\\n\\nTable 1. Colorization results on 10k images in the ImageNet validation set [28], as\\nused in [23]. AuC refers to the area under the curve of the cumulative error distribution\\nover ab space [22]. Results column 2 shows the class-balanced variant of this metric.\\nColumn 3 is the classi\\ufb01cation accuracy after colorization using the VGG-16 [5] network.\\nColumn 4 shows results from our AMT real vs. fake test (with mean and standard error\\nreported, estimated by bootstrap [34]). Note that an algorithm that produces ground\\ntruth images would achieve 50% performance in expectation. Higher is better for all\\nmetrics. Rows refer to di\\ufb00erent algorithms; see text for a description of each. Parameter\\nand feature memory, and runtime, were measured on a Titan X GPU using Ca\\ufb00e [35].\\n\\n3. Ours (L2) Our network trained from scratch, with L2 regression loss, de-\\n\\nscribed in Equation 1, following the same training protocol.\\n\\n4. Ours (L2, ft) Our network trained with L2 regression loss, \\ufb01ne-tuned from\\n\\nour full classi\\ufb01cation with rebalancing network.\\n\\n5. Larsson et al. [23] A CNN method that also appears in these proceedings.\\n6. Dahl [2] A previous model using a Laplacian pyramid on VGG features,\\n\\ntrained with L2 regression loss.\\n\\n7. Gray Colors every pixel gray, with (a, b) = 0.\\n8. Random Copies the colors from a random image from the training set.\\n\\nEvaluating the quality of synthesized images is well-known to be a di\\ufb03cult\\ntask, as simple quantitative metrics, like RMS error on pixel values, often fail to\\ncapture visual realism. To address the shortcomings of any individual evaluation,\\nwe test three that measure di\\ufb00erent senses of quality, shown in Table 1.\\n\\n1. Perceptual realism (AMT): For many applications, such as those in\\ngraphics, the ultimate test of colorization is how compelling the colors look to a\\nhuman observer. To test this, we ran a real vs. fake two-alternative forced choice\\nexperiment on Amazon Mechanical Turk (AMT). Participants in the experiment\\nwere shown a series of pairs of images. Each pair consisted of a color photo next\\nto a re-colorized version, produced by either our algorithm or a baseline. Par-\\nticipants were asked to click on the photo they believed contained fake colors\\ngenerated by a computer program. Individual images of resolution 256\\xd7256 were\\nshown for one second each, and after each pair, participants were given unlim-\\nited time to respond. Each experimental session consisted of 10 practice trials\\n\\n\\x0c10\\n\\nZhang, Isola, Efros\\n\\nFig. 6. Images sorted by how often AMT participants chose our algorithm\\u2019s colorization\\nover the ground truth. In all pairs to the left of the dotted line, participants believed\\nour colorizations to be more real than the ground truth on \\u2265 50% of the trials. In some\\ncases, this may be due to poor white balancing in the ground truth image, corrected\\nby our algorithm, which predicts a more prototypical appearance. Right of the dotted\\nline are examples where participants were never fooled.\\n\\n(excluded from subsequent analysis), followed by 40 test pairs. On the practice\\ntrials, participants were given feedback as to whether or not their answer was\\ncorrect. No feedback was given during the 40 test pairs. Each session tested\\nonly a single algorithm at a time, and participants were only allowed to com-\\nplete at most one session. A total of 40 participants evaluated each algorithm.\\nTo ensure that all algorithms were tested in equivalent conditions (i.e. time of\\nday, demographics, etc.), all experiment sessions were posted simultaneously and\\ndistributed to Turkers in an i.i.d. fashion.\\n\\nTo check that participants were competent at this task, 10% of the trials\\npitted the ground truth image against the Random baseline described above.\\nParticipants successfully identi\\ufb01ed these random colorizations as fake 87% of\\nthe time, indicating that they understood the task and were paying attention.\\n\\nFigure 6 gives a better sense of the participants\\u2019 competency at detecting\\nsubtle errors made by our algorithm. The far right column shows example pairs\\nwhere participants identi\\ufb01ed the fake image successfully in 100% of the trials.\\nEach of these pairs was scored by at least 10 participants. Close inspection reveals\\nthat on these images, our colorizations tend to have giveaway artifacts, such as\\nthe yellow blotches on the two trucks, which ruin otherwise decent results.\\n\\nNonetheless, our full algorithm fooled participants on 32% of trials, as shown\\nin Table 1. This number is signi\\ufb01cantly higher than all compared algorithms\\n(p < 0.05 in each case) except for Larsson et al., against which the di\\ufb00erence\\nwas not signi\\ufb01cant (p = 0.10; all statistics estimated by bootstrap [34]). These\\nresults validate the e\\ufb00ectiveness of using both a classi\\ufb01cation loss and class-\\nrebalancing.\\n\\nGround truthOurs82%67%64%64%60%58%55%55%55%55%55%50%0%0%0%0%Fooled more oftenFooled less oftenGround truthOursGround truthOursGround truthOurs\\x0cColorful Image Colorization\\n\\n11\\n\\nNote that if our algorithm exactly reproduced the ground truth colors, the\\nforced choice would be between two identical images, and participants would be\\nfooled 50% of the time on expectation. Interestingly, we can identify cases where\\nparticipants were fooled more often than 50% of the time, indicating our results\\nwere deemed more realistic than the ground truth. Some examples are shown\\nin the \\ufb01rst three columns of Figure 6. In many case, the ground truth image\\nis poorly white balanced or has unusual colors, whereas our system produces a\\nmore prototypical appearance.\\n\\n2. Semantic interpretability (VGG classi\\ufb01cation): Does our method\\nproduce realistic enough colorizations to be interpretable to an o\\ufb00-the-shelf ob-\\nject classi\\ufb01er? We tested this by feeding our fake colorized images to a VGG\\nnetwork [5] that was trained to predict ImageNet classes from real color photos.\\nIf the classi\\ufb01er performs well, that means the colorizations are accurate enough\\nto be informative about object class. Using an o\\ufb00-the-shelf classi\\ufb01er to assess\\nthe realism of synthesized data has been previously suggested by [12].\\n\\nThe results are shown in the second column from the right of Table 1. Classi-\\n\\ufb01er performance drops from 68.3% to 52.7% after ablating colors from the input.\\nAfter re-colorizing using our full method, the performance is improved to 56.0%\\n(other variants of our method achieve slightly higher results). The Larsson et al.\\n[23] method achieves the highest performance on this metric, reaching 59.4%. For\\nreference, a VGG classi\\ufb01cation network \\ufb01ne-tuned on grayscale inputs reaches a\\nperformance of 63.5%.\\n\\nIn addition to serving as a perceptual metric, this analysis demonstrates a\\npractical use for our algorithm: without any additional training or \\ufb01ne-tuning, we\\ncan improve performance on grayscale image classi\\ufb01cation, simply by colorizing\\nimages with our algorithm and passing them to an o\\ufb00-the-shelf classi\\ufb01er.\\n\\n3. Raw accuracy (AuC): As a low-level test, we compute the percentage\\nof predicted pixel colors within a thresholded L2 distance of the ground truth\\nin ab color space. We then sweep across thresholds from 0 to 150 to produce\\na cumulative mass function, as introduced in [22], integrate the area under the\\ncurve (AuC), and normalize. Note that this AuC metric measures raw prediction\\naccuracy, whereas our method aims for plausibility.\\n\\nOur network, trained on classi\\ufb01cation without rebalancing, outperforms our\\nL2 variant (when trained from scratch). When the L2 net is instead \\ufb01ne-tuned\\nfrom a color classi\\ufb01cation network, it matches the performance of the classi\\ufb01ca-\\ntion network. This indicates that the L2 metric can achieve accurate coloriza-\\ntions, but has di\\ufb03culty in optimization from scratch. The Larsson et al. [23]\\nmethod achieves slightly higher accuracy. Note that this metric is dominated by\\ndesaturated pixels, due to the distribution of ab values in natural images (Figure\\n3(b)). As a result, even predicting gray for every pixel does quite well, and our\\nfull method with class rebalancing achieves approximately the same score.\\n\\nPerceptually interesting regions of images, on the other hand, tend to have a\\ndistribution of ab values with higher values of saturation. As such, we compute\\na class-balanced variant of the AuC metric by re-weighting the pixels inversely\\nby color class probability (Equation 4, setting \\u03bb = 0). Under this metric, our\\nfull method outperforms all variants and compared algorithms, indicating that\\nclass-rebalancing in the training objective achieved its desired e\\ufb00ect.\\n\\n\\x0c12\\n\\nZhang, Isola, Efros\\n\\nDataset and Task Generalization on PASCAL [37]\\n\\nClass.\\n\\nDet.\\n\\nSeg.\\n\\n[Ref ] fc8 fc6-8 all\\n\\n-\\n\\n\\u2013\\n\\n\\u2013\\n\\n\\ufb01ne-tune layers\\nImageNet [38]\\nGaussian\\nAutoencoder\\nk-means [36]\\nAgrawal et al. [8]\\nWang & Gupta [15]\\n*Doersch et al. [14]\\n*Pathak et al. [10]\\n*Donahue et al. [16]\\nOurs (gray)\\nOurs (color)\\n\\n(%mAP)\\n\\n(%mAP) (%mIU)\\n[Ref ] all\\n[Ref ] all\\n76.8 78.9 79.9 [36] 56.8 [42] 48.0\\n[10]\\n53.3 [10] 43.4 [10] 19.8\\n[16] 24.8 16.0 53.8 [10] 41.9 [10] 25.2\\n[16] 32.0 39.2 56.6 [36] 45.6 [16] 32.6\\n[16] 31.2 31.0 54.2 [36] 43.9\\n28.1 52.2 58.7 [36] 47.4\\n[16] 44.7 55.1 65.3 [36] 51.1\\n[10]\\n\\n56.5 [10] 44.5 [10] 29.7\\n38.2 50.2 58.6 [16] 46.2 [16] 34.9\\n35.0\\n52.4 61.5 65.9\\n52.4 61.5 65.6\\n35.6\\n\\n46.1\\n46.9\\n\\n\\u2013\\n\\u2013\\n\\u2013\\n\\n\\u2013\\n\\u2013\\n\\n\\u2013\\n\\n\\u2013\\n\\u2013\\n\\u2013\\n\\n\\u2013\\n\\n\\u2013\\n\\n\\u2013\\n\\u2013\\n\\u2013\\n\\n\\u2013\\n\\u2013\\n\\nFig. 7. ImageNet Linear Classi\\ufb01cation\\n\\nTable 2. PASCAL Tests\\n\\nFig. 7. Task Generalization on ImageNet We freeze pre-trained networks and\\nlearn linear classi\\ufb01ers on internal layers for ImageNet [28] classi\\ufb01cation. Features are\\naverage-pooled, with equal kernel and stride sizes, until feature dimensionality is\\nbelow 10k. ImageNet [38], k-means [36], and Gaussian initializations were run with\\ngrayscale inputs, shown with dotted lines, as well as color inputs, shown with solid\\nlines. Previous [14,10] and concurrent [16] self-supervision methods are shown.\\nTab. 2. Task and Dataset Generalization on PASCAL Classi\\ufb01cation and\\ndetection on PASCAL VOC 2007 [39] and segmentation on PASCAL VOC 2012 [40],\\nusing standard mean average precision (mAP) and mean intersection over union\\n(mIU) metrics for each task. We \\ufb01ne-tune our network with grayscale inputs (gray)\\nand color inputs (color). Methods noted with a * only pre-trained a subset of the\\nAlexNet layers. The remaining layers were initialized with [36]. Column Ref indicates\\nthe source for a value obtained from a previous paper.\\n\\n3.2 Cross-Channel Encoding as Self-Supervised Feature Learning\\n\\nIn addition to making progress on the graphics task of colorization, we evaluate\\nhow colorization can serve as a pretext task for representation learning. Our\\nmodel is akin to an autoencoder, except that the input and output are di\\ufb00erent\\nimage channels, suggesting the term cross-channel encoder.\\n\\nTo evaluate the feature representation learned through this kind of cross-\\nchannel encoding, we run two sets of tests on our network. First, we test the\\ntask generalization capability of the features by \\ufb01xing the learned representa-\\ntion and training linear classi\\ufb01ers to perform object classi\\ufb01cation on already seen\\ndata (Figure 7). Second, we \\ufb01ne-tune the network on the PASCAL dataset [37]\\nfor the tasks of classi\\ufb01cation, detection, and segmentation. Here, in addition to\\ntesting on held-out tasks, this group of experiments tests the learned represen-\\ntation on dataset generalization. To fairly compare to previous feature learning\\nalgorithms, we retrain an AlexNet [38] network on the colorization task, using\\nour full method, for 450k iterations. We \\ufb01nd that the resulting learned repre-\\nsentation achieves higher performance on object classi\\ufb01cation and segmentation\\ntasks relative to previous methods tested (Table 2).\\n\\nImageNet classi\\ufb01cation The network was pre-trained to colorize images\\nfrom the ImageNet dataset, without semantic label information. We test how well\\n\\n\\x0cColorful Image Colorization\\n\\n13\\n\\nthe learned features represent the object-level semantics. To do this, we freeze\\nthe weights of the network, provide semantic labels, and train linear classi\\ufb01ers\\non each convolutional layer. The results are shown in Figure 7.\\n\\nAlexNet directly trained on ImageNet classi\\ufb01cation achieves the highest per-\\nformance, and serves as the ceiling for this test. Random initialization, with\\nGaussian weights or the k-means scheme implemented in [36], peak in the mid-\\ndle layers. Because our representation is learned on grayscale images, the network\\nis handicapped at the input. To quantify the e\\ufb00ect of this loss of information,\\nwe \\ufb01ne-tune AlexNet on grayscale image classi\\ufb01cation, and also run the random\\ninitialization schemes on grayscale images. Interestingly, for all three methods,\\nthere is a 6% performance gap between color and grayscale inputs, which remains\\napproximately constant throughout the network.\\n\\nWe compare our model to other recent self-supervised methods pre-trained on\\nImageNet [14,10,16]. To begin, our conv1 representation results in worse linear\\nclassi\\ufb01cation performance than competiting methods [14,16], but is comparable\\nto other methods which have a grayscale input. However, this performance gap\\nis immediately bridged at conv2, and our network achieves competitive perfor-\\nmance to [14,16] throughout the remainder of the network. This indicates that\\ndespite the input handicap, solving the colorization task encourages representa-\\ntions that linearly separate semantic classes in the trained data distribution.\\n\\nPASCAL classi\\ufb01cation, detection, and segmentation We test our\\nmodel on the commonly used self-supervision benchmarks on PASCAL classi\\ufb01-\\ncation, detection, and segmentation, introduced in [14,36,10]. Results are shown\\nin Table 2. Our network achieves strong performance across all three tasks, and\\nstate-of-the-art numbers in classi\\ufb01cation and segmentation. We use the method\\nfrom [36], which rescales the layers so they \\u201clearn\\u201d at the same rate. We test\\nour model in two modes: (1) keeping the input grayscale by disregarding color\\ninformation (Ours (gray)) and (2) modifying conv1 to receive a full 3-channel\\nLab input, initializing the weights on the ab channels to be zero (Ours (color)).\\nWe \\ufb01rst test the network on PASCAL VOC 2007 [39] classi\\ufb01cation, following\\nthe protocol in [16]. The network is trained by freezing the representation up to\\ncertain points, and \\ufb01ne-tuning the remainder. Note that when conv1 is frozen,\\nthe network is e\\ufb00ectively only able to interpret grayscale images. Across all three\\nclassi\\ufb01cation tests, we achieve state-of-the-art accuracy.\\n\\nWe also test detection on PASCAL VOC 2007, using Fast R-CNN [41], fol-\\nlowing the procedure in [36]. Doersch et al. [14] achieves 51.1%, while we reach\\n46.9% and 47.9% with grayscale and color inputs, respectively. Our method is\\nwell above the strong k-means [36] baseline of 45.6%, but all self-supervised meth-\\nods still fall short of pre-training with ImageNet semantic supervision, which\\nreaches 56.8%.\\n\\nFinally, we test semantic segmentation on PASCAL VOC 2012 [40], using\\nthe FCN architecture of [42], following the protocol in [10]. Our colorization\\ntask shares similarities to the semantic segmentation task, as both are per-pixel\\nclassi\\ufb01cation problems. Our grayscale \\ufb01ne-tuned network achieves performance\\nof 35.0%, approximately equal to Donahue et al. [16], and adding in color infor-\\nmation increases performance to 35.6%, above other tested algorithms.\\n\\n\\x0c14\\n\\nZhang, Isola, Efros\\n\\nFig. 8. Applying our method to legacy black and white photos. Left to right: photo\\nby David Fleay of a Thylacine, now extinct, 1936; photo by Ansel Adams of Yosemite;\\namateur family photo from 1956; Migrant Mother by Dorothea Lange, 1936.\\n\\n3.3 Legacy Black and White Photos\\n\\nSince our model was trained using \\u201cfake\\u201d grayscale images generated by strip-\\nping ab channels from color photos, we also ran our method on real legacy black\\nand white photographs, as shown in Figure 8 (additional results can be viewed\\non our project webpage). One can see that our model is still able to produce\\ngood colorizations, even though the low-level image statistics of the legacy pho-\\ntographs are quite di\\ufb00erent from those of the modern-day photos on which it\\nwas trained.\\n\\n4 Conclusion\\n\\nWhile image colorization is a boutique computer graphics task, it is also an in-\\nstance of a di\\ufb03cult pixel prediction problem in computer vision. Here we have\\nshown that colorization with a deep CNN and a well-chosen objective function\\ncan come closer to producing results indistinguishable from real color photos.\\nOur method not only provides a useful graphics output, but can also be viewed\\nas a pretext task for representation learning. Although only trained to color,\\nour network learns a representation that is surprisingly useful for object clas-\\nsi\\ufb01cation, detection, and segmentation, performing strongly compared to other\\nself-supervised pre-training methods.\\n\\nAcknowledgements\\n\\nThis research was supported, in part, by ONR MURI N000141010934, NSF SMA-\\n1514512, an Intel research grant, and a hardware donation by NVIDIA Corp. We thank\\nmembers of the Berkeley Vision Lab and Aditya Deshpande for helpful discussions,\\nPhilipp Kr\\xa8ahenb\\xa8uhl and Je\\ufb00 Donahue for help with self-supervision experiments, and\\nGustav Larsson for providing images for comparison to [23].\\n\\n\\x0cColorful Image Colorization\\n\\n15\\n\\nReferences\\n\\n1. Cheng, Z., Yang, Q., Sheng, B.: Deep colorization. In: Proceedings of the IEEE\\n\\nInternational Conference on Computer Vision. (2015) 415\\u2013423\\n\\n2. Dahl, R.: Automatic colorization. In: http://tinyclouds.org/colorize/. (2016)\\n3. Charpiat, G., Hofmann, M., Sch\\xa8olkopf, B.: Automatic image colorization via mul-\\ntimodal predictions. In: Computer Vision\\u2013ECCV 2008. Springer (2008) 126\\u2013139\\n4. Ramanarayanan, G., Ferwerda, J., Walter, B., Bala, K.: Visual equivalence: to-\\nwards a new standard for image \\ufb01delity. ACM Transactions on Graphics (TOG)\\n26(3) (2007) 76\\n\\n5. Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale\\n\\nimage recognition. arXiv preprint arXiv:1409.1556 (2014)\\n\\n6. Bengio, Y., Courville, A., Vincent, P.: Representation learning: A review and new\\nperspectives. IEEE transactions on pattern analysis and machine intelligence 35(8)\\n(2013) 1798\\u20131828\\n\\n7. Ngiam, J., Khosla, A., Kim, M., Nam, J., Lee, H., Ng, A.Y.: Multimodal deep\\nlearning. In: Proceedings of the 28th international conference on machine learning\\n(ICML-11). (2011) 689\\u2013696\\n\\n8. Agrawal, P., Carreira, J., Malik, J.: Learning to see by moving. In: Proceedings of\\n\\nthe IEEE International Conference on Computer Vision. (2015) 37\\u201345\\n\\n9. Jayaraman, D., Grauman, K.: Learning image representations tied to ego-motion.\\nIn: Proceedings of the IEEE International Conference on Computer Vision. (2015)\\n1413\\u20131421\\n\\n10. Pathak, D., Kr\\xa8ahenb\\xa8uhl, P., Donahue, J., Darrell, T., Efros, A.: Context encoders:\\n\\nFeature learning by inpainting. In: CVPR. (2016)\\n\\n11. Lotter, W., Kreiman, G., Cox, D.: Deep predictive coding networks for video\\n\\nprediction and unsupervised learning. arXiv preprint arXiv:1605.08104 (2016)\\n\\n12. Owens, A., Isola, P., McDermott, J., Torralba, A., Adelson, E.H., Freeman, W.T.:\\n\\nVisually indicated sounds. CVPR (2016)\\n\\n13. Owens, A., Wu, J., McDermott, J.H., Freeman, W.T., Torralba, A.: Ambient sound\\n\\nprovides supervision for visual learning. In: ECCV. (2016)\\n\\n14. Doersch, C., Gupta, A., Efros, A.A.: Unsupervised visual representation learning\\nby context prediction. In: Proceedings of the IEEE International Conference on\\nComputer Vision. (2015) 1422\\u20131430\\n\\n15. Wang, X., Gupta, A.: Unsupervised learning of visual representations using videos.\\nIn: Proceedings of the IEEE International Conference on Computer Vision. (2015)\\n2794\\u20132802\\n\\n16. Donahue, J., Kr\\xa8ahenb\\xa8uhl, P., Darrell, T.: Adversarial feature learning. arXiv\\n\\npreprint arXiv:1605.09782 (2016)\\n\\n17. Hertzmann, A., Jacobs, C.E., Oliver, N., Curless, B., Salesin, D.H.: Image analo-\\nIn: Proceedings of the 28th annual conference on Computer graphics and\\n\\ngies.\\ninteractive techniques, ACM (2001) 327\\u2013340\\n\\n18. Welsh, T., Ashikhmin, M., Mueller, K.: Transferring color to greyscale images.\\n\\nACM Transactions on Graphics (TOG) 21(3) (2002) 277\\u2013280\\n\\n19. Gupta, R.K., Chia, A.Y.S., Rajan, D., Ng, E.S., Zhiyong, H.: Image colorization\\nusing similar images. In: Proceedings of the 20th ACM international conference\\non Multimedia, ACM (2012) 369\\u2013378\\n\\n20. Liu, X., Wan, L., Qu, Y., Wong, T.T., Lin, S., Leung, C.S., Heng, P.A.: Intrinsic\\ncolorization. In: ACM Transactions on Graphics (TOG). Volume 27., ACM (2008)\\n152\\n\\n\\x0c16\\n\\nZhang, Isola, Efros\\n\\n21. Chia, A.Y.S., Zhuo, S., Gupta, R.K., Tai, Y.W., Cho, S.Y., Tan, P., Lin, S.: Seman-\\ntic colorization with internet images. In: ACM Transactions on Graphics (TOG).\\nVolume 30., ACM (2011) 156\\n\\n22. Deshpande, A., Rock, J., Forsyth, D.: Learning large-scale automatic image col-\\nIn: Proceedings of the IEEE International Conference on Computer\\n\\norization.\\nVision. (2015) 567\\u2013575\\n\\n23. Larsson, G., Maire, M., Shakhnarovich, G.: Learning representations for automatic\\n\\ncolorization. European Conference on Computer Vision (2016)\\n\\n24. Iizuka, S., Simo-Serra, E., Ishikawa, H.: Let there be Color!: Joint End-to-end\\nLearning of Global and Local Image Priors for Automatic Image Colorization with\\nSimultaneous Classi\\ufb01cation. ACM Transactions on Graphics (Proc. of SIGGRAPH\\n2016) 35(4) (2016)\\n\\n25. Hariharan, B., Arbel\\xb4aez, P., Girshick, R., Malik, J.: Hypercolumns for object\\nsegmentation and \\ufb01ne-grained localization. In: Proceedings of the IEEE Conference\\non Computer Vision and Pattern Recognition. (2015) 447\\u2013456\\n\\n26. Chen, L.C., Papandreou, G., Kokkinos, I., Murphy, K., Yuille, A.L.: Deeplab:\\nSemantic image segmentation with deep convolutional nets, atrous convolution,\\nand fully connected crfs. arXiv preprint arXiv:1606.00915 (2016)\\n\\n27. Yu, F., Koltun, V.: Multi-scale context aggregation by dilated convolutions. In-\\n\\nternational Conference on Learning Representations (2016)\\n\\n28. Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z.,\\nKarpathy, A., Khosla, A., Bernstein, M., et al.: Imagenet large scale visual recog-\\nnition challenge. International Journal of Computer Vision 115(3) (2015) 211\\u2013252\\n29. Zhou, B., Lapedriza, A., Xiao, J., Torralba, A., Oliva, A.: Learning deep features\\nfor scene recognition using places database. In: Advances in neural information\\nprocessing systems. (2014) 487\\u2013495\\n\\n30. Io\\ufb00e, S., Szegedy, C.: Batch normalization: Accelerating deep network training by\\n\\nreducing internal covariate shift. arXiv preprint arXiv:1502.03167 (2015)\\n\\n31. Hinton, G., Vinyals, O., Dean, J.: Distilling the knowledge in a neural network.\\n\\narXiv preprint arXiv:1503.02531 (2015)\\n\\n32. Farabet, C., Couprie, C., Najman, L., LeCun, Y.: Learning hierarchical features\\nfor scene labeling. Pattern Analysis and Machine Intelligence, IEEE Transactions\\non 35(8) (2013) 1915\\u20131929\\n\\n33. Kirkpatrick, S., Vecchi, M.P., et al.: Optimization by simmulated annealing. science\\n\\n220(4598) (1983) 671\\u2013680\\n\\n34. Efron, B.: Bootstrap methods: another look at the jackknife. In: Breakthroughs\\n\\nin Statistics. Springer (1992) 569\\u2013593\\n\\n35. Jia, Y., Shelhamer, E., Donahue, J., Karayev, S., Long, J., Girshick, R., Guadar-\\nrama, S., Darrell, T.: Ca\\ufb00e: Convolutional architecture for fast feature embedding.\\nIn: Proceedings of the 22nd ACM international conference on Multimedia, ACM\\n(2014) 675\\u2013678\\n\\n36. Kr\\xa8ahenb\\xa8uhl, P., Doersch, C., Donahue, J., Darrell, T.: Data-dependent initial-\\nizations of convolutional neural networks. International Conference on Learning\\nRepresentations (2016)\\n\\n37. Everingham, M., Van Gool, L., Williams, C.K., Winn, J., Zisserman, A.: The\\nInternational journal of computer\\n\\npascal visual object classes (voc) challenge.\\nvision 88(2) (2010) 303\\u2013338\\n\\n38. Krizhevsky, A., Sutskever, I., Hinton, G.E.: Imagenet classi\\ufb01cation with deep con-\\nvolutional neural networks. In: Advances in neural information processing systems.\\n(2012) 1097\\u20131105\\n\\n39. Everingham, M., Van Gool, L., Williams, C.K.I., Winn, J., Zisserman, A.:\\nThe PASCAL Visual Object Classes Challenge 2007 (VOC2007) Results.\\nhttp://www.pascal-network.org/challenges/VOC/voc2007/workshop/index.html\\n\\n\\x0cColorful Image Colorization\\n\\n17\\n\\n40. Everingham, M., Van Gool, L., Williams, C.K.I., Winn, J., Zisserman, A.:\\nThe PASCAL Visual Object Classes Challenge 2012 (VOC2012) Results.\\nhttp://www.pascal-network.org/challenges/VOC/voc2012/workshop/index.html\\n\\n41. Girshick, R.: Fast r-cnn. In: Proceedings of the IEEE International Conference on\\n\\nComputer Vision. (2015) 1440\\u20131448\\n\\n42. Long, J., Shelhamer, E., Darrell, T.: Fully convolutional networks for semantic\\nsegmentation. In: Proceedings of the IEEE Conference on Computer Vision and\\nPattern Recognition. (2015) 3431\\u20133440\\n\\n43. Noroozi, M., Favaro, P.: Unsupervised learning of visual representations by solving\\n\\njigsaw puzzles. arXiv preprint arXiv:1603.09246 (2016)\\n\\n44. Chakrabarti, A.: Color constancy by learning to predict chromaticity from lumi-\\n\\nnance. In: Advances in Neural Information Processing Systems. (2015) 163\\u2013171\\n\\n45. Xiao, J., Hays, J., Ehinger, K.A., Oliva, A., Torralba, A.: Sun database: Large-scale\\nscene recognition from abbey to zoo. In: Computer vision and pattern recognition\\n(CVPR), 2010 IEEE conference on, IEEE (2010) 3485\\u20133492\\n\\n46. Ratli\\ufb00, N.D., Silver, D., Bagnell, J.A.: Learning to search: Functional gradient\\n\\ntechniques for imitation learning. Autonomous Robots 27(1) (2009) 25\\u201353\\n\\n47. Patterson, G., Hays, J.: Sun attribute database: Discovering, annotating, and rec-\\nognizing scene attributes. In: Computer Vision and Pattern Recognition (CVPR),\\n2012 IEEE Conference on, IEEE (2012) 2751\\u20132758\\n\\nAppendix\\n\\nThe main paper is our ECCV 2016 camera ready submission. All networks were re-\\ntrained from scratch, and are referred to as the v2 model. Due to space constraints, we\\nwere unable to include many of the analyses presented in our original arXiv v1 paper.\\nWe include these analyses in this Appendix, which were generated from a previous v1\\nversion of the model. All models are publicly available on our website.\\n\\nSection 5 contains additional representation learning experiments. Section 6 inves-\\ntigates additional analysis on the VGG semantic interpretability test. In Section 7, we\\nexplore how low-level queues a\\ufb00ect the output. Section 8 examines the multi-modality\\nlearned in the network. Section 9 de\\ufb01nes the network architecture used. In Section 10,\\nwe compare our algorithm to previous approaches [22] and [1], and show additional\\nexamples on legacy grayscale images.\\n\\n5 Cross-Channel Encoding as Self-Supervised Feature\\n\\nLearning (continued)\\n\\nIn Section 3.2, we discussed using colorization as a pretext task for representation learn-\\ning. In addition to learning linear classi\\ufb01ers on internal layers for ImageNet classi\\ufb01ers,\\nwe run the additional experiment of learning non-linear classi\\ufb01ers, as proposed in [43].\\nEach internal layer is frozen, along with all preceding layers, and the layers on top\\nare randomly reinitialized and trained for classi\\ufb01cation. Performance is summarized in\\nTable 3. Of the unsupervised models, Noroozi et al. [43] have the highest performance\\nacross all layers. The architectural modi\\ufb01cations result in 5.6\\xd7 feature map size and\\n7.35\\xd7 model run-time, up to the pool5 layer, relative to an unmodi\\ufb01ed Alexnet. Of\\nthe remaining methods, Donahue et al. [16] performs best at conv2 and Doersch et\\nal. performs best at conv3 and conv4. Our method performs strongly throughout, and\\nbest across methods at the conv5 layer.\\n\\n\\x0c18\\n\\nZhang, Isola, Efros\\n\\nAuthor\\n\\nTraining Input\\n\\nModel\\n\\nParams Feats Runtime\\n\\n[Ref ]\\n\\nLayers\\n\\nconv2 conv3 conv4 conv5\\n\\nKrizhevsky et al. [38]\\nKrizhevsky et al. [38]\\nNoroozi & Favaro [43]\\n\\nGaussian\\n\\nDoersch et al. [14]\\nWang & Gupta [15]\\nDonahue et al. [16]\\n\\nOurs\\n\\nlabels\\nlabels\\n\\nimagenet\\nimagenet\\nimagenet\\n\\nvideos\\n\\nimagenet\\nimagenet\\n\\nrgb\\nL\\nrgb\\nrgb\\nrgb\\nrgb\\nrgb\\nL\\n\\n1.00\\n0.99\\n1.00\\n1.00\\n1.61\\n1.00\\n1.00\\n0.99\\n\\n1.00\\n1.00\\n5.60\\n1.00\\n1.00\\n1.00\\n0.87\\n0.87\\n\\n1.00\\n0.92\\n7.35\\n1.00\\n2.82\\n1.00\\n0.96\\n0.84\\n\\n\\u2013\\n\\u2013\\n\\n[43]\\n[43]\\n[43]\\n[43]\\n[16]\\n\\n\\u2013\\n\\n56.5\\n50.5\\n56.0\\n41.0\\n47.6\\n46.9\\n51.9\\n46.6\\n\\n56.5\\n50.5\\n52.4\\n34.8\\n48.7\\n42.8\\n47.3\\n43.5\\n\\n56.5\\n50.5\\n48.3\\n27.1\\n45.6\\n38.8\\n41.9\\n40.7\\n\\n56.5\\n50.5\\n38.1\\n12.0\\n30.4\\n29.8\\n31.1\\n35.2\\n\\nTable 3. ImageNet classi\\ufb01cation with nonlinear layers, as proposed in [43]. Note\\nthat some models have architectural di\\ufb00erences. We note the e\\ufb00ect of these modi\\ufb01ca-\\ntions by the number of model parameters, number of features per image, and run-time,\\nas a multiple of Alexnet [38] without modi\\ufb01cations, up to the pool5 layer. Noroozi et al.\\n[43] performs best on all layers, with denser feature maps due to smaller stride in conv1\\nlayer, along with LRN and pool ordering switched. Doersch et al. [14] remove groups\\nin conv layers. Donahue et al. [16] remove LRN layers and change ReLU to leakyReLU\\nunits. Ours removes LRN and uses a single channel input. We also note the source of\\nperformance numbers. Column Ref indicates the source for a value obtained from a\\nprevious paper.\\n\\n6 Semantic Interpretability of Colorizations\\n\\nIn Section 3.1, we investigated using the VGG classi\\ufb01er to evaluate the semantic in-\\nterpretability of our colorization results. In Section 6.1, we show the categories which\\nperform well, and the ones which perform poorly, using this metric. In Section 6.2, we\\nshow commonly confused categories after recolorization.\\n\\n6.1 Category Performance\\n\\nIn Figure 9, we show a selection of classes that have the most improvement in VGG\\nclassi\\ufb01cation with respect to grayscale, along with the classes for which our colorizations\\nhurt the most. Interestingly, many of the top classes actually have a color in their\\nname, such as the green snake, orange, and gold\\ufb01nch. The bottom classes show some\\ncommon errors of our system, such as coloring clothing incorrectly and inconsistently\\nand coloring an animal with a plausible but incorrect color. This analysis was performed\\nusing 48k images from the ImageNet validation set, and images in the top and bottom\\n10 classes are provided on the website.\\n\\nOur process for sorting categories and images is described below. For each cat-\\negory, we compute the top-5 classi\\ufb01cation performance on grayscale and recolorized\\nimages, agray, arecolor \\u2208 [0, 1]C , where C = 1000 categories. We sort the categories\\nby arecolor \\u2212 agray. The re-colored vs grayscale performance per category is shown in\\nFigure 11(a), with top and bottom 50 categories highlighted. For the top example cat-\\negories, the individual images are sorted by ascending rank of the correct classi\\ufb01cation\\nof the recolorizeed image, with tiebreakers on descending rank of the correct classi\\ufb01ca-\\ntion of the grayscale image. For the bottom example categories, the images are sorted\\nin reverse, in order to highlight the instances when recolorization results in an errant\\nclassi\\ufb01cation relative to the grayscale image.\\n\\n\\x0cColorful Image Colorization\\n\\n19\\n\\nFig. 9. Images colorized by our algorithm from selected categories. Categories are\\nsorted by VGG object classi\\ufb01cation accuracy of our colorized images relative to ac-\\ncuracy on gracyscale images. Top: example categories where our colorization helps the\\nmost. Bottom: example categories where our colorization hurts the most. Number in\\nparentheses indicates category rank amongst all 1000. Notice that the categories most\\na\\ufb00ected by colorization are those for which color information is highly diagnostic, such\\nas birds and fruits. The bottom examples show several kinds of failures: 1) arti\\ufb01cial\\nobjects such as modems and clothes have ambiguous colors; color is not very infor-\\nmative for classi\\ufb01cation, and moreover, our algorithm tends to predict an incoherent\\ndistribution of red and blue, 2) for certain categories, like the gray fox, our algorithm\\nsystematically predicts the wrong color, confusing the species.\\n\\n6.2 Common Confusions\\n\\nTo further investigate the biases in our system, we look at the common classi\\ufb01cation\\nconfusions that often occur after image recolorization, but not with the original ground\\ntruth image. Examples for some top confusions are shown in Figure 10. An image of a\\n\\u201cminibus\\u201d is often colored yellow, leading to a misclassi\\ufb01cation as \\u201cschool bus\\u201d. Animal\\nclasses are sometimes colored di\\ufb00erently than ground truth, leading to misclassi\\ufb01cation\\nto related species. Note that the colorizations are often visually realistic, even though\\nthey lead to a misclassi\\ufb01cation.\\nTo \\ufb01nd common confusions, we compute the rate of top-5 confusion Corig, Crecolor \\u2208\\n[0, 1]C\\xd7C , with ground truth colors and after recolorization. A value of Cc,d = 1 means\\nthat every image in category c was classi\\ufb01ed as category d in the top-5. We \\ufb01nd the\\nclass-confusion added after recolorization by computing A = Crecolor \\u2212 Corig, and\\nsort the o\\ufb00-diagonal entries. Figure 11(b) shows all C \\xd7 (C \\u2212 1) o\\ufb00-diagonal entries\\nof Crecolor vs Corig, with the top 100 entries from A highlighted. For each category\\npair (c, d), we extract the images that contained the confusion after recolorization, but\\n\\nGreen  snake (6)Orange (9)Gold\\ufb01nch (10)Lorikeet (2)Rapeseed (1)Pomegranate (5)Rock beauty (26)Jelly\\ufb01sh (43)Military (-1)Modem (-6)Grey fox (-26)Sweatshirt (-15)\\x0c20\\n\\nZhang, Isola, Efros\\n\\nnot with the original colorization. We then sort the images in descending order of the\\nclassi\\ufb01cation score of the confused category.\\n\\nFig. 10. Examples of some most-confused categories. Top rows show ground truth im-\\nage. Bottom rows show recolorized images. Rank of common confusion in parentheses.\\nGround truth and confused categories after recolorization are labeled.\\n\\n\\x0cColorful Image Colorization\\n\\n21\\n\\nFig. 11. (a) Performance of VGG top-5 classi\\ufb01cation on recolorized images vs grayscale\\nimages per category (b) Top-5 confusion rates with recolorizations and original colors.\\nTest was done on last 48,000 images in ImageNet validation set.\\n\\n7 Is the network exploiting low-level cues?\\n\\nUnlike many computer vision tasks that can be roughly categorized as low, mid or\\nhigh-level vision, color prediction requires understanding an image at both the pixel\\nand the semantic-level. We have investigated how colorization generalizes to high-level\\nsemantic tasks in Section 3.2. Studies of natural image statistics have shown that the\\nlightness value of a single pixel can highly constrain the likely color of that pixel: darker\\nlightness values tend to be correlated with more saturated colors [44].\\n\\nCould our network be exploiting a simple, low-level relationship like this, in order to\\npredict color?4 We tested this hypothesis with the simple demonstration in Figure 12.\\nGiven a grayscale Macbeth color chart as input, our network was unable to recover\\nits colors. This is true, despite the fact that the lightness values vary considerably for\\nthe di\\ufb00erent color patches in this image. On the other hand, given two recognizable\\nvegetables that are roughly isoluminant, the system is able to recover their color.\\n\\nIn Figure 12, we also demonstrate that the prediction is somewhat stable with\\nrespect to low-level lightness and contrast changes. Blurring, on the other hand, has a\\nbigger e\\ufb00ect on the predictions in this example, possibly because the operation removes\\nthe diagnostic texture pattern of the zucchini.\\n\\n8 Does our model learn multimodal color distributions?\\n\\nAs discussed in Section 2.1, formulating color prediction as a multinomial classi\\ufb01cation\\nproblem allows the system to predict multimodal distributions, and can capture the\\ninherent ambiguity in the color of natural objects. In Figure 13, we illustrate the\\n\\n4 E.g., previous work showed that CNNs can learn to use chromatic aberration cues\\n\\nto predict, given an image patch, its (x,y) location within an image [14].\\n\\n\\x0c22\\n\\nZhang, Isola, Efros\\n\\nFig. 12. Left: pixel lightness on its own does not reveal color, as shown by the color\\nchart. In contrast, two vegetables that are nearly isoluminant are recognized as having\\ndi\\ufb00erent colors. Right: stability of the network predictions with respect to low-level\\nimage transformations.\\n\\nprobability outputs(cid:98)Z and demonstrate that the network does indeed learn multimodal\\ndistributions. The system output (cid:98)Y is shown in the top-left of Figure 13. Each block\\nillustrates the probability map (cid:98)Zq \\u2208 [0, 1]H,W given ab bin q in the output space. For\\n\\nclarity, we show a subsampling of the Q total output bins and coarsely quantize the\\nprobability values. In Figure 13(a), the system clearly predicts a di\\ufb00erent distribution\\nfor the background vegetation and the foreground bird. The background is predicted\\nto be green, yellow, or brown, while the foreground bird is predicted to be red or\\nblue. Figure 13(b) shows that oranges can be predicted to be di\\ufb00erent colors. Lastly,\\nin Figure 13(c), the man\\u2019s sarong is predicted to be either red, pink, or purple, while\\nhis shirt is classi\\ufb01ed as turquoise, cyan or light orange. Note that despite the multi-\\nmodality of the prediction, taking the annealed-mean of the distribution produces a\\nspatially consistent prediction.\\n\\n9 Network architecture\\n\\nFigure 2 showed a diagram of our network architecture. Table 4 in this document thor-\\noughly lists the layers used in our architecture during training time. During testing,\\nthe temperature adjustment, softmax, mean, and bilinear upsampling are all imple-\\nmented as subsequent layers in a feed-forward network. Note the column showing the\\n\\n\\x0cColorful Image Colorization\\n\\n23\\n\\nFig. 13. The output probability distributions per image. The top-left image is \\ufb01nal\\nprediction of our system. The black sub-images are quantized blocks of the ab gamut.\\nHigh probabilities are shown as higher luminance and are quantized for clarity. (a)\\nBackground of bird is predicted to be green or brown. Foreground bird has distri-\\nbution across blue and red colors. (b) Oranges are predicted to be di\\ufb00erent colors.\\n(c) The person\\u2019s shirt and sarong has uncertainty across turqoise/cyan/orange and\\nred/pink/purple colors, respectively. Note that despite the multimodality of the per-\\npixel distributions, the results after taking the annealed-mean are typically spatially\\nconsistent.\\n\\ne\\ufb00ective dilation. The e\\ufb00ective dilation is the spacing at which consecutive elements\\nof the convolutional kernel are evaluated, relative to the input pixels, and is computed\\nby the product of the accumulated stride and the layer dilation. Through each convo-\\nlutional block from conv1 to conv5, the e\\ufb00ective dilation of the convolutional kernel is\\nincreased. From conv6 to conv8, the e\\ufb00ective dilation is decreased.\\n\\n10 Colorization comparisons on held-out datasets\\n\\n10.1 Comparison to LEARCH [22]\\n\\nThough our model was trained on object-centric ImageNet dataset, we demonstrate\\nthat it nonetheless remains e\\ufb00ective for photos from the scene-centric SUN dataset [45]\\nselected by Deshpande et al. [22]. Deshpande et al. recently established a benchmark\\nfor colorization using a subset of the SUN dataset and reported top results using an\\nalgorithm based on LEARCH [46]. Table 5 provides a quantitative comparison of our\\nmethod to Deshpande et al.. For fair comparison, we use the same grayscale input\\nas [22], which is R+G+B\\n. Note that this input space is non-linearly related to the\\nL channel on which we trained. Despite di\\ufb00erences in grayscale space and training\\ndataset, our method outperforms Deshpande et al. in both the raw accuracy AuC\\nCMF and perceptual realism AMT metrics. Figure 14 shows qualitative comparisons\\nbetween our method and Deshpande et al., one from each of the six scene categories.\\nA complete comparison on all 240 images are included in the supplementary material.\\nOur results are able to fool participants in the real vs. fake task 17.2% of the time,\\nsigni\\ufb01cantly higher than Deshpande et al. at 9.8%.\\n\\n3\\n\\n10.2 Comparison to Deep Colorization [1]\\n\\nWe provide qualitative comparisons to the 23 test images in [1] on the website, which\\nwe obtained by manually cropping from the paper. Our results are about the same\\n\\n\\x0c24\\n\\nZhang, Isola, Efros\\n\\nLayer\\n\\nX C S D Sa De BN L\\n\\ndata\\n\\nconv1 1\\nconv1 2\\nconv2 1\\nconv2 1\\nconv3 1\\nconv3 2\\nconv3 3\\nconv4 1\\nconv4 2\\nconv4 3\\nconv5 1\\nconv5 2\\nconv5 3\\nconv6 1\\nconv6 2\\nconv6 3\\nconv7 1\\nconv7 2\\nconv7 3\\nconv8 1\\nconv8 2\\nconv8 3\\n\\n3\\n64\\n64\\n\\n-\\n-\\n224\\n1\\n1\\n224\\n1\\n112\\n2\\n1\\n112 128 1\\n1\\n128 2\\n56\\n1\\n256 1\\n56\\n1\\n56\\n256 1\\n1\\n256 2\\n28\\n1\\n512 1\\n28\\n1\\n512 1\\n28\\n1\\n512 1\\n28\\n2\\n512 1\\n28\\n2\\n28\\n512 1\\n2\\n512 1\\n28\\n2\\n512 1\\n28\\n2\\n512 1\\n28\\n2\\n512 1\\n28\\n1\\n256 1\\n28\\n1\\n28\\n256 1\\n256 1\\n28\\n1\\n128 .5 1\\n56\\n1\\n128 1\\n56\\n56\\n128 1\\n1\\n\\n-\\n1\\n1\\n2\\n2\\n4\\n4\\n4\\n8\\n8\\n8\\n8\\n8\\n8\\n8\\n8\\n8\\n8\\n8\\n8\\n4\\n4\\n4\\n\\n-\\n-\\n-\\n1\\n-\\n-\\n(cid:88) -\\n1\\n2\\n-\\n-\\n(cid:88) -\\n2\\n4\\n-\\n-\\n4\\n-\\n-\\n(cid:88) -\\n4\\n8\\n-\\n-\\n8\\n-\\n-\\n(cid:88) -\\n8\\n16\\n-\\n-\\n16\\n-\\n-\\n16 (cid:88) -\\n16\\n-\\n16\\n-\\n16 (cid:88) -\\n-\\n-\\n8\\n-\\n8\\n-\\n(cid:88) -\\n8\\n-\\n-\\n4\\n4\\n-\\n-\\n- (cid:88)\\n4\\n\\n-\\n-\\n\\nTable 4. Our network architecture. X spatial resolution of output, C number of chan-\\nnels of output; S computation stride, values greater than 1 indicate downsampling\\nfollowing convolution, values less than 1 indicate upsampling preceding convolution;\\nD kernel dilation; Sa accumulated stride across all preceding layers (product over all\\nstrides in previous layers); De e\\ufb00ective dilation of the layer with respect to the input\\n(layer dilation times accumulated stride); BN whether BatchNorm layer was used after\\nlayer; L whether a 1x1 conv and cross-entropy loss layer was imposed\\n\\nqualitative level as [1]. Note that Deep Colorization [1] has several advantages in this\\nsetting: (1) the test images are from the SUN dataset [47], which we did not train on and\\n(2) the 23 images were hand-selected from 1344 by the authors, and is not necessarily\\nrepresentative of algorithm performance. We were unable to obtain the 1344 test set\\nresults through correspondence with the authors.\\n\\nAdditionally, we compare the methods on several important dimensions in Table 6:\\nalgorithm pipeline, learning, dataset, and run-time. Our method is faster, straightfor-\\nward to train and understand, has fewer hand-tuned parameters and components, and\\nhas been demonstrated on a broader and more diverse set of test images than Deep\\nColorization [1].\\n\\n10.3 Additional Examples on Legacy Grayscale Images\\n\\nHere, we show additional qualitative examples of applying our model to legacy black\\nand white photographs. Figures 16, 17, and 18 show examples including work of\\nrenowned photographers, such as Ansel Adams and Henri Cartier-Bresson, photographs\\nof politicians and celebrities, and old family photos. One can see that our model is often\\nable to produce good colorizations, even though the low-level image statistics of old\\nlegacy photographs are quite di\\ufb00erent from those of modern-day photos.\\n\\n\\x0cColorful Image Colorization\\n\\n25\\n\\nResults on LEARCH [22] dataset\\nAuC AMT\\nCMF Labeled\\n(%) Real (%)\\n90.1 17.2\\xb11.9\\n9.8\\xb11.5\\n\\nOurs\\nDeshpande et al. [22] 88.8\\nGrayscale\\n89.3\\n100\\nGround Truth\\n\\nAlgorithm\\n\\n\\u2013\\n50\\n\\nTable 5. Results on LEARCH [22] test set, containing 240 images from 6 categories\\nbeach, outdoor, castle, bedroom, kitchen, and living room. Results column 1 shows the\\nAuC of thresholded CMF over ab space. Results column 2 are from our AMT real vs.\\nfake test.\\n\\nFig. 14. CMF on the LEARCH [22] test set\\n\\nDeep Colorization [1]\\n\\nOurs\\n\\n(1) Extract feature sets\\n(a) 7x7 patch (b) DAISY\\n(c) FCN on 47 categories\\n(2) 3-layer NN regressor\\n(3) Joint-bilateral \\ufb01lter\\n\\nFeed-forward CNN\\n\\nExtract features. Train FCN [42]\\n\\nTrain CNN from pixels to\\n\\non pre-de\\ufb01ned categories.\\nTrain 3-layer NN regressor.\\n\\n2688/1344 images from\\nSUN [47] for train/test.\\n\\nLimited variety with\\n\\nonly scenes.\\n4.9s/image on\\n\\ncolor distribution. Tune single\\n\\nparameter on validation.\\n1.3M/10k images from\\n\\nImageNet [28] for train/test.\\n\\nBroad and diverse\\n\\nset of objects and scenes.\\n\\n21.1ms/image in Ca\\ufb00e\\n\\nAlgorithm\\n\\nLearning\\n\\nDataset\\n\\nRun-time\\n\\nMatlab implementation\\n\\non K40 GPU\\n\\nTable 6. Comparison to Deep Colorization [1]\\n\\n\\x0c26\\n\\nZhang, Isola, Efros\\n\\nFig. 15. Our model generalizes well to datasets on which it was not trained. Here\\nwe show results on the dataset from [22], which consists of six scene categories from\\nSUN [45]. Compared to the state of the art algorithm on this dataset [22], our method\\nproduces more perceptually plausible colorization (see also Table 5 and Figure 14).\\nPlease visit http://richzhang.github.io/colorization/ to see the results on all\\n240 images.\\n\\nGround truthDeshpande  et al. 2015OursInputBeachBedroomCastleKitchenLiving roomOutdoor\\x0cColorful Image Colorization\\n\\n27\\n\\nFig. 16. Applying our method to black and white photographs by Ansel Adams.\\n\\n\\x0c28\\n\\nZhang, Isola, Efros\\n\\nFig. 17. Applying our method to black and white photographs by Henri Cartier-\\nBresson.\\n\\n\\x0cColorful Image Colorization\\n\\n29\\n\\nFig. 18. Applying our method to legacy black and white photographs. Top to bottom,\\nleft to right: photo of Elvis Presley, photo of Migrant Mother by Dorothea Lange, photo\\nof Marilyn Monroe, an amateur family photo, photo by Henri Cartier-Bresson, photo\\nby Dr. David Fleay of Benjamin, the last captive thylacine which went extinct in 1936.\\n\\n\\x0c', u'IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 21, NO. 5, MAY 2013\\n\\n1\\n\\nMachine Learning Paradigms for Speech Recognition:\\n\\nAn Overview\\n\\nLi Deng, Fellow, IEEE, and Xiao Li, Member, IEEE\\n\\nAbstract\\u2014Automatic Speech Recognition (ASR) has histori-\\ncally been a driving force behind many machine learning (ML)\\nincluding the ubiquitously used hidden Markov\\ntechniques,\\nmodel, discriminative learning, structured sequence learning,\\nBayesian learning, and adaptive learning. Moreover, ML can and\\noccasionally does use ASR as a large-scale, realistic application\\nto rigorously test the effectiveness of a given technique, and to\\ninspire new problems arising from the inherently sequential and\\ndynamic nature of speech. On the other hand, even though ASR\\nis available commercially for some applications, it is largely an\\nunsolved problem\\u2014for almost all applications, the performance\\nof ASR is not on par with human performance. New insight from\\nmodern ML methodology shows great promise to advance the\\nstate-of-the-art in ASR technology. This overview article provides\\nreaders with an overview of modern ML techniques as utilized in\\nthe current and as relevant to future ASR research and systems.\\nThe intent is to foster further cross-pollination between the ML\\nand ASR communities than has occurred in the past. The article\\nis organized according to the major ML paradigms that are either\\npopular already or have potential for making signi\\ufb01cant contribu-\\ntions to ASR technology. The paradigms presented and elaborated\\nin this overview include: generative and discriminative learning;\\nsupervised, unsupervised, semi-supervised, and active learning;\\nadaptive and multi-task learning; and Bayesian learning. These\\nlearning paradigms are motivated and discussed in the context of\\nASR technology and applications. We \\ufb01nally present and analyze\\nrecent developments of deep learning and learning with sparse\\nrepresentations, focusing on their direct relevance to advancing\\nASR technology.\\n\\nIndex Terms\\u2014Machine learning,\\n\\nsu-\\npervised, unsupervised, discriminative, generative, dynamics,\\nadaptive, Bayesian, deep learning.\\n\\nspeech recognition,\\n\\nI. INTRODUCTION\\n\\nI N recent years, the machine learning (ML) and automatic\\n\\nspeech recognition (ASR) communities have had increasing\\nin\\ufb02uences on each other. This is evidenced by a number of ded-\\nicated workshops by both communities recently, and by the fact\\nthat major ML-centric conferences contain speech processing\\nsessions and vice versa. Indeed, it is not uncommon for the ML\\n\\nManuscript received December 02, 2011; revised June 04, 2012 and October\\n13, 2012; accepted December 21, 2012. Date of publication January 30, 2013;\\ndate of current version nulldate. The associate editor coordinating the review of\\nthis manuscript and approving it for publication was Prof. Zhi-Quan (Tom) Luo.\\nL. Deng is with Microsoft Research, Redmond, WA 98052 USA (e-mail:\\n\\ndeng@microsoft.com).\\n\\nX. Li was with Microsoft Research, Redmond, WA 98052 USA. She is now\\nwith Facebook Corporation, Palo Alto, CA 94025 USA (e-mail: mimily@gmail.\\ncom).\\n\\nColor versions of one or more of the \\ufb01gures in this paper are available online\\n\\nat http://ieeexplore.ieee.org.\\n\\nDigital Object Identi\\ufb01er 10.1109/TASL.2013.2244083\\n\\ncommunity to make assumptions about a problem, develop pre-\\ncise mathematical theories and algorithms to tackle the problem\\ngiven those assumptions, but then evaluate on data sets that are\\nrelatively small and sometimes synthetic. ASR research, on the\\nother hand, has been driven largely by rigorous empirical eval-\\nuations conducted on very large, standard corpora from real\\nworld. ASR researchers often found formal theoretical results\\nand mathematical guarantees from ML of less use in prelimi-\\nnary work. Hence they tend to pay less attention to these results\\nthan perhaps they should, possibly missing insight and guidance\\nprovided by the ML theories and formal frameworks even if the\\ncomplex ASR tasks are often beyond the current state-of-the-art\\nin ML.\\n\\nThis overview article is intended to provide readers of\\nIEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE\\nPROCESSING with a thorough overview of the \\ufb01eld of modern\\nML as exploited in ASR\\u2019s theories and applications, and to\\nfoster technical communications and cross pollination between\\nthe ASR and ML communities. The importance of such cross\\npollination is twofold: First, ASR is still an unsolved problem\\ntoday even though it appears in many commercial applications\\n(e.g. iPhone\\u2019s Siri) and is sometimes perceived, incorrectly, as\\na solved problem. The poor performance of ASR in many con-\\ntexts, however, renders ASR a frustrating experience for users\\nand thus precludes including ASR technology in applications\\nwhere it could be extraordinarily useful. The existing techniques\\nfor ASR, which are based primarily on the hidden Markov\\nmodel (HMM) with Gaussian mixture output distributions,\\nappear to be facing diminishing returns, meaning that as more\\ncomputational and data resources are used in developing an\\nASR system, accuracy improvements are slowing down. This\\nis especially true when the test conditions do not well match\\nthe training conditions [1], [2]. New methods from ML hold\\npromise to advance ASR technology in an appreciable way.\\nSecond, ML can use ASR as a large-scale, realistic problem to\\nrigorously test the effectiveness of the developed techniques,\\nand to inspire new problems arising from special sequential\\nproperties of speech and their solutions. All this has become\\nrealistic due to the recent advances in both ASR and ML. These\\nadvances are re\\ufb02ected notably in the emerging development\\nof the ML methodologies that are effective in modeling deep,\\ndynamic structures of speech, and in handling time series or\\nsequential data and nonlinear interactions between speech and\\nthe acoustic environmental variables which can be as complex\\nas mixing speech from other talkers; e.g., [3]\\u2013[5].\\n\\nThe main goal of this article is to offer insight from mul-\\ntiple perspectives while organizing a multitude of ASR tech-\\nniques into a set of well-established ML schemes. More specif-\\nically, we provide an overview of common ASR techniques by\\nestablishing several ways of categorization and characteriza-\\ntion of the common ML paradigms, grouped by their learning\\n\\n1558-7916/$31.00 \\xa9 2013 IEEE\\n\\n\\x0c2\\n\\nIEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 21, NO. 5, MAY 2013\\n\\nstyles. The learning styles upon which the categorization of the\\nlearning techniques are established refer to the key attributes of\\nthe ML algorithms, such as the nature of the algorithm\\u2019s input\\nor output, the decision function used to determine the classi\\ufb01ca-\\ntion or recognition output, and the loss function used in training\\nthe models. While elaborating on the key distinguishing factors\\nassociated with the different classes of the ML algorithms, we\\nalso pay special attention to the related arts developed in ASR\\nresearch.\\n\\nIn its widest scope, the aim of ML is to develop automatic\\nsystems capable of generalizing from previously observed ex-\\namples, and it does so by constructing or learning functional de-\\npendencies between arbitrary input and output domains. ASR,\\nwhich is aimed to convert the acoustic information in speech se-\\nquence data into its underlying linguistic structure, typically in\\nthe form of word strings, is thus fundamentally an ML problem;\\ni.e., given examples of inputs as the continuous-valued acoustic\\nfeature sequences (or possibly sound waves) and outputs as the\\nnominal (categorical)-valued label (word, phone, or phrase) se-\\nquences, the goal is to predict the new output sequence from a\\nnew input sequence. This prediction task is often called classi\\ufb01-\\ncation when the temporal segment boundaries of the output la-\\nbels are assumed known. Otherwise, the prediction task is called\\nrecognition. For example, phonetic classi\\ufb01cation and phonetic\\nrecognition are two different tasks: the former with the phone\\nboundaries given in both training and testing data, while the\\nlatter requires no such boundary information and is thus more\\ndif\\ufb01cult. Likewise, isolated word \\u201crecognition\\u201d is a standard\\nclassi\\ufb01cation task in ML, except with a variable dimension in\\nthe input space due to the variable length of the speech input.\\nAnd continuous speech recognition is a special type of struc-\\ntured ML problems, where the prediction has to satisfy addi-\\ntional constraints with the output having structure. These ad-\\nditional constraints for the ASR problem include: 1) linear se-\\nquence in the discrete output of either words, syllables, phones,\\nor other \\ufb01ner-grained linguistic units; and 2) segmental prop-\\nerty that the output units have minimal and variable durations\\nand thus cannot switch their identities freely.\\n\\nThe major components and topics within the space of ASR\\nare: 1) feature extraction; 2) acoustic modeling; 3) pronuncia-\\ntion modeling; 4) language modeling; and 5) hypothesis search.\\nHowever, to limit the scope of this article, we will provide the\\noverview of ML paradigms mainly on the acoustic modeling\\ncomponent, which is arguably the most important one with\\ngreatest contributions to and from ML.\\n\\nThe remaining portion of this paper is organized as follows:\\nWe provide background material in Section II, including math-\\nematical notations, fundamental concepts of ML, and some es-\\nsential properties of speech subject to the recognition process. In\\nSections III and IV, two most prominent ML paradigms, gener-\\native and discriminative learning, are presented. We use the two\\naxes of modeling and loss function to categorize and elaborate\\non numerous techniques developed in both ML and ASR areas,\\nand provide an overview on the generative and discriminative\\nmodels in historical and current use for ASR. The many types of\\nloss functions explored and adopted in ASR are also reviewed.\\nIn Section V, we embark on the discussion of active learning\\nand semi-supervised learning, two different but closely related\\nML paradigms widely used in ASR. Section VI is devoted to\\ntransfer learning, consisting of adaptive learning and multi-task\\n\\nTABLE I\\n\\nDEFINITIONS OF A SUBSET OF COMMONLY USED\\n\\nSYMBOLS AND NOTATIONS IN THIS ARTICLE\\n\\nlearning, where the former has a long and prominent history of\\nresearch in ASR and the latter is often embedded in the ASR\\nsystem design. Section VII is devoted to two emerging areas of\\nML that are beginning to make inroad into ASR technology with\\nsome signi\\ufb01cant contributions already accomplished. In partic-\\nular, as we started writing this article in 2009, deep learning\\ntechnology was only taking shape, and now in 2013 it is gaining\\nfull momentum in both ASR and ML communities. Finally,\\nin Section VIII, we summarize the paper and discuss future\\ndirections.\\n\\nII. BACKGROUND\\n\\nA. Fundamentals\\n\\nIn this section, we establish some fundamental concepts in\\nML most relevant to the ASR discussions in the remainder of\\nthis paper. We \\ufb01rst introduce our mathematical notations in\\nTable 1.\\n\\nConsider the canonical setting of classi\\ufb01cation or regression\\n\\nin machine learning. Assume that we have a training set\\n\\n,\\n\\ndrawn from the distribution\\n\\n,\\n. The goal of learning is to \\ufb01nd a decision function\\nthat correctly predicts the output of a future input\\ndrawn from the same distribution. The prediction task is called\\nclassi\\ufb01cation when the output takes categorical values, which\\nwe assume in this work. ASR is fundamentally a classi\\ufb01cation\\nproblem. In a multi-class setting, a decision function is deter-\\nmined by a set of discriminant functions, i.e.,\\n\\n(1)\\n\\nEach discriminant function\\n. In binary classi\\ufb01cation where\\ncommon to use a single \\u201cdiscriminant function\\u201d as follows,\\n\\nis a class-dependent function of\\n, however, it is\\n\\n(2)\\n\\nFormally, learning is concerned with \\ufb01nding a decision func-\\ntion (or equivalently a set of discriminant functions) that mini-\\nmizes the expected risk, i.e.,\\n\\nunder some loss function\\nmeasures the \\u201ccost\\u201d of making the decision\\n\\n. Here the loss function\\nwhile the true\\n\\n(3)\\n\\n\\x0cDENG AND LI: MACHINE LEARNING PARADIGMS FOR SPEECH RECOGNITION: AN OVERVIEW\\n\\n3\\n\\noutput is\\n; and the expected risk is simply the expected value\\nof such a cost. In ML, it is important to understand the differ-\\nence between the decision function and the loss function. The\\nformer is often referred to as the \\u201cmodel\\u201d. For example, a linear\\nmodel is a particular form of the decision function, meaning that\\ninput features are linearly combined at classi\\ufb01cation time. On\\nthe other hand, how the parameters of a linear model are esti-\\nmated depends on the loss function (or, equivalently, the training\\nobjective). A particular model can be estimated using different\\nloss functions, while the same loss function can be applied to\\na variety of models. We will discuss the choice of models and\\nloss functions in more detail in Section III and Section IV.\\n\\nApparently, the expected risk is hard to optimize directly as\\nis generally unknown. In practice, we often aim to \\ufb01nd\\n\\na decision function that minimizes the empirical risk, i.e.,\\n\\n(4)\\n\\nconverges to\\n\\nwith respect to the training set. It has been shown that, if\\nsat-\\nis\\ufb01es certain constraints,\\nin prob-\\nability for any\\n[6]. The training set, however, is almost always\\ninsuf\\ufb01cient. It is therefore crucial to apply certain type of reg-\\nularization to improve generalization. This leads to a practical\\ntraining objective referred to as accuracy-regularization which\\ntakes the following general form:\\n\\nwhere\\nand\\n\\nis a tradeoff parameter.\\n\\n(5)\\n\\n,\\n\\nis a regularizer that measures \\u201ccomplexity\\u201d of\\n\\nIn fact, a fundamental problem in ML is to derive such\\nforms of\\nthat guarantee the generalization performance\\nof learning. Among the most popular theorems on generaliza-\\ntion error bound is the VC bound theorem [7]. According to\\nthe theorem, if two models describe the training data equally\\nwell, the model with the smallest VC dimension has better\\ngeneralization performance. The VC dimension, therefore, can\\nnaturally serve as a regularizer in empirical risk minimization,\\nprovided that it has a mathematically convenient form, as in the\\ncase of large-margin hyperplanes [7], [8].\\n\\nAlternatively, regularization can be viewed from a Bayesian\\nitself is considered a random variable. One\\n, before seeing\\n. In contrast, the posterior probability of\\n\\nperspective, where\\nneeds to specify a prior belief, denoted as\\nthe training data\\nthe model is derived after training data is observed:\\n\\n(6)\\n\\nMaximizing (6) is known as maximum a posteriori (MAP) esti-\\nmation. Notice that by taking logarithm, this learning objective\\nis now represented by a\\n\\ufb01ts the general form of (5);\\nparticular loss function\\nand\\n.\\nThe choice of the prior distribution has usually been a compro-\\nmise between a realistic assessment of beliefs and choosing a\\nparametric form that simpli\\ufb01es analytical calculations. In prac-\\ntice, certain forms of the prior are preferred due mainly to their\\n\\nby\\n\\nmathematical tractability. For example, in the case of genera-\\ntive models, a conjugate prior\\nwith respect to the joint\\nsample distribution\\nis often used, so that the posterior\\n\\nbelongs to the same functional family as the prior.\\n\\nAll discussions above are based on the goal of \\ufb01nding a point-\\nestimate of the model. In the Bayesian approach, it is often ben-\\ne\\ufb01cial to have a decision function that takes into account the\\nuncertainty of the model itself. A Bayesian predictive classi\\ufb01er\\nis precisely for this purpose:\\n\\n(7)\\n\\nIn other words, instead of using one point-estimate of the model\\n(as is in MAP), we consider the entire posterior distribution,\\nthereby making the classi\\ufb01cation decision less subject to the\\nvariance of the model.\\n\\nThe use of Bayesian predictive classi\\ufb01ers apparently leads\\nto a different learning objective; it is now the posterior dis-\\ntribution\\nthat we are interested in estimating as opposed\\nto a particular\\n. As a result, the training objective becomes\\n. Similar to our earlier discussion, this objective\\ncan be estimated via empirical risk minimization with regular-\\nization. For example, McAllester\\u2019s PAC-Bayesian bound [9]\\nsuggests the following training objective,\\n\\n(8)\\n\\nwhich \\ufb01nds a posterior distribution that minimizes both the\\nmarginalized empirical risk as well as the divergence from the\\nprior distribution of the model. Similarly, Maximum entropy\\ndiscrimination [10] seeks\\nunder the constraints that\\n\\nthat minimizes\\n\\nFinally, it is worth noting that Bayesian predictive classi\\ufb01ers\\nshould be distinguished from the notion of Bayesian minimum\\nrisk (BMR) classi\\ufb01ers. The latter is a form of point-estimate\\nclassi\\ufb01ers in (1) that are based on Bayesian probabilities. We\\nwill discuss BMR in detail in the discriminative learning para-\\ndigm in Section IV.\\n\\n.\\n\\nB. Speech Recognition: A Structured Sequence Classi\\ufb01cation\\nProblem in Machine Learning\\n\\nHere we address the fundamental problem of ASR. From\\na functional view, ASR is the conversion process from the\\nacoustic data sequence of speech into a word sequence. From\\nthe technical view of ML, this conversion process of ASR re-\\nquires a number of sub-processes including the use of (discrete)\\ntime stamps, often called frames, to characterize the speech\\nwaveform data or acoustic features, and the use of categorical\\nlabels (e.g. words, phones, etc.) to index the acoustic data\\nsequence. The fundamental issues in ASR lie in the nature of\\nsuch labels and data. It is important to clearly understand the\\nunique attributes of ASR, in terms of both input data and output\\nlabels, as a central motivation to connect the ASR and ML\\nresearch areas and to appreciate their overlap.\\n\\nFrom the output viewpoint, ASR produces sentences that con-\\nsist of a variable number of words. Thus, at least in principle, the\\nnumber of possible classes (sentences) for the classi\\ufb01cation is so\\nlarge that it is virtually impossible to construct ML models for\\ncomplete sentences without the use of structure. From the input\\n\\n\\x0c4\\n\\nIEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 21, NO. 5, MAY 2013\\n\\nviewpoint, the acoustic data are also a sequence with a variable\\nlength, and typically, the length of data input is vastly different\\nfrom that of label output, giving rise to the special problem of\\nsegmentation or alignment that the \\u201cstatic\\u201d classi\\ufb01cation prob-\\nlems in ML do not encounter. Combining the input and output\\nviewpoints, we state the fundamental problem as a structured\\nsequence classi\\ufb01cation task, where a (relatively long) sequence\\nof acoustic data is used to infer a (relatively short) sequence of\\nthe linguistic units such as words. More detailed exposition on\\nthe structured nature of input and output of the ASR problem\\ncan be found in [11], [12].\\n\\nIt is worth noting that the sequence structure (i.e. sentence)\\nin the output of ASR is generally more complex than most of\\nclassi\\ufb01cation problems in ML where the output is a \\ufb01xed, \\ufb01nite\\nset of categories (e.g., in image classi\\ufb01cation tasks). Further,\\nwhen sub-word units and context dependency are introduced to\\nconstruct structured models for ASR, even greater complexity\\ncan arise than the straightforward word sequence output in ASR\\ndiscussed above.\\n\\nThe more interesting and unique problem in ASR, however,\\nis on the input side, i.e., the variable-length acoustic-feature se-\\nquence. The unique characteristic of speech as the acoustic input\\nto ML algorithms makes it a sometimes more dif\\ufb01cult object for\\nthe study than other (static) patterns such as images. As such, in\\nthe typical ML literature, there has typically been less emphasis\\non speech and related \\u201ctemporal\\u201d patterns than on other signals\\nand patterns.\\n\\nThe unique characteristic of speech lies primarily in its tem-\\nporal dimension\\u2014in particular, in the huge variability of speech\\nassociated with the elasticity of this temporal dimension. As a\\nconsequence, even if two output word sequences are identical,\\nthe input speech data typically have distinct lengths; e.g., dif-\\nferent input samples from the same sentence usually contain dif-\\nferent data dimensionality depending on how the speech sounds\\nare produced. Further, the discriminative cues among separate\\nspeech classes are often distributed over a reasonably long tem-\\nporal span, which often crosses neighboring speech units. Other\\nspecial aspects of speech include class-dependent acoustic cues.\\nThese cues are often expressed over diverse time spans that\\nwould bene\\ufb01t from different lengths of analysis windows in\\nspeech analysis and feature extraction. Finally, distinguished\\nfrom other classi\\ufb01cation problems commonly studied in ML,\\nthe ASR problem is a special class of structured pattern recog-\\nnition where the recognized patterns (such as phones or words)\\nare embedded in the overall temporal sequence pattern (such as\\na sentence).\\n\\nConventional wisdom posits that speech is a one-dimensional\\ntemporal signal in contrast to image and video as higher di-\\nmensional signals. This view is simplistic and does not capture\\nthe essence and dif\\ufb01culties of the ASR problem. Speech is best\\nviewed as a two-dimensional signal, where the spatial (or fre-\\nquency or tonotopic) and temporal dimensions have vastly dif-\\nferent characteristics, in contrast to images where the two spatial\\ndimensions tend to have similar properties. The \\u201cspatial\\u201d dimen-\\nsion in speech is associated with the frequency distribution and\\nrelated transformations, capturing a number of variability types\\nincluding primarily those arising from environments, speakers,\\naccent, speaking style and rate. The latter type induces correla-\\n\\nFig. 1. An overview of ML paradigms and their distinct characteristics.\\n\\ntions between spatial and temporal dimensions, and the environ-\\nment factors include microphone characteristics, speech trans-\\nmission channel, ambient noise, and room reverberation.\\n\\nThe temporal dimension in speech, and in particular its\\ncorrelation with the spatial or frequency-domain properties of\\nspeech, constitutes one of the unique challenges for ASR. Some\\nof the advanced generative models associated with the genera-\\ntive learning paradigm of ML as discussed in Section III have\\naimed to address this challenge, where Bayesian approaches\\nare used to provide temporal constraints as prior knowledge\\nabout the human speech generation process.\\n\\nC. A High-Level Summary of Machine Learning Paradigms\\n\\nBefore delving into the overview detail, here in Fig. 1 we\\nprovide a brief summary of the major ML techniques and\\nparadigms to be covered in the remainder of this article. The\\nfour columns in Fig. 1 represent the key attributes based on\\nwhich we organize our overview of a series of ML paradigms.\\nIn short, using the nature of the loss function (as well as the\\ndecision function), we divide the major ML paradigms into\\ngenerative and discriminative learning categories. Depending\\non what kind of training data are available for learning, we\\nalternatively categorize the ML paradigms into supervised,\\nsemi-supervised, unsupervised, and active learning classes.\\nWhen disparity between source and target distributions arises,\\na more common situation in ASR than many other areas of ML\\napplications, we classify the ML paradigms into single-task,\\nmulti-task, and adaptive learning. Finally, using the attribute of\\ninput representation, we have sparse learning and deep learning\\nparadigms, both more recent developments in ML and ASR\\nand connected to other ML paradigms in multiple ways.\\n\\nIII. GENERATIVE LEARNING\\n\\nGenerative learning and discriminative learning are the two\\nmost prevalent, antagonistically paired ML paradigms devel-\\noped and deployed in ASR. There are two key factors that distin-\\nguish generative learning from discriminative learning: the na-\\nture of the model (and hence the decision function) and the loss\\nfunction (i.e., the core term in the training objective). Brie\\ufb02y\\nspeaking, generative learning consists of\\n\\n\\u2022 Using a generative model, and\\n\\u2022 Adopting a training objective function based on the joint\\n\\nlikelihood loss de\\ufb01ned on the generative model.\\n\\nDiscriminative learning, on the other hand, requires either\\n\\n\\u2022 Using a discriminative model, or\\n\\n\\x0cDENG AND LI: MACHINE LEARNING PARADIGMS FOR SPEECH RECOGNITION: AN OVERVIEW\\n\\n5\\n\\n\\u2022 Applying a discriminative training objective function to a\\n\\ngenerative model.\\n\\nIn this and the next sections, we will discuss generative vs.\\ndiscriminative learning from both the model and loss function\\nperspectives. While historically there has been a strong associ-\\nation between a model and the loss function chosen to train the\\nmodel, there has been no necessary pairing of these two com-\\nponents in the literature [13]. This section will offer a decou-\\npled view of the models and loss functions commonly used in\\nASR for the purpose of illustrating the intrinsic relationship and\\ncontrast between the paradigms of generative vs. discrimina-\\ntive learning. We also show the hybrid learning paradigm con-\\nstructed using mixed generative and discriminative learning.\\n\\nThis section, starting below, is devoted to the paradigm of\\ngenerative learning, and the next Section IV to the discrimina-\\ntive learning counterpart.\\n\\nA. Models\\n\\nGenerative learning requires using a generative model and\\nhence a decision function derived therefrom. Speci\\ufb01cally, a\\ngenerative model is one that describes the joint distribution\\ndenotes generative model parameters. In\\nclassi\\ufb01cation, the discriminant functions have the following\\ngeneral form:\\n\\n, where\\n\\n(9)\\n\\n.\\n\\nAs a result, the output of the decision function in (1) is the class\\nlabel that produces the highest joint likelihood. Notice that de-\\npending on the form of the generative model, the discriminant\\nfunction and hence the decision function can be greatly sim-\\npli\\ufb01ed. For example, when\\nare Gaussian distributions\\nwith the same covariance matrix,\\n, for all classes can be\\nreplaced by an af\\ufb01ne function of\\n\\nOne simplest form of generative models is the na\\xefve Bayes\\nclassi\\ufb01er, which makes strong independence assumptions that\\nfeatures are independent of each other given the class label. Fol-\\nlowing this assumption,\\nis decomposed to a product of\\nsingle-dimension feature distributions\\n. The fea-\\nture distribution at one dimension can be either discrete or con-\\ntinuous, either parametric or non-parametric. In any case, the\\nbeauty of the na\\xefve Bayes approach is that the estimation of\\none feature distribution is completely decoupled from the es-\\ntimation of others. Some applications have observed bene\\ufb01ts\\nby going beyond the na\\xefve Bayes assumption and introducing\\ndependency, partially or completely, among feature variables.\\nOne such example is a multivariate Gaussian distribution with\\na block-diagonal or full convariance matrix.\\n\\nOne can introduce latent variables to model more complex\\ndistributions. For example, latent topic models such as proba-\\nbilistic Latent Semantic Analysis (pLSA) and Latent Dirichilet\\nAllocation (LDA), are widely used as generative models for text\\ninputs. Gaussian mixture models (GMM) are able to approxi-\\nmate any continuous distribution with suf\\ufb01cient precision. More\\ngenerally, dependencies between latent and observed variables\\ncan be represented in a graphical model framework [14].\\n\\nThe notion of graphical models is especially interesting when\\ndealing with structured output. Dynamic Bayesian network is a\\ndirected acyclic graph with vertices representing variables and\\nedges representing possible direct dependence relations among\\n\\nthe variables. A Bayesian network represents all probability\\ndistributions that validly factor according to the network. The\\njoint distribution of all variables in a distribution corresponding\\nto the network factorizes over variables given their parents,\\ni.e.\\n. By having fewer\\nedges in the graph, the network has stronger conditional inde-\\npendence properties and the resulting model has fewer degrees\\nof freedom. When an integer expansion parameter representing\\ndiscrete time is associated with a Bayesian network, and a set of\\nrules is given to connect together two successive such \\u201cchunks\\u201d\\nof Bayesian network, then a dynamic Bayesian network arises.\\nFor example, hidden Markov models (HMMs), with simple\\ngraph structures, are among the most popularly used dynamic\\nBayesian networks.\\n\\nSimilar to a Bayesian network, a Markov random \\ufb01eld (MRF)\\nis a graph that expresses requirements over a family of proba-\\nbility distributions. A MRF, however, is an undirected graph,\\nand thus is capable of representing certain distributions that\\na Bayesian network can not represent. In this case, the joint\\ndistribution of the variables is the product of potential func-\\ntions over cliques (the maximal fully-connected sub-graphs).\\nFormally,\\nis\\nthe potential function for clique\\nis a normalization\\nconstant. Again, the graph structure has a strong relation to the\\nmodel complexity.\\n\\n, where\\n\\n, and\\n\\nB. Loss Functions\\n\\nAs mentioned in the beginning of this section, generative\\nlearning requires using a generative model and a training ob-\\njective based on joint likelihood loss, which is given by\\n\\n(10)\\n\\nand\\n\\nOne advantage of using the joint likelihood loss is that the loss\\nfunction can often be decomposed into independent sub-prob-\\nlems which can be optimized separately. This is especially ben-\\ne\\ufb01cial when the problem is to predict structured output (such\\nas a sentence output of an ASR system), denoted as bolded .\\nFor example, in a Beysian network,\\ncan be conveniently\\nrewritten as\\ncan be\\nfurther decomposed according to the input and output structure.\\nIn the following subsections, we will present several joint like-\\nlihood forms widely used in ASR.\\n\\n, where each of\\n\\nThe generative model\\u2019s parameters learned using the above\\ntraining objective are referred to as maximum likelihood esti-\\nmates (MLE), which is statistically consistent under the assump-\\ntions that (a) the generative model structure is correct, (b) the\\ntraining data is generated from the true distribution, and (c) we\\nhave an in\\ufb01nite amount of such training data. In practice, how-\\never, the model structure we choose can be wrong and training\\ndata is almost never suf\\ufb01cient, making MLE suboptimal for\\nlearning tasks. Discriminative loss functions, as will be intro-\\nduced in Section IV, aim at directly optimizing predicting per-\\nformance rather than solving a more dif\\ufb01cult density estimation\\nproblem.\\n\\nC. Generative Learning in Speech Recognition\\u2014An Overview\\nIn ASR, the most common generative learning approach\\nis based on Gaussian-Mixture-Model based Hidden Markov\\nmodels, or GMM-HMM; e.g., [15]\\u2013[18]. A GMM-HMM is\\n\\n\\x0c6\\n\\nIEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 21, NO. 5, MAY 2013\\n\\n.\\n\\nis a set where\\n\\nis a vector of state prior\\nparameterized by\\nprobabilities;\\nis a state transition probability matrix;\\nand\\nrepresents the Gaussian\\nmixture model of state . The state is typically associated with a\\nsub-segment of a phone in speech. One important innovation in\\nASR is the introduction of context-dependent states (e.g. [19]),\\nmotivated by the desire to reduce output variability associated\\nwith each state, a common strategy for \\u201cdetailed\\u201d generative\\nmodeling. A consequence of using context dependency is a\\nvast expansion of the HMM state space, which, fortunately,\\ncan be controlled by regularization methods such as state\\ntying. (It turns out that such context dependency also plays\\na critical role in the more recent advance of ASR in the area\\nof discriminative-based deep learning [20], to be discussed in\\nSection VII-A.)\\n\\nThe introduction of the HMM and the related statistical\\nmethods to ASR in mid 1970s [21], [22] can be regarded the\\nmost signi\\ufb01cant paradigm shift in the \\ufb01eld, as discussed in [1].\\nOne major reason for this early success was due to the highly\\nef\\ufb01cient MLE method invented about ten years earlier [23].\\nThis MLE method, often called the Baum-Welch algorithm,\\nhad been the principal way of training the HMM-based ASR\\nsystems until 2002, and is still one major step (among many)\\nin training these systems nowadays. It is interesting to note\\nthat the Baum-Welch algorithm serves as one major motivating\\nexample for the later development of the more general Expec-\\ntation-Maximization (EM) algorithm [24].\\n\\nThe goal of MLE is to minimize the empirical risk with re-\\nspect to the joint likelihood loss (extended to sequential data),\\ni.e.,\\n\\nto the well-known weaknesses of the HMM. The remaining part\\nof this section and part of Section VII will aim to address ways\\nof using more advanced ML models and techniques for speech.\\nAnother clear success of the generative learning paradigm in\\nASR is the use of GMM-HMM as prior \\u201cknowledge\\u201d within\\nthe Bayesian framework for environment-robust ASR. The\\nmain idea is as follows. When the speech signal, to be recog-\\nnized, is mixed with noise or another non-intended speaker,\\nthe observation is a combination of the signal of interest\\nand interference of no interest, both unknown. Without prior\\ninformation, the recovery of the speech of interest and its\\nrecognition would be ill de\\ufb01ned and subject to gross errors.\\nExploiting generative models of Gaussian-mixture HMM (also\\nserving the dual purpose of recognizer), or often a simpler\\nGaussian mixture or even a single Gaussian, as Bayesian prior\\nfor \\u201cclean\\u201d speech overcomes the ill-posed problem. Further,\\nthe generative approach allows probabilistic construction of the\\nmodel for the relationship among the noisy speech observation,\\nclean speech, and interference, which is typically nonlinear\\nwhen the log-domain features are used. A set of generative\\nlearning approaches in ASR following this philosophy are vari-\\nably called \\u201cparallel model combination\\u201d [26], vector Taylor\\nseries (VTS) method [27], [28], and Algonquin [29]. Notably,\\nthe comprehensive application of such a generative learning\\nparadigm for single-channel multitalker speech recognition is\\nreported and reviewed in [5], where the authors apply success-\\nfully a number of well established ML methods including loopy\\nbelief propagation and structured mean-\\ufb01eld approximation.\\nUsing this generative learning scheme, ASR accuracy with loud\\ninterfering speakers is shown to exceed human performance.\\n\\n(11)\\n\\nD. Trajectory/Segment Models\\n\\nrepresents acoustic data, usually in the form of a se-\\nwhere\\nquence feature vectors extracted at frame-level;\\nrepresents a\\nsequence of linguistic units. In large-vocabulary ASR systems,\\nit is normally the case that word-level labels are provided, while\\nstate-level labels are latent. Moreover, in training HMM-based\\nASR systems, parameter tying is often used as a type of reg-\\nularization [25]. For example, similar acoustic states of the tri-\\nphones can share the same Gaussian mixture model. In this case,\\nthe\\n\\nterm in (5) is expressed by\\n\\n(12)\\n\\nwhere\\n\\nrepresents a set of tied state pairs.\\n\\nThe use of the generative model of HMMs, including the most\\npopular Gaussian-mixture HMM, for representing the (piece-\\nwise stationary) dynamic speech pattern and the use of MLE for\\ntraining the tied HMM parameters constitute one most promi-\\nnent and successful example of generative learning in ASR.\\nThis success was \\ufb01rmly established by the ASR community,\\nand has been widely spread to the ML and related communi-\\nties; in fact, HMM has become a standard tool not only in ASR\\nbut also in ML and their related \\ufb01elds such as bioinformatics\\nand natural language processing. For many ML as well as ASR\\nresearchers, the success of HMM in ASR is a bit surprising due\\n\\nDespite some success of GMM-HMMs in ASR, their weak-\\nnesses, such as the conditional independence assumption, have\\nbeen well known for ASR applications [1], [30]. Since early\\n1990\\u2019s, ASR researchers have begun the development of statis-\\ntical models that capture the dynamic properties of speech in\\nthe temporal dimension more faithfully than HMM. This class\\nof beyond-HMM models have been variably called stochastic\\nsegment model [31], [32], trended or nonstationary-state HMM\\n[33], [34], trajectory segmental model [32], [35], trajectory\\nHMMs [36], [37], stochastic trajectory models [38], hidden dy-\\nnamic models [39]\\u2013[45], buried Markov models [46], structured\\nspeech model [47], and hidden trajectory model [48] depending\\non different \\u201cprior knowledge\\u201d applied to the temporal structure\\nof speech and on various simplifying assumptions to facilitate\\nthe model implementation. Common to all these beyond-HMM\\nmodels is some temporal trajectory structure built into the\\nmodels, hence trajectory models. Based on the nature of such\\nstructure, we can classify these models into two main cate-\\ngories. In the \\ufb01rst category are the models focusing on temporal\\ncorrelation structure at the \\u201csurface\\u201d acoustic level. The second\\ncategory consists of hidden dynamics, where the underlying\\nspeech production mechanisms are exploited as the Bayesian\\nprior to represent the \\u201cdeep\\u201d temporal structure that accounts\\nfor the observed speech pattern. When the mapping from the\\nhidden dynamic layer to the observation layer limited to linear\\n(and deterministic), then the generative hidden dynamic models\\nin the second category reduces to the \\ufb01rst category.\\n\\n\\x0cDENG AND LI: MACHINE LEARNING PARADIGMS FOR SPEECH RECOGNITION: AN OVERVIEW\\n\\n7\\n\\nThe temporal span of the generative trajectory models in both\\ncategories above is controlled by a sequence of linguistic labels,\\nwhich segment the full sentence into multiple regions from left\\nto right; hence segment models.\\n\\nIn a general form, the trajectory/segment models with hidden\\ndynamics makes use of the switching state space formulation,\\nintensely studied in ML as well as in signal processing and\\ncontrol. They use temporal recursion to de\\ufb01ne the hidden dy-\\nnamics,\\n, which may correspond to articulatory movement\\nduring human speech production. Each discrete region or seg-\\nment,\\n, of such dynamics is characterized by the -dependent\\nparameter set\\n.\\nThe memory-less nonlinear mapping function is exploited to\\nlink the hidden dynamic vector\\nto the observed acoustic\\nfeature vector\\n, with the \\u201cobservation noise\\u201d denoted by\\n, and parameterized also by segment-dependent parame-\\nters. The combined \\u201cstate equation\\u201d (13) and \\u201cobservation equa-\\ntion\\u201d (14) below form a general switching nonlinear dynamic\\nsystem model:\\n\\n, with the \\u201cstate noise\\u201d denoted by\\n\\n(13)\\n(14)\\n\\nwhere subscripts\\n\\nand\\nare time varying and may be asynchronous with each other.\\ndenotes the dynamic region correlated with phonetic\\n\\nindicate that the functions\\n\\nand\\n\\nor\\n\\ncategories.\\n\\nand\\n\\nThere have been several studies on switching nonlinear state\\nspace models for ASR, both theoretical [39], [49] and experi-\\nmental [41]\\u2013[43], [50]. The speci\\ufb01c forms of the functions of\\nand their parameterization are\\ndetermined by prior knowledge based on current understanding\\nof the nature of the temporal dimension in speech. In particular,\\nstate equation (13) takes into account the temporal elasticity in\\nspontaneous speech and its correlation with the \\u201cspatial\\u201d prop-\\nerties in hidden speech dynamics such as articulatory positions\\nor vocal tract resonance frequencies; see [45] for a comprehen-\\nsive review of this body of work.\\n\\nWhen nonlinear functions of\\n\\nand\\n\\nin (13) and (14) are reduced to linear functions (and when syn-\\nchrony between the two equations are eliminated), the switching\\nnonlinear dynamic system model is reduced to its linear coun-\\nterpart, or switching linear dynamic system (SLDS). The SLDS\\ncan be viewed as a hybrid of standard HMMs and linear dynam-\\nical systems, with a general mathematical description of\\n\\n(15)\\n(16)\\n\\nThere has also been an interesting set of work on SLDS\\napplied to ASR. The early set of studies have been carefully\\nreviewed in [32] for generative speech modeling and for its\\nASR applications. More recently, the studies reported in [51],\\n[52] applied SLDS to noise-robust ASR and explored several\\napproximate inference techniques, overcoming intractability in\\ndecoding and parameter learning. The study reported in [53]\\napplied another approximate inference technique, a special type\\nof Gibbs sampling commonly used in ML, to an ASR problem.\\ntrajectory/segment models\\nfor ASR, a number of ML techniques invented originally\\n\\nDuring the development of\\n\\nin non-ASR communities, e.g. variational\\nlearning [50],\\npseudo-Bayesian [43], [51], Kalman \\ufb01ltering [32], extended\\nKalman \\ufb01ltering [39], [45], Gibbs sampling [53], orthogonal\\npolynomial regression [34], etc., have been usefully applied\\nwith modi\\ufb01cations and improvement to suit the speech-speci\\ufb01c\\nproperties and ASR applications. However, the success has\\nmostly been limited to small-scale tasks. We can identify four\\nmain sources of dif\\ufb01culty (as well as new opportunities) in suc-\\ncessful applications of trajectory/segment models to large-scale\\nASR. First, scienti\\ufb01c knowledge on the precise nature of the\\nunderlying articulatory speech dynamics and its deeper articu-\\nlatory control mechanisms is far from complete. Coupled with\\nthe need for ef\\ufb01cient computation in training and decoding\\nfor ASR applications, such knowledge was forced to be again\\nsimpli\\ufb01ed, reducing the modeling power and precision further.\\nSecond, most of the work in this area has been placed within\\nthe generative learning setting, having a goal of providing\\nparsimonious accounts (with small parameter sets) for speech\\nvariations due to contextual factors and co-articulation. In con-\\ntrast, the recent joint development of deep learning by both ML\\nand ASR communities, which we will review in Section VII,\\ncombines generative and discriminative learning paradigms\\nand makes use of massive instead of parsimonious parameters.\\nThere is a huge potential for synergy of research here. Third,\\nalthough structural ML learning of switching dynamic systems\\nvia Bayesian nonparametrics has been maturing and producing\\nsuccessful applications in a number of ML and signal pro-\\ncessing tasks (e.g. the tutorial paper [54]), it has not entered\\nmainstream ASR; only isolated studies have been reported\\non using Bayesian nonparametrics for modeling aspects of\\nspeech dynamics [55] and for language modeling [56]. Finally,\\nmost of the trajectory/segment models developed by the ASR\\ncommunity have focused on only isolated aspects of speech\\ndynamics rooted in deep human production mechanisms, and\\nhave been constructed using relatively simple and largely stan-\\ndard forms of dynamic systems. More comprehensive modeling\\nand learning/inference algorithm development would require\\nthe use of more general graphical modeling tools advanced by\\nthe ML community. It is this topic that the next subsection is\\ndevoted to.\\n\\nE. Dynamic Graphical Models\\n\\nThe generative trajectory/segment models for speech dy-\\nnamics just described typically took specialized forms of the\\nmore general dynamic graphical model. Overviews on the\\ngeneral use of dynamic Bayesian networks, which belong to\\ndirected form of graphical models, for ASR have been provided\\nin [4], [57], [58]. The undirected form of graphical models,\\nincluding Markov random \\ufb01eld and the product of experts\\nmodel as its special case, has been applied successfully in\\nHMM-based parametric speech synthesis research and systems\\n[59]. However, the use of undirected graphical models has not\\nbeen as popular and successful. Only quite recently, a restricted\\nform of the Markov random \\ufb01eld, called restricted Boltzmann\\nmachine (RBM), has been successfully used as one of the\\nseveral components in the speech model for use in ASR. We\\nwill discuss RBM for ASR in Section VII-A.\\n\\nAlthough the dynamic graphical networks have provided\\nhighly generalized forms of generative models for speech\\n\\n\\x0c8\\n\\nIEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 21, NO. 5, MAY 2013\\n\\nmodeling, some key sequential properties of the speech signal,\\ne.g. those reviewed in Section II-B, have been expressed in\\nspecially tailored forms of dynamic speech models, or the tra-\\njectory/segment models reviewed in the preceding subsection.\\nSome of these models applied to ASR have been formulated and\\nexplored using the dynamic Bayesian network framework [4],\\n[45], [60], [61], but they have focused on only isolated aspects\\nof speech dynamics. Here, we expand the previous use of the\\ndynamic Bayesian network and provide more comprehensive\\nmodeling of deep generative mechanisms of human speech.\\n\\nof length\\n\\nShown in Fig. 2 is an example of the directed graphical\\nmodel or Bayesian network representation of the observable\\ndistorted speech feature sequence\\ngiven its \\u201cdeep\\u201d generative causes from both top-down and\\nbottom up directions. The top-down causes represented in Fig. 2\\ninclude the phonological/pronunciation model (denoted by se-\\nquence\\n), articulatory control model (denoted by\\nsequence\\n), articulatory dynamic model (denoted\\nby sequence\\n), and the articultory-to-acoustic\\nmapping model (denoted by the conditional relation from\\n). The bottom-up causes in-\\nclude nonstationary distortion model, and the interaction model\\namong \\u201chidden\\u201d clean speech, observed distorted speech, and\\nthe environmental distortion such as channel and noise.\\n\\nThe semantics of the Bayesian network in Fig. 2, which spec-\\ni\\ufb01es dependency among a set of time varying random variables\\ninvolved in the full speech production process and its interac-\\ntions with acoustic environments, is summarized below. First,\\nthe probabilistic segmental property of the target process is rep-\\nresented by the conditional probability [62]:\\n\\nto\\n\\nSecond, articulatory dynamics controlled by the target\\n\\nprocess is given by the conditional probability:\\n\\n,\\n\\n.\\n\\n(17)\\n\\nequivalently the\\n\\nor\\nstate-space formulation [63]:\\n\\ntarget-directed state\\n\\n(18)\\n\\nequation with\\n\\n(19)\\n\\nThird, the \\u201cobservation\\u201d equation in the state-space model\\ngoverning the relationship between distortion-free acoustic fea-\\ntures of speech and the corresponding articulatory con\\ufb01guration\\nis represented by\\n\\ncounterpart\\nstationary channel distortion\\n\\n, on the non-stationary noise\\n\\nis represented by\\n\\n, and on the\\n\\n(21)\\n\\nwhere the distribution\\non the prediction residual has typically\\ntaken a Gaussian form with a constant variance [29] or with an\\nSNR-dependent variance [64].\\n\\nInference and learning in the comprehensive generative\\nmodel of speech shown in Fig. 2 are clearly not tractable.\\nNumerous sub-problems and model components associated\\nwith the overall model have been explored or solved using\\ninference and learning algorithm developed in ML; e.g. varia-\\ntional learning [50] and other approximate inference methods\\n[5], [45], [53]. Recently proposed new techniques for learning\\ngraphical model parameters given all sorts of approximations\\n(in inference, decoding, and graphical model structure) are in-\\nteresting alternatives to overcoming the intractability problem\\n[65].\\n\\nDespite the intractable nature of the learning problem in com-\\nprehensive graphical modeling of the generative process for\\nhuman speech, it is our belief that accurate \\u201cgenerative\\u201d rep-\\nresentation of structured speech dynamics holds a key to the\\nultimate success of ASR. As will be discussed in Section VII,\\nrecent advance of deep learning has reduced ASR errors sub-\\nstantially more than the purely generative graphical modeling\\napproach while making much weaker use of the properties of\\nspeech dynamics. Part of that success comes from well designed\\nintegration of (unstructured) generative learning with discrimi-\\nnative learning (although more serious but dif\\ufb01cult modeling of\\ndynamic processes with temporal memory based on deep recur-\\nrent neural networks is a new trend). We devote the next section\\nto discriminative learning, noting a strong future potential of\\nintegrating structured generative learning discussed in this sec-\\ntion with the increasingly successful deep learning scheme with\\na hybrid generative-discriminative learning scheme, a subject of\\nSection VII-A.\\n\\nIV. DISCRIMINATIVE LEARNING\\n\\nAs discussed earlier, the paradigm of discriminative learning\\ninvolves either using a discriminative model or applying dis-\\ncriminative training to a generative model. In this section, we\\n\\ufb01rst provide a general discussion of the discriminative models\\nand of the discriminative loss functions used in training, fol-\\nlowed by an overview of the use of discriminative learning in\\nASR applications including its successful hybrid with genera-\\ntive learning.\\n\\n(20)\\n\\nA. Models\\n\\nis the distortion-free speech vector,\\n\\nwhere\\nservation noise vector uncorrelated with the state noise\\n\\nis the ob-\\n, and\\nis the static memory-less transformation from the articula-\\nwas imple-\\n\\ntory vector to its corresponding acoustic vector.\\nmented by a neural network in [63].\\n\\nFinally, the dependency of the observed environmentally-dis-\\non its distortion-free\\n\\ntorted acoustic features of speech\\n\\nDiscriminative models make direct use of the conditional re-\\nlation of labels given input vectors. One major school of such\\nmodels are referred to as Bayesian Mininum Risk (BMR) clas-\\nsi\\ufb01ers [66]\\u2013[68]:\\n\\n(22)\\n\\n\\x0cDENG AND LI: MACHINE LEARNING PARADIGMS FOR SPEECH RECOGNITION: AN OVERVIEW\\n\\n9\\n\\nthe discriminant functions in (24) can be equivalently replaced\\nby (25), by ignoring their common denominators.\\n\\nB. Loss Functions\\n\\nThis section introduces a number of discriminative loss func-\\ntions. The \\ufb01rst group of loss functions are based on probabilistic\\nmodels, while the second group on the notion of margin.\\n\\n1) Probability-Based Loss: Similar to the joint likelihood\\nloss discussed in the preceding section on generative learning,\\nconditional likelihood loss is a probability-based loss function\\nbut is de\\ufb01ned upon the conditional relation of class labels given\\ninput features:\\n\\n(26)\\n\\nThis loss function is strongly tied to probabilistic discrimina-\\ntive models such as conditional log linear models and MLPs,\\nwhile they can be applied to generative models as well, leading\\nto a school of discriminative training methods which will be\\ndiscussed shortly. Moreover, conditional likelihood loss can be\\nnaturally extended to predicting structure output. For example,\\nwhen applying (26) to Markov random \\ufb01elds, we obtain the\\ntraining objective of conditional random \\ufb01elds (CRFs) [70]:\\n\\n(27)\\n\\nis a normalization factor.\\n\\nis a\\nThe partition function\\nweight vector and\\nis a vector of feature functions re-\\nferred to as a feature vector. In ASR tasks where state-level la-\\nbels are usually unknown, hidden CRF have been introduced to\\nmodel conditional likelihood with the presence of hidden vari-\\nables [71], [72]:\\n\\n(28)\\n\\nNote that in most of the ML as well as the ASR literature, one\\noften calls the training method using the conditional likelihood\\nloss above as simply maximal likelihood estimation (MLE).\\nReaders should not confuse this type of discriminative learning\\nwith the MLE in the generative learning paradigm we discussed\\nin the preceding section.\\n\\nA generalization of conditional likelihood loss is Minimum\\nBayes Risk training. This is consistent with the criterion of MBR\\nclassi\\ufb01ers described in the previous subsection. The loss func-\\ntion of (MBR) in training is given by\\n\\n(29)\\n\\nwhere\\nis the cost (loss) function used in classi\\ufb01cation. This\\nloss function is especially useful in models with structured\\noutput; dissimilarity between different outputs\\ncan be formu-\\nlated using the cost function, e.g., word or phone error rates\\nin speech recognition [73]\\u2013[75], and BLEU score in machine\\ntranslation [76]\\u2013[78]. When\\nis based on 0\\u20131 loss, (29) is\\nreduced to conditional likelihood loss.\\n\\n2) Margin-Based Loss: Margin-based loss, as discussed and\\nanalyzed in detail in [6], represents another class of loss func-\\ntions. In binary classi\\ufb01cation, they follow a general expression\\nis the discriminant func-\\n\\n, where\\n\\ntion de\\ufb01ned in (2), and\\n\\nis known as the margin.\\n\\nFig. 2. A directed graphical model, or Bayesian network, which represents the\\ndeep generative process of human speech production and its interactions with\\nthe distorting acoustic environment; adopted from [45], where the variables\\nrepresent the \\u201cvisible\\u201d or measurable distorted speech features which are de-\\nnoted by\\n\\nin the text.\\n\\n.\\n\\nrepresents the cost of classifying\\n\\nwhere\\nas while\\nthe true classi\\ufb01cation is\\nis sometimes referred to as \\u201closs\\nfunction\\u201d, but this loss function is applied at classi\\ufb01cation time,\\nwhich should be distinguished from the loss function applied at\\ntraining time as in (3).\\n\\nWhen 0\\u20131 loss is used in classi\\ufb01cation, (22) is reduced to\\n\\ufb01nding the class label that yields the highest conditional proba-\\nbility, i.e.,\\n\\n(23)\\n\\nThe corresponding discriminant function can be represented as\\n\\n(24)\\n\\nConditional log linear models (Chapter 4 in [69]) and multi-\\nlayer perceptrons (MLPs) with softmax output (Chapter 5 in\\n[69]) are both of this form.\\n\\nAnother major school of discriminative models focus on the\\ndecision boundary instead of the probabilistic conditional dis-\\ntribution. In support vector machines (SVMs, see (Chapter 7\\nin [69])), for example, the discriminant functions (extended to\\nmulti-class classi\\ufb01cation) can be written as\\n\\n(25)\\n\\nis a feature vector derived from the input and\\nwhere\\nthe class label, and is implicitly determined by a reproducing\\nkernel. Notice that for conditional log linear models and MLPs,\\n\\n\\x0c10\\n\\nIEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 21, NO. 5, MAY 2013\\n\\non the same input data. When\\nreduced to (31).\\n\\nis based on 0\\u20131 loss, (32) is\\n\\nC. Discriminative Learning in Speech Recognition\\u2014An\\nOverview\\n\\nFig. 3. Convex surrogates of 0\\u20131 loss as discussed and analyzed in [6].\\n\\nMargin-based loss functions, including logistic loss, hinge\\nloss used in SVMs, and exponential loss used in boosting, are all\\nmotivated by upper bounds of 0\\u20131 loss, as illustrated in Fig. 3,\\nwith the highly desirable convexity property for ease of op-\\ntimization. Empirical risk minimization under such loss func-\\ntions are related to the minimization of classi\\ufb01cation error rate.\\nIn a multi-class setting, the notion of \\u201cmargin\\u201d can be gener-\\nally viewed as a discrimination metric between the discriminant\\nfunction of the true class and those of the competing classes,\\ne.g.,\\n. Margin-based loss, then,\\ncan be de\\ufb01ned accordingly such that minimizing the loss would\\nenlarge the \\u201cmargins\\u201d between\\n\\n, for all\\n\\nOne functional form that \\ufb01ts this intuition is introduced in the\\nminimum classi\\ufb01cation error (MCE) training [79], [80] com-\\nmonly used in ASR:\\n\\nand\\n\\n,\\n\\n.\\n\\n(30)\\n\\nwhere\\nis a smooth function, which is non-convex and\\nwhich maps the \\u201cmargin\\u201d to a 0\\u20131 continuum. It is easy to\\nsee that in a binary setting where\\nand where\\n, this loss function can be sim-\\nwhich has\\npli\\ufb01ed to\\nexactly the same form as logistic loss for binary classi\\ufb01cation\\n[6].\\n\\nSimilarly, there have been a host of work that generalizes\\nhinge loss to the multi-class setting. One well known approach\\n[81] is to have\\n\\n(31)\\n\\n(where sum is often replaced by max). Again when there are\\nonly two classes, (31) is reduced to hinge loss\\n\\nTo be even more general, margin based loss can be extended\\nto structured output as well. In [82], loss functions are de\\ufb01ned\\nbased on\\nis a measure of discrepancy be-\\ntween two output structures. Analogous to (31), we have\\n\\n, where\\n\\n.\\n\\nIntuitively, if two output structures are more similar, their dis-\\ncriminant functions should produce more similar output values\\n\\n(32)\\n\\nHaving introduced the models and loss functions for the gen-\\neral discriminative learning settings, we now review the use of\\nthese models and loss functions in ASR applications.\\n\\n1) Models: When applied to ASR,\\n\\nthere are \\u201cdirect\\u201d\\napproaches which use maximum entropy Markov models\\n(MEMMs) [83], conditional random \\ufb01elds (CRFs) [84], [85],\\nhidden CRFs (HCRFs) [71], augmented CRFs [86], segmental\\nCRFs (SCARFs) [72], and deep-structured CRFs [87], [88].\\nThe use of neural networks in the form of MLP (typically with\\none hidden layer) with the softmax nonlinear function at the\\n\\ufb01nal layer was popular in 1990\\u2019s. Since the output of the MLP\\ncan be interpreted as the conditional probability [89], when the\\noutput is fed into an HMM, a good discriminative sequence\\nmodel, or hybrid MLP-HMM, can be created. The use of this\\ntype of discriminative model for ASR has been documented\\nand summarized in detail in [90]\\u2013[92] and analyzed recently in\\n[93]. Due mainly to the dif\\ufb01culty in learning MLPs, this line of\\nresearch has been switched to a new direction where the MLP\\nsimply produces a subset of \\u201cfeature vectors\\u201d in combination\\nwith the traditional features for use in the generative HMM\\n[94]. Only recently,\\nthe dif\\ufb01culty associated with learning\\nMLPs has been actively addressed, which we will discuss in\\nSection VII. All these models are examples of the probabilistic\\ndiscriminative models expressed in the form of conditional\\nprobabilities of speech classes given the acoustic features as\\nthe input.\\n\\nThe second school of discriminative models focus on deci-\\nsion boundaries instead of class-conditional probabilities. Anal-\\nogous to MLP-HMMs, SVM-HMMs have been developed to\\nprovide more accurate state/phone classi\\ufb01cation scores, with in-\\nteresting results reported [95]\\u2013[97]. Recent work has attempted\\nto directly exploit structured SVMs [98], and have obtained sig-\\nni\\ufb01cant performance gains in noise-robustness ASR.\\n\\n2) Conditional Likelihood: The loss functions in discrimi-\\nnative learning for ASR applications have also taken more than\\none form. The conditional likelihood loss, while being most nat-\\nural for use in probabilistic discriminative models, can also be\\napplied to generative models. The maximum mutual informa-\\ntion estimation (MMIE) of generative models, highly popular\\nin ASR, uses an equivalent loss function to the conditional like-\\nlihood loss that leads to the empirical risk of\\n\\n(33)\\n\\nSee a simple proof of their equivalence in [74]. Due to its\\ndiscriminative nature, MMIE has demonstrated signi\\ufb01cant\\nperformance improvement over using the joint likelihood loss\\nin training Gaussian-mixture HMM systems [99]\\u2013[101].\\n\\nFor non-generative or direct models in ASR, the conditional\\nlikelihood loss has been naturally used in training. These dis-\\ncriminative probabilistic models including MEMMs [83], CRFs\\n[85], hidden CRFs [71], semi-Markov CRFs [72], and MLP-\\nHMMs [91], all belonging to the class of conditional log linear\\nmodels. The empirical risk has the same form as (33) except\\n\\n\\x0cDENG AND LI: MACHINE LEARNING PARADIGMS FOR SPEECH RECOGNITION: AN OVERVIEW\\n\\n11\\n\\nthat\\nmodels by\\n\\ncan be computed directly from the conditional\\n\\nsuch large-margin methods. When using a generative model dis-\\ncriminant function\\n\\n, we have\\n\\nFor the conditional log linear models, it is common to apply a\\nGaussian prior on model parameters, i.e.,\\n\\nSimilarly, by using\\nmargin training objective for conditional models:\\n\\n, we obtain a large-\\n\\n(34)\\n\\n(36)\\n\\n(35)\\n\\n3) Bayesian Minimum Risk: Loss functions based on\\nBayesian minimum risk or BMR (of which the conditional\\nlikelihood loss is a special case) have received strong success in\\nASR, as their optimization objectives are more consistent with\\nASR performance metrics. Using sentence error, word error\\nand phone error as\\nin (29) leads to their respective methods\\ncommonly called Minimum Classi\\ufb01cation Error (MCE), Min-\\nimum Word Error (MWE) and Minimum Phone Error (MPE)\\nin the ASR literature. In practice, due to the non-continuity\\nof these objectives, they are often substituted by continuous\\napproximations, making them closer to margin-based loss in\\nnature.\\n\\nThe MCE loss, as represented by (30) is among the earliest\\nadoption of BMR with margin-based loss form in ASR. It\\nwas originated from MCE training of the generative model of\\nGaussian-mixture HMM [79], [102]. The analogous use of the\\nMPE loss has been developed in [73]. With a slight modi\\ufb01-\\ncation of the original MCE objective function where the bias\\nparameter in the sigmoid smoothing function is annealed over\\neach training iteration, highly desirable discriminative margin\\nis achieved while producing the best ASR accuracy result for a\\nstandard ASR task (TI-Digits) in the literature [103], [104].\\n\\nWhile the MCE loss function has been developed originally\\nand used pervasively for generative models of HMM in ASR,\\nthe same MCE concept can be applied to training discrimina-\\ntive models. As pointed out in [105], the underlying principle\\nof MCE is decision feedback, where the discriminative deci-\\nsion function that is used as the scoring function in the decoding\\nprocess becomes a part of the optimization procedure of the en-\\ntire system. Using this principle, a new MCE-based learning al-\\ngorithm is developed in [106] with success for a speech under-\\nstanding task which embeds ASR as a sub-component, where\\nthe parameters of a log linear model is learned via a general-\\nized MCE criterion. More recently, a similar MCE-based deci-\\nsion-feedback principle is applied to develop a more advanced\\nlearning algorithm with success for a speech translation task\\nwhich also embeds ASR as a sub-component [107].\\n\\nMost recently, excellent results on large-scale ASR are re-\\nported in [108] using the direct BMR (state-level) criterion to\\ntrain massive sets of ASR model parameters. This is enabled\\nby distributed computing and by a powerful technique called\\nHessian-free optimization. The ASR system is constructed in a\\nsimilar framework to the deep neural networks of [20], which\\nwe will describe in more detail in Section VII-A.\\n\\n4) Large Margin: Further, the hinge loss and its variations\\nlead to a variety of large-margin training methods for ASR.\\nEquation (32) represents a uni\\ufb01ed framework for a number of\\n\\nIn [109], a quadratic discriminant function of\\n\\n(37)\\n\\n(38)\\n\\n,\\nis de\\ufb01ned as the decision function for ASR, where\\nare positive semide\\ufb01nite matrices that incorporate means and\\ncovariance matrices of Gaussians. Note that due to the missing\\nlog-variance term in (38), the underlying ASR model is no\\nlonger probabilistic and generative. The goal of learning in the\\napproach developed in [109] is to minimize the empirical risk\\nunder the hinge loss function in (31), i.e.,\\n\\n,\\n\\nwhile regularizing on model parameters:\\n\\n(39)\\n\\n(40)\\n\\ncan be solved as a con-\\nThe minimization of\\nstrained convex optimization problem, which gives a huge com-\\nputational advantage over most other discriminative learning al-\\ngorithms in training ASR which are non-convex in the objective\\nfunctions. The readers are referred to a recent special issue of\\nIEEE Signal Processing Magazine on the key roles that convex\\noptimization plays in signal processing including speech recog-\\nnition [110].\\n\\nA different but related margin-based loss function was ex-\\nplored in the work of [111], [112], where the empirical risk is\\nexpressed by\\n\\n(41)\\n\\nfollowing the standard de\\ufb01nition of multiclass separation\\nmargin developed in the ML community for probabilistic\\ngenerative models; e.g., [113], and the discriminant function\\nin (41) is taken to be the log likelihood function of the input\\ndata. Here, the main difference between the two approaches\\nto the use of large margin for discriminative training in ASR\\nis that one is based on the probabilistic generative model of\\nHMM [111], [114], and the other based in non-generative\\ndiscriminant function [109], [115]. However, similar to [109],\\n[115], the work described in [111], [114], [116], [117] also\\nexploits convexity of the optimization objective by using\\nconstraints imposed on model parameters, offering similar\\nkind of compensational advantage. A geometric perspective on\\nlarge-margin training that analyzes the above two types of loss\\n\\n\\x0c12\\n\\nIEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 21, NO. 5, MAY 2013\\n\\nfunctions has appeared recently in [118], which is tested in a\\nvowel classi\\ufb01cation task.\\n\\nIn order to improve discrimination, many methods have been\\ndeveloped for combining different ASR systems. This is one\\narea with interesting overlaps between the ASR and ML com-\\nmunities. Due to space limitation, we will not cover this en-\\nsemble learning paradigm in this paper, except to point out that\\nmany common techniques from ML in this area have not made\\nstrong impact in ASR and further research is needed.\\n\\nThe above discussions have touched only lightly on discrim-\\ninative learning for HMM [79], [111], while focusing on the\\ntwo general aspects of discriminative learning for ASR with re-\\nspect to modeling and to the use of loss functions. Nevertheless,\\nthere has been a very large body of work in the ASR literature,\\nwhich belongs to the more speci\\ufb01c category of the discrimi-\\nnative learning paradigm when the generative model takes the\\nform of GMM-HMM. Recent surveys have provided detailed\\nanalysis on and comparisons among the various popular tech-\\nniques within this speci\\ufb01c paradigm pertaining to HMM-like\\ngenerative models, as well as a uni\\ufb01ed treatment of these tech-\\nniques [74], [114], [119], [120]. We now turn to a brief overview\\non this body of work.\\n\\nD. Discriminative Learning for HMM and Related Generative\\nModels\\n\\nThe overview article of [74] provides the de\\ufb01nitions and intu-\\nitions of four popular discriminative learning criteria in use for\\nHMM-based ASR, all being originally developed and steadily\\nmodi\\ufb01ed and improved by ASR researchers since mid-1980\\u2019s.\\nThey include: 1) MMI [101], [121]; 2) MCE, which can be inter-\\npreted as minimal sentence error rate [79] or approximate min-\\nimal phone error rate [122]; 3) MPE or minimal phone error\\n[73], [123]; and 4) MWE or minimal word error. A discrimina-\\ntive learning objective function is the empirical average of the\\nrelated loss function over all training samples.\\n\\nThe essence of the work presented in [74] is to reformu-\\nlate all the four discriminative learning criteria for an HMM\\ninto a common, uni\\ufb01ed mathematical form of rational functions.\\nThis is trivial for MMI by the de\\ufb01nition, but non-trivial for\\nMCE, MPE, and MWE. The critical difference between MMI\\nand MCE/MPE/MWE is the product form vs. the summation\\nform in the respective loss function, while the form of rational\\nfunction requires the product form and requires a non-trivial\\nconversion for the MCE/MPE/MWE criteria in order to arrive\\nat a uni\\ufb01ed mathematical expression with MMI. The tremen-\\ndous advantage gained by the uni\\ufb01cation is the enabling of a nat-\\nural application of the powerful and ef\\ufb01cient optimization tech-\\nnique, called growth-transformation or extended Baum-Welch\\nalgorithm, to optimization all parameters in parametric genera-\\ntive models. One important step in developing the growth-trans-\\nformation algorithm is to derive two key auxiliary functions for\\nintermediate levels of optimization. Technical details including\\nmajor steps in the derivation of the estimation formulas are pro-\\nvided for growth-transformation based parameter optimization\\nfor both the discrete HMM and the Gaussian HMM. Full tech-\\nnical details including the HMM with the output distributions\\nusing the more general exponential family, the use of lattices\\nin computing the needed quantities in the estimation formulas,\\nand the supporting experimental results in ASR are provided in\\n[119].\\n\\nThe overview article of [114] provides an alternative uni\\ufb01ed\\nview of various discriminative learning criteria for an HMM.\\nThe uni\\ufb01ed criteria include 1) MMI; 2) MCE; and 3) LME\\n(large-margin estimate). Note the LME is the same as (41) when\\nthe discriminant function\\ntakes the form of log likelihood\\nfunction of the input data in an HMM. The uni\\ufb01cation proceeds\\nby \\ufb01rst de\\ufb01ning a \\u201cmargin\\u201d as the difference between the HMM\\nlog likelihood on the data for the correct class minus the geo-\\nmetric average the HMM log likelihoods on the data for all in-\\ncorrect classes. This quantity can be intuitively viewed as a mea-\\nsure of distance from the data to the current decision boundary,\\nand hence \\u201cmargin\\u201d. Then, given the \\ufb01xed margin function def-\\ninition, three different functions of the same margin function\\nover the training data samples give rise to 1) MMI as a sum of\\nthe margins over the data; 2) MCE as sum of exponential func-\\ntions of the margin over the data; and 3) LME as a minimum of\\nthe margins over the data.\\n\\nBoth the motivation and the mathematical form of the uni\\ufb01ed\\ndiscriminative learning criteria presented in [114] are quite dif-\\nferent from those presented in [74], [119]. There is no common\\nrational functional form to enable the use of the extended Baum-\\nWelch algorithm. Instead, the interesting constrained optimiza-\\ntion technique was developed by the authors and presented.\\nThe technique consists of two steps: 1) Approximation step,\\nwhere the uni\\ufb01ed objective function is approximated by an aux-\\niliary function in the neighborhood of the current model param-\\neters; and 2) Maximization step, where the approximated aux-\\niliary function was optimized using the locality constraint. Im-\\nportantly, a relaxation method was exploited, which was also\\nused in [117] with an alternative approach, to further approxi-\\nmate the auxiliary function into a form of positive semi-de\\ufb01nite\\nmatrix. Thus, the ef\\ufb01cient convex optimization technique for a\\nsemi-de\\ufb01nite programming problem can be developed for this\\nM-step.\\n\\nThe work described in [124] also presents a uni\\ufb01ed formula\\nfor the objective function of discriminative learning for MMI,\\nMP/MWE, and MCE. Similar to [114], both contain a generic\\nnonlinear function, with its varied forms corresponding to dif-\\nferent objective functions. Again, the most important distinction\\nbetween the product vs. summation forms of the objective func-\\ntions was not explicitly addressed.\\n\\nOne interesting area of ASR research on discriminative\\nlearning for HMM has been to extend the learning of HMM pa-\\nrameters to the learning of parametric feature extractors. In this\\nway, one can achieve end-to-end optimization for the full ASR\\nsystem instead of just the model component. One earliest work\\nin this area was from [125], where dimensionality reduction in\\nthe Mel-warped discrete Fourier transform (DFT) feature space\\nwas investigated subject to maximal preservation of speech\\nclassi\\ufb01cation information. An optimal linear transformation\\non the Mel-warped DFT was sought, jointly with the HMM\\nparameters, using the MCE criterion for optimization. This\\napproach was later extended to use \\ufb01lter-bank parameters, also\\njointly with the HMM parameters, with similar success [126].\\nIn [127], an auditory-based feature extractor was parameterized\\nby a set of weights in the auditory \\ufb01lters, and had its output fed\\ninto an HMM speech recognizer. The MCE-based discrimina-\\ntive learning procedure was applied to both \\ufb01lter parameters\\nand HMM parameters, yielding superior performance over\\nthe separate training of auditory \\ufb01lter parameters and HMM\\n\\n\\x0cDENG AND LI: MACHINE LEARNING PARADIGMS FOR SPEECH RECOGNITION: AN OVERVIEW\\n\\n13\\n\\nparameters. The end-to-end approach to speech understanding\\ndescribed in [106] and to speech translation described in\\n[107] can be regarded as extensions of the earlier set of work\\ndiscussed here on \\u201cjoint discriminative feature extraction and\\nmodel training\\u201d developed for ASR applications.\\n\\nIn addition to the many uses of discriminative learning for\\nHMM as a generative model, for other more general forms of\\ngenerative models for speech that are surveyed in Section III,\\ndiscriminative learning has been applied with success in ASR.\\nThe early work in the area can be found in [128], where MCE\\nis used to discriminatively learn all the polynomial coef\\ufb01cients\\nin the trajectory model discussed in Section III. The extension\\nfrom the generative learning for the same model as described\\nin [34] to the discriminative learning (via MCE, e.g.) is mo-\\ntivated by the new model space for smoothness-constrained,\\nstate-bound speech trajectories. Discriminative learning offers\\nthe potential to re-structure the new, constrained model space\\nand hence to provide stronger power to disambiguate the obser-\\nvational trajectories generated from nonstationary sources cor-\\nresponding to different speech classes. In more recent work of\\n[129] on the trajectory model, the time variation of the speech\\ndata is modeled as a semi-parametric function of the observation\\nsequence via a set of centroids in the acoustic space. The model\\nparameters of this model are learned discriminatively using the\\nMPE criterion.\\n\\nE. Hybrid Generative-Discriminative Learning Paradigm\\n\\nToward the end of discussing generative and discriminative\\nlearning paradigms, here we would like to provide a brief\\noverview on the hybrid paradigm between the two. Discrimi-\\nnative classi\\ufb01ers directly relate to classi\\ufb01cation boundaries, do\\nnot rely on assumptions on the data distribution, and tend to be\\nsimpler for the design. On the other hand, generative classi\\ufb01ers\\nare most robust to the use of unlabeled data, have more princi-\\npled ways of treating missing information and variable-length\\ndata, and are more amenable to model diagnosis and error\\nanalysis. They are also coherent, \\ufb02exible, and modular, and\\nmake it relatively easy to embed knowledge and structure\\nabout the data. The modularity property is a particularly key\\nadvantage of generative models: due to local normalization\\nproperties, different knowledge sources can be used to train\\ndifferent parts of the model (e.g., web data can train a language\\nmodel independent of how much acoustic data there is to train\\nan acoustic model). See [130] for a comprehensive review of\\nhow speech production knowledge is embedded into design\\nand improvement of ASR systems.\\n\\nThe strengths of both generative and discriminative learning\\nparadigms can be combined for complementary bene\\ufb01ts. In the\\nML literature, there are several approaches aimed at this goal.\\nThe work of [131] makes use of the Fisher kernel to exploit\\ngenerative models in discriminative classi\\ufb01ers. Structured dis-\\ncriminability as developed in the graphical modeling framework\\nalso belongs to the hybrid paradigm [57], where the structure\\nof the model is formed to be inherently discriminative so that\\neven a generative loss function yields good classi\\ufb01cation per-\\nformance. Other approaches within the hybrid paradigm use the\\nloss functions that blend the joint likelihood with the conditional\\nlikelihood by linearly interpolating them [132] or by conditional\\nmodeling with a subset of the observation data. The hybrid par-\\nadigm can also be implemented by staging generative learning\\n\\nahead of discriminative learning. A prime example of this hy-\\nbrid style is the use of a generative model to produce features\\nthat are fed to the discriminative learning module [133], [134]\\nin the framework of deep belief network, which we will return\\nto in Section VII. Finally, we note that with appropriate parame-\\nterization some classes of generative and discriminative models\\ncan be made mathematically equivalent [135].\\n\\nV. SEMI-SUPERVISED AND ACTIVE LEARNING\\n\\nThe preceding overview of generative and discriminative ML\\nparadigms uses the attributes of loss and decision functions to\\norganize a multitude of ML techniques. In this section, we use\\na different set of attributes, namely the nature of the training\\ndata in relation to their class labels. Depending on the way that\\ntraining samples are labeled or otherwise, we can classify many\\nexisting ML techniques into several separate paradigms, most of\\nwhich have been in use in the ASR practice. Supervised learning\\nassumes that all training samples are labeled, while unsuper-\\nvised learning assumes none. Semi-supervised learning, as the\\nname suggests, assumes that both labeled and unlabeled training\\nsamples are available. Supervised, unsupervised and semi-su-\\npervised learning are typically referred to under the passive\\nlearning setting, where labeled training samples are generated\\nrandomly according to an unknown probability distribution. In\\ncontrast, active learning is a setting where the learner can intel-\\nligently choose which samples to label, which we will discuss at\\nthe end of this section. In this section, we concentrate mainly on\\nsemi-supervised and active learning paradigms. This is because\\nsupervised learning is reasonably well understood and unsuper-\\nvised learning does not directly aim at predicting outputs from\\ninputs (and hence is beyond the focus of this article); We will\\ncover these two topics only brie\\ufb02y.\\n\\nA. Supervised Learning\\n\\nIn supervised learning, the training set consists of pairs of\\ninputs and outputs drawn from a joint distribution. Using nota-\\ntions introduced in Section II-A,\\n\\n\\u2022\\n\\nand the corresponding output labels\\n\\nThe learning objective is again empirical risk minimization with\\nregularization, i.e.,\\n, where both input data\\nare provided. In\\nSections III and IV, we provided an overview of the generative\\nand discriminative approaches and their uses in ASR all under\\nthe setting of supervised learning.\\n\\nNotice that there may exist multiple levels of label variables,\\nnotably in ASR. In this case, we should distinguish between\\nthe fully supervised case, where labels of all levels are known,\\nthe partially supervised case, where labels at certain levels\\nare missing. In ASR, for example, it is often the case that the\\ntraining set consists of waveforms and their corresponding\\nword-level transcriptions as the labels, while the phone-level\\ntranscriptions and time alignment information between the\\nwaveforms and the corresponding phones are missing.\\n\\nTherefore, strictly speaking, what is often called supervised\\nlearning in ASR is actually partially supervised learning. It is\\ndue to this \\u201cpartial\\u201d supervision that ASR often uses EM algo-\\nrithm [24], [136], [137]. For example, in the Gaussian mixture\\nmodel for speech, we may have a label variable\\nrepresenting\\n\\n\\x0c14\\n\\nIEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 21, NO. 5, MAY 2013\\n\\nthe Gaussian mixture ID and\\nrepresenting the Gaussian com-\\nponent ID. In the latter case, our goal is to maximize the incom-\\nplete likelihood\\n\\n(42)\\n\\nwhich cannot be optimized directly. However, we can apply EM\\nalgorithm that iteratively maximizes its lower bound. The opti-\\nmization objective at each iteration, then, is given by\\n\\n(43)\\n\\nB. Unsupervised Learning\\n\\nIn ML, unsupervised learning in general refers to learning\\nwith the input data only. This learning paradigm often aims at\\nbuilding representations of the input that can be used for predic-\\ntion, decision making or classi\\ufb01cation, and data compression.\\nFor example, density estimation, clustering, principle compo-\\nnent analysis and independent component analysis are all impor-\\ntant forms of unsupervised learning. Use of vector quantization\\n(VQ) to provide discrete inputs to ASR is one early successful\\napplication of unsupervised learning to ASR [138].\\n\\nMore recently, unsupervised learning has been developed\\nas a component of staged hybrid generative-discriminative\\nparadigm in ML. This emerging technique, based on the deep\\nlearning framework, is beginning to make impact on ASR,\\nwhich we will discuss in Section VII. Learning sparse speech\\nrepresentations, to be discussed in Section VII also, can also be\\nregarded as unsupervised feature learning, or learning feature\\nrepresentations in absence of classi\\ufb01cation labels.\\n\\nC. Semi-Supervised Learning\\u2014An Overview\\n\\nThe semi-supervised learning paradigm is of special signi\\ufb01-\\ncance in both theory and applications. In many ML applications\\nincluding ASR, unlabeled data is abundant but labeling is ex-\\npensive and time-consuming. It is possible and often helpful to\\nleverage information from unlabeled data to in\\ufb02uence learning.\\nSemi-supervised learning is targeted at precisely this type of\\nscenario, and it assumes the availability of both labeled\\nand\\nunlabeled\\n\\ndata, i.e.,\\n\\nThe goal is to leverage both data sources to improve learning\\nperformance.\\n\\nThere have been a large number of semi-supervised learning\\nalgorithms proposed in the literature and various ways of\\ngrouping these approaches. An excellent survey can be found\\nin [139]. Here we categorize semi-supervised learning methods\\nbased on their inductive or transductive nature. The key dif-\\nference between inductive and transductive learning is the\\noutcome of learning. In the former setting, the goal is to \\ufb01nd a\\ndecision function that not only correctly classi\\ufb01es training set\\nsamples, but also generalizes to any future sample. In contrast,\\ntransductive learning aims at directly predicting the output\\nlabels of a test set, without the need of generalizing to other\\nsamples. In this regard, the direct outcome of transductive\\nsemi-supervised learning is a set of labels instead of a deci-\\n\\n\\u2022\\n\\u2022\\n\\nsion function. All learning paradigms we have presented in\\nSections III and IV are inductive in nature.\\n\\nAn important characteristic of transductive learning is that\\nboth training and test data are explicitly leveraged in learning.\\nFor example, in transductive SVMs [7], [140], test-set outputs\\nare estimated such that the resulting hyper-plane separates\\nboth training and test data with maximum margin. Although\\ntransductive SVMs implicitly use a decision function (hyper-\\nplane), the goal is no longer to generalize to future samples\\nbut to predict as accurately as possible the outputs of the test\\nset. Alternatively, transductive learning can be conducted using\\ngraph-based methods that utilize the similarity matrix of the\\ninput [141], [142]. It is worth noting that transductive learning\\nis often mistakenly equated to semi-supervised learning, as both\\nlearning paradigms receive partially labeled data for training.\\nIn fact, semi-supervised learning can be either inductive or\\ntransductive, depending on the outcome of learning. Of course,\\nmany transductive algorithms can produce models that can be\\nused in the same fashion as would the outcome of an inductive\\nlearner. For example, graph-based transductive semi-super-\\nvised learning can produce a non-parametric model that can be\\nused to classify any new point, not in the training and \\u201ctest\\u201d\\nset, by \\ufb01nding where in the graph any new point might lie, and\\nthen interpolating the outputs.\\n\\n1) Inductive Approaches: Inductive approaches to semi-su-\\npervised learning require the construction of classi\\ufb01cation\\nmodels\\n. A general semi-supervised learning objective can be\\nexpressed as\\n\\n(44)\\n\\nwhere\\n\\nagain is the empirical risk on labeled data\\n\\n,\\n\\nis a \\u201crisk\\u201d measured on unlabeled data\\n\\n.\\n\\nFor generative models (Section III), a common measure on\\n\\nunlabeled data is the incomplete-data likelihood, i.e.,\\n\\n(45)\\n\\nThe goal of semi-supervised learning, therefore, becomes to\\nmaximize the complete-data likelihood on\\nand the incom-\\n. One way of solving this optimiza-\\nplete-data likelihood on\\ntion problem is applying the EM algorithm or its variations to\\nunlabeled data [143], [144]. Furthermore, when discriminative\\nloss functions, e.g., (26), (29), or (32), are used in\\n,\\nthe learning objective becomes equivalent to applying discrim-\\ninative training on\\nand while applying maximum likelihood\\nestimation on\\n\\nThe above approaches, however, are not applicable to dis-\\ncriminative models (which model conditional relations rather\\nthan joint distributions). For conditional models, one solution\\nto semi-supervised learning is minimum entropy regularization\\n[145], [146] that de\\ufb01nes\\nas the conditional entropy of\\nunlabeled data:\\n\\n.\\n\\nThe semi-supervised learning objective is then to maximize the\\nwhile minimizing the conditional\\nconditional likelihood of\\n\\n(46)\\n\\n\\x0cDENG AND LI: MACHINE LEARNING PARADIGMS FOR SPEECH RECOGNITION: AN OVERVIEW\\n\\n15\\n\\nentropy of\\nmodels which can be data-sensitive in practice.\\n\\n. This approach generally would result in \\u201csharper\\u201d\\n\\nAnother set of results makes an additional assumption that\\nprior knowledge can be utilized in learning. Generalized ex-\\npectation criteria [147] represent prior knowledge as labeled\\nfeatures,\\n\\n(47)\\n\\nand\\n\\nboth refer to conditional distributions\\nIn the last term,\\nof labels given a feature. While the former is speci\\ufb01ed by prior\\nknowledge, and the latter is estimated by applying model\\non\\nunlabeled data. In [148], prior knowledge is encoded as vir-\\ntual evidence [149], denoted as\\n. They model the distribution\\nexplicitly and formulate the semi-supervised learning\\n\\nproblem as follows,\\n\\n(48)\\n\\nwhere\\ncan be optimized in an EM fashion. This\\ntype of methods has been most used in sequence models, where\\nprior knowledge on frame- or segment-level features/labels is\\navailable. This can be potentially interesting to ASR as a way\\nof incorporating linguistic knowledge into data-driven systems.\\nwas origi-\\nnally inspired by transductive SVMs [7]. The intuition is to \\ufb01nd\\na labeling of\\nand newly la-\\nbeled would have the largest margin. In a binary classi\\ufb01cation\\nsetting, the learning objective is given by a\\nbased on\\nhinge loss and\\n\\nThe concept of semi-supervised SVMs\\n\\nsuch that the SVM trained on\\n\\n(49)\\n\\nrepresents a linear function;\\n\\nis derived\\nwhere\\nfrom\\n. Various works have been pro-\\nposed to approximate the optimization problem (which is no\\nlonger convex due to the second term), e.g., [140], [150]\\u2013[152].\\nIn fact, a transductive SVM is in the strict sense an inductive\\nlearner, although it is by convention called \\u201ctransductive\\u201d for\\nits intention to minimize the generalization error bound on the\\ntarget inputs.\\n\\nWhile the methods introduced above are model-dependent,\\nthere are inductive algorithms that can be applied across dif-\\nferent models. Self-training [153] extends the idea of EM to a\\nwider range of classi\\ufb01cation models\\u2014the algorithm iteratively\\ntrains a seed classi\\ufb01er using the labeled data, and uses predic-\\ntions on the unlabeled data to expand the training set. Typically\\nthe most con\\ufb01dent predictions are added to the training set. The\\nEM algorithm on generative models can be considered a spe-\\ncial case of self-training in that all unlabeled samples are used\\nin re-training, weighted by their posterior probabilities. The dis-\\nadvantage of self-training is that it lacks a theoretical justi\\ufb01ca-\\ntion for optimality and convergence, unless certain conditions\\nare satis\\ufb01ed [153].\\n\\nCo-training [154] assumes that the input features can be split\\ninto two conditionally independent subsets, and that each subset\\nis suf\\ufb01cient for classi\\ufb01cation. Under these assumptions, the\\nalgorithm trains two separate classi\\ufb01ers on these two subsets\\n\\nof features, and each classi\\ufb01er\\u2019s predictions on new unlabeled\\nsamples are used to enlarge the training set of the other. Similar\\nto self-training, co-training often selects data based on con\\ufb01-\\ndence. Certain work has found it bene\\ufb01cial to probabilistically\\n, leading to the co-EM paradigm [155]. Some variations\\nlabel\\nof co-training include split data and ensemble learning.\\n\\n2) Transductive Approaches: Transductive approaches do\\nnot necessarily require a classi\\ufb01cation model. Instead, the goal\\nis to produce a set of labels\\n. Such approaches are\\noften based on graphs, with nodes representing labeled and un-\\nlabeled samples and edges representing the similarity between\\nthe samples. Let\\nsimilarity ma-\\ntrix,\\nmatrix representing classi\\ufb01cation\\ndenote an\\ndenote another\\nscores of all with respect to all classes, and\\nmatrix representing known label information. The\\ngoal of graph-based learning is to \\ufb01nd a classi\\ufb01cation of all data\\nthat satis\\ufb01es the constraints imposed by the labeled data and is\\nsmooth over the entire graph. This can be expressed by a gen-\\neral objective function of\\n\\ndenote an\\n\\nfor\\n\\nby\\n\\nby\\n\\nby\\n\\n(50)\\n\\nwhich consists of a loss term and regularization term. The loss\\nterm evaluates the discrepancy between classi\\ufb01cation outputs\\nand known labels while the regularization term ensures that\\nsimilar inputs have similar outputs. Different graph-based algo-\\nrithms, including mincut [156], random walk [157], label prop-\\nagation [158], local and global consistency [159] and manifold\\nregularization [160], and measure propagation [161] vary only\\nin the forms of the loss and regularization functions.\\n\\nNotice that compared to inductive approaches to semi-super-\\nvised learning, transductive learning has rarely been used in\\nASR. This is mainly because of the usually very large amount\\nof data involved in training ASR systems, which makes it pro-\\nhibitive to directly use af\\ufb01nity between data samples in learning.\\nThe methods we will review shortly below all \\ufb01t into the in-\\nductive category. We believe, however, it is important to in-\\ntroduce readers to some powerful transductive learning tech-\\nniques and concepts which have made fundamental impact to\\nmachine learning. They also have the potential for make impact\\nin ASR as example- or template-based approaches have increas-\\ningly been explored in ASR more recently. Some of the recent\\nwork of this type will be discussed in Section VII-B.\\n\\nD. Semi-Supervised Learning in Speech Recognition\\n\\nWe \\ufb01rst point out that the standard description of semi-su-\\npervised learning discussed above in the ML literature has been\\nused loosely in the ASR literature, and often been referred\\nto as unsupervised learning or unsupervised training. This\\n(minor) confusion is caused by the fact that while there are both\\ntranscribed/labeled and un-transcribed sets of training data, the\\nlatter is signi\\ufb01cantly greater in the amount than the former.\\nTechnically, the need for semi-supervised learning in ASR\\nis obvious. State of the art performance in large vocabulary\\nASR systems usually requires thousands of hours of manually\\nannotated speech and millions of words of text. The manual\\ntranscription is often too expensive or impractical. Fortunately,\\nwe can rely upon the assumption that any domain which re-\\nquires ASR technology will have thousands of hours of audio\\n\\n\\x0c16\\n\\nIEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 21, NO. 5, MAY 2013\\n\\navailable. Unsupervised acoustics model training builds initial\\nmodels from small amounts of transcribed acoustic data and\\nthen use them to decode much larger amounts of un-transcribed\\ndata. One then trains new models using part or all of these\\nautomatic transcripts as the label. This drastically reduces the\\nlabeling requirements for ASR in the sparse domains.\\n\\nThe above training paradigm falls into the self-training cat-\\negory of semi-supervised learning described in the preceding\\nsubsection. Representative work includes [162]\\u2013[164], where\\nan ASR trained on a small transcribed set is used to generate\\ntranscriptions for larger quantities of un-transcribed data \\ufb01rst.\\nThe recognized transcriptions are selected then based on con\\ufb01-\\ndence measures. The selected transcriptions are treated as the\\ncorrect ones and are used to train the \\ufb01nal recognizer. Spe-\\nci\\ufb01c techniques include incremental training where the high-\\ncon\\ufb01dence (as determined with a threshold) utterances are com-\\nbined with transcribed utterances to retrain or to adapt the rec-\\nognizer. Then the retrained recognizer is used to transcribe the\\nnext batch of utterances. Often, generalized expectation maxi-\\nmization is used where all utterances are used but with different\\nweights determined by the con\\ufb01dence measure. This approach\\n\\ufb01ts into the general framework of (44), and has also been ap-\\nplied to combining discriminative training with semi-supervised\\nlearning [165]. While straightforward, it has been shown that\\nsuch con\\ufb01dence-based self-training approaches are associated\\nwith the weakness of reinforcing what the current model already\\nknows and sometimes even reinforcing the errors. Divergence is\\nfrequently observed when the performance of the current model\\nis relatively poor.\\n\\nSimilar to the objective of (46), in the work of [166] the global\\nentropy de\\ufb01ned over the entire training data set is used as the\\nbasis for assigning labels in the un-transcribed portion of the\\ntraining utterances for semi-supervised learning. This approach\\ndiffers from the previous ones by making the decision based on\\nthe global dataset instead of individual utterances only. More\\nspeci\\ufb01cally, the developed algorithm focuses on the improve-\\nment to the overall system performance by taking into consid-\\neration not only the con\\ufb01dence of each utterance but also the\\nfrequency of similar and contradictory patterns in the un-tran-\\nscribed set when determining the right utterance-transcription\\npair to be included in the semi-supervised training set. The al-\\ngorithm estimates the expected entropy reduction which the ut-\\nterance-transcription pair may cause on the full un-transcribed\\ndataset.\\n\\nOther ASR work [167] in semi-supervised learning lever-\\nages prior knowledge, e.g., closed-captions, which are consid-\\nered as low-quality or noisy labels, as constraints in otherwise\\nstandard self-training. The idea is akin to (48). One particular\\nconstraint exploited is to align the closed captions with recog-\\nnized transcriptions and to select only segments that agree. This\\napproach is called lightly supervised training in [167]. Alter-\\nnatively, recognition has been carried out by using a language\\nmodel which is trained on the closed captions.\\n\\nWe would like to point out that many effective semi-su-\\npervised learning algorithms developed in ML as surveyed in\\nSection V-D have yet to be explored in ASR, and this is one\\narea expecting growing contributions from the ML community.\\n\\nE. Active Learning\\u2014An overview\\n\\nActive learning is a similar setting to semi-supervised\\nlearning in that, in addition to a small amount of labeled data\\n\\n, there is a large amount of unlabeled data\\n\\u2022\\n\\u2022\\n\\navailable; i.e.,\\n\\nThe goal of active learning, however, is to query the most infor-\\nmative set of inputs to be labeled, hoping to improve classi\\ufb01-\\ncation performance with the minimum number of queries. That\\nis, in active learning, the learner may play an active role in de-\\nciding the data set\\n\\nrather than it be passively given.\\n\\nThe key idea behind active learning is that a ML algorithm\\ncan achieve greater performance, e.g., higher classi\\ufb01cation\\naccuracy, with fewer training labels if it is allowed to choose\\nthe subset of data that has labels. An active learner may pose\\nqueries, usually in the form of unlabeled data instances to be\\nlabeled (often by a human). For this reason, it is sometimes\\ncalled query learning. Active learning is well-motivated in\\nmany modern ML problems, where unlabeled data may be\\nabundant or easily obtained, but labels are dif\\ufb01cult, time-con-\\nsuming, or expensive to obtain. This is the situation for speech\\nrecognition. Broadly, active learning comes in two forms: batch\\nactive learning, where a subset of data is chosen, a priori in\\na batch to be labeled. The labels of the instances in the batch\\nchosen to be labeled may not, under this approach, in\\ufb02uence\\nother instances to be selected since all instances are chosen at\\nonce. In online active learning, on the other hand, instances are\\nchosen one-by-one, and the true labels of all previously labeled\\ninstances may be used to select other instances to be labeled.\\nFor this reason, online active learning is sometimes considered\\nmore powerful.\\n\\nA recent survey of active learning can be found in [168].\\nBelow we brie\\ufb02y review a few commonly used approaches with\\nrelevance to ASR.\\n\\n1) Uncertainty Sampling: Uncertainty sampling is probably\\nthe simplest approach to active learning. In this framework, un-\\nlabeled inputs are selected based on an uncertainty (informa-\\ntiveness) measure,\\n\\n(51)\\n\\n. There are\\nwhere\\nvarious choices of the certainty measure [169]\\u2013[171], including\\n\\ndenote model parameters estimated on\\n\\nwhere\\n\\n, where\\n\\n;\\n\\n\\u2022 posterior:\\n\\n\\u2022 margin:\\n\\n\\u2022 entropy:\\n\\nand\\nmodel\\n\\nare the \\ufb01rst and second most likely label under\\n; and\\n\\nFor non-probabilistic models, similar measures can be con-\\nstructed from discriminant functions. For example, the distance\\nto the decision boundary is used as a measure for active learning\\nassociated with SVM [172].\\n\\n2) Query-by-Committee: The query-by committee algo-\\nrithm enjoys a more theoretical explanation [173], [174].\\nThe idea is to construct a committee of learners, denoted by\\n\\n\\x0cDENG AND LI: MACHINE LEARNING PARADIGMS FOR SPEECH RECOGNITION: AN OVERVIEW\\n\\n17\\n\\n, all trained on labeled samples. The\\nunlabeled samples upon which the committee disagree the most\\nare selected to be labeled by human, i.e.,\\n\\n(52)\\n\\nThis analysis shows that active learning and semi-supervised\\nlearning attack the same problem from opposite directions.\\nWhile semi-supervised methods exploit what the learner thinks\\nit knows about the unlabeled data, active methods attempt to\\nexplore the unknown aspects.\\n\\nThe key problems in committee-based methods consist of\\n(1) constructing a committee\\nthat represents competing\\n. The\\nhypotheses and (2) having a measure of disagreement\\n\\ufb01rst problem is often tackled by sampling the model space, by\\nsplitting the training data or by splitting the feature space. For\\nthe second problem, one popularly used disagreement measure\\nis vote entropy [175]\\nis\\nthe number of votes the class\\nreceives from the committee\\nregarding input\\n\\nis the committee size.\\n\\n3) Exploiting Structures in Data: Both uncertainty sampling\\nand query-by committee may encounter the sampling bias\\nproblem; i.e., the selected inputs are not representatives of the\\ntrue input distribution. Recent work proposed to select inputs\\nnot only based on an uncertainty/disagreement measure but\\nalso on a \\u201cdensity\\u201d measure [171], [176]. Mathematically, the\\ndecision is\\n\\nwhere\\n\\nand\\n\\n(53)\\n\\nwhere\\n\\ncan be either\\n\\nin query-by-committee;\\n\\nin uncertainty sampling of\\nis a density term that can\\nbe estimated by computing similarity with other inputs with or\\nwithout clustering. Such methods have achieved active learning\\nperformance superior to those that do not take structure or den-\\nsity into consideration.\\n\\n4) Submodular Active Selection: A recent and novel ap-\\nproach to batch active learning for speech recognition was\\nproposed in [177] that made use of sub-modular functions; in\\nthis work, results outperformed many of the active learning\\nmethods mentioned above. Sub-modular functions are a rich\\nclass of functions on discrete sets and subsets thereof that cap-\\nture the notion of diminishing returns\\u2014an item is worth less\\nas the context in which it is evaluated gets larger. Sub-modular\\nfunctions are relevant for batch active learning either in speech\\nrecognition and other areas of machine learning [178], [179].\\n\\n5) Comparisons Between Semi-Supervised and Active\\nLearning: Active learning and semi-supervised learning both\\naim at making the most out of unlabeled data. As a result, there\\nare conceptual overlaps between these two paradigms of ML.\\nAs an example, in self-training of semi-supervised technique\\nas discussed earlier, the classi\\ufb01er is \\ufb01rst trained with a small\\namount of labeled data, and then used to classify the unlabeled\\ndata. Typically the most con\\ufb01dent unlabeled instances, together\\nwith their predicted labels, are added to the training set, and the\\nprocess repeats. A corresponding technique in active learning\\nis uncertainty sampling, where the instances about which the\\nmodel is least con\\ufb01dent are selected for querying. As another\\nexample, co-training in semi-supervised learning initially trains\\nseparate models with the labeled data. The models then classify\\nthe unlabeled data, and \\u201cteach\\u201d the other models with a few\\nunlabeled examples about which they are most con\\ufb01dent. This\\ncorresponds to the query-by-committee approach in active\\nlearning.\\n\\nF. Active Learning in Speech Recognition\\n\\nThe main motivation for exploiting active learning paradigm\\nin ASR to improve the systems performance in the applications\\nwhere the initial accuracy is very low and only a small amount\\nof data can be transcribed. A typical example is the voice search\\napplication, with which users may search for information such\\nas phone numbers of a business with voice. In the ASR com-\\nponent of a voice search system, the vocabulary size is usu-\\nally very large, and the users often interact with the system\\nusing free-style instantaneous speech under real noisy environ-\\nments. Importantly, acquisition of un-transcribed acoustic data\\nfor voice systems is usually as inexpensive as logging the user\\ninteractions with the system, while acquiring transcribed or la-\\nbeled acoustic data is very costly. Hence, active learning is of\\nspecial importance for ASR here. In light of the recent popu-\\nlarity of and availability of infrastructure for crowding sourcing,\\nwhich has the potential to stimulate a paradigm shift in active\\nlearning, the importance of active learning in ASR applications\\nin the future is expected to grow.\\n\\nAs described above, the basic approach of active learning is\\nto actively ask a question based on all the information available\\nso far, so that some objective function can be optimized when\\nthe answer becomes known. In many ASR related tasks, such\\nas designing dialog systems and improving acoustic models, the\\nquestion to be asked is limited to selecting an utterance for tran-\\nscribing from a set of un-transcribed utterances.\\n\\nThere have been many studies on how to select appropriate\\nutterance for human transcription in ASR. The key issue here is\\nthe criteria for selecting utterances. First, con\\ufb01dence measures\\nis used as the criterion as in the standard uncertainty sampling\\nmethod discussed earlier [180]\\u2013[182]. The initial recognizer in\\nthese approaches, which is prepared beforehand, is \\ufb01rst used\\nto recognize all the utterances in the training set. Those utter-\\nances that have recognition results with less con\\ufb01dence are then\\nselected. The word posterior probabilities for each utterance\\nhave often been used as con\\ufb01dence measures. Second, in the\\nquery-by-committee based approach proposed in [183], sam-\\nples that cause the largest different opinions from a set of rec-\\nognizers (committee) are selected. These multiple recognizers\\nare also prepared beforehand, and the recognition results pro-\\nduced by these recognizers are used for selecting utterances.\\nThe authors apply the query-by-committee technique not only to\\nacoustic models but also to language models and their combina-\\ntion. Further, in [184], the confusion or entropy reduction based\\napproach is developed where samples that reduce the entropy\\nabout the true model parameters are selected for transcribing.\\nSimilarly, in the error rate-based approach the samples that can\\nminimize the expected error rate most is selected.\\n\\nA rather unique technique of active learning for ASR is de-\\nveloped in [166]. It recognizes the weakness of the most com-\\nmonly used, con\\ufb01dence-based approach as follows. Frequently,\\n\\n\\x0c18\\n\\nIEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 21, NO. 5, MAY 2013\\n\\nthe con\\ufb01dence-based active learning algorithm is prone to se-\\nlect noise and garbage utterances since these utterances typi-\\ncally have low con\\ufb01dence scores. Unfortunately, transcribing\\nthese utterances is usually dif\\ufb01cult and carries little value in im-\\nproving the overall ASR performance. This limitation originates\\nfrom the utterance-by-utterance decision, which is based on the\\ninformation from each individual utterance only. that is, tran-\\nscribing the least con\\ufb01dent utterance may signi\\ufb01cantly help rec-\\nognize that utterance but it may not help improve the recognition\\naccuracy on other utterances. Consider two speech utterances A\\nand B. Say A has a slightly lower con\\ufb01dence score than B. If A\\nis observed only once and B occurs frequently in the dataset, a\\nreasonable choice is to transcribe B instead of A. This is because\\ntranscribing B would correct a larger fraction of errors in the test\\ndata than transcribing A and thus has better potential to improve\\nthe performance of the whole system. This example shows that\\nthe active learning algorithm should select the utterances that\\ncan provide the most bene\\ufb01t to the full dataset. Such a global cri-\\nterion for active learning has been implemented in [166] based\\non maximizing the expected lattice entropy reduction over all\\nun-transcribed data. Optimizing the entropy is shown to be more\\nrobust than optimizing the top choice [184], since it considers\\nall possible outcomes weighted with probabilities.\\n\\nVI. TRANSFER LEARNING\\n\\nThe ML paradigms and algorithms discussed so far in this\\npaper have the goal of producing a classi\\ufb01er that generalizes\\nacross samples drawn from the same distribution. Transfer\\nlearning, or learning with \\u201cknowledge transfer\\u201d, is a new ML\\nparadigm that emphasizes producing a classi\\ufb01er that general-\\nizes across distributions, domains, or tasks. Transfer learning\\nis gaining growing importance in ML in recent years but is in\\ngeneral less familiar to the ASR community than other learning\\nparadigms discussed so far. Indeed, numerous highly successful\\nadaptation techniques developed in ASR are aimed to solve\\none of the most prominent problems that transfer learning\\nresearchers in ML try to address\\u2014mismatch between training\\nand test conditions. However, the scope of transfer learning in\\nML is wider than this, and it also encompasses a number of\\nschemes familiar to ASR researchers such as audio-visual ASR,\\nmulti-lingual and cross-lingual ASR, pronunciation learning\\nfor word recognition, and detection-based ASR. We organize\\nsuch diverse ASR methodologies into a uni\\ufb01ed categorization\\nscheme under the very broad transfer learning paradigm in\\nthis section, which would otherwise be viewed as isolated\\nASR applications. We also use the standard ML notations in\\nSection II to describe all ASR topics in this section.\\n\\nThere is vast ML literature on transfer learning. To organize\\nour presentation with considerations to existing ASR applica-\\ntions, we create the four-way categorization of major transfer\\nlearning techniques, as shown in Table II, using the following\\ntwo axes. The \\ufb01rst axis is the manner in which knowledge is\\ntransferred. Adaptive learning is one form of transfer learning\\nin which knowledge transfer is done in a sequential manner,\\ntypically from a source task to a target\\ntask. In contrast,\\nmulti-task learning is concerned with learning multiple tasks\\nsimultaneously.\\n\\nFOUR-WAY CATEGORIZATION OF TRANSFER LEARNING\\n\\nTABLE II\\n\\nTransfer learning can be orthogonally categorized using the\\nsecond axis as to whether the input/output space of the target\\ntask is different from that of the source task. It is called homo-\\ngeneous if the source and target task have the same input/output\\nspace, and is heterogeneous otherwise. Note that both adaptive\\nlearning and multi-task learning can be either homogeneous or\\nheterogeneous.\\n\\nA. Homogeneous Transfer\\n\\nInterestingly, homogeneous transfer, i.e., adaptation, is one\\nparadigm of transfer learning that has been more extensively de-\\nveloped (and also earlier) in the speech community rather than\\nthe ML community. To be consistent with earlier sections, we\\n\\ufb01rst present adaptive learning from the ML theoretical perspec-\\ntive, and then discuss how it is applied to ASR.\\n\\n1) Basics: At this point, it is helpful for the readers to review\\nthe notations set up in Section II which will be used intensively\\nin this section. In this setting, the input space\\nin the target\\ntask is the same as that in the source task, so is the output space\\n. Most of the ML techniques discussed earlier in this article\\nassume that the source-task (training) and target-task (test) sam-\\nples are generated from the same underlying distribution\\nover\\n\\ufb01er\\n\\n. Often, however, in most ASR applications classi-\\nis trained on samples drawn from a source distribution\\nthat is different from, yet similar to, the target distri-\\nbution\\n. Moreover, while there may be a large amount\\nof training data from the source task, only a limited amount of\\ndata (labeled and/or unlabeled) from the target task is available.\\nThe problem of adaptation, then, is to learn a new classi\\ufb01er\\nleveraging the available information from the source and target\\ntasks, ideally to minimize\\n\\nHomogeneous adaptation is important\\n\\nto many machine\\nlearning applications. In ASR, a source model (e.g., speaker-in-\\ndependent HMM for ASR) may be trained on a dataset\\nconsisting of samples from a large number of individuals, but\\nthe target distribution would correspond only to a speci\\ufb01c user.\\nIn image classi\\ufb01cation, the lighting condition at application\\ntime may vary from that when training-set images are collected.\\nIn spam detection, the wording styles of spam emails or web\\npages are constantly evolving.\\n\\nHomogeneous adaptation can be formulated in various ways\\ndepending on the type of source/target information available at\\nadaptation time. Information from the source task may consist\\nof the following:\\n\\n.\\n\\n\\u2022\\n\\n\\u2022\\n\\n, i.e., labeled\\n\\ntraining data from the source task. A typical example of\\nin ASR is the transcribed speech data for training speaker-\\nindependent and environment-independent HMMs.\\n\\n: a source model or classi\\ufb01er which is either an accu-\\nrate representation or an approximately correct estimate\\nof\\n, i.e., the risk minimizer for the source\\n\\n\\x0cDENG AND LI: MACHINE LEARNING PARADIGMS FOR SPEECH RECOGNITION: AN OVERVIEW\\n\\n19\\n\\ntask. A typical example of\\nin ASR is the HMM trained\\nalready using speaker-independent and environment-inde-\\npendent training data.\\n\\nFor the target task, one or both of the following data sources\\n\\nmay be available:\\n\\n\\u2022\\n\\n\\u2022\\n\\n, i.e., labeled\\nadaptation data from the target task. A typical example\\nof\\nin ASR is the enrollment data for speech dictation\\nsystems.\\n\\n, i.e., unlabeled adaptation data\\nfrom the target task. A typical example of\\nin ASR is\\nthe actual conversation speech from the users of interactive\\nvoice response systems.\\n\\nBelow we present and analyze two major classes of methods\\n\\nfor homogeneous adaptation.\\n\\n2) Data Combination: When\\n\\nis available at adaptation\\ntime, a natural approach is to seek intelligent ways of com-\\nbining\\n). The work by [185]\\nderived generalization error bounds for a learner that minimizes\\na convex combination of source and target empirical risks,\\n\\n(and sometimes\\n\\nand\\n\\n(54)\\n\\nand\\n\\nand\\n\\nare de\\ufb01ned with respect to\\n\\nwhere\\nrespectively. Data combination is also implicitly used in many\\npractical studies on SVM adaptation. In [116], [186], [187], the\\nsupport vectors as derived data from\\n,\\nwith different weights, for retraining a target model.\\n\\nare combined with\\n\\nin adaptation. In ASR, for example,\\n\\nIn many applications, however, it is not always feasible to\\nuse\\nmay consist of\\nhundreds or even thousands of hours of speech, making any data\\ncombination approach prohibitive.\\n\\n3) Model Adaptation: Here we focus on alternative classes\\nof approaches which attempt to adapt directly from . These\\napproaches can be less optimal (due to the loss of information)\\nbut much more ef\\ufb01cient compared with data combination. De-\\npending on which target-data source is used, adaptation of\\ncan be conducted in a supervised or unsupervised fashion. Un-\\nsupervised adaptation is akin to the semi-supervised learning\\nsetting already discussed in Section V-C, which we do not re-\\npeat here.\\n\\nIn supervised adaptation, labeled data\\n\\n, usually in a very\\nsmall amount, is used to adapt\\n. The learning objective con-\\nsists of minimizing the target empirical risk while regularizing\\ntoward the source model,\\n\\n(55)\\n\\nDifferent adaptation techniques essentially differ in how regu-\\nlarization works.\\n\\nOne school of methods are based on Bayesian model selec-\\ntion. In other words, regularization is achieved by a prior distri-\\nbution on model parameters, i.e.,\\n\\nwhere the hyper-parameters of the prior distribution are usually\\nderived from source model parameters. The function form of\\n\\n(56)\\n\\nthe prior distribution depends on classi\\ufb01cation model. For gen-\\nerative models, it is mathematically convenient to use the con-\\njugate prior of the likelihood function such that the posterior\\nbelongs to the same function family as the prior. For example,\\nnormal-Wishart priors have been used in adapting Gaussians\\n[188], [189]; Dirichlet priors have been used in adapting multi-\\nnomial [188]\\u2013[190]. For discriminative models such as condi-\\ntional maximum entropy models, SVMs and MLPs, Gaussian\\npriors are commonly used [116], [191]. A uni\\ufb01ed view of these\\npriors can be found in [116], which also relates the general-\\nization error bound to the KL divergence of source and target\\nsample distributions.\\n\\nAnother group of methods adapt model parameters in a more\\nstructured way by forcing the target model to be a transforma-\\ntion of the source model. The regularization term can be ex-\\npressed as follows,\\n\\n(57)\\n\\nrepresents a transform function. For example, max-\\nwhere\\nimum likelihood linear regression (MLLR) [192], [193] adapts\\nGaussian parameters through shared transform functions. In\\n[194], [195], the target MLP is obtained by augmenting the\\nsource MLP with an additional linear input layer.\\n\\nFinally, other studies on model adaptation have related the\\nsource and target models via shared components. Both [196]\\nand [197] proposed to construct MLPs whose input-to-hidden\\nlayer is shared by multiple related tasks. This layer represents\\nan \\u201cinternal representation\\u201d which, once learned, is \\ufb01xed during\\nadaptation. In [198], the source and target distributions were\\neach assumed to a mixture of two components, with one mixture\\ncomponent shared between source and target tasks. [199], [200]\\nassumed that the target distribution is a mixture of multiple\\nsource distributions. They proposed to combine source models\\nweighted by source distributions, which has an expected loss\\nguarantee with respect to any mixture.\\n\\nB. Homogeneous Transfer in Speech Recognition\\n\\nThe ASR community is actually among the \\ufb01rst to systemati-\\ncally investigate homogeneous adaptation, mostly in the context\\nof speaker or noise adaptation. A recent survey on noise adap-\\ntation techniques for ASR can be found in [201].\\n\\nOne of the commonly used homogeneous adaptation tech-\\nniques in ASR is maximum a posteriori (MAP) method [188],\\n[189], [202], which places adaptation within the Bayesian\\nlearning framework and involves using a prior distribution on\\nthe model parameters as in (56). Speci\\ufb01cally, to adapt Gaussian\\nmixture models, MAP method applies a normal-Wishart prior\\non Gaussian means and covariance matrices, and a Dirichlet\\nprior on mixture component weights.\\n\\nMaximum likelihood linear regression (MLLR) [192], [193]\\nregularizes the model space in a more structured way than MAP\\nin many cases. MLLR adapts Gaussian mixture parameters in\\nHMMs through shared af\\ufb01ne transforms such that each HMM\\nstate is more likely to generate the adaptation data and hence\\nthe target distribution. There are various techniques to combine\\nthe structural information captured by linear regression with the\\nprior knowledge utilized in the Bayesian learning framework.\\n\\n\\x0c20\\n\\nIEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 21, NO. 5, MAY 2013\\n\\nMaximum a posteriori linear regression (MAPLR) and its vari-\\nations [203], [204] improve over MLLR by assuming a prior\\ndistribution on af\\ufb01ne transforms.\\n\\nYet another important family of adaptation techniques have\\nbeen developed, unique in ASR and not seen in the ML liter-\\nature, in the frameworks of speaker adaptive training (SAT)\\n[205] and noise adaptive training (NAT) [201], [206], [207].\\nThese frameworks utilize speaker or acoustic-environment\\nadaptation techniques, such as MLLR [192], [193], SPLICE\\n[206], [208], [209], and vector Taylor series approximation\\n[210], [211], during training to explicitly address speaker-in-\\nduced or environment-induced variations. Since speaker and\\nacoustic-environment variability has been explicitly accounted\\nfor by the transformations in training, the resulting speaker-in-\\ndependent and environment-independent models only need\\nto address intrinsic phonetic variability and are hence more\\ncompact than conventional models.\\n\\nThere are a few extensions to the SAT and NAT frameworks\\nbased on the notion of \\u201cspeaker clusters\\u201d or \\u201cenvironment clus-\\nters\\u201d [212], [213]. For example, [213] proposed cluster adap-\\ntive training where all Gaussian components in the system are\\npartitioned into Gaussian classes, and all training speakers are\\npartitioned into speaker clusters. It is assumed that a speaker-de-\\npendent model (either in adaptive training or in recognition)\\nis a linear combination of cluster-conditional models, and that\\nall Gaussian components in the same Gaussian class share the\\nsame set of weights. In a similar spirit, eigenvoice [214] con-\\nstrains a speaker-dependent model to be a linear combination of\\na number of basis models. During recognition, a new speaker\\u2019s\\nsuper-vector is a linear combination of eigen-voices where the\\nweights are estimated to maximize the likelihood of the adapta-\\ntion data.\\n\\nC. Heterogeneous Transfer\\n\\n1) Basics: Heterogeneous transfer involves a higher level of\\ngeneralization. The goal is to transfer knowledge learned from\\none task to a new task of a different nature. For example, an\\nimage classi\\ufb01cation task may bene\\ufb01t from a text classi\\ufb01cation\\ntask although they do not have the same input spaces. Speech\\nrecognition of a low-resource language can borrow information\\nfrom a resource-rich language ASR system, despite the differ-\\nence in their output spaces (i.e., different languages).\\n\\nFormally, we de\\ufb01ne the input spaces\\n\\nfor the\\nsource and target tasks, respectively. Similarly, we de\\ufb01ne the\\ncorresponding output spaces as\\nand\\n, respectively. While\\nhomogeneous adaptation assumes that\\n\\n, or both spaces are different. Let\\n\\n, heterogeneous adaptation assumes that either\\n\\nor\\njoint distribution over\\ndistribution over\\ntation is then to minimize\\n(1) source task information in the form of\\ntarget task information in the form of\\n\\n,\\ndenote the\\ndenote the joint\\n. The goal of heterogeneous adap-\\nleveraging two data sources:\\n; (2)\\n\\nand/or\\n.\\n\\n, and Let\\n\\nand/or\\n\\nBelow we discuss the methods associated with two main\\nconditions under which heterogeneous adaptation is typically\\napplied.\\n\\nand\\n\\nand\\n\\n2)\\n\\nand\\n\\n: In this case, we often leverage\\nthe relationship between\\nand\\nfor knowledge transfer.\\nThe basic idea is to map\\nto the same space where\\nand\\nhomogeneous adaptation can be applied. The mapping can be\\ndone directly from\\n\\n, i.e.,\\n\\nto\\n\\n(58)\\n\\nFor example, a bilingual dictionary represents such a mapping\\nthat can be used in cross-language text categorization or re-\\ntrieval [139], [215], where two languages are considered as two\\ndifferent domains or tasks.\\n\\ncan be transformed to a\\n\\nAlternatively, both\\n\\nto\\n\\ncommon latent space [216], [217]:\\n\\nThe mapping can also be modeled probabilistically in the form\\nof a \\u201ctranslation\\u201d model [218],\\n\\n(59)\\n\\n(60)\\n\\nThe above relationships can be estimated if we have a large\\n. For\\nnumber of correspondence data\\nexample, the study of [218] uses images with text annotations as\\naligned input pairs to estimate\\n. When correspondence\\ndata is not available, the study of [217] learns the mappings to\\nthe latent space that preserve the local geometry and neighbor-\\nhood relationship.\\n\\n: In this scenario, it is the re-\\nlationship between the output spaces that methods of hetero-\\ngeneous adaptation will leverage. Often, there may exist direct\\nmappings between output spaces. For example, phone recogni-\\ntion (source task) has an output space consisting of phoneme\\nsequences. Word recognition (target task), then, can be cast into\\na phone recognition problem followed by a phoneme-to-word\\ntransducer:\\n\\nand\\n\\n3)\\n\\nAlternatively, the output spaces\\n\\nand\\n\\nrelated to each other via a latent space:\\n\\n(61)\\n\\ncan also be made\\n\\n(62)\\n\\nand\\n\\ncan be both transformed from a hidden\\nFor example,\\nlayer space using MLPs [196]. Additionally, the relationship can\\nbe modeled in the form of constraints. In [219], the source task is\\npart-of-speech tagging and the target task is named-entity recog-\\nnition. By imposing constraints on the output variables, e.g.,\\nnamed entities should not be part of verb phrases, the author\\nshowed both theoretically and experimentally that it is possible\\nto learn\\n\\nwith fewer samples from\\n\\n.\\n\\nD. Multi-Task Learning\\n\\nFinally, we brie\\ufb02y discuss the multi-task learning setting.\\nWhile adaptive learning just described aims at transferring\\nknowledge sequentially from a source task to a target task,\\nmulti-task learning focuses on learning different yet related\\ntasks simultaneously. Let\\u2019s index the individual tasks in the\\n\\n\\x0cDENG AND LI: MACHINE LEARNING PARADIGMS FOR SPEECH RECOGNITION: AN OVERVIEW\\n\\n21\\n\\nby\\n\\nby\\n\\nand\\n\\ninput/output spaces are the same across tasks, i.e.,\\n\\nmulti-task learning setting by\\n. We denote\\nthe input and output spaces of task\\n, respec-\\ntively, and denote the joint input/output distribution for task\\n. Note that the tasks are homogeneous if the\\nand\\nfor any ; and are otherwise heterogeneous. Multi-task\\nlearning described in ML literature is usually heterogeneous in\\nnature. Furthermore, we assume a training set\\nis available\\nfor each task with samples drawn from the corresponding\\njoint distribution. The tasks relate to each other via a meta-pa-\\nrameter\\n, the form of which will be discussed shortly. The goal\\nof multi-task learning is to jointly \\ufb01nd a meta-parameter\\nand\\na set of decision functions\\nthat minimize\\nthe average expected risk, i.e.,\\n\\n(63)\\n\\nIt has been theoretically proved that learning multiple tasks\\njointly is guaranteed to have better generalization performance\\nthan learning them independently, given that these tasks are re-\\nlated [197], [220]\\u2013[223]. A common approach is to minimize\\nthe empirical risk of each task while applying regularization that\\ncaptures the relatedness between tasks, i.e.,\\n\\n(64)\\n\\nwhere\\n\\ndenotes the empirical risk on data set\\n\\n, and\\n\\nis a regularization term that is parameterized by .\\n\\nAs in the case of adaptation, regularization is the key to the\\nsuccess of multi-task learning. There have been many regular-\\nization strategies that exploit different types of relatedness. A\\nlarge body of work is based on hierarchical Bayesian inference\\n[220], [224]\\u2013[228]. The basic idea is to assume that (1)\\nare\\neach generated from a prior\\nare each gener-\\nated from the same hyper prior\\n. Another approach, and\\nprobably one of the earliest to multi-task learning, is to let the\\ndecision functions of different tasks share common structures.\\nFor example, in [196], [197], some layers of MLPs are shared\\nby all tasks while the remaining layers are task-dependent. With\\na similar motivation, other works apply various forms of regu-\\nlarization such that\\nof similar tasks are close to each other in\\nthe model parameter space [223], [229], [230].\\n\\n; and (2)\\n\\nRecently, multi-task learning, and transfer learning in gen-\\neral, has been approached by the ML community using a new,\\ndeep learning framework. The basic idea is that the feature rep-\\nresentations learned in an unsupervised manner at higher layers\\nin the hierarchical architectures tend to share the properties\\ncommon among different tasks; e.g., [231]. We will brie\\ufb02y dis-\\ncuss an application of this new approach to multi-task learning\\nto ASR next, and will devote the \\ufb01nal section of this article to\\na more general introduction of deep learning.\\n\\nlearning usually involves heterogeneous inputs or outputs, and\\nthe information transfer can go both directions between tasks.\\nOne most interesting application of heterogeneous transfer\\nand multi-task learning is multimodal speech recognition and\\nsynthesis, as well as recognition and synthesis of other sources\\nof modality information such as video and image. In the recent\\nstudy of [231], an instance of heterogeneous multi-task learning\\narchitecture of [196] is developed using more advanced hier-\\narchical architectures and deep learning techniques. This deep\\nlearning model is then applied to a number of tasks including\\nspeech recognition, where the audio data of speech (in the form\\nof spectrogram) and video data are fused to learn the shared rep-\\nresentation of both speech and video in the mid layers of a deep\\narchitecture. This multi-task deep architecture extends the ear-\\nlier deep architectures developed for single-task deep learning\\narchitecture for image pixels [133], [134] and for speech spec-\\ntrograms [232] alone. The preliminary results reported in [231]\\nshow that both video and speech recognition tasks are improved\\nwith multi-task learning based on the deep architectures en-\\nabling shared speech and video representations.\\n\\nAnother successful example of heterogeneous transfer and\\nmulti-task learning in ASR is multi-lingual or cross-lingual\\nspeech recognition, where speech recognition for different\\nlanguages is considered as different tasks. Various approaches\\nhave been taken to attack this rather challenging acoustic\\nmodeling problem for ASR, where the dif\\ufb01culty lies in low\\nresources in either data or transcriptions or both due to eco-\\nnomic considerations in developing ASR for all languages\\nof the world. Cross-language data sharing and data weighing\\nare common and useful approaches [233]. Another successful\\napproach is to map pronunciation units across languages either\\nvia knowledge-based or data-driven methods [234].\\n\\nFinally, when we consider phone recognition and word recog-\\nnition as different tasks, e.g., phone recognition results are used\\nnot for producing text outputs but for language-type identi\\ufb01ca-\\ntion or for spoken document retrieval, then the use of pronun-\\nciation dictionary in almost all ASR systems to bridge phones\\nto words can constitute another excellent example of heteroge-\\nneous transfer. More advanced frameworks in ASR have pushed\\nthis direction further by advocating the use of even \\ufb01ner units\\nof speech than phones to bridge the raw acoustic information\\nof speech to semantic content of speech via a hierarchy of lin-\\nguistic structure. These atomic speech units include \\u201cspeech at-\\ntributes\\u201d [235], [236] in the detection-based and knowledge-\\nrich modeling framework, and overlapping articulatory features\\nin the framework that enables the exploitation of articulatory\\nconstraints and speech co-articulatory mechanisms for \\ufb02uent\\nspeech recognition; e.g., [130], [237], [238]. When the articula-\\ntory information during speech can be recovered during speech\\nrecognition using articulatory based recognizers, such informa-\\ntion can be usefully applied to a different task of pronunciation\\ntraining.\\n\\nE. Heterogeneous Transfer and Multi-Task Learning in Speech\\nRecognition\\n\\nVII. EMERGING MACHINE LEARNING PARADIGMS\\n\\nThe terms heterogeneous transfer and multi-task learning\\nare often used exchangeably in the ML literature, as multi-task\\n\\nIn this \\ufb01nal section, we will provide an overview on two\\nemerging and rather signi\\ufb01cant developments within both ASR\\n\\n\\x0c22\\n\\nIEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 21, NO. 5, MAY 2013\\n\\nand ML communities in recent years: learning with deep ar-\\nchitectures and learning with sparse representations. These de-\\nvelopments share the commonality that they focus on learning\\ninput representations of the signals including speech, as shown\\nin the last column of Fig. 1. Deep learning is intrinsically linked\\nto the use of multiple layers of nonlinear transformations to\\nderive speech features, while learning with sparsity involves\\nthe use of examplar-based representations for speech features\\nwhich have high dimensionality but mostly empty entries.\\n\\nConnections between the emerging learning paradigms re-\\nviewed in this section and those discussed in previous sections\\ncan be drawn. Deep learning described in Section VII-A below\\nis an excellent example of hybrid generative and discrimina-\\ntive learning paradigms elaborated in Sections III and IV, where\\ngenerative learning is used as \\u201cpre-training\\u201d and discrimina-\\ntive learning is used as \\u201c\\ufb01ne tuning\\u201d. Since the \\u201cpre-training\\u201d\\nphase typically does not make use of labels for classi\\ufb01cation,\\nthis also falls into the unsupervised learning paradigm discussed\\nin Section V-B. Sparse representation in Section VII-B below is\\nalso linked to unsupervised learning; i.e. learning feature repre-\\nsentations in absence of classi\\ufb01cation labels. It further relates to\\nregularization in supervised or semi-supervised learning.\\n\\nA. Learning Deep Architectures\\n\\nLearning deep architectures, or more commonly called deep\\nlearning or hierarchical learning, has emerged since 2006 ig-\\nnited by the publications of [133], [134]. It links and expands a\\nnumber of ML paradigms that we have reviewed so far in this\\npaper, including generative, discriminative, supervised, unsu-\\npervised, and multi-task learning. Within the past few years, the\\ntechniques developed from deep learning research have already\\nbeen impacting a wide range of signal and information pro-\\ncessing including notably ASR; e.g., [20], [108], [239]\\u2013[256].\\nDeep learning refers to a class of ML techniques, where\\nmany layers of information processing stages in hierarchical\\narchitectures are exploited for unsupervised feature learning\\nand for pattern classi\\ufb01cation. It is in the intersections among\\nthe research areas of neural network, graphical modeling,\\noptimization, pattern recognition, and signal processing. Two\\nimportant reasons for the popularity of deep learning today are\\nthe signi\\ufb01cantly lowered cost of computing hardware and the\\ndrastically increased chip processing abilities (e.g., GPU units).\\nSince 2006, researchers have demonstrated the success of deep\\nlearning in diverse applications of computer vision, phonetic\\nrecognition, voice search, spontaneous speech recognition,\\nspeech and image feature coding, semantic utterance classi\\ufb01ca-\\ntion, hand-writing recognition, audio processing, information\\nretrieval, and robotics.\\n\\n1) A Brief Historical Account: Until recently, most ML tech-\\nniques had exploited shallow-structured architectures. These ar-\\nchitectures typically contain a single layer of nonlinear fea-\\nture transformations and they lack multiple layers of adaptive\\nnon-linear features. Examples of the shallow architectures are\\nconventional HMMs which we discussed in Section III, linear or\\nnonlinear dynamical systems, conditional random \\ufb01elds, max-\\nimum entropy models, support vector machines, logistic regres-\\nsion, kernel regression, and multi-layer perceptron with a single\\nhidden layer. A property common to these shallow learning\\nmodels is the simple architecture that consists of only one layer\\n\\nresponsible for transforming the raw input signals or features\\ninto a problem-speci\\ufb01c feature space, which may be unobserv-\\nable. Take the example of a SVM. It is a shallow linear separa-\\ntion model with one or zero feature transformation layer when\\nkernel trick is and is not used, respectively. Shallow architec-\\ntures have been shown effective in solving many simple or well-\\nconstrained problems, but their limited modeling and represen-\\ntational power can cause dif\\ufb01culties when dealing with more\\ncomplicated real-world applications involving natural signals\\nsuch as human speech, natural sound and language, and natural\\nimage and visual scenes.\\n\\nHistorically, the concept of deep learning was originated\\nfrom arti\\ufb01cial neural network research. It was not until recently\\nthat the well known optimization dif\\ufb01culty associated with\\nthe deep models was empirically alleviated when a reasonably\\nef\\ufb01cient, unsupervised learning algorithm was introduced in\\n[133], [134]. A class of deep generative models was introduced,\\ncalled deep belief networks (DBNs, not to be confused with\\nDynamic Bayesian Networks discussed in Section III). A core\\ncomponent of the DBN is a greedy, layer-by-layer learning\\nalgorithm which optimizes DBN weights at time complexity\\nlinear to the size and depth of the networks. The building block\\nof the DBN is the restricted Boltzmann machine, a special type\\nof Markov random \\ufb01eld, discussed in Section III-A, that has\\none layer of stochastic hidden units and one layer of stochastic\\nobservable units.\\n\\nThe DBN training procedure is not the only one that makes\\ndeep learning possible. Since the publication of the seminal\\nwork in [133], [134], a number of other researchers have been\\nimproving and developing alternative deep learning techniques\\nwith success. For example, one can alternatively pre-train the\\ndeep networks layer by layer by considering each pair of layers\\nas a de-noising auto-encoder [257].\\n\\n2) A Review of Deep Architectures and Their Learning: A\\nbrief overview is provided here on the various architectures of\\ndeep learning, including and beyond the original DBN. As de-\\nscribed earlier, deep learning refers to a rather wide class of ML\\ntechniques and architectures, with the hallmark of using many\\nlayers of non-linear information processing stages that are hier-\\narchical in nature. Depending on how the architectures and tech-\\nniques are intended for use, e.g., synthesis/generation or recog-\\nnition/classi\\ufb01cation, one can categorize most of the work in this\\narea into three types summarized below.\\n\\nThe \\ufb01rst type consists of generative deep architectures, which\\nare intended to characterize the high-order correlation proper-\\nties of the data or joint statistical distributions of the visible data\\nand their associated classes. Use of Bayes rule can turn this type\\nof architecture into a discriminative one. Examples of this type\\nare various forms of deep auto-encoders, deep Boltzmann ma-\\nchine, sum-product networks, the original form of DBN and its\\nextension to the factored higher-order Boltzmann machine in\\nits bottom layer. Various forms of generative models of hidden\\nspeech dynamics discussed in Section III-D and III-E, the deep\\ndynamic Bayesian network model discussed in Fig. 2, also be-\\nlong to this type of generative deep architectures.\\n\\nThe second type of deep architectures are discriminative in\\nnature, which are intended to provide discriminative power for\\npattern classi\\ufb01cation and to do so by characterizing the poste-\\nrior distributions of class labels conditioned on the visible data.\\n\\n\\x0cDENG AND LI: MACHINE LEARNING PARADIGMS FOR SPEECH RECOGNITION: AN OVERVIEW\\n\\n23\\n\\nExamples include deep-structured CRF, tandem-MLP architec-\\nture [94], [258], deep convex or stacking network [248] and its\\ntensor version [242], [243], [259], and detection-based ASR ar-\\nchitecture [235], [236], [260].\\n\\nin Section III-D and Section III-E, into the discriminative deep\\narchitectures explored vigorously by both ML and ASR com-\\nmunities in recent years is a fruitful research direction.\\n\\nActive research is currently ongoing by a growing number of\\ngroups, both academic and industrial, in applying deep learning\\nto ASR. New and more effective deep architectures and related\\nlearning algorithms have been reported in every major ASR-\\nrelated and ML-related conferences and workshops since 2010.\\nThis trend is expected to continue in coming years.\\n\\nB. Sparse Representations\\n\\n1) A Review of Recent Work: In recent years, another ac-\\ntive area of ASR research that is closely related to ML has\\nbeen the use of sparse representation. This refers to a set of\\ntechniques used to reconstruct a structured signal from a lim-\\nited number of training examples, a problem which arises in\\nmany ML applications where reconstruction relates to adap-\\ntively \\ufb01nding a dictionary which best represents the signal on\\na per-sample basis. The dictionary can either include random\\nprojections, as is typically done for signal reconstruction, or in-\\nclude actual training samples from the data, as explored also in\\nmany ML applications. Like deep learning, sparse representa-\\ntion is another emerging and rapidly growing area with contri-\\nbutions in a variety of signal processing and ML conferences,\\nincluding ASR in recent years.\\n\\nWe review the recent applications of sparse representation\\nto ASR here, highlighting the relevance to and contributions\\nfrom ML. In [262], [263], exemplar-based sparse representa-\\ntions are systematically explored to map test features into the\\nlinear span of training examples. They share the same \\u201cnon-\\nparametric\\u201d ML principle as the nearest-neighbor approach ex-\\nplored in [264] and the SVM method in directly utilizing infor-\\nmation about individual training examples. Speci\\ufb01cally, given\\na set of acoustic-feature sequences from the training set that\\nserve as a dictionary, the test data is represented as a linear com-\\nbination of these training examples by solving a least square\\nregression problem constrained by sparseness on the weight\\nsolution. The use of such constraints is typical of regulariza-\\ntion techniques, which are fundamental in ML and discussed in\\nSection II. The sparse features derived from the sparse weights\\nand dictionary are then used to map the test samples back into\\nthe linear span of training examples in the dictionary. The re-\\nsults show that the frame-level speech classi\\ufb01cation accuracy\\nusing sparse representations exceeds that of Gaussian mixture\\nmodel. In addition, sparse representations not only move test\\nfeatures closer to training, they also move the features closer\\nto the correct class. Such sparse representations are used as ad-\\nditional features to the existing high-quality features and error\\nrate reduction is reported in both phone recognition and large\\nvocabulary continuous speech recognition tasks with detailed\\nexperimental conditions provided in [263].\\n\\nIn the studies of [265], [266], various uncertainty measures\\nare developed to characterize the expected accuracy of a sparse\\nimputation, an exemplar-based reconstruction method based on\\nrepresenting segments of the noisy speech signal as linear com-\\nbinations of as few clean speech example segments as possible.\\nThe exemplars used are time-frequency patches of real speech,\\neach spanning multiple time frames. Then after the distorted\\nspeech is modeled as a linear combination of noise and speech\\n\\nIn the third type, or hybrid deep architectures, the goal is dis-\\ncrimination but this is assisted (often in a signi\\ufb01cant way) with\\nthe outcomes of generative architectures. In the existing hybrid\\narchitectures published in the literature, the generative com-\\nponent is mostly exploited to help with discrimination as the\\n\\ufb01nal goal of the hybrid architecture. How and why generative\\nmodeling can help with discriminative can be examined from\\ntwo viewpoints: 1)The optimization viewpoint where genera-\\ntive models can provide excellent initialization points in highly\\nnonlinear parameter estimation problems (The commonly used\\nterm of \\u201cpre-training\\u201d in deep learning has been introduced for\\nthis reason); and/or 2) The regularization perspective where\\ngenerative models can effectively control the complexity of\\nthe overall model. When the generative deep architecture of\\nDBN is subject to further discriminative training, commonly\\ncalled \\u201c\\ufb01ne-tuning\\u201d in the literature, we obtain an equivalent\\narchitecture of deep neural network (DNN, which is sometimes\\nalso called DBN or deep MLP in the literature). In a DNN, the\\nweights of the network are \\u201cpre-trained\\u201d from DBN instead\\nof the usual random initialization. The surprising success of\\nthis hybrid generative-discriminative deep architecture in the\\nform of DNN in large vocabulary ASR was \\ufb01rst reported in\\n[20], [250], soon veri\\ufb01ed by a series of new and bigger ASR\\ntasks carried out vigorously by a number of major ASR labs\\nworldwide.\\n\\nAnother typical example of the hybrid deep architecture was\\ndeveloped in [261]. This is a hybrid of DNN with a shallow dis-\\ncriminative architecture of CRF. Here, the overall architecture\\nof DNN-CRF is learned using the discriminative criterion of\\nsentence-level conditional probability of labels given the input\\ndata sequence. It can be shown that such DNN-CRF is equiva-\\nlent to a hybrid deep architecture of DNN and HMM, whose pa-\\nrameters are learned jointly using the full-sequence maximum\\nmutual information (MMI) between the entire label sequence\\nand the input data sequence. This architecture is more recently\\nextended to have sequential connections or temporal depen-\\ndency in the hidden layers of DBN, in addition to the output\\nlayer [244].\\n\\n3) Analysis and Perspectives: As analyzed in Section III,\\nmodeling structured speech dynamics and capitalizing on the\\nessential temporal properties of speech are key to high accu-\\nracy ASR. Yet the DBN-DNN approach, while achieving dra-\\nmatic error reduction, has made little use of such structured dy-\\nnamics. Instead, it simply accepts the input of a long window\\nof speech features as its acoustic context and outputs a very\\nlarge number of context-dependent sub-phone units, using many\\nhidden layers one on top of another with massive weights.\\n\\nThe de\\ufb01ciency in temporal aspects of the DBN-DNN ap-\\nproach has been recognized and much of current research has\\nfocused on recurrent neural network using the same massive-\\nweight methodology. It is not clear such a brute-force approach\\ncan adequately capture the underlying structured dynamic prop-\\nerties of speech, but it is clearly superior to the earlier use of\\nlong, \\ufb01xed-sized windows in DBN-DNN. How to integrate the\\npower of generative modeling of speech dynamics, elaborated\\n\\n\\x0c24\\n\\nIEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 21, NO. 5, MAY 2013\\n\\nexemplars, an algorithm is developed and applied to recover the\\nsparse linear combination of exemplars from the observed noisy\\nspeech. In experiments on noisy large vocabulary speech data,\\nthe use of observation uncertainties and sparse representations\\nimproves ASR performance signi\\ufb01cantly.\\n\\nIn a further study reported in [232], [267], [268], in deriving\\nsparse feature representations for speech, an auto-associative\\nneural network is used, whose internal hidden-layer output is\\nconstrained to be sparse. In [268], the fundamental concept of\\nregularization in ML is used, where a sparse regularization term\\nis added to the original reconstruction error or a cross-entropy\\ncost function and by updating the parameters of the network to\\nminimize the overall cost. Signi\\ufb01cant phonetic recognition error\\nreduction is reported.\\n\\nnodes for feature representations as done in [232], [267], [268],\\nsparseness is exploited for reducing non-zero DNN weights.\\nThe experimental results in [271] on a large scale ASR task\\nshow not only the DNN model size is reduced by 66% to 88%,\\nthe error rate is also slightly reduced by 0.2\\u20130.3%. It is a fruitful\\nresearch direction to exploit sparseness in multiple ways for\\nASR, and the highly successful deep sparse coding schemes\\ndeveloped by ML and computer vision researchers have yet to\\nenter ASR.\\n\\nVIII. DISCUSSION AND CONCLUSIONS\\n\\nIn this overview article, we introduce a set of prominent ML\\nparadigms that are motivated in the context of ASR technology\\nand applications. Throughout this review, readers can see that\\nML is deeply ingrained within ASR technology, and vice versa.\\nOn the one hand, ASR can be regarded only as an instance of a\\nML problem, just as is any \\u201capplication\\u201d of ML such as com-\\nputer vision, bioinformatics, and natural language processing.\\nWhen seen in this way, ASR is a particularly useful ML appli-\\ncation since it has extremely large training and test corpora, it\\nis computationally challenging, it has a unique sequential struc-\\nture in the input, it is also an instance of ML with structured\\noutput, and, perhaps most importantly, it has a large commu-\\nnity of researchers who are energetically advancing the under-\\nlying technology. On the other hand, ASR has been the source\\nof many critical ideas in ML, including the ubiquitous HMM,\\nthe concept of classi\\ufb01er adaptation, and the concept of discrim-\\ninative training on generative models such as HMM\\u2014all these\\nwere developed and used in the ASR community long before\\nthey caught the interest of the ML community. Indeed, our main\\nhypothesis in this review is that these two communities can and\\nshould be communicating regularly with each other. Our belief\\nis that the historical and mutually bene\\ufb01cial in\\ufb02uence that the\\ncommunities have had on each other will continue, perhaps at\\nan even more fruitful pace. It is hoped that this overview paper\\nwill indeed foster such communication and advancement.\\n\\nTo this end, throughout this overview we have elaborated on\\nthe key ML notion of structured classi\\ufb01cation as a fundamental\\nproblem in ASR\\u2014with respect to both the symbolic sequence\\nas the ASR classi\\ufb01er\\u2019s output and the continuous-valued vector\\nfeature sequence as the ASR classi\\ufb01er\\u2019s input. In presenting\\neach of the ML paradigms, we have highlighted the most\\nrelevant ML concepts to ASR, and emphasized the kind of\\nML approaches that are effective in dealing with the special\\ndif\\ufb01culties of ASR including deep/dynamic structure in human\\nspeech and strong variability in the observations. We have\\nalso paid special attention to discussing and analyzing the\\nmajor ML paradigms and results that have been con\\ufb01rmed\\nby ASR experiments. The main examples discussed in this\\narticle include HMM-related and dynamics-oriented generative\\nlearning, discriminative learning for HMM-like generative\\nmodels, complexity control (regularization) of ASR systems\\nby principled parameter tying, adaptive and Bayesian learning\\nfor environment-robust and speaker-robust ASR, and hybrid\\nsupervised/unsupervised learning or hybrid generative/dis-\\ncriminative learning as exempli\\ufb01ed in the more recent \\u201cdeep\\nlearning\\u201d scheme involving DBN and DNN. However, we have\\nalso discussed a set of ASR models and methods that have not\\nbecome mainstream but that have solid theoretical foundation\\n\\nFinally, motivated by the sparse Bayesian learning technique\\nand relevance vector machines developed by the ML commu-\\nnity (e.g. [269]), an extension is made from the generic unstruc-\\ntured data to structured data of speech and to ASR applications\\nby ASR researchers. In the Bayesian-sensing HMM reported\\nin [270], speech feature sequences are represented using a set\\nof HMM state-dependent basis vectors. Again, model regular-\\nization is used to perform sparse Bayesian sensing in face of\\nheterogeneous training data. By incorporating a prior density\\non sensing weights, the relevance of different bases to a feature\\nvector is determined by the corresponding precision parameters.\\nThe model parameters that consist of the basis vectors, the pre-\\ncision matrices of sensing weights and the precision matrices of\\nreconstruction errors, are jointly estimated using a recursive so-\\nlution, in which the standard Bayesian technique of marginal-\\nization (over the weight priors) is exploited. Experimental re-\\nsults reported in [270] as well as in a series of earlier work on a\\nlarge-scale ASR task show consistent improvements.\\n\\n2) Analysis and Perspectives: Sparse representation has\\nclose links to fundamental ML concepts of regularization and\\nunsupervised feature learning, and also has a deep root in\\nneuroscience. However, its applications to ASR are quite recent\\nand their success, compared with deep learning, is more limited\\nin scope and size, despite the huge success of sparse coding\\nand (sparse) compressive sensing in ML and signal/image\\nprocessing with a relatively long history.\\n\\nOne possible limiting factor is that the underlying structure of\\nspeech features is less prone to sparsi\\ufb01cation and compression\\nthan the image counterpart. Nevertheless, the initial promising\\nASR results as reviewed above should encourage more work in\\nthis direction. It is possible that different types of raw speech\\nfeatures from what have been experimented will have greater\\npotential and effectiveness for sparse representations. As an ex-\\nample, speech waveforms are obviously not a natural candidate\\nfor sparse representation but the residual signals after linear pre-\\ndiction would be.\\n\\nFurther, sparseness may not necessarily be exploited for rep-\\nresentation purposes only in the unsupervised learning setting.\\nJust as the success of deep learning comes from hybrid between\\nunsupervised generative learning (pre-training) and supervised\\ndiscriminative learning (\\ufb01ne-tuning), sparseness can be ex-\\nploited in a similar way. The recent work reported in [271]\\nformulates parameter sparseness as soft regularization and\\nconvex constrained optimization problems in a DNN system.\\nInstead of placing sparseness constraint in the DNN\\u2019s hidden\\n\\n\\x0cDENG AND LI: MACHINE LEARNING PARADIGMS FOR SPEECH RECOGNITION: AN OVERVIEW\\n\\n25\\n\\nin ML and speech science, and in combination with other\\nlearning paradigms, they offer a potential to make signi\\ufb01cant\\ncontributions. We provide suf\\ufb01cient context and offer insight\\nin discussing such models and ASR examples in connection\\nwith the relevant ML paradigms, and analyze their potential\\ncontributions.\\n\\nASR technology is fast changing in recent years, partly\\npropelled by a number of emerging applications in mobile\\ncomputing, natural user interface, and AI-like personal as-\\nsistant technology. So is the infusion of ML techniques into\\nASR. A comprehensive overview on the topic of this nature\\nunavoidably contains bias as we suggest important research\\nproblems and future directions where the ML paradigms would\\noffer the potential to spur next waves of ASR advancement,\\nand as we take position and carry out analysis on a full range of\\nthe ASR work spanning over 40 years. In the future, we expect\\nmore integrated ML paradigms to be usefully applied to ASR\\nas exempli\\ufb01ed by the two emerging ML schemes presented and\\nanalyzed in Section VII. We also expect new ML techniques\\nthat make an intelligent use of large supply of training data with\\nwide diversity and large-scale optimization (e.g., [272]) to im-\\npact ASR, where active learning, semi-supervised learning, and\\neven unsupervised learning will play more important roles than\\nin the past and at present as surveyed in Section V. Moreover,\\neffective exploration and exploitation of deep, hierarchical\\nstructure in conjunction with spatially invariant and temporary\\ndynamic properties of speech is just beginning (e.g., [273]).\\nThe recent renewed interest in recurrent neural network with\\ndeep, multiple-level representations from both ASR and ML\\ncommunities using more powerful optimization techniques\\nthan in the past is an example of the research moving towards\\nthis direction. To reap full fruit by such an endeavor will require\\nintegrated ML methodologies within and possibly beyond the\\nparadigms we have covered in this paper.\\n\\nACKNOWLEDGMENT\\n\\nThe authors thank Prof. Jeff Bilmes for contributions during\\nthe early phase (2010) of developing this paper, and for valuable\\ndiscussions with Geoff Hinton, John Platt, Mark Gales, Nelson\\nMorgan, Hynek Hermansky, Alex Acero, and Jason Eisner. Ap-\\npreciations also go to MSR for the encouragement and support\\nof this \\u201cmentor-mentee project\\u201d, to Helen Meng as the previous\\nEIC for handling the white-paper reviews during 2009, and to\\nthe reviewers whose desire for perfection has made various ver-\\nsions of the revision steadily improve the paper\\u2019s quality as new\\nadvances on ML and ASR frequently broke out throughout the\\nwriting and revision over past 3 years.\\n\\nREFERENCES\\n\\n[1] J. Baker, L. Deng, J. Glass, S. Khudanpur, C.-H. Lee, N. Morgan, and\\nD. O\\u2019Shgughnessy, \\u201cResearch developments and directions in speech\\nrecognition and understanding, part i,\\u201d IEEE Signal Process. Mag., vol.\\n26, no. 3, pp. 75\\u201380, 2009.\\n\\n[2] X. Huang and L. Deng, \\u201cAn overview of modern speech recognition,\\u201d\\nin Handbook of Natural Language Processing, Second Edition, N. In-\\ndurkhya and F. J. Damerau, Eds. Boca Raton, FL, USA: CRC, Taylor\\nand Francis.\\n\\n[3] M. Jordan, E. Sudderth, M. Wainwright, and A. Wilsky, \\u201cMajor ad-\\nvances and emerging developments of graphical models, special issue,\\u201d\\nIEEE Signal Process. Mag., vol. 27, no. 6, pp. 17\\u2013138, Nov. 2010.\\n\\n[4] J. Bilmes, \\u201cDynamic graphical models,\\u201d IEEE Signal Process. Mag.,\\n\\nvol. 33, no. 6, pp. 29\\u201342, Nov. 2010.\\n\\n[5] S. Rennie, J. Hershey, and P. Olsen, \\u201cSingle-channel multitalker speech\\nrecognition\\u2014Graphical modeling approaches,\\u201d IEEE Signal Process.\\nMag., vol. 33, no. 6, pp. 66\\u201380, Nov. 2010.\\n\\n[6] P. L. Bartlett, M. I. Jordan, and J. D. McAuliffe, \\u201cConvexity, classi-\\n\\ufb01cation, risk bounds,\\u201d J. Amer. Statist. Assoc., vol. 101, pp. 138\\u2013156,\\n2006.\\n\\n[7] V. N. Vapnik, Statistical Learning Theory. New York, NY, USA:\\n\\n[8] C. Cortes and V. Vapnik, \\u201cSupport vector networks,\\u201d Mach. Learn.,\\n\\nWiley-Interscience, 1998.\\n\\npp. 273\\u2013297, 1995.\\n\\n[9] D. A. McAllester, \\u201cSome PAC-Bayesian theorems,\\u201d in Proc. Workshop\\n\\nComput. Learn. Theory, 1998.\\n\\n[10] T. Jaakkola, M. Meila, and T. Jebara, \\u201cMaximum entropy discrimi-\\nnation,\\u201d Mass. Inst. of Technol., Artif. Intell. Lab., Tech. Rep. AITR-\\n1668, 1999.\\n\\n[11] M. Gales, S. Watanabe, and E. Fosler-Lussier, \\u201cStructured discrimina-\\ntive models for speech recognition,\\u201d IEEE Signal Process. Mag., vol.\\n29, no. 6, pp. 70\\u201381, Nov. 2012.\\n\\n[12] S. Zhang and M. Gales, \\u201cStructured SVMs for automatic speech recog-\\nnition,\\u201d IEEE Trans. Audio, Speech, Lang. Process., vol. 21, no. 3, pp.\\n544\\u2013555, Mar. 2013.\\n\\n[13] F. Pernkopf and J. Bilmes, \\u201cDiscriminative versus generative param-\\neter and structure learning of Bayesian network classi\\ufb01ers,\\u201d in Proc.\\nInt. Conf. Mach. Learn., Bonn, Germany, 2005.\\n\\n[14] D. Koller and N. Friedman, Probabilistic Graphical Models: Princi-\\n\\nples and Techniques. Cambridge, MA, USA: MIT Press, 2009.\\n\\n[15] L. Rabiner and B.-H. Juang, Fundamentals of Speech Recognition.\\n\\nUpper Saddle River, NJ, USA: Prentice-Hall, 1993.\\n\\n[16] B.-H. Juang, S. E. Levinson, and M. M. Sondhi, \\u201cMaximum likelihood\\nestimation for mixture multivariate stochastic observations of Markov\\nchains,\\u201d IEEE Trans. Inf. Theory, vol. IT-32, no. 2, pp. 307\\u2013309, Mar.\\n1986.\\n\\n[17] L. Deng, P. Kenny, M. Lennig, V. Gupta, F. Seitz, and P. Mermelsten,\\n\\u201cPhonemic hidden Markov models with continuous mixture output\\ndensities for large vocabulary word recognition,\\u201d IEEE Trans. Acoust.,\\nSpeech, Signal Process., vol. 39, no. 7, pp. 1677\\u20131681, Jul. 1991.\\n\\n[18] J. Bilmes, \\u201cWhat HMMs can do,\\u201d IEICE Trans. Inf. Syst., vol. E89-D,\\n\\nno. 3, pp. 869\\u2013891, Mar. 2006.\\n\\n[19] L. Deng, M. Lennig, F. Seitz, and P. Mermelstein, \\u201cLarge vocabulary\\nword recognition using context-dependent allophonic hidden Markov\\nmodels,\\u201d Comput., Speech, Lang., vol. 4, pp. 345\\u2013357, 1991.\\n\\n[20] G. Dahl, D. Yu, L. Deng, and A. Acero, \\u201cContext-dependent pre-trained\\ndeep neural networks for large-vocabulary speech recognition,\\u201d IEEE\\nTrans. Audio, Speech, Lang. Process., vol. 20, no. 1, pp. 30\\u201342, Jan.\\n2012.\\n\\n[21] J. Baker, \\u201cStochastic modeling for automatic speech recognition,\\u201d in\\nSpeech Recogn., D. R. Reddy, Ed. New York, NY, USA: Academic,\\n1976.\\n\\n[22] F. Jelinek, \\u201cContinuous speech recognition by statistical methods,\\u201d\\n\\nProc. IEEE, vol. 64, no. 4, pp. 532\\u2013557, Apr. 1976.\\n\\n[23] L. E. Baum and T. Petrie, \\u201cStatistical inference for probabilistic func-\\ntions of \\ufb01nite state Markov chains,\\u201d Ann. Math. Statist., vol. 37, no. 6,\\npp. 1554\\u20131563, 1966.\\n\\n[24] A. P. Dempster, N. M. Laird, and D. B. Rubin, \\u201cMaximum-likelihood\\nfrom incomplete data via the EM algorithm,\\u201d J. R. Statist. Soc. Ser. B.,\\nvol. 39, pp. 1\\u201338, 1977.\\n\\n[25] X. D. Huang, A. Acero, and H.-W. Hon, Spoken Language Processing:\\nA Guide to Theory, Algorithm, System Development. Upper Saddle\\nRiver, NJ, USA: Prentice-Hall, 2001.\\n\\n[26] M. Gales and S. Young, \\u201cRobust continuous speech recognition using\\nparallel model combination,\\u201d IEEE Trans. Speech Audio Process., vol.\\n4, no. 5, pp. 352\\u2013359, Sep. 1996.\\n\\n[27] A. Acero, L. Deng, T. Kristjansson, and J. Zhang, \\u201cHMM adaptation\\nusing vector taylor series for noisy speech recognition,\\u201d in Proc. Int.\\nConf. Spoken Lang, Process., 2000, pp. 869\\u2013872.\\n\\n[28] L. Deng, J. Droppo, and A. Acero, \\u201cA Bayesian approach to speech\\nfeature enhancement using the dynamic cepstral prior,\\u201d in Proc. IEEE\\nInt. Conf. Acoust., Speech, Signal Process., May 2002, vol. 1, pp.\\nI-829\\u2013I-832.\\n\\n[29] B. Frey, L. Deng, A. Acero, and T. Kristjansson, \\u201cAlgonquin: Iterating\\nLaplaces method to remove multiple types of acoustic distortion for\\nrobust speech recognition,\\u201d in Proc. Eurospeech, 2000.\\n\\n[30] J. Baker, L. Deng, J. Glass, S. Khudanpur, C.-H. Lee, N. Morgan, and\\nD. O\\u2019Shgughnessy, \\u201cUpdated MINDS report on speech recognition\\nand understanding,\\u201d IEEE Signal Process. Mag., vol. 26, no. 4, pp.\\n78\\u201385, Jul. 2009.\\n\\n[31] M. Ostendorf, A. Kannan, O. Kimball, and J. Rohlicek, \\u201cContinuous\\nword recognition based on the stochastic segment model,\\u201d in Proc.\\nDARPA Workshop CSR, 1992.\\n\\n\\x0c26\\n\\nIEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 21, NO. 5, MAY 2013\\n\\n[32] M. Ostendorf, V. Digalakis, and O. Kimball, \\u201cFrom HMM\\u2019s to segment\\nmodels: A uni\\ufb01ed view of stochastic modeling for speech recognition,\\u201d\\nIEEE Trans. Speech Audio Process., vol. 4, no. 5, pp. 360\\u2013378, Sep.\\n1996.\\n\\n[33] L. Deng, \\u201cA generalized hidden Markov model with state-conditioned\\ntrend functions of time for the speech signal,\\u201d Signal Process., vol. 27,\\nno. 1, pp. 65\\u201378, 1992.\\n\\n[34] L. Deng, M. Aksmanovic, D. Sun, and J. Wu, \\u201cSpeech recognition\\nusing hidden Markov models with polynomial regression functions as\\nnon-stationary states,\\u201d IEEE Trans. Acoust., Speech, Signal Process.,\\nvol. 2, no. 4, pp. 101\\u2013119, Oct. 1994.\\n\\n[35] W. Holmes and M. Russell, \\u201cProbabilistic-trajectory segmental\\n\\nHMMs,\\u201d Comput. Speech Lang., vol. 13, pp. 3\\u201337, 1999.\\n\\n[36] H. Zen, K. Tokuda, and T. Kitamura, \\u201cAn introduction of trajectory\\nmodel into HMM-based speech synthesis,\\u201d in Proc. ISCA SSW5, 2004,\\npp. 191\\u2013196.\\n\\n[37] L. Zhang and S. Renals, \\u201cAcoustic-articulatory modelling with the tra-\\njectory HMM,\\u201d IEEE Signal Process. Lett., vol. 15, pp. 245\\u2013248, 2008.\\n[38] Y. Gong, I. Illina, and J.-P. Haton, \\u201cModeling long term variability\\ninformation in mixture stochastic trajectory framework,\\u201d in Proc. Int.\\nConf. Spoken Lang, Process., 1996.\\n\\n[39] L. Deng, G. Ramsay, and D. Sun, \\u201cProduction models as a structural\\nbasis for automatic speech recognition,\\u201d Speech Commun., vol. 33, no.\\n2\\u20133, pp. 93\\u2013111, Aug. 1997.\\n\\n[40] L. Deng, \\u201cA dynamic, feature-based approach to the interface between\\nphonology and phonetics for speech modeling and recognition,\\u201d\\nSpeech Commun., vol. 24, no. 4, pp. 299\\u2013323, 1998.\\n\\n[41] J. Picone, S. Pike, R. Regan, T. Kamm, J. Bridle, L. Deng, Z. Ma,\\nH. Richards, and M. Schuster, \\u201cInitial evaluation of hidden dynamic\\nmodels on conversational speech,\\u201d in Proc. IEEE Int. Conf. Acoust.,\\nSpeech, Signal Process., 1999, pp. 109\\u2013112.\\n\\n[42] J. Bridle, L. Deng, J. Picone, H. Richards, J. Ma, T. Kamm, M.\\nSchuster, S. Pike, and R. Reagan, \\u201cAn investigation fo segmental\\nhidden dynamic models of speech coarticulation for automatic speech\\nrecognition,\\u201d Final Rep. for 1998 Workshop on Language Engineering,\\nCLSP, Johns Hopkins 1998.\\n\\n[43] J. Ma and L. Deng, \\u201cA path-stack algorithm for optimizing dynamic\\nregimes in a statistical hidden dynamic model of speech,\\u201d Comput.\\nSpeech Lang., vol. 14, pp. 101\\u2013104, 2000.\\n\\n[44] M. Russell and P. Jackson, \\u201cA multiple-level linear/linear segmental\\nHMM with a formant-based intermediate layer,\\u201d Comput. Speech\\nLang., vol. 19, pp. 205\\u2013225, 2005.\\n\\n[45] L. Deng, Dynamic Speech Models\\u2014Theory, Algorithm, Applica-\\n\\ntions. San Rafael, CA, USA: Morgan and Claypool, 2006.\\n\\n[46] J. Bilmes, \\u201cBuried Markov models: A graphical modeling approach\\nto automatic speech recognition,\\u201d Comput. Speech Lang., vol. 17, pp.\\n213\\u2013231, Apr.\\u2013Jul. 2003.\\n\\n[47] L. Deng, D. Yu, and A. Acero, \\u201cStructured speech modeling,\\u201d IEEE\\nTrans. Speech Audio Process., vol. 14, no. 5, pp. 1492\\u20131504, Sep. 2006.\\n[48] L. Deng, D. Yu, and A. Acero, \\u201cA bidirectional target \\ufb01ltering model of\\nspeech coarticulation: Two-stage implementation for phonetic recogni-\\ntion,\\u201d IEEE Trans. Speech Audio Process., vol. 14, no. 1, pp. 256\\u2013265,\\nJan. 2006.\\n\\n[49] L. Deng, \\u201cComputational models for speech production,\\u201d in Computa-\\ntional Models of Speech Pattern Processing. New York, NY, USA:\\nSpringer-Verlag, 1999, pp. 199\\u2013213.\\n\\n[50] L. Lee, H. Attias, and L. Deng, \\u201cVariational inference and learning for\\nsegmental switching state space models of hidden speech dynamics,\\u201d\\nin Proc. IEEE Int. Conf. Acoust., Speech, Signal Process., Apr. 2003,\\nvol. 1, pp. I-872\\u2013I-875.\\n\\n[51] J. Droppo and A. Acero, \\u201cNoise robust speech recognition with a\\nswitching linear dynamic model,\\u201d in Proc. IEEE Int. Conf. Acoust.,\\nSpeech, Signal Process., May 2004, vol. 1, pp. I-953\\u2013I-956.\\n\\n[52] B. Mesot and D. Barber, \\u201cSwitching linear dynamical systems for noise\\nrobust speech recognition,\\u201d IEEE Audio, Speech, Lang. Process., vol.\\n15, no. 6, pp. 1850\\u20131858, Aug. 2007.\\n\\n[53] A. Rosti and M. Gales, \\u201cRao-blackwellised gibbs sampling for\\nswitching linear dynamical systems,\\u201d in Proc. IEEE Int. Conf. Acoust.,\\nSpeech, Signal Process., May 2004, vol. 1, pp. I-809\\u2013I-812.\\n\\n[54] E. B. Fox, E. B. Sudderth, M. I. Jordan, and A. S. Willsky, \\u201cBayesian\\nnonparametric methods for learning Markov switching processes,\\u201d\\nIEEE Signal Process. Mag., vol. 27, no. 6, pp. 43\\u201354, Nov. 2010.\\n\\n[55] E. Ozkan, I. Y. Ozbek, and M. Demirekler, \\u201cDynamic speech spectrum\\nrepresentation and tracking variable number of vocal tract resonance\\nfrequencies with time-varying Dirichlet process mixture models,\\u201d\\nIEEE Audio, Speech, Lang. Process., vol. 17, no. 8, pp. 1518\\u20131532,\\nNov. 2009.\\n\\n[56] J.-T. Chien and C.-H. Chueh, \\u201cDirichlet class language models for\\nspeech recognition,\\u201d IEEE Audio, Speech, Lang. Process., vol. 27, no.\\n3, pp. 43\\u201354, Mar. 2011.\\n\\n[57] J. Bilmes, \\u201cGraphical models and automatic speech recognition,\\u201d in\\nMathematical Foundations of Speech and Language Processing, R.\\nRosenfeld, M. Ostendorf, S. Khudanpur, and M. Johnson, Eds. New\\nYork, NY, USA: Springer-Verlag, 2003.\\n\\n[58] J. Bilmes and C. Bartels, \\u201cGraphical model architectures for speech\\nrecognition,\\u201d IEEE Signal Process. Mag., vol. 22, no. 5, pp. 89\\u2013100,\\nSep. 2005.\\n\\n[59] H. Zen, M. J. F. Gales, Y. Nankaku, and K. Tokuda, \\u201cProduct of experts\\nfor statistical parametric speech synthesis,\\u201d IEEE Audio, Speech, Lang.\\nProcess., vol. 20, no. 3, pp. 794\\u2013805, Mar. 2012.\\n\\n[60] D. Barber and A. Cemgil, \\u201cGraphical models for time series,\\u201d IEEE\\n\\nSignal Process. Mag., vol. 33, no. 6, pp. 18\\u201328, Nov. 2010.\\n\\n[61] A. Miguel, A. Ortega, L. Buera, and E. Lleida, \\u201cBayesian networks for\\ndiscrete observation distributions in speech recognition,\\u201d IEEE Audio,\\nSpeech, Lang. Process., vol. 19, no. 6, pp. 1476\\u20131489, Aug. 2011.\\n\\n[62] L. Deng, \\u201cSwitching dynamic system models for speech articulation\\nand acoustics,\\u201d in Mathematical Foundations of Speech and Lan-\\nguage Processing. New York, NY, USA: Springer-Verlag, 2003, pp.\\n115\\u2013134.\\n\\n[63] L. Deng and J. Ma, \\u201cSpontaneous speech recognition using a statistical\\ncoarticulatory model for the hidden vocal-tract-resonance dynamics,\\u201d\\nJ. Acoust. Soc. Amer., vol. 108, pp. 3036\\u20133048, 2000.\\n\\n[64] L. Deng, J. Droppo, and A. Acero, \\u201cEnhancement of log mel power\\nspectra of speech using a phase-sensitive model of the acoustic environ-\\nment and sequential estimation of the corrupting noise,\\u201d IEEE Trans.\\nSpeech Audio Process., vol. 12, no. 2, pp. 133\\u2013143, Mar. 2004.\\n\\n[65] V. Stoyanov, A. Ropson, and J. Eisner, \\u201cEmpirical risk minimization\\nof graphical model parameters given approximate inference, decoding,\\nmodel structure,\\u201d in Proc. AISTAT, 2011.\\n\\n[66] V. Goel and W. Byrne, \\u201cMinimum Bayes-risk automatic speech recog-\\n\\nnition,\\u201d Comput. Speech Lang., vol. 14, no. 2, pp. 115\\u2013135, 2000.\\n\\n[67] V. Goel, S. Kumar, and W. Byrne, \\u201cSegmental minimum Bayes-risk\\ndecoding for automatic speech recognition,\\u201d IEEE Trans. Speech Audio\\nProcess., vol. 12, no. 3, pp. 234\\u2013249, May 2004.\\n\\n[68] R. Schluter, M. Nussbaum-Thom, and H. Ney, \\u201cOn the relationship\\nbetween Bayes risk and word error rate in ASR,\\u201d IEEE Audio, Speech,\\nLang. Process., vol. 19, no. 5, pp. 1103\\u20131112, Jul. 2011.\\n\\n[69] C. Bishop, Pattern Recognition and Mach. Learn.. New York, NY,\\n\\nUSA: Springer, 2006.\\n\\n[70] J. Lafferty, A. McCallum, and F. Pereira, \\u201cConditional random \\ufb01elds:\\nProbabilistic models for segmenting and labeling sequence data,\\u201d in\\nProc. Int. Conf. Mach. Learn., 2001, pp. 282\\u2013289.\\n\\n[71] A. Gunawardana, M. Mahajan, A. Acero, and J. Platt, \\u201cHidden con-\\nditional random \\ufb01elds for phone classi\\ufb01cation,\\u201d in Proc. Interspeech,\\n2005.\\n\\n[72] G. Zweig and P. Nguyen, \\u201cSCARF: A segmental conditional random\\n\\n\\ufb01eld toolkit for speech recognition,\\u201d in Proc. Interspeech, 2010.\\n\\n[73] D. Povey and P. Woodland, \\u201cMinimum phone error and i-smoothing\\nfor improved discriminative training,\\u201d in Proc. IEEE Int. Conf. Acoust.,\\nSpeech, Signal Process., 2002, pp. 105\\u2013108.\\n\\n[74] X. He, L. Deng, and W. Chou, \\u201cDiscriminative learning in sequen-\\ntial pattern recognition\\u2014A unifying review for optimization-oriented\\nspeech recognition,\\u201d IEEE Signal Process. Mag., vol. 25, no. 5, pp.\\n14\\u201336, 2008.\\n\\n[75] J. Pylkkonen and M. Kurimo, \\u201cAnalysis of extended Baum-Welch and\\nconstrained optimization for discriminative training of HMMs,\\u201d IEEE\\nAudio, Speech, Lang. Process., vol. 20, no. 9, pp. 2409\\u20132419, 2012.\\n\\n[76] S. Kumar and W. Byrne, \\u201cMinimum Bayes-risk decoding for statistical\\n\\nmachine translation,\\u201d in Proc. HLT-NAACL, 2004.\\n\\n[77] X. He and L. Deng, \\u201cSpeech recognition, machine translation, speech\\ntranslation\\u2014A uni\\ufb01ed discriminative learning paradigm,\\u201d IEEE Signal\\nProcess. Mag., vol. 27, no. 5, pp. 126\\u2013133, Sep. 2011.\\n\\n[78] X. He and L. Deng, \\u201cMaximum expected BLEU training of phrase\\nand lexicon translation models,\\u201d Proc. Assoc. Comput. Linguist., pp.\\n292\\u2013301, 2012.\\n\\n[79] B.-H. Juang, W. Chou, and C.-H. Lee, \\u201cMinimum classi\\ufb01cation error\\nrate methods for speech recognition,\\u201d IEEE Trans. Speech Audio\\nProcess., vol. 5, no. 3, pp. 257\\u2013265, May 1997.\\n\\n[80] Q. Fu, Y. Zhao, and B.-H. Juang, \\u201cAutomatic speech recognition based\\non non-uniform error criteria,\\u201d IEEE Audio, Speech, Lang. Process.,\\nvol. 20, no. 3, pp. 780\\u2013793, Mar. 2012.\\n\\n[81] J. Weston and C. Watkins, \\u201cSupport vector machines for multi-class\\npattern recognition,\\u201d in Eur. Symp. Artif. Neural Netw., 1999, pp.\\n219\\u2013224.\\n\\n[82] I. Tsochantaridis, T. Hofmann, T. Joachims, and Y. Altun, \\u201cSupport\\nvector machine learning for interdependent and structured output\\nspaces,\\u201d in Proc. Int. Conf. Mach. Learn., 2004.\\n\\n[83] J. Kuo and Y. Gao, \\u201cMaximum entropy direct models for speech\\nrecognition,\\u201d IEEE Audio, Speech, Lang. Process., vol. 14, no. 3, pp.\\n873\\u2013881, May 2006.\\n\\n\\x0cDENG AND LI: MACHINE LEARNING PARADIGMS FOR SPEECH RECOGNITION: AN OVERVIEW\\n\\n27\\n\\n[84] J. Morris and E. Fosler-Lussier, \\u201cCombining phonetic attributes using\\nconditional random \\ufb01elds,\\u201d in Proc. Interspeech, 2006, pp. 597\\u2013600.\\n[85] I. Heintz, E. Fosler-Lussier, and C. Brew, \\u201cDiscriminative input stream\\ncombination for conditional random \\ufb01eld phone recognition,\\u201d IEEE\\nAudio, Speech, Lang. Process., vol. 17, no. 8, pp. 1533\\u20131546, Nov.\\n2009.\\n\\n[86] Y. Hifny and S. Renals, \\u201cSpeech recognition using augmented condi-\\ntional random \\ufb01elds,\\u201d IEEE Audio, Speech, Lang. Process., vol. 17, no.\\n2, pp. 354\\u2013365, Mar. 2009.\\n\\n[87] D. Yu, L. Deng, and A. Acero, \\u201cHidden conditional random \\ufb01eld with\\ndistribution constraints for phone classi\\ufb01cation,\\u201d in Proc. Interspeech,\\n2009, pp. 676\\u2013679.\\n\\n[88] D. Yu and L. Deng, \\u201cDeep-structured hidden conditional random \\ufb01elds\\nfor phonetic recognition,\\u201d in Proc. IEEE Int. Conf. Acoust., Speech,\\nSignal Process., 2010.\\n\\n[89] S. Renals, N. Morgan, H. Boulard, M. Cohen, and H. Franco, \\u201cCon-\\nnectionist probability estimators in HMM speech recognition,\\u201d IEEE\\nTrans. Speech Audio Process., vol. 2, no. 1, pp. 161\\u2013174, Jan. 1994.\\n\\n[90] H. Boulard and N. Morgan, \\u201cContinuous speech recognition by con-\\nnectionist statistical methods,\\u201d IEEE Trans. Neural Netw., vol. 4, no.\\n6, pp. 893\\u2013909, Nov. 1993.\\n\\n[91] H. Bourlard and N. Morgan, Connectionist Speech Recognition: A Hy-\\nbrid Approach, ser. The Kluwer International Series in Engineering and\\nComputer Science. Boston, MA, USA: Kluwer, 1994, vol. 247.\\n\\n[92] H. Bourlard and N. Morgan, \\u201cHybrid HMM/ANN systems for speech\\nrecognition: Overview and new research directions,\\u201d in Adaptive Pro-\\ncessing of Sequences and Data Structures. London, U.K.: Springer-\\nVerlag, 1998, pp. 389\\u2013417.\\n\\n[93] J. Pinto, S. Garimella, M. Magimai-Doss, H. Hermansky, and H.\\nBourlard, \\u201cAnalysis of MLP-based hierarchical phoneme posterior\\nprobability estimator,\\u201d IEEE Audio, Speech, Lang. Process., vol. 19,\\nno. 2, pp. 225\\u2013241, Feb. 2011.\\n\\n[94] N. Morgan, Q. Zhu, A. Stolcke, K. Sonmez, S. Sivadas, T. Shinozaki,\\nM. Ostendorf, P. Jain, H. Hermansky, D. Ellis, G. Doddington, B.\\nChen, O. Cretin, H. Bourlard, and M. Athineos, \\u201cPushing the enve-\\nlope\\u2014Aside [speech recognition],\\u201d IEEE Signal Process. Mag., vol.\\n22, no. 5, pp. 81\\u201388, Sep. 2005.\\n\\n[95] A. Ganapathiraju, J. Hamaker, and J. Picone, \\u201cHybrid SVM/HMM ar-\\nchitectures for speech recognition,\\u201d in Proc. Adv. Neural Inf. Process.\\nSyst., 2000.\\n\\n[96] J. Stadermann and G. Rigoll, \\u201cA hybrid SVM/HMM acoustic modeling\\napproach to automatic speech recognition,\\u201d in Proc. Interspeech, 2004.\\n[97] M. Hasegawa-Johnson, J. Baker, S. Borys, K. Chen, E. Coogan, S.\\nGreenberg, A. Juneja, K. Kirchhoff, K. Livescu, S. Mohan, J. Muller,\\nK. Sonmez, and T. Wang, \\u201cLandmark-based speech recognition: Re-\\nport of the 2004 johns hopkins summer workshop,\\u201d in Proc. IEEE Int.\\nConf. Acoust., Speech, Signal Process., 2005, pp. 213\\u2013216.\\n\\n[98] S. Zhang, A. Ragni, and M. Gales, \\u201cStructured log linear models for\\nnoise robust speech recognition,\\u201d IEEE Signal Process. Lett., vol. 17,\\n2010.\\n\\n[99] L. R. Bahl, P. F. Brown, P. V. de Souza, and R. L. Mercer, \\u201cMaximum\\nmutual information estimation of HMM parameters for speech recog-\\nnition,\\u201d in Proc. IEEE Int. Conf. Acoust., Speech, Signal Process., Dec.\\n1986, pp. 49\\u201352.\\n\\n[100] Y. Ephraim and L. Rabiner, \\u201cOn the relation between modeling ap-\\nproaches for speech recognition,\\u201d IEEE Trans. Inf. Theory, vol. 36, no.\\n2, pp. 372\\u2013380, Mar. 1990.\\n\\n[101] P. C. Woodland and D. Povey, \\u201cLarge scale discriminative training\\nof hidden Markov models for speech recognition,\\u201d Comput. Speech\\nLang., vol. 16, pp. 25\\u201347, 2002.\\n\\n[102] E. McDermott, T. Hazen, J. L. Roux, A. Nakamura, and S. Katagiri,\\n\\u201cDiscriminative training for large vocabulary speech recognition using\\nminimum classi\\ufb01cation error,\\u201d IEEE Audio, Speech, Lang. Process.,\\nvol. 15, no. 1, pp. 203\\u2013223, Jan. 2007.\\n\\n[103] D. Yu, L. Deng, X. He, and A. Acero, \\u201cUse of incrementally regu-\\nlated discriminative margins in mce training for speech recognition,\\u201d\\nin Proc. Int. Conf. Spoken Lang, Process., 2006, pp. 2418\\u20132421.\\n\\n[104] D. Yu, L. Deng, X. He, and A. Acero, \\u201cLarge-margin minimum clas-\\nsi\\ufb01cation error training: A theoretical risk minimization perspective,\\u201d\\nComput. Speech Lang., vol. 22, pp. 415\\u2013429, 2008.\\n\\n[105] C.-H. Lee and Q. Huo, \\u201cOn adaptive decision rules and decision param-\\neter adaptation for automatic speech recognition,\\u201d Proc. IEEE, vol. 88,\\nno. 8, pp. 1241\\u20131269, Aug. 2000.\\n\\n[106] S. Yaman, L. Deng, D. Yu, Y. Wang, and A. Acero, \\u201cAn integrative\\nand discriminative technique for spoken utterance classi\\ufb01cation,\\u201d IEEE\\nAudio, Speech, Lang. Process., vol. 16, no. 6, pp. 1207\\u20131215, Aug.\\n2008.\\n\\n[107] Y. Zhang, L. Deng, X. He, and A. Aceero, \\u201cA novel decision function\\nand the associated decision-feedback learning for speech translation,\\u201d\\nin Proc. IEEE Int. Conf. Acoust., Speech, Signal Process., 2011, pp.\\n5608\\u20135611.\\n\\n[108] B. Kingsbury, T. Sainath, and H. Soltau, \\u201cScalable minimum Bayes\\nrisk training of deep neural network acoustic models using distributed\\nhessian-free optimization,\\u201d in Proc. Interspeech, 2012.\\n\\n[109] F. Sha and L. Saul, \\u201cLarge margin hidden Markov models for automatic\\nspeech recognition,\\u201d in Adv. Neural Inf. Process. Syst., 2007, vol. 19,\\npp. 1249\\u20131256.\\n\\n[110] Y. Eldar, Z. Luo, K. Ma, D. Palomar, and N. Sidiropoulos, \\u201cConvex\\noptimization in signal processing,\\u201d IEEE Signal Process. Mag., vol.\\n27, no. 3, pp. 19\\u2013145, May 2010.\\n\\n[111] H. Jiang, X. Li, and C. Liu, \\u201cLarge margin hidden Markov models for\\nspeech recognition,\\u201d IEEE Audio, Speech, Lang. Process., vol. 14, no.\\n5, pp. 1584\\u20131595, Sep. 2006.\\n\\n[112] X. Li and H. Jiang, \\u201cSolving large-margin hidden Markov model es-\\ntimation via semide\\ufb01nite programming,\\u201d IEEE Trans. Audio, Speech,\\nLang. Process., vol. 15, no. 8, pp. 2383\\u20132392, Nov. 2007.\\n\\n[113] K. Crammer and Y. Singer, \\u201cOn the algorithmic implementation of\\nmulti-class kernel-based vector machines,\\u201d J. Mach. Learn. Res., vol.\\n2, pp. 265\\u2013292, 2001.\\n\\n[114] H. Jiang and X. Li, \\u201cParameter estimation of statistical models using\\nconvex optimization,\\u201d IEEE Signal Process. Mag., vol. 27, no. 3, pp.\\n115\\u2013127, May 2010.\\n\\n[115] F. Sha and L. Saul, \\u201cLarge margin Gaussian mixture modeling for pho-\\nnetic classi\\ufb01cation and recognition,\\u201d in Proc. IEEE Int. Conf. Acoust.,\\nSpeech, Signal Process., Toulouse, France, 2006, pp. 265\\u2013268.\\n\\n[116] X. Li and J. Bilmes, \\u201cA Bayesian divergence prior for classi\\ufb01er adap-\\n\\ntation,\\u201d in Proc. Int. Conf. Artif. Intell. Statist., 2007.\\n\\n[117] T.-H. Chang, Z.-Q. Luo, L. Deng, and C.-Y. Chi, \\u201cA convex opti-\\nmization method for joint mean and variance parameter estimation of\\nlarge-margin CDHMM,\\u201d in Proc. IEEE Int. Conf. Acoust., Speech,\\nSignal Process., 2008, pp. 4053\\u20134056.\\n\\n[118] L. Xiao and L. Deng, \\u201cA geometric perspective of large-margin training\\nof Gaussian models,\\u201d IEEE Signal Process. Mag., vol. 27, no. 6, pp.\\n118\\u2013123, Nov. 2010.\\n\\n[119] X. He and L. Deng, Discriminative Learning for Speech Recognition:\\nTheory and Practice. San Rafael, CA, USA: Morgan & Claypool,\\n2008.\\n\\n[120] G. Heigold, S. Wiesler, M. Nubbaum-Thom, P. Lehnen, R. Schluter,\\nand H. Ney, \\u201cDiscriminative HMMs. log-linear models, CRFs: What\\nis the difference?,\\u201d in Proc. IEEE Int. Conf. Acoust., Speech, Signal\\nProcess., 2010, pp. 5546\\u20135549.\\n\\n[121] C. Liu, Y. Hu, and H. Jiang, \\u201cA trust region based optimization for max-\\nimum mutual information estimation of HMMs in speech recognition,\\u201d\\nIEEE Audio, Speech, Lang. Process., vol. 19, no. 8, pp. 2474\\u20132485,\\nNov. 2011.\\n\\n[122] Q. Fu and L. Deng, \\u201cPhone-discriminating minimum classi\\ufb01cation\\nerror (p-mce) training for phonetic recognition,\\u201d in Proc. Interspeech,\\n2007.\\n\\n[123] M. Gibson and T. Hain, \\u201cError approximation and minimum phone\\nerror acoustic model estimation,\\u201d IEEE Audio, Speech, Lang. Process.,\\nvol. 18, no. 6, pp. 1269\\u20131279, Aug. 2010.\\n\\n[124] R. Schlueter, W. Macherey, B. Mueller, and H. Ney, \\u201cComparison of\\ndiscriminative training criteria and optimization methods for speech\\nrecognition,\\u201d Speech Commun., vol. 31, pp. 287\\u2013310, 2001.\\n\\n[125] R. Chengalvarayan and L. Deng, \\u201cHMM-based speech recogni-\\ntion using state-dependent, discriminatively derived transforms on\\nmel-warped DFT features,\\u201d IEEE Trans. Speech Audio Process., vol.\\n5, no. 3, pp. 243\\u2013256, May 1997.\\n\\n[126] A. Biem, S. Katagiri, E. McDermott, and B. H. Juang, \\u201cAn application\\nof discriminative feature extraction to \\ufb01lter-bank-based speech recog-\\nnition,\\u201d IEEE Trans. Speech Audio Process., vol. 9, no. 2, pp. 96\\u2013110,\\nFeb. 2001.\\n\\n[127] B. Mak, Y. Tam, and P. Li, \\u201cDiscriminative auditory-based features for\\nrobust speech recognition,\\u201d IEEE Trans. Speech Audio Process., vol.\\n12, no. 1, pp. 28\\u201336, Jan. 2004.\\n\\n[128] R. Chengalvarayan and L. Deng, \\u201cSpeech trajectory discrimination\\nusing the minimum classi\\ufb01cation error learning,\\u201d IEEE Trans. Speech\\nAudio Process., vol. 6, no. 6, pp. 505\\u2013515, Nov. 1998.\\n\\n[129] K. Sim and M. Gales, \\u201cDiscriminative semi-parametric trajectory\\nmodel for speech recognition,\\u201d Comput. Speech Lang., vol. 21, pp.\\n669\\u2013687, 2007.\\n\\n[130] S. King, J. Frankel, K. Livescu, E. McDermott, K. Richmond, and M.\\nWester, \\u201cSpeech production knowledge in automatic speech recogni-\\ntion,\\u201d J. Acoust. Soc. Amer., vol. 121, pp. 723\\u2013742, 2007.\\n\\n[131] T. Jaakkola and D. Haussler, \\u201cExploiting generative models in discrim-\\n\\ninative classi\\ufb01ers,\\u201d in Adv. Neural Inf. Process. Syst., 1998, vol. 11.\\n\\n[132] A. McCallum, C. Pal, G. Druck, and X. Wang, \\u201cMulti-conditional\\nlearning: Generative/discriminative training for clustering and classi-\\n\\ufb01cation,\\u201d in Proc. AAAI, 2006.\\n\\n[133] G. Hinton and R. Salakhutdinov, \\u201cReducing the dimensionality of data\\nwith neural networks,\\u201d Science, vol. 313, no. 5786, pp. 504\\u2013507, 2006.\\n\\n\\x0c28\\n\\nIEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 21, NO. 5, MAY 2013\\n\\n[134] G. Hinton, S. Osindero, and Y. Teh, \\u201cA fast learning algorithm for deep\\n\\nbelief nets,\\u201d Neural Comput., vol. 18, pp. 1527\\u20131554, 2006.\\n\\n[135] G. Heigold, H. Ney, P. Lehnen, T. Gass, and R. Schluter, \\u201cEquiva-\\nlence of generative and log-linear models,\\u201d IEEE Audio, Speech, Lang.\\nProcess., vol. 19, no. 5, pp. 1138\\u20131148, Jul. 2011.\\n\\n[136] R. J. A. Little and D. B. Rubin, Statistical Analysis With Missing\\n\\nData. New York, NY, USA: Wiley, 1987.\\n\\n[137] J. Bilmes, \\u201cA gentle tutorial of the EM algorithm and its application\\nto parameter estimation for Gaussian mixture and hidden Markov\\nmodels,\\u201d ICSI, Tech. Rep. TR-97-021, 1997.\\n\\n[138] L. Rabiner, \\u201cTutorial on hidden Markov models and selected applica-\\ntions in speech recognition,\\u201d Proc. IEEE, vol. 77, no. 2, pp. 257\\u2013286,\\nFeb. 1989.\\n\\n[139] J. Zhu, \\u201cSemi-supervised learning literature survey,\\u201d Computer Sci-\\n\\nences, Univ. of Wisconsin-Madison, Tech. Rep., 2006.\\n\\n[140] T. Joachims, \\u201cTransductive inference for text classi\\ufb01cation using sup-\\n\\nport vector machines,\\u201d in Proc. Int. Conf. Mach. Learn., 1999.\\n\\n[141] X. Zhu and Z. Ghahramani, \\u201cLearning from labeled and unlabeled\\ndata with label propagation,\\u201d Carnegie Mellon Univ., Philadelphia, PA,\\nUSA, Tech. Rep. CMU-CALD-02, 2002.\\n\\n[142] T. Joachims, \\u201cTransductive learning via spectral graph partitioning,\\u201d\\n\\nin Proc. Int. Conf. Mach. Learn., 2003.\\n\\n[143] D. Miller and H. Uyar, \\u201cA mixture of experts classi\\ufb01er with learning\\nbased on both labeled and unlabeled data,\\u201d in Proc. Adv. Neural Inf.\\nProcess. Syst., 1996.\\n\\n[144] K. Nigam, A. McCallum, S. Thrun, and T. Mitchell, \\u201cText classi\\ufb01cation\\nfrom labeled and unlabeled documents using EM,\\u201d Mach. Learn., vol.\\n39, pp. 103\\u2013134, 2000.\\n\\n[145] Y. Grandvalet and Y. Bengio, \\u201cSemi-supervised learning by entropy\\n\\nminimization,\\u201d in Proc. Adv. Neural Inf. Process. Syst., 2004.\\n\\n[146] F. Jiao, S. Wang, C. Lee, R. Greiner, and D. Schuurmans, \\u201cSemi-super-\\nvised conditional random \\ufb01elds for improved sequence segmentation\\nand labeling,\\u201d in Proc. Assoc. Comput. Linguist., 2006.\\n\\n[147] G. Mann and A. McCallum, \\u201cGeneralized expectation criteria for\\nsemi-supervised learning of conditional random \\ufb01elds,\\u201d in Proc.\\nAssoc. Comput. Linguist., 2008.\\n\\n[148] X. Li, \\u201cOn the use of virtual evidence in conditional random \\ufb01elds,\\u201d in\\n\\nProc. EMNLP, 2009.\\n\\n[149] J. Bilmes, \\u201cOn soft evidence in Bayesian networks,\\u201d Univ. of Wash-\\nington, Dept. of Elect. Eng., Tech. Rep. UWEETR-2004-0016, 2004.\\n[150] K. P. Bennett and A. Demiriz, \\u201cSemi-supervised support vector ma-\\nchines,\\u201d in Proc. Adv. Neural Inf. Process. Syst., 1998, pp. 368\\u2013374.\\n[151] O. Chapelle, M. Chi, and A. Zien, \\u201cA continuation method for semi-\\n\\nsupervised SVMs,\\u201d in Proc. Int. Conf. Mach. Learn., 2006.\\n\\n[152] R. Collobert, F. Sinz, J. Weston, and L. Bottou, \\u201cLarge scale transduc-\\n\\ntive SVMs,\\u201d J. Mach. Learn. Res., 2006.\\n\\n[153] D. Yarowsky, \\u201cUnsupervised word sense disambiguation rivaling\\nsupervised methods,\\u201d in Proc. Assoc. Comput. Linguist., 1995, pp.\\n189\\u2013196.\\n\\n[154] A. Blum and T. Mitchell, \\u201cCombining labeled and unlabeled data with\\n\\nco-training,\\u201d in Proc. Workshop Comput. Learn. Theory, 1998.\\n\\n[155] K. Nigam and R. Ghani, \\u201cAnalyzing the effectiveness and applicability\\n\\nof co-training,\\u201d in Proc. Int. Conf. Inf. Knowl. Manage., 2000.\\n\\n[156] A. Blum and S. Chawla, \\u201cLearning from labeled and unlabeled data\\n\\nusing graph mincut,\\u201d in Proc. Int. Conf. Mach. Learn., 2001.\\n\\n[157] M. Szummer and T. Jaakkola, \\u201cPartially labeled classi\\ufb01cation with\\nMarkov random walks,\\u201d in Proc. Adv. Neural Inf. Process. Syst., 2001,\\nvol. 14.\\n\\n[158] X. Zhu, Z. Ghahramani, and J. Lafferty, \\u201cSemi-supervised learning\\nusing Gaussian \\ufb01elds and harmonic functions,\\u201d in Proc. Int. Conf.\\nMach. Learn., 2003.\\n\\n[159] D. Zhou, O. Bousquet, J. Weston, T. N. Lal, and B. Schlkopf, \\u201cLearning\\nwith local and global consistency,\\u201d in Proc. Adv. Neural Inf. Process.\\nSyst., 2003.\\n\\n[160] V. Sindhwani, M. Belkin, P. Niyogi, and P. Bartlett, \\u201cManifold regu-\\nlarization: A geometric framework for learning from labeled and unla-\\nbeled examples,\\u201d J. Mach. Learn. Res., vol. 7, Nov. 2006.\\n\\n[161] A. Subramanya and J. Bilmes, \\u201cEntropic graph regularization in non-\\nparametric semi-supervised classi\\ufb01cation,\\u201d in Proc. Adv. Neural Inf.\\nProcess. Syst., Vancouver, BC, Canada, Dec. 2009.\\n\\n[162] T. Kemp and A. Waibel, \\u201cUnsupervised training of a speech recognizer:\\n\\nRecent experiments,\\u201d in Proc. Eurospeech, 1999.\\n\\n[163] D. Charlet, \\u201cCon\\ufb01dence-measure-driven unsupervised incremental\\nadaptation for HMM-based speech recognition,\\u201d in Proc. IEEE Int.\\nConf. Acoust., Speech, Signal Process., 2001, pp. 357\\u2013360.\\n\\n[164] F. Wessel and H. Ney, \\u201cUnsupervised training of acoustic models for\\nlarge vocabulary continuous speech recognition,\\u201d IEEE Audio, Speech,\\nLang. Process., vol. 13, no. 1, pp. 23\\u201331, Jan. 2005.\\n\\n[165] J.-T. Huang and M. Hasegawa-Johnson, \\u201cMaximum mutual infor-\\nmation estimation with unlabeled data for phonetic classi\\ufb01cation,\\u201d in\\nProc. Interspeech, 2008.\\n\\n[166] D. Yu, L. Deng, B. Varadarajan, and A. Acero, \\u201cActive learning and\\nsemi-supervised learning for speech recognition: A uni\\ufb01ed framework\\nusing the global entropy reduction maximization criterion,\\u201d Comput.\\nSpeech Lang., vol. 24, pp. 433\\u2013444, 2009.\\n\\n[167] L. Lamel, J.-L. Gauvain, and G. Adda, \\u201cLightly supervised and unsu-\\npervised acoustic model training,\\u201d Comput. Speech Lang., vol. 16, pp.\\n115\\u2013129, 2002.\\n\\n[168] B. Settles, \\u201cActive learning literature survey,\\u201d Univ. of Wisconsin,\\n\\nMadison, WI, USA, Tech. Rep. 1648, 2010.\\n\\n[169] D. Lewis and J. Catlett, \\u201cHeterogeneous uncertainty sampling for su-\\n\\npervised learning,\\u201d in Proc. Int. Conf. Mach. Learn., 1994.\\n\\n[170] T. Scheffer, C. Decomain, and S. Wrobel, \\u201cActive hidden Markov\\nmodels for information extraction,\\u201d in Proc. Int. Conf. Adv. Intell.\\nData Anal. (CAIDA), 2001.\\n\\n[171] B. Settles and M. Craven, \\u201cAn analysis of active learning strategies for\\n\\nsequence labeling tasks,\\u201d in Proc. EMNLP, 2008.\\n\\n[172] S. Tong and D. Koller, \\u201cSupport vector machine active learning with\\napplications to text classi\\ufb01cation,\\u201d in Proc. Int. Conf. Mach. Learn.,\\n2000, pp. 999\\u20131006.\\n\\n[173] H. S. Seung, M. Opper, and H. Sompolinsky, \\u201cQuery by committee,\\u201d\\n\\nin Proc. ACM Workshop Comput. Learn. Theory, 1992.\\n\\n[174] Y. Freund, H. S. Seung, E. Shamir, and N. Tishby, \\u201cSelective sampling\\nusing the query by committee algorithm,\\u201d Mach. Learn., pp. 133\\u2013168,\\n1997.\\n\\n[175] I. Dagan and S. P. Engelson, \\u201cCommittee-based sampling for training\\n\\nprobabilistic classi\\ufb01ers,\\u201d in Proc. Int. Conf. Mach. Learn., 1995.\\n\\n[176] H. Nguyen and A. Smeulders, \\u201cActive learning using pre-clustering,\\u201d\\n\\nin Proc. Int. Conf. Mach. Learn., 2004, pp. 623\\u2013630.\\n\\n[177] H. Lin and J. Bilmes, \\u201cHow to select a good training-data subset for\\ntranscription: Submodular active selection for sequences,\\u201d in Proc. In-\\nterspeech, 2009.\\n\\n[178] A. Guillory and J. Bilmes, \\u201cInteractive submodular set cover,\\u201d in Proc.\\n\\nInt. Conf. Mach. Learn., Haifa, Israel, 2010.\\n\\n[179] D. Golovin and A. Krause, \\u201cAdaptive submodularity: A new approach\\nto active learning and stochastic optimization,\\u201d in Proc. Int. Conf.\\nLearn. Theory, 2010.\\n\\n[180] G. Riccardi and D. Hakkani-Tur, \\u201cActive learning: Theory and appli-\\ncations to automatic speech recognition,\\u201d IEEE Trans. Speech Audio\\nProcess., vol. 13, no. 4, pp. 504\\u2013511, Jul. 2005.\\n\\n[181] D. Hakkani-Tur, G. Tur, M. Rahim, and G. Riccardi, \\u201cUnsupervised\\nand active learning in automatic speech recognition for call classi\\ufb01ca-\\ntion,\\u201d in Proc. IEEE Int. Conf. Acoust., Speech, Signal Process., 2004,\\npp. 429\\u2013430.\\n\\n[182] D. Hakkani-Tur and G. Tur, \\u201cActive learning for automatic speech\\nrecognition,\\u201d in Proc. IEEE Int. Conf. Acoust., Speech, Signal Process.,\\n2002, pp. 3904\\u20133907.\\n\\n[183] Y. Hamanaka, K. Shinoda, S. Furui, T. Emori, and T. Koshinaka,\\n\\u201cSpeech modeling based on committee-based active learning,\\u201d in\\nProc. IEEE Int. Conf. Acoust., Speech, Signal Process., 2010, pp.\\n4350\\u20134353.\\n\\n[184] H.-K. J. Kuo and V. Goel, \\u201cActive learning with minimum expected\\nerror for spoken language understanding,\\u201d in Proc. Interspeech, 2005.\\n[185] J. Blitzer, K. Crammer, A. Kulesza, F. Pereira, and J. Wortman,\\n\\u201cLearning bounds for domain adaptation,\\u201d in Proc. Adv. Neural Inf.\\nProcess. Syst., 2008.\\n\\n[186] S. R\\xfcping, \\u201cIncremental learning with support vector machines,\\u201d in\\n\\nProc. IEEE. Int. Conf. Data Mining, 2001.\\n\\n[187] P. Wu and T. G. Dietterich, \\u201cImproving svm accuracy by training on\\n\\nauxiliary data sources,\\u201d in Proc. Int. Conf. Mach. Learn., 2004.\\n\\n[188] J.-L. Gauvain and C.-H. Lee, \\u201cBayesian learning of Gaussian mixture\\ndensities for hidden Markov models,\\u201d in Proc. DARPA Speech and Nat-\\nural Language Workshop, 1991, pp. 272\\u2013277.\\n\\n[189] J.-L. Gauvain and C.-H. Lee, \\u201cMaximum a posteriori estimation for\\nmultivariate Gaussian mixture observations of Markov chains,\\u201d IEEE\\nTrans. Speech Audio Process., vol. 2, no. 2, pp. 291\\u2013298, Apr. 1994.\\n[190] M. Bacchiani and B. Roark, \\u201cUnsupervised language model adapta-\\ntion,\\u201d in Proc. IEEE Int. Conf. Acoust., Speech, Signal Process., 2003,\\npp. 224\\u2013227.\\n\\n[191] C. Chelba and A. Acero, \\u201cAdaptation of maximum entropy capitalizer:\\n\\nLittle data can help a lot,\\u201d in Proc. EMNLP, July 2004.\\n\\n[192] C. Leggetter and P. Woodland, \\u201cMaximum likelihood linear regression\\nfor speaker adaptation of continuous density hidden Markov models,\\u201d\\nComput. Speech Lang., vol. 9, 1995.\\n\\n[193] M. Gales and P. Woodland, \\u201cMean and variance adaptation within the\\n\\nmllr framework,\\u201d Comput. Speech Lang., vol. 10, 1996.\\n\\n[194] J. Neto, L. Almeida, M. Hochberg, C. Martins, L. Nunes, S. Renals, and\\nT. Robinson, \\u201cSpeaker-adaptation for hybrid HMM-ANN continuous\\nspeech recognition system,\\u201d in Proc. Eurospeech, 1995.\\n\\n[195] V. Abrash, H. Franco, A. Sankar, and M. Cohen, \\u201cConnectionist\\n\\nspeaker normalization and adaptation,\\u201d in Proc. Eurospeech, 1995.\\n\\n\\x0cDENG AND LI: MACHINE LEARNING PARADIGMS FOR SPEECH RECOGNITION: AN OVERVIEW\\n\\n29\\n\\n[196] R. Caruana, \\u201cMultitask learning,\\u201d Mach. Learn., vol. 28, pp. 41\\u201375,\\n\\n[225] T. Heskes, \\u201cEmpirical Bayes for learning to learn,\\u201d in Proc. Int. Conf.\\n\\nMach. Learn., 2000.\\n\\n[226] K. Yu, A. Schwaighofer, and V. Tresp, \\u201cLearning Gaussian processes\\n\\nfrom multiple tasks,\\u201d in Proc. Int. Conf. Mach. Learn., 2005.\\n\\n[227] Y. Xue, X. Liao, and L. Carin, \\u201cMulti-task learning for classi\\ufb01cation\\nwith Dirichlet process priors,\\u201d J. Mach. Learn. Res., vol. 8, pp. 35\\u201363,\\n2007.\\n\\n[228] H. Daume, \\u201cBayesian multitask learning with latent hierarchies,\\u201d in\\n\\nProc. Uncertainty in Artif. Intell., 2009.\\n\\n[229] T. Evgeniou, C. A. Micchelli, and M. Pontil, \\u201cLearning multiple tasks\\nwith kernel methods,\\u201d J. Mach. Learn. Res., vol. 6, pp. 615\\u2013637, 2005.\\n[230] A. Argyriou, C. A. Micchelli, M. Pontil, and Y. Ying, \\u201cSpectral regu-\\nlarization framework for multi-task structure learning,\\u201d in Proc. Adv.\\nNeural Inf. Process. Syst., 2007.\\n\\n[231] J. Ngiam, A. Khosla, M. Kim, J. Nam, H. Lee, and A. Ng, \\u201cMultimodal\\n\\ndeep learning,\\u201d in Proc. Int. Conf. Mach. Learn., 2011.\\n\\n[232] L. Deng, M. Seltzer, D. Yu, A. Acero, A. Mohamed, and G. Hinton,\\n\\u201cBinary coding of speech spectrograms using a deep auto-encoder,\\u201d in\\nProc. Interspeech, 2010.\\n\\n[233] H. Lin, L. Deng, D. Yu, Y. Gong, and A. Acero, \\u201cA study on multilin-\\ngual acoustic modeling for large vocabulary ASR,\\u201d in Proc. IEEE Int.\\nConf. Acoust., Speech, Signal Process., 2009, pp. 4333\\u20134336.\\n\\n[234] D. Yu, L. Deng, P. Liu, J. Wu, Y. Gong, and A. Acero, \\u201cCross-lingual\\nspeech recognition under run-time resource constraints,\\u201d in Proc. IEEE\\nInt. Conf. Acoust., Speech, Signal Process., 2009, pp. 4193\\u20134196.\\n\\n[235] C.-H. Lee, \\u201cFrom knowledge-ignorant to knowledge-rich modeling: A\\nnew speech research paradigm for next-generation automatic speech\\nrecognition,\\u201d in Proc. Int. Conf. Spoken Lang, Process., 2004, pp.\\n109\\u2013111.\\n\\n[236] I. Bromberg, Q. Qian, J. Hou, J. Li, C. Ma, B. Matthews, A. Moreno-\\nDaniel, J. Morris, M. Siniscalchi, Y. Tsao, and Y. Wang, \\u201cDetection-\\nbased ASR in the automatic speech attribute transcription project,\\u201d in\\nProc. Interspeech, 2007, pp. 1829\\u20131832.\\n\\n[237] L. Deng and D. Sun, \\u201cA statistical approach to automatic speech recog-\\nnition using the atomic speech units constructed from overlapping ar-\\nticulatory features,\\u201d J. Acoust. Soc. Amer., vol. 85, pp. 2702\\u20132719,\\n1994.\\n\\n[238] J. Sun and L. Deng, \\u201cAn overlapping-feature based phonological model\\nincorporating linguistic constraints: Applications to speech recogni-\\ntion,\\u201d J. Acoust. Soc. Amer., vol. 111, pp. 1086\\u20131101, 2002.\\n\\n[239] G. Hinton, L. Deng, D. Yu, G. Dahl, A. Mohamed, N. Jaitly, A. Senior,\\nV. Vanhoucke, P. Nguyen, T. Sainath, and B. Kingsbury, \\u201cDeep neural\\nnetworks for acoustic modeling in speech recognition,\\u201d IEEE Signal\\nProcess. Mag., vol. 29, no. 6, pp. 82\\u201397, Nov. 2012.\\n\\n[240] D. C. Ciresan, U. Meier, L. M. Gambardella, and J. Schmidhuber,\\n\\u201cDeep, big, simple neural nets for handwritten digit recognition,\\u201d\\nNeural Comput., vol. 22, pp. 3207\\u20133220, 2010.\\n\\n[241] A. Mohamed, G. Dahl, and G. Hinton, \\u201cAcoustic modeling using deep\\nbelief networks,\\u201d IEEE Audio, Speech, Lang. Process., vol. 20, no. 1,\\npp. 14\\u201322, Jan. 2012.\\n\\n[242] B. Hutchinson, L. Deng, and D. Yu, \\u201cA deep architecture with bilinear\\nmodeling of hidden representations: Applications to phonetic recogni-\\ntion,\\u201d in Proc. IEEE Int. Conf. Acoust., Speech, Signal Process., 2012,\\npp. 4805\\u20134808.\\n\\n[243] B. Hutchinson, L. Deng, and D. Yu, \\u201cTensor deep stacking networks,\\u201d\\n\\nIEEE Trans. Pattern Anal. Mach. Intell., 2013, to be published.\\n\\n[244] G. Andrew and J. Bilmes, \\u201cSequential deep belief networks,\\u201d in Proc.\\nIEEE Int. Conf. Acoust., Speech, Signal Process., 2012, pp. 4265\\u20134268.\\n[245] D. Yu, S. Siniscalchi, L. Deng, and C. Lee, \\u201cBoosting attribute and\\nphone estimation accuracies with deep neural networks for detection-\\nbased speech recognition,\\u201d in Proc. IEEE Int. Conf. Acoust., Speech,\\nSignal Process., 2012, pp. 4169\\u20134172.\\n\\n[246] G. Dahl, D. Yu, L. Deng, and A. Acero, \\u201cLarge vocabulary contin-\\nuous speech recognition with context-dependent DBN-HMMs,\\u201d in\\nProc. IEEE Int. Conf. Acoust., Speech, Signal Process., 2011, pp.\\n4688\\u20134691.\\n\\n[247] T. N. Sainath, B. Kingsbury, and B. Ramabhadran, \\u201cAuto-encoder bot-\\ntleneck features using deep belief networks,\\u201d in Proc. IEEE Int. Conf.\\nAcoust., Speech, Signal Process., 2012, pp. 4153\\u20134156.\\n\\n[248] L. Deng, D. Yu, and J. Platt, \\u201cScalable stacking and learning for\\nbuilding deep architectures,\\u201d in Proc. IEEE Int. Conf. Acoust., Speech,\\nSignal Process., 2012, pp. 2133\\u20132136.\\n\\n[249] O. Abdel-Hamid, A. Mohamed, H. Jiang, and G. Penn, \\u201cApplying con-\\nvolutional neural networks concepts to hybrid NN-HMM model for\\nspeech recognition,\\u201d in Proc. IEEE Int. Conf. Acoust., Speech, Signal\\nProcess., 2012, pp. 4277\\u20134280.\\n\\n[250] D. Yu, L. Deng, and G. Dahl, \\u201cRoles of pre-training and \\ufb01ne-tuning\\nin context-dependent DBN-HMMs for real-world speech recognition,\\u201d\\nin Proc. NIPS Workshop Deep Learn. Unsupervised Feature Learn.,\\n2010.\\n\\n1997.\\n\\n[197] J. Baxter, \\u201cLearning internal representations,\\u201d in Proc. Workshop\\n\\nComput. Learn. Theory, 1995.\\n\\n[198] H. Daum\\xe9 and D. Marcu, \\u201cDomain adaptation for statistical classi-\\n\\n\\ufb01ers,\\u201d J. Artif. Intell. Res., vol. 26, pp. 1\\u201315, 2006.\\n\\n[199] Y. Mansour, M. Mohri, and A. Rostamizadeh, \\u201cMultiple source adap-\\ntation and the Renyi divergence,\\u201d in Proc. Uncertainty Artif. Intell.,\\n2009.\\n\\n[200] Y. Mansour, M. Mohri, and A. Rostamizadeh, \\u201cDomain adaptation:\\nLearning bounds and algorithms,\\u201d in Proc. Workshop Comput. Learn.\\nTheory, 2009.\\n\\n[201] L. Deng, Front-End, Back-End, Hybrid Techniques to Noise-Robust\\nSpeech Recognition. Chapter 4 in Book: Robust Speech Recognition\\nof Uncertain Data. Berlin, Germany: Springer-Verlag, 2011.\\n\\n[202] G. Zavaliagkos, R. Schwarz, J. McDonogh, and J. Makhoul, \\u201cAdap-\\nlarge scale HMM recognizers,\\u201d in Proc.\\n\\ntation algorithms for\\nEurospeech, 1995.\\n\\n[203] C. Chesta, O. Siohan, and C. Lee, \\u201cMaximum a posteriori linear re-\\ngression for hidden Markov model adaptation,\\u201d in Proc. Eurospeech,\\n1999.\\n\\n[204] T. Myrvoll, O. Siohan, C.-H. Lee, and W. Chou, \\u201cStructural maximum\\na posteriori linear regression for unsupervised speaker adaptation,\\u201d in\\nProc. Int. Conf. Spoken Lang, Process., 2000.\\n\\n[205] T. Anastasakos, J. McDonough, R. Schwartz, and J. Makhoul, \\u201cA com-\\npact model for speaker-adaptive training,\\u201d in Proc. Int. Conf. Spoken\\nLang, Process., 1996, pp. 1137\\u20131140.\\n\\n[206] L. Deng, A. Acero, M. Plumpe, and X. D. Huang, \\u201cLarge vocabulary\\nspeech recognition under adverse acoustic environment,\\u201d in Proc. Int.\\nConf. Spoken Lang, Process., 2000, pp. 806\\u2013809.\\n\\n[207] O. Kalinli, M. L. Seltzer, J. Droppo, and A. Acero, \\u201cNoise adaptive\\ntraining for robust automatic speech recognition,\\u201d IEEE Audio, Speech,\\nLang. Process., vol. 18, no. 8, pp. 1889\\u20131901, Nov. 2010.\\n\\n[208] L. Deng, K. Wang, A. Acero, H. Hon, J. Droppo, Y. Wang, C. Boulis,\\nD. Jacoby, M. Mahajan, C. Chelba, and X. Huang, \\u201cDistributed speech\\nprocessing in mipad\\u2019s multimodal user interface,\\u201d IEEE Audio, Speech,\\nLang. Process., vol. 20, no. 9, pp. 2409\\u20132419, Nov. 2012.\\n\\n[209] L. Deng, J. Droppo, and A. Acero, \\u201cRecursive estimation of nonsta-\\ntionary noise using iterative stochastic approximation for robust speech\\nrecognition,\\u201d IEEE Trans. Speech Audio Process., vol. 11, no. 6, pp.\\n568\\u2013580, Nov. 2003.\\n\\n[210] J. Li, L. Deng, D. Yu, Y. Gong, and A. Acero, \\u201cHigh-performance\\nHMM adaptation with joint compensation of additive and convolutive\\ndistortions via vector Taylor series,\\u201d in Proc. IEEE Workshop Autom.\\nSpeech Recogn. Understand., Dec. 2007, pp. 65\\u201370.\\n\\n[211] J. Y. Li, L. Deng, Y. Gong, and A. Acero, \\u201cA uni\\ufb01ed framework of\\nHMM adaptation with joint compensation of additive and convolutive\\ndistortions,\\u201d Comput. Speech Lang., vol. 23, pp. 389\\u2013405, 2009.\\n\\n[212] M. Padmanabhan, L. R. Bahl, D. Nahamoo, and M. Picheny, \\u201cSpeaker\\nclustering and transformation for speaker adaptation in speech recog-\\nnition systems,\\u201d IEEE Trans. Speech Audio Process., vol. 6, no. 1, pp.\\n71\\u201377, Jan. 1998.\\n\\n[213] M. Gales, \\u201cCluster adaptive training of hidden Markov models,\\u201d IEEE\\n\\nTrans. Speech Audio Process., vol. 8, no. 4, pp. 417\\u2013428, Jul. 2000.\\n\\n[214] R. Kuhn, J.-C. Junqua, P. Nguyen, and N. Niedzielski, \\u201cRapid speaker\\nadaptation in eigenvoice space,\\u201d IEEE Trans. Speech Audio Process.,\\nvol. 8, no. 4, pp. 417\\u2013428, Jul. 2000.\\n\\n[215] A. Gliozzo and C. Strapparava, \\u201cExploiting comparable corpora and\\nbilingual dictionaries for cross-language text categorization,\\u201d in Proc.\\nAssoc. Comput. Linguist., 2006.\\n\\n[216] J. Ham, D. Lee, and L. Saul, \\u201cSemisupervised alignment of manifolds,\\u201d\\n\\nin Proc. Int. Workshop Artif. Intell. Statist., 2005.\\n\\n[217] C. Wang and S. Mahadevan, \\u201cManifold alignment without correspon-\\n\\ndence,\\u201d in Proc. 21st Int. Joint Conf. Artif. Intell., 2009.\\n\\n[218] W. Dai, Y. Chen, G. Xue, Q. Yang, and Y. Yu, \\u201cTranslated learning:\\nTransfer learning across different feature spaces,\\u201d in Proc. Adv. Neural\\nInf. Process. Syst., 2008.\\n\\n[219] H. Daume, \\u201cCross-task knowledge-constrained self training,\\u201d in Proc.\\n\\n[222] S. Ben-David and R. Schuller, \\u201cExploiting task relatedness for multiple\\n\\ntask learning,\\u201d in Proc. Comput. Learn. Theory, 2003.\\n\\n[223] R. Ando and T. Zhang, \\u201cA framework for learning predictive structures\\nfrom multiple tasks and unlabeled data,\\u201d J. Mach. Learn. Res., vol. 6,\\npp. 1817\\u20131853, 2005.\\n\\n[224] J. Baxter, \\u201cA Bayesian/information theoretic model of learning to learn\\n\\nvia multiple task sampling,\\u201d Mach. Learn., pp. 7\\u201339, 1997.\\n\\n[220] J. Baxter, \\u201cA model of inductive bias learning,\\u201d J. Artif. Intell. Res.,\\n\\n[221] S. Thrun and L. Y. Pratt, Learning To Learn. Boston, MA, USA:\\n\\nEMNLP, 2008.\\n\\nvol. 12, pp. 149\\u2013198, 2000.\\n\\nKluwer, 1998.\\n\\n\\x0c30\\n\\nIEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 21, NO. 5, MAY 2013\\n\\n[251] A. Mohamed, T. Sainath, G. Dahl, B. Ramabhadran, G. Hinton, and\\nM. Picheny, \\u201cDeep belief networks using discriminative features for\\nphone recognition,\\u201d in Proc. IEEE Int. Conf. Acoust., Speech, Signal\\nProcess., May 2011, pp. 5060\\u20135063.\\n\\n[252] D. Yu, L. Deng, and F. Seide, \\u201cLarge vocabulary speech recognition\\n\\nusing deep tensor neural networks,\\u201d in Proc. Interspeech, 2012.\\n\\n[253] Z. Tuske, M. Sundermeyer, R. Schluter, and H. Ney, \\u201cContext-depen-\\ndent MLPs for LVCSR: Tandem, hybrid or both,\\u201d in Proc. Interspeech,\\n2012.\\n\\n[254] G. Saon and B. Kingbury, \\u201cDiscriminative feature-space transforms\\n\\nusing deep neural networks,\\u201d in Proc. Interspeech, 2012.\\n\\n[255] R. Gens and P. Domingos, \\u201cDiscriminative learning of sum-product\\n\\nnetworks,\\u201d in Proc. Adv. Neural Inf. Process. Syst., 2012.\\n\\n[256] O. Vinyals, Y. Jia, L. Deng, and T. Darrell, \\u201cLearning with recursive\\nperceptual representations,\\u201d in Proc. Adv. Neural Inf. Process. Syst.,\\n2012.\\n\\n[257] Y. Bengio, \\u201cLearning deep architectures for AI,\\u201d Foundations and\\n\\nTrends in Mach. Learn., vol. 2, no. 1, pp. 1\\u2013127, 2009.\\n\\n[258] N. Morgan, \\u201cDeep and wide: Multiple layers in automatic speech\\nrecognition,\\u201d IEEE Audio, Speech, Lang. Process., vol. 20, no. 1, pp.\\n7\\u201313, Jan. 2012.\\n\\n[259] D. Yu, L. Deng, and F. Seide, \\u201cThe deep tensor neural network with\\napplications to large vocabulary speech recognition,\\u201d IEEE Audio,\\nSpeech, Lang. Process., vol. 21, no. 2, pp. 388\\u2013396, Feb. 2013.\\n\\n[260] M. Siniscalchi, L. Deng, D. Yu, and C.-H. Lee, \\u201cExploiting deep neural\\nnetworks for detection-based speech recognition,\\u201d Neurocomputing,\\n2013.\\n\\n[261] A. Mohamed, D. Yu, and L. Deng, \\u201cInvestigation of full-sequence\\ntraining of deep belief networks for speech recognition,\\u201d in Proc.\\nInterspeech, 2010.\\n\\n[262] T. Sainath, B. Ramabhadran, D. Nahamoo, D. Kanevsky, and A.\\nSethy, \\u201cExemplar-based sparse representation features for speech\\nrecognition,\\u201d in Proc. Interspeech, 2010.\\n\\n[263] T. Sainath, B. Ramabhadran, M. Picheny, D. Nahamoo, and D.\\nKanevsky, \\u201cExemplar-based sparse representation features: From\\nTIMIT to LVCSR,\\u201d IEEE Audio, Speech, Lang. Process., vol. 19, no.\\n8, pp. 2598\\u20132613, Nov. 2011.\\n\\n[264] M. De Wachter, M. Matton, K. Demuynck, P. Wambacq, R. Cools,\\nand D. Van Compernolle, \\u201cTemplate-based continuous speech recog-\\nnition,\\u201d IEEE Audio, Speech, Lang. Process., vol. 15, no. 4, pp.\\n1377\\u20131390, May 2007.\\n\\n[265] J. Gemmeke, U. Remes, and K. J. Palomki, \\u201cObservation uncertainty\\n\\nmeasures for sparse imputation,\\u201d in Proc. Interspeech, 2010.\\n\\n[266] J. Gemmeke, T. Virtanen, and A. Hurmalainen, \\u201cExemplar-based\\nsparse representations for noise robust automatic speech recognition,\\u201d\\nIEEE Audio, Speech, Lang. Process., vol. 19, no. 7, pp. 2067\\u20132080,\\nSep. 2011.\\n\\n[267] G. Sivaram, S. Ganapathy, and H. Hermansky, \\u201cSparse auto-associa-\\ntive neural networks: Theory and application to speech recognition,\\u201d\\nin Proc. Interspeech, 2010.\\n\\n[268] G. Sivaram and H. Hermansky, \\u201cSparse multilayer perceptron for\\nphoneme recognition,\\u201d IEEE Audio, Speech, Lang. Process., vol. 20,\\nno. 1, pp. 23\\u201329, Jan. 2012.\\n\\n[269] M. Tipping, \\u201cSparse Bayesian learning and the relevance vector ma-\\n\\nchine,\\u201d J. Mach. Learn. Res., pp. 211\\u2013244, 2001.\\n\\n[270] G. Saon and J. Chien, \\u201cBayesian sensing hidden Markov models,\\u201d\\nIEEE Audio, Speech, Lang. Process., vol. 20, no. 1, pp. 43\\u201354, Jan.\\n2012.\\n\\n[271] D. Yu, F. Seide, G. Li, and L. Deng, \\u201cExploiting sparseness in\\ndeep neural networks for large vocabulary speech recognition,\\u201d in\\nProc. IEEE Int. Conf. Acoust., Speech, Signal Process., 2012, pp.\\n4409\\u20134412.\\n\\n[272] J. Dean, G. Corrado, R. Monga, K. Chen, M. Devin, Q. Le, M. Mao,\\nM. Ranzato, A. Senior, P. Tucker, K. Yang, and A. Ng, \\u201cLarge scale\\ndistributed deep networks,\\u201d in Proc. Adv. Neural Inf. Process. Syst.,\\n2012.\\n\\n[273] L. Deng, G. Hinton, and B. Kingsbury, \\u201cNew types of deep neural\\nnetwork learning for speech recognition and related applications: An\\noverview,\\u201d in Proc. Int. Conf. Acoust., Speech, Signal Process., 2013,\\nto be published.\\n\\nLi Deng (F\\u201905) received the Ph.D. degree from the\\nUniversity of Wisconsin-Madison. He joined Dept.\\nElectrical and Computer Engineering, University of\\nWaterloo, Ontario, Canada in 1989 as an assistant\\nprofessor, where he became a tenured full professor\\nin 1996. In 1999, he joined Microsoft Research,\\nRedmond, WA as a Senior Researcher, where he\\nis currently a Principal Researcher. Since 2000,\\nhe has also been an Af\\ufb01liate Full Professor and\\ngraduate committee member in the Department of\\nElectrical Engineering at University of Washington,\\nSeattle. Prior to MSR, he also worked or taught at Massachusetts Institute of\\nTechnology, ATR Interpreting Telecom. Research Lab. (Kyoto, Japan), and\\nHKUST. In the general areas of speech/language technology, machine learning,\\nand signal processing, he has published over 300 refereed papers in leading\\njournals and conferences and 3 books, and has given keynotes, tutorials, and\\ndistinguished lectures worldwide. He is a Fellow of the Acoustical Society\\nof America, a Fellow of the IEEE, and a Fellow of ISCA. He served on the\\nBoard of Governors of the IEEE Signal Processing Society (2008\\u20132010).\\nMore recently, he served as Editor-in-Chief for the IEEE Signal Processing\\nMagazine (2009\\u20132011), which earned the highest impact factor among all IEEE\\npublications and for which he received the 2011 IEEE SPS Meritorious Service\\nAward. He currently serves as Editor-in-Chief for the IEEE TRANSACTIONS\\nON AUDIO, SPEECH AND LANGUAGE PROCESSING. His recent technical work\\n(since 2009) and leadership on industry-scale deep learning with colleagues\\nand collaborators have created signi\\ufb01cant impact on speech recognition, signal\\nprocessing, and related applications.\\n\\nXiao Li (M\\u201907) received the B.S.E.E degree from\\nTsinghua University, Beijing, China, in 2001 and the\\nPh.D. degree from the University of Washington,\\nSeattle,\\nin 2007. In 2007, she joined Microsoft\\nResearch, Redmond as a researcher. Her research\\ninterests include speech and language understanding,\\ninformation retrieval, and machine learning. She\\nhas published over 30 referred papers in these areas,\\nand is a reviewer of a number of IEEE, ACM, and\\nACL journals and conferences. At MSR she worked\\non search engines by detecting and understanding a\\nuser\\u2019s intent with a search query, for which she was honored with MIT Tech-\\nnology Reviews TR35 Award in 2011. After working at Microsoft Research\\nfor over four years, she recently embarked on a new adventure at Facebook\\nInc. as a research scientist.\\n\\n\\x0c', u'Generative Adversarial Text to Image Synthesis\\n\\nScott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Logeswaran\\nBernt Schiele, Honglak Lee\\n1 University of Michigan, Ann Arbor, MI, USA (UMICH.EDU)\\n2 Max Planck Institute for Informatics, Saarbr\\xa8ucken, Germany (MPI-INF.MPG.DE)\\n\\nREEDSCOT1, AKATA2, XCYAN1, LLAJAN1\\nSCHIELE2,HONGLAK1\\n\\nAbstract\\n\\nAutomatic synthesis of realistic images from text\\nwould be interesting and useful, but current AI\\nsystems are still far from this goal. However, in\\nrecent years generic and powerful recurrent neu-\\nral network architectures have been developed\\nto learn discriminative text feature representa-\\ntions. Meanwhile, deep convolutional generative\\nadversarial networks (GANs) have begun to gen-\\nerate highly compelling images of speci\\ufb01c cat-\\negories, such as faces, album covers, and room\\ninteriors. In this work, we develop a novel deep\\narchitecture and GAN formulation to effectively\\nbridge these advances in text and image model-\\ning, translating visual concepts from characters\\nto pixels. We demonstrate the capability of our\\nmodel to generate plausible images of birds and\\n\\ufb02owers from detailed text descriptions.\\n\\n1. Introduction\\nIn this work we are interested in translating text in the form\\nof single-sentence human-written descriptions directly into\\nimage pixels. For example, \\u201cthis small bird has a short,\\npointy orange beak and white belly\\u201d or \\u201dthe petals of this\\n\\ufb02ower are pink and the anther are yellow\\u201d. The problem of\\ngenerating images from visual descriptions gained interest\\nin the research community, but it is far from being solved.\\nTraditionally this type of detailed visual information about\\nan object has been captured in attribute representations -\\ndistinguishing characteristics the object category encoded\\ninto a vector (Farhadi et al., 2009; Kumar et al., 2009;\\nParikh & Grauman, 2011; Lampert et al., 2014), in partic-\\nular to enable zero-shot visual recognition (Fu et al., 2014;\\nAkata et al., 2015), and recently for conditional image gen-\\neration (Yan et al., 2015).\\nWhile the discriminative power and strong generalization\\n\\nProceedings of the 33 rd International Conference on Machine\\nLearning, New York, NY, USA, 2016. JMLR: W&CP volume\\n48. Copyright 2016 by the author(s).\\n\\nFigure 1. Examples of generated images from text descriptions.\\nLeft: captions are from zero-shot (held out) categories, unseen\\ntext. Right: captions are from the training set.\\n\\nproperties of attribute representations are attractive, at-\\ntributes are also cumbersome to obtain as they may require\\ndomain-speci\\ufb01c knowledge.\\nIn comparison, natural lan-\\nguage offers a general and \\ufb02exible interface for describing\\nobjects in any space of visual categories. Ideally, we could\\nhave the generality of text descriptions with the discrimi-\\nnative power of attributes.\\nRecently, deep convolutional and recurrent networks for\\ntext have yielded highly discriminative and generaliz-\\nable (in the zero-shot learning sense) text representations\\nlearned automatically from words and characters (Reed\\net al., 2016). These approaches exceed the previous state-\\nof-the-art using attributes for zero-shot visual recognition\\non the Caltech-UCSD birds database (Wah et al., 2011),\\nand also are capable of zero-shot caption-based retrieval.\\nMotivated by these works, we aim to learn a mapping di-\\nrectly from words and characters to image pixels.\\nTo solve this challenging problem requires solving two sub-\\nproblems: \\ufb01rst, learn a text feature representation that cap-\\ntures the important visual details; and second, use these fea-\\n\\nthis small bird has a pink breast and crown, and black primaries and secondaries.the flower has petals that are bright pinkish purple with white stigmathis magnificent fellow is almost all black with a red crest, and white cheek patch.this white and yellow flower have thin white petals and a round yellow stamen\\x0cGenerative Adversarial Text to Image Synthesis\\n\\ntures to synthesize a compelling image that a human might\\nmistake for real. Fortunately, deep learning has enabled\\nenormous progress in both subproblems - natural language\\nrepresentation and image synthesis - in the previous several\\nyears, and we build on this for our current task.\\nHowever, one dif\\ufb01cult remaining issue not solved by deep\\nlearning alone is that the distribution of images conditioned\\non a text description is highly multimodal, in the sense that\\nthere are very many plausible con\\ufb01gurations of pixels that\\ncorrectly illustrate the description. The reverse direction\\n(image to text) also suffers from this problem but learning\\nis made practical by the fact that the word or character se-\\nquence can be decomposed sequentially according to the\\nchain rule; i.e. one trains the model to predict the next\\ntoken conditioned on the image and all previous tokens,\\nwhich is a more well-de\\ufb01ned prediction problem.\\nThis conditional multi-modality is thus a very natural ap-\\nplication for generative adversarial networks (Goodfellow\\net al., 2014), in which the generator network is optimized to\\nfool the adversarially-trained discriminator into predicting\\nthat synthetic images are real. By conditioning both gen-\\nerator and discriminator on side information (also studied\\nby Mirza & Osindero (2014) and Denton et al. (2015)), we\\ncan naturally model this phenomenon since the discrimina-\\ntor network acts as a \\u201csmart\\u201d adaptive loss function.\\nOur main contribution in this work is to develop a sim-\\nple and effective GAN architecture and training strat-\\negy that enables compelling text to image synthesis of\\nbird and \\ufb02ower images from human-written descriptions.\\nWe mainly use the Caltech-UCSD Birds dataset and the\\nOxford-102 Flowers dataset along with \\ufb01ve text descrip-\\ntions per image we collected as our evaluation setting. Our\\nmodel is trained on a subset of training categories, and we\\ndemonstrate its performance both on the training set cate-\\ngories and on the testing set, i.e. \\u201czero-shot\\u201d text to image\\nsynthesis. In addition to birds and \\ufb02owers, we apply our\\nmodel to more general images and text descriptions in the\\nMS COCO dataset (Lin et al., 2014).\\n2. Related work\\nKey challenges in multimodal learning include learning\\na shared representation across modalities, and to predict\\nmissing data (e.g. by retrieval or synthesis) in one modal-\\nity conditioned on another. Ngiam et al. (2011) trained a\\nstacked multimodal autoencoder on audio and video sig-\\nnals and were able to learn a shared modality-invariant rep-\\nresentation. Srivastava & Salakhutdinov (2012) developed\\na deep Boltzmann machine and jointly modeled images and\\ntext tags. Sohn et al. (2014) proposed a multimodal con-\\nditional prediction framework (hallucinating one modality\\ngiven the other) and provided theoretical justi\\ufb01cation.\\nMany researchers have recently exploited the capability of\\n\\ndeep convolutional decoder networks to generate realistic\\nimages. Dosovitskiy et al. (2015) trained a deconvolutional\\nnetwork (several layers of convolution and upsampling) to\\ngenerate 3D chair renderings conditioned on a set of graph-\\nics codes indicating shape, position and lighting. Yang et al.\\n(2015) added an encoder network as well as actions to this\\napproach. They trained a recurrent convolutional encoder-\\ndecoder that rotated 3D chair models and human faces con-\\nditioned on action sequences of rotations. Reed et al. (2015)\\nencode transformations from analogy pairs, and use a con-\\nvolutional decoder to predict visual analogies on shapes,\\nvideo game characters and 3D cars.\\nGenerative adversarial networks (Goodfellow et al., 2014)\\nhave also bene\\ufb01ted from convolutional decoder networks,\\nfor the generator network module. Denton et al. (2015)\\nused a Laplacian pyramid of adversarial generator and dis-\\ncriminators to synthesize images at multiple resolutions.\\nThis work generated compelling high-resolution images\\nand could also condition on class labels for controllable\\ngeneration. Radford et al. (2016) used a standard convolu-\\ntional decoder, but developed a highly effective and stable\\narchitecture incorporating batch normalization to achieve\\nstriking image synthesis results.\\nThe main distinction of our work from the conditional\\nGANs described above is that our model conditions on text\\ndescriptions instead of class labels. To our knowledge it\\nis the \\ufb01rst end-to-end differentiable architecture from the\\ncharacter level to pixel level. Furthermore, we introduce a\\nmanifold interpolation regularizer for the GAN generator\\nthat signi\\ufb01cantly improves the quality of generated sam-\\nples, including on held out zero shot categories on CUB.\\nThe bulk of previous work on multimodal learning from\\nimages and text uses retrieval as the target task, i.e. fetch\\nrelevant images given a text query or vice versa. How-\\never, in the past year, there has been a breakthrough in\\nusing recurrent neural network decoders to generate text\\ndescriptions conditioned on images (Vinyals et al., 2015;\\nMao et al., 2015; Karpathy & Li, 2015; Donahue et al.,\\n2015). These typically condition a Long Short-Term Mem-\\nory (Hochreiter & Schmidhuber, 1997) on the top-layer\\nfeatures of a deep convolutional network to generate cap-\\ntions using the MS COCO (Lin et al., 2014) and other cap-\\ntioned image datasets. Xu et al. (2015) incorporated a re-\\ncurrent visual attention mechanism for improved results.\\nOther tasks besides conditional generation have been con-\\nsidered in recent work. Ren et al. (2015) generate answers\\nto questions about the visual content of images. This ap-\\nproach was extended to incorporate an explicit knowledge\\nbase (Wang et al., 2015). Zhu et al. (2015) applied se-\\nquence models to both text (in the form of books) and\\nmovies to perform a joint alignment.\\n\\n\\x0cGenerative Adversarial Text to Image Synthesis\\n\\nIn contemporary work Mansimov et al. (2016) generated\\nimages from text captions, using a variational recurrent\\nautoencoder with attention to paint the image in multiple\\nsteps, similar to DRAW (Gregor et al., 2015). Impressively,\\nthe model can perform reasonable synthesis of completely\\nnovel (unlikely for a human to write) text such as \\u201ca stop\\nsign is \\ufb02ying in blue skies\\u201d, suggesting that it does not sim-\\nply memorize. While the results are encouraging, the prob-\\nlem is highly challenging and the generated images are not\\nyet realistic, i.e., mistakeable for real. Our model can in\\nmany cases generate visually-plausible 64\\xd764 images con-\\nditioned on text, and is also distinct in that our entire model\\nis a GAN, rather only using GAN for post-processing.\\nBuilding on ideas from these many previous works, we\\ndevelop a simple and effective approach for text-based\\nimage synthesis using a character-level text encoder and\\nclass-conditional GAN. We propose a novel architecture\\nand learning strategy that leads to compelling visual re-\\nsults. We focus on the case of \\ufb01ne-grained image datasets,\\nfor which we use the recently collected descriptions for\\nCaltech-UCSD Birds and Oxford Flowers with 5 human-\\ngenerated captions per image (Reed et al., 2016). We train\\nand test on class-disjoint sets, so that test performance can\\ngive a strong indication of generalization ability which we\\nalso demonstrate on MS COCO images with multiple ob-\\njects and various backgrounds.\\n3. Background\\nIn this section we brie\\ufb02y describe several previous works\\nthat our method is built upon.\\n\\n3.1. Generative adversarial networks\\nGenerative adversarial networks (GANs) consist of a gen-\\nerator G and a discriminator D that compete in a two-\\nplayer minimax game: The discriminator tries to distin-\\nguish real training data from synthetic images, and the gen-\\nerator tries to fool the discriminator. Concretely, D and G\\nplay the following game on V(D,G):\\n\\nmin\\nG\\n\\nmax\\nD\\n\\nV (D, G) = Ex\\u223cpdata(x)[log D(x)]+\\n\\n(1)\\n\\nEx\\u223cpz(z)[log(1 \\u2212 D(G(z)))]\\n\\nGoodfellow et al. (2014) prove that this minimax game has\\na global optimium precisely when pg = pdata, and that un-\\nder mild conditions (e.g. G and D have enough capacity)\\npg converges to pdata. In practice, in the start of training\\nsamples from D are extremely poor and rejected by D with\\nhigh con\\ufb01dence. It has been found to work better in prac-\\ntice for the generator to maximize log(D(G(z))) instead of\\nminimizing log(1 \\u2212 D(G(z))).\\n3.2. Deep symmetric structured joint embedding\\nTo obtain a visually-discriminative vector representation of\\ntext descriptions, we follow the approach of Reed et al.\\n\\n(2016) by using deep convolutional and recurrent text en-\\ncoders that learn a correspondence function with images.\\nThe text classi\\ufb01er induced by the learned correspondence\\nfunction ft is trained by optimizing the following struc-\\ntured loss:\\n\\nN(cid:88)\\n\\nn=1\\n\\n1\\nN\\n\\n\\u2206(yn, fv(vn)) + \\u2206(yn, ft(tn))\\n\\n(2)\\n\\nwhere {(vn, tn, yn) : n = 1, ..., N} is the training data set,\\n\\u2206 is the 0-1 loss, vn are the images, tn are the correspond-\\ning text descriptions, and yn are the class labels. Classi\\ufb01ers\\nfv and ft are parametrized as follows:\\n\\nfv(v) = arg max\\n\\ny\\u2208Y\\n\\nft(t) = arg max\\n\\ny\\u2208Y\\n\\nEt\\u223cT (y)[\\u03c6(v)T \\u03d5(t))]\\nEv\\u223cV(y)[\\u03c6(v)T \\u03d5(t))]\\n\\n(3)\\n\\n(4)\\n\\nwhere \\u03c6 is the image encoder (e.g. a deep convolutional\\nneural network), \\u03d5 is the text encoder (e.g. a character-\\nlevel CNN or LSTM), T (y) is the set of text descriptions\\nof class y and likewise V(y) for images. The intuition here\\nis that a text encoding should have a higher compatibility\\nscore with images of the correspondong class compared to\\nany other class and vice-versa.\\nTo train the model a surrogate objective related to Equa-\\ntion 2 is minimized (see Akata et al. (2015) for details). The\\nresulting gradients are backpropagated through \\u03d5 to learn\\na discriminative text encoder. Reed et al. (2016) found that\\ndifferent text encoders worked better for CUB versus Flow-\\ners, but for full generality and robustness to typos and large\\nvocabulary, in this work we always used a hybrid character-\\nlevel convolutional-recurrent network.\\n4. Method\\nOur approach is to train a deep convolutional generative\\nadversarial network (DC-GAN) conditioned on text fea-\\ntures encoded by a hybrid character-level convolutional-\\nrecurrent neural network. Both the generator network G\\nand the discriminator network D perform feed-forward in-\\nference conditioned on the text feature.\\n\\n4.1. Network architecture\\nWe use the following notation. The generator network is\\ndenoted G : RZ \\xd7 RT \\u2192 RD, the discriminator as D :\\nRD \\xd7 RT \\u2192 {0, 1}, where T is the dimension of the text\\ndescription embedding, D is the dimension of the image,\\nand Z is the dimension of the noise input to G. We illustrate\\nour network architecture in Figure 2.\\nIn the generator G, \\ufb01rst we sample from the noise prior\\nz \\u2208 RZ \\u223c N (0, 1) and we encode the text query t us-\\ning text encoder \\u03d5. The description embedding \\u03d5(t) is \\ufb01rst\\ncompressed using a fully-connected layer to a small dimen-\\nsion (in practice we used 128) followed by leaky-ReLU and\\n\\n\\x0cGenerative Adversarial Text to Image Synthesis\\n\\nFigure 2. Our text-conditional convolutional GAN architecture. Text encoding \\u03d5(t) is used by both generator and discriminator. It is\\nprojected to a lower-dimensions and depth concatenated with image feature maps for further stages of convolutional processing.\\n\\nthen concatenated to the noise vector z. Following this, in-\\nference proceeds as in a normal deconvolutional network:\\nwe feed-forward it through the generator G; a synthetic im-\\nage \\u02c6x is generated via \\u02c6x \\u2190 G(z, \\u03d5(t)). Image generation\\ncorresponds to feed-forward inference in the generator G\\nconditioned on query text and a noise sample.\\nIn the discriminator D, we perform several layers of stride-\\n2 convolution with spatial batch normalization (Ioffe &\\nSzegedy, 2015) followed by leaky ReLU. We again reduce\\nthe dimensionality of the description embedding \\u03d5(t) in a\\n(separate) fully-connected layer followed by recti\\ufb01cation.\\nWhen the spatial dimension of the discriminator is 4 \\xd7 4,\\nwe replicate the description embedding spatially and per-\\nform a depth concatenation. We then perform a 1 \\xd7 1 con-\\nvolution followed by recti\\ufb01cation and a 4 \\xd7 4 convolution\\nto compute the \\ufb01nal score from D. Batch normalization is\\nperformed on all convolutional layers.\\n\\n4.2. Matching-aware discriminator (GAN-CLS)\\nThe most straightforward way to train a conditional GAN\\nis to view (text, image) pairs as joint observations and train\\nthe discriminator to judge pairs as real or fake. This type of\\nconditioning is naive in the sense that the discriminator has\\nno explicit notion of whether real training images match\\nthe text embedding context.\\nHowever, as discussed also by (Gauthier, 2015),\\nthe\\ndynamics of learning may be different from the non-\\nconditional case. In the beginning of training, the discrim-\\ninator ignores the conditioning information and easily re-\\njects samples from G because they do not look plausible.\\nOnce G has learned to generate plausible images, it must\\nalso learn to align them with the conditioning information,\\nand likewise D must learn to evaluate whether samples\\nfrom G meet this conditioning constraint.\\nIn naive GAN, the discriminator observes two kinds of in-\\nputs: real images with matching text, and synthetic images\\nwith arbitrary text. Therefore, it must implicitly separate\\ntwo sources of error: unrealistic images (for any text), and\\n\\nAlgorithm 1 GAN-CLS training algorithm with step size\\n\\u03b1, using minibatch SGD for simplicity.\\n1: Input: minibatch images x, matching text t, mis-\\n\\nmatching \\u02c6t, number of training batch steps S\\n\\nh \\u2190 \\u03d5(t) {Encode matching text description}\\n\\u02c6h \\u2190 \\u03d5(\\u02c6t) {Encode mis-matching text description}\\nz \\u223c N (0, 1)Z {Draw sample of random noise}\\n\\u02c6x \\u2190 G(z, h) {Forward through generator}\\nsr \\u2190 D(x, h) {real image, right text}\\nsw \\u2190 D(x, \\u02c6h) {real image, wrong text}\\nsf \\u2190 D(\\u02c6x, h) {fake image, right text}\\nLD \\u2190 log(sr) + (log(1 \\u2212 sw) + log(1 \\u2212 sf ))/2\\nLG \\u2190 log(sf )\\n\\n2: for n = 1 to S do\\n3:\\n4:\\n5:\\n6:\\n7:\\n8:\\n9:\\n10:\\n11: D \\u2190 D \\u2212 \\u03b1\\u2202LD/\\u2202D {Update discriminator}\\n12:\\n13: G \\u2190 G \\u2212 \\u03b1\\u2202LG/\\u2202G {Update generator}\\n14: end for\\n\\nrealistic images of the wrong class that mismatch the con-\\nditioning information. Based on the intuition that this may\\ncomplicate learning dynamics, we modi\\ufb01ed the GAN train-\\ning algorithm to separate these error sources. In addition\\nto the real / fake inputs to the discriminator during train-\\ning, we add a third type of input consisting of real im-\\nages with mismatched text, which the discriminator must\\nlearn to score as fake. By learning to optimize image / text\\nmatching in addition to the image realism, the discrimina-\\ntor can provide an additional signal to the generator.\\nAlgorithm 1 summarizes the training procedure. After en-\\ncoding the text, image and noise (lines 3-5) we generate the\\nfake image (\\u02c6x, line 6). sr indicates the score of associat-\\ning a real image and its corresponding sentence (line 7), sw\\nmeasures the score of associating a real image with an ar-\\nbitrary sentence (line 8), and sf is the score of associating\\na fake image with its corresponding text (line 9). Note that\\nwe use \\u2202LD/\\u2202D to indicate the gradient of D\\u2019s objective\\nwith respect to its parameters, and likewise for G. Lines\\n11 and 13 are meant to indicate taking a gradient step to\\nupdate network parameters.\\n\\nThis flower has small, round violet petals with a dark purple center\\u03c6\\u03c6z ~ N(0,1)This flower has small, round violet petals with a dark purple centerGenerator NetworkDiscriminator Network\\u03c6(t)x := G(z,\\u03c6(t))D(x\\u2019,\\u03c6(t))\\x0cGenerative Adversarial Text to Image Synthesis\\n\\n4.3. Learning with manifold interpolation (GAN-INT)\\nDeep networks have been shown to learn representations\\nin which interpolations between embedding pairs tend to\\nbe near the data manifold (Bengio et al., 2013; Reed et al.,\\n2014). Motivated by this property, we can generate a large\\namount of additional text embeddings by simply interpolat-\\ning between embeddings of training set captions. Critically,\\nthese interpolated text embeddings need not correspond to\\nany actual human-written text, so there is no additional la-\\nbeling cost. This can be viewed as adding an additional\\nterm to the generator objective to minimize:\\n\\nEt1,t2\\u223cpdata [log(1 \\u2212 D(G(z, \\u03b2t1 + (1 \\u2212 \\u03b2)t2)))]\\n\\n(5)\\n\\nwhere z is drawn from the noise distribution and \\u03b2 inter-\\npolates between text embeddings t1 and t2. In practice we\\nfound that \\ufb01xing \\u03b2 = 0.5 works well.\\nBecause the interpolated embeddings are synthetic, the dis-\\ncriminator D does not have \\u201creal\\u201d corresponding image\\nand text pairs to train on. However, D learns to predict\\nwhether image and text pairs match or not. Thus, if D does\\na good job at this, then by satisfying D on interpolated text\\nembeddings G can learn to \\ufb01ll in gaps on the data manifold\\nin between training points. Note that t1 and t2 may come\\nfrom different images and even different categories.1\\n\\n4.4. Inverting the generator for style transfer\\nIf the text encoding \\u03d5(t) captures the image content (e.g.\\n\\ufb02ower shape and colors), then in order to generate a real-\\nistic image the noise sample z should capture style factors\\nsuch as background color and pose. With a trained GAN,\\none may wish to transfer the style of a query image onto\\nthe content of a particular text description. To achieve this,\\none can train a convolutional network to invert G to regress\\nfrom samples \\u02c6x \\u2190 G(z, \\u03d5(t)) back onto z. We used a\\nsimple squared loss to train the style encoder:\\n\\nLstyle = Et,z\\u223cN (0,1)||z \\u2212 S(G(z, \\u03d5(t)))||2\\n\\n2\\n\\n(6)\\n\\nwhere S is the style encoder network. With a trained gen-\\nerator and style encoder, style transfer from a query image\\nx onto text t proceeds as follows:\\n\\ns \\u2190 S(x), \\u02c6x \\u2190 G(s, \\u03d5(t))\\n\\nwhere \\u02c6x is the result image and s is the predicted style.\\n5. Experiments\\nIn this section we \\ufb01rst present results on the CUB dataset\\nof bird images and the Oxford-102 dataset of \\ufb02ower im-\\nages. CUB has 11,788 images of birds belonging to one of\\n\\n1In our experiments, we used \\ufb01ne-grained categories (e.g.\\nbirds are similar enough to other birds, \\ufb02owers to other \\ufb02owers,\\netc.), and interpolating across categories did not pose a problem.\\n\\n200 different categories. The Oxford-102 contains 8,189\\nimages of \\ufb02owers from 102 different categories.\\nAs in Akata et al. (2015) and Reed et al. (2016), we split\\nthese into class-disjoint training and test sets. CUB has\\n150 train+val classes and 50 test classes, while Oxford-102\\nhas 82 train+val and 20 test classes. For both datasets, we\\nused 5 captions per image. During mini-batch selection for\\ntraining we randomly pick an image view (e.g. crop, \\ufb02ip)\\nof the image and one of the captions.\\nFor text features, we \\ufb01rst pre-train a deep convolutional-\\nrecurrent text encoder on structured joint embedding of\\ntext captions with 1,024-dimensional GoogLeNet image\\nembedings (Szegedy et al., 2015) as described in subsec-\\ntion 3.2. For both Oxford-102 and CUB we used a hybrid\\nof character-level ConvNet with a recurrent neural network\\n(char-CNN-RNN) as described in (Reed et al., 2016). Note,\\nhowever that pre-training the text encoder is not a require-\\nment of our method and we include some end-to-end results\\nin the supplement. The reason for pre-training the text en-\\ncoder was to increase the speed of training the other com-\\nponents for faster experimentation. We also provide some\\nqualitative results obtained with MS COCO images of the\\nvalidation set to show the generalizability of our approach.\\nWe used the same GAN architecture for all datasets. The\\ntraining image size was set to 64 \\xd7 64 \\xd7 3. The text en-\\ncoder produced 1, 024-dimensional embeddings that were\\nprojected to 128 dimensions in both the generator and dis-\\ncriminator before depth concatenation into convolutional\\nfeature maps.\\nAs indicated in Algorithm 1, we take alternating steps of\\nupdating the generator and the discriminator network. We\\nused the same base learning rate of 0.0002, and used the\\nADAM solver (Ba & Kingma, 2015) with momentum 0.5.\\nThe generator noise was sampled from a 100-dimensional\\nunit normal distribution. We used a minibatch size of 64\\nand trained for 600 epochs. Our implementation was built\\non top of dcgan.torch2.\\n\\n5.1. Qualitative results\\nWe compare the GAN baseline, our GAN-CLS with image-\\ntext matching discriminator (subsection 4.2), GAN-INT\\nlearned with text manifold interpolation (subsection 4.3)\\nand GAN-INT-CLS which combines both.\\nResults on CUB can be seen in Figure 3. GAN and GAN-\\nCLS get some color information right, but the images do\\nnot look real. However, GAN-INT and GAN-INT-CLS\\nshow plausible images that usually match all or at least part\\nof the caption. We include additional analysis on the ro-\\nbustness of each GAN variant on the CUB dataset in the\\nsupplement.\\n\\n2https://github.com/soumith/dcgan.torch\\n\\n\\x0cGenerative Adversarial Text to Image Synthesis\\n\\nFigure 3. Zero-shot (i.e. conditioned on text from unseen test set categories) generated bird images using GAN, GAN-CLS, GAN-INT\\nand GAN-INT-CLS. We found that interpolation regularizer was needed to reliably achieve visually-plausible results.\\n\\nFigure 4. Zero-shot generated \\ufb02ower images using GAN, GAN-CLS, GAN-INT and GAN-INT-CLS. All variants generated plausible\\nimages. Although some shapes of test categories were not seen during training (e.g. columns 3 and 4), the color information is preserved.\\n\\nResults on the Oxford-102 Flowers dataset can be seen in\\nFigure 4. In this case, all four methods can generate plau-\\nsible \\ufb02ower images that match the description. The basic\\nGAN tends to have the most variety in \\ufb02ower morphology\\n(i.e. one can see very different petal types if this part is left\\nunspeci\\ufb01ed by the caption), while other methods tend to\\ngenerate more class-consistent images. We speculate that\\nit is easier to generate \\ufb02owers, perhaps because birds have\\nstronger structural regularities across species that make it\\neasier for D to spot a fake bird than to spot a fake \\ufb02ower.\\nMany additional results with GAN-INT and GAN-INT-\\nCLS as well as GAN-E2E (our end-to-end GAN-INT-CLS\\nwithout pre-training the text encoder \\u03d5(t)) for both CUB\\nand Oxford-102 can be found in the supplement.\\n\\n5.2. Disentangling style and content\\nIn this section we investigate the extent to which our model\\ncan separate style and content. By content, we mean the\\nvisual attributes of the bird itself, such as shape, size and\\ncolor of each body part. By style, we mean all of the other\\nfactors of variation in the image such as background color\\nand the pose orientation of the bird.\\nThe text embedding mainly covers content information and\\ntypically nothing about style, e.g. captions do not mention\\nthe background or the bird pose. Therefore, in order to\\ngenerate realistic images then GAN must learn to use noise\\nsample z to account for style variations.\\nTo quantify the degree of disentangling on CUB we set up\\ntwo prediction tasks with noise z as the input: pose veri\\ufb01-\\n\\na tiny bird, with a tiny beak, tarsus and feet, a blue crown, blue coverts, and black cheek patchthis small bird has a yellow breast, brown crown, and black superciliaryan all black bird with a distinct thick, rounded bill.this bird is different shades of brown all over with white and black spots on its head and backGAN - CLSGAN - INTGANGAN - INT- CLSthe gray bird has a light grey head and grey webbed feetGTGAN - CLSGAN - INTGANGAN - INT - CLSthis flower is white and pink in color, with petals that have veins.these flowers have petals that start off white in color and end in a dark purple towards the tips.bright droopy yellow petals with burgundy streaks, and a yellow stigma.a flower with long pink petals and raised orange stamen.the flower shown has a blue petals with a white pistil in the centerGT\\x0cGenerative Adversarial Text to Image Synthesis\\n\\nFigure 5. ROC curves using cosine distance between predicted\\nstyle vector on same vs. different style image pairs. Left: im-\\nage pairs re\\ufb02ect same or different pose. Right: image pairs re\\ufb02ect\\nsame or different average background color.\\n\\ncation and background color veri\\ufb01cation. For each task, we\\n\\ufb01rst constructed similar and dissimilar pairs of images and\\nthen computed the predicted style vectors by feeding the\\nimage into a style encoder (trained to invert the input and\\noutput of generator). If GAN has disentangled style using\\nz from image content, the similarity between images of the\\nsame style (e.g. similar pose) should be higher than that of\\ndifferent styles (e.g. different pose).\\nTo recover z, we inverted the each generator network as\\ndescribed in subsection 4.4. To construct pairs for veri\\ufb01ca-\\ntion, we grouped images into 100 clusters using K-means\\nwhere images from the same cluster share the same style.\\nFor background color, we clustered images by the average\\ncolor (RGB channels) of the background; for bird pose, we\\nclustered images by 6 keypoint coordinates (beak, belly,\\nbreast, crown, forehead, and tail).\\nFor evaluation, we compute the actual predicted style vari-\\nables by feeding pairs of images style encoders for GAN,\\nGAN-CLS, GAN-INT and GAN-INT-CLS. We verify the\\nscore using cosine similarity and report the AU-ROC (aver-\\naging over 5 folds). As a baseline, we also compute cosine\\nsimilarity between text features from our text encoder.\\nWe present results on Figure 5. As expected, captions alone\\nare not informative for style prediction. Moreover, con-\\nsistent with the qualitative results, we found that models\\nincorporating interpolation regularizer (GAN-INT, GAN-\\nINT-CLS) perform the best for this task.\\n\\n5.3. Pose and background style transfer\\nWe demonstrate that GAN-INT-CLS with trained style en-\\ncoder (subsection 4.4) can perform style transfer from an\\nunseen query image onto a text description. Figure 6 shows\\nthat images generated using the inferred styles can accu-\\nrately capture the pose information.\\nIn several cases the\\nstyle transfer preserves detailed background information\\nsuch as a tree branch upon which the bird is perched.\\nDisentangling the style by GAN-INT-CLS is interesting be-\\ncause it suggests a simple way of generalization. This way\\n\\nFigure 6. Transfering style from the top row (real) images to the\\ncontent from the query text, with G acting as a deterministic de-\\ncoder. The bottom three rows are captions made up by us.\\nwe can combine previously seen content (e.g. text) and pre-\\nviously seen styles, but in novel pairings so as to generate\\nplausible images very different from any seen image during\\ntraining. Another way to generalize is to use attributes that\\nwere previously seen (e.g. blue wings, yellow belly) as in\\nthe generated parakeet-like bird in the bottom row of Fig-\\nure 6. This way of generalization takes advantage of text\\nrepresentations capturing multiple visual aspects.\\n\\n5.4. Sentence interpolation\\nFigure 8 demonstrates the learned text manifold by inter-\\npolation (Left). Although there is no ground-truth text for\\nthe intervening points, the generated images appear plau-\\nsible. Since we keep the noise distribution the same, the\\nonly changing factor within each row is the text embedding\\nthat we use. Note that interpolations can accurately re\\ufb02ect\\ncolor information, such as a bird changing from blue to red\\nwhile the pose and background are invariant.\\nAs well as interpolating between two text encodings, we\\nshow results on Figure 8 (Right) with noise interpolation.\\nHere, we sample two random noise vectors. By keeping the\\ntext encoding \\ufb01xed, we interpolate between these two noise\\nvectors and generate bird images with a smooth transition\\nbetween two styles by keeping the content \\ufb01xed.\\n\\n5.5. Beyond birds and \\ufb02owers\\nWe trained a GAN-CLS on MS-COCO to show the gen-\\neralization capability of our approach on a general set of\\nimages that contain multiple objects and variable back-\\ngrounds. We use the same text encoder architecture,\\nsame GAN architecture and same hyperparameters (learn-\\ning rate, minibatch size and number of epochs) as in CUB\\n\\nThe bird has a yellow breast with grey features and a small beak.This is a large white bird with black wings and a red head.A small bird with a black head and wings and features grey wings.This bird has a white breast, brown and white coloring on its head and wings, and a thin pointy beak.A small bird with white base and black stripes throughout its belly, head, and feathers.A small sized bird that has a cream belly and a short pointed bill.This bird is completely red.This bird is completely white.This is a yellow bird. The wings are bright blue.Text descriptions(content)Images (style)\\x0cGenerative Adversarial Text to Image Synthesis\\n\\nFigure 7. Generating images of general concepts using our GAN-CLS on the MS-COCO validation set. Unlike the case of CUB and\\nOxford-102, the network must (try to) handle multiple objects and diverse backgrounds.\\n\\nroughly correspond to the query, but AlignDRAW samples\\nmore noticably re\\ufb02ect single-word changes in the selected\\nqueries from that work. Incorporating temporal structure\\ninto the GAN-CLS generator network could potentially im-\\nprove its ability to capture these text variations.\\n6. Conclusions\\nIn this work we developed a simple and effective model\\nfor generating images based on detailed visual descriptions.\\nWe demonstrated that the model can synthesize many plau-\\nsible visual interpretations of a given text caption. Our\\nmanifold interpolation regularizer substantially improved\\nthe text to image synthesis on CUB. We showed disentan-\\ngling of style and content, and bird pose and background\\ntransfer from query images onto text descriptions. Finally\\nwe demonstrated the generalizability of our approach to\\ngenerating images with multiple objects and variable back-\\ngrounds with our results on MS-COCO dataset. In future\\nwork, we aim to further scale up the model to higher reso-\\nlution images and add more types of text.\\nAcknowledgments\\nThis work was supported in part by NSF CAREER\\nIIS-1453651, ONR N00014-13-1-0762 and NSF CMMI-\\n1266184.\\n\\nReferences\\nAkata, Z., Reed, S., Walter, D., Lee, H., and Schiele, B.\\nEvaluation of Output Embeddings for Fine-Grained Im-\\nage Classi\\ufb01cation. In CVPR, 2015.\\n\\nBa, J. and Kingma, D. Adam: A method for stochastic\\n\\noptimization. In ICLR, 2015.\\n\\nBengio, Y., Mesnil, G., Dauphin, Y., and Rifai, S. Better\\n\\nFigure 8. Left: Generated bird images by interpolating between\\ntwo sentences (within a row the noise is \\ufb01xed). Right: Interpolat-\\ning between two randomly-sampled noise vectors.\\n\\nand Oxford-102. The only difference in training the text\\nencoder is that COCO does not have a single object cat-\\negory per class. However, we can still learn an instance\\nlevel (rather than category level) image and text matching\\nfunction, as in (Kiros et al., 2014).\\nSamples and ground truth captions and their corresponding\\nimages are shown on Figure 7. A common property of all\\nthe results is the sharpness of the samples, similar to other\\nGAN-based image synthesis models. We also observe di-\\nversity in the samples by simply drawing multiple noise\\nvectors and using the same \\ufb01xed text encoding.\\nFrom a distance the results are encouraging, but upon\\nclose inspection it is clear that the generated scenes are\\nnot usually coherent; for example the human-like blobs in\\nthe baseball scenes lack clearly articulated parts.\\nIn fu-\\nture work, it may be interesting to incorporate hierarchical\\nstructure into the image synthesis model in order to better\\nhandle complex multi-object scenes.\\nA qualitative comparison with AlignDRAW (Mansimov\\net al., 2016) can be found in the supplement. GAN-\\nCLS generates sharper and higher-resolution samples that\\n\\na group of people on skis stand on the snow.a table with many plates of food and drinkstwo giraffe standing next to each other in a forest.a large blue octopus kite flies above the people having fun at the beach.a man in a wet suit riding a surfboard on a wave.two plates of food that include beans, guacamole and rice.a green plant that is growing out of the ground.there is only one horse in the grassy field.a pitcher is about to throw the ball to the batter. a sheep standing in a open grass field.a picture of a very clean living room.a toilet in a small room with a window and unfinished walls.GTOursGTOursGTOurs\\u2018Blue bird with black beak\\u2019 \\u2192 \\u2018Red bird with black beak\\u2019\\u2018Small blue bird with black wings\\u2019 \\u2192 \\u2018Small yellow bird with black wings\\u2019\\u2018This bird is bright.\\u2019 \\u2192 \\u2018This bird is dark.\\u2019\\u2018This bird is completely red with black wings\\u2019\\u2018This is a yellow bird. The wings are bright blue\\u2019\\u2018this bird is all blue, the top part of the bill is blue, but the bottom half is white\\u2019\\x0cGenerative Adversarial Text to Image Synthesis\\n\\nmixing via deep representations. In ICML, 2013.\\n\\nDenton, E. L., Chintala, S., Fergus, R., et al. Deep gener-\\native image models using a laplacian pyramid of adver-\\nsarial networks. In NIPS, 2015.\\n\\nDonahue, J., Hendricks, L. A., Guadarrama, S., Rohrbach,\\nM., Venugopalan, S., Saenko, K., and Darrell, T. Long-\\nterm recurrent convolutional networks for visual recog-\\nnition and description. In CVPR, 2015.\\n\\nDosovitskiy, A., Tobias Springenberg, J., and Brox, T.\\nLearning to generate chairs with convolutional neural\\nnetworks. In CVPR, 2015.\\n\\nFarhadi, A., Endres, I., Hoiem, D., and Forsyth, D. De-\\n\\nscribing objects by their attributes. In CVPR, 2009.\\n\\nFu, Y., Hospedales, T. M., Xiang, T., Fu, Z., and Gong, S.\\nTransductive multi-view embedding for zero-shot recog-\\nnition and annotation. In ECCV, 2014.\\n\\nGauthier, J. Conditional generative adversarial nets for\\n\\nconvolutional face generation. Technical report, 2015.\\n\\nGoodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B.,\\nWarde-Farley, D., Ozair, S., Courville, A., and Bengio,\\nY. Generative adversarial nets. In NIPS, 2014.\\n\\nGregor, K., Danihelka, I., Graves, A., Rezende, D., and\\nWierstra, D. Draw: A recurrent neural network for image\\ngeneration. In ICML, 2015.\\n\\nHochreiter, S. and Schmidhuber, J. Long short-term mem-\\n\\nory. Neural computation, 9(8):1735\\u20131780, 1997.\\n\\nIoffe, S. and Szegedy, C. Batch normalization: Accelerat-\\ning deep network training by reducing internal covariate\\nshift. In ICML, 2015.\\n\\nKarpathy, A. and Li, F. Deep visual-semantic alignments\\n\\nfor generating image descriptions. In CVPR, 2015.\\n\\nKiros, R., Salakhutdinov, R., and Zemel, R. S. Unify-\\ning visual-semantic embeddings with multimodal neural\\nlanguage models. In ACL, 2014.\\n\\nMansimov, E., Parisotto, E., Ba, J. L., and Salakhutdi-\\nnov, R. Generating images from captions with attention.\\nICLR, 2016.\\n\\nMao, J., Xu, W., Yang, Y., Wang, J., and Yuille, A. Deep\\ncaptioning with multimodal recurrent neural networks\\n(m-rnn). ICLR, 2015.\\n\\nMirza, M. and Osindero, S. Conditional generative adver-\\n\\nsarial nets. arXiv preprint arXiv:1411.1784, 2014.\\n\\nNgiam, J., Khosla, A., Kim, M., Nam, J., Lee, H., and Ng,\\n\\nA. Y. Multimodal deep learning. In ICML, 2011.\\n\\nParikh, D. and Grauman, K. Relative attributes. In ICCV,\\n\\n2011.\\n\\nRadford, A., Metz, L., and Chintala, S. Unsupervised rep-\\nresentation learning with deep convolutional generative\\nadversarial networks. 2016.\\n\\nReed, S., Sohn, K., Zhang, Y., and Lee, H. Learning to dis-\\nentangle factors of variation with manifold interaction.\\nIn ICML, 2014.\\n\\nReed, S., Zhang, Y., Zhang, Y., and Lee, H. Deep visual\\n\\nanalogy-making. In NIPS, 2015.\\n\\nReed, S., Akata, Z., Lee, H., and Schiele, B. Learning deep\\nIn\\n\\nrepresentations for \\ufb01ne-grained visual descriptions.\\nCVPR, 2016.\\n\\nRen, M., Kiros, R., and Zemel, R. Exploring models and\\n\\ndata for image question answering. In NIPS, 2015.\\n\\nSohn, K., Shang, W., and Lee, H.\\n\\ndeep learning with variation of information.\\n2014.\\n\\nImproved multimodal\\nIn NIPS,\\n\\nSrivastava, N. and Salakhutdinov, R. R. Multimodal learn-\\n\\ning with deep boltzmann machines. In NIPS, 2012.\\n\\nSzegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S.,\\nAnguelov, D., Erhan, D., Vanhoucke, V., and Rabi-\\nnovich, A. Going deeper with convolutions. In CVPR,\\n2015.\\n\\nKumar, N., Berg, A. C., Belhumeur, P. N., and Nayar, S. K.\\nAttribute and simile classi\\ufb01ers for face veri\\ufb01cation. In\\nICCV, 2009.\\n\\nVinyals, O., Toshev, A., Bengio, S., and Erhan, D. Show\\nIn CVPR,\\n\\nand tell: A neural image caption generator.\\n2015.\\n\\nLampert, C. H., Nickisch, H., and Harmeling, S. Attribute-\\nbased classi\\ufb01cation for zero-shot visual object catego-\\nrization. TPAMI, 36(3):453\\u2013465, 2014.\\n\\nWah, C., Branson, S., Welinder, P., Perona, P., and Be-\\nlongie, S. The caltech-ucsd birds-200-2011 dataset.\\n2011.\\n\\nLin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P.,\\nRamanan, D., Doll\\xb4ar, P., and Zitnick, C. L. Microsoft\\ncoco: Common objects in context. In ECCV. 2014.\\n\\nWang, P., Wu, Q., Shen, C., Hengel, A. v. d., and Dick, A.\\nExplicit knowledge-based reasoning for visual question\\nanswering. arXiv preprint arXiv:1511.02570, 2015.\\n\\n\\x0cGenerative Adversarial Text to Image Synthesis\\n\\nXu, K., Ba, J., Kiros, R., Courville, A., Salakhutdinov, R.,\\nZemel, R., and Bengio, Y. Show, attend and tell: Neural\\nimage caption generation with visual attention. In ICML,\\n2015.\\n\\nYan, X., Yang, J., Sohn, K., and Lee, H. Attribute2image:\\nimage generation from visual attributes.\\n\\nConditional\\narXiv preprint arXiv:1512.00570, 2015.\\n\\nYang, J., Reed, S., Yang, M.-H., and Lee, H. Weakly-\\nsupervised disentangling with recurrent transformations\\nfor 3d view synthesis. In NIPS, 2015.\\n\\nZhu, Y., Kiros, R., Zemel, R., Salakhutdinov, R., Urta-\\nsun, R., Torralba, A., and Fidler, S. Aligning books\\nand movies: Towards story-like visual explanations by\\nwatching movies and reading books. In ICCV, 2015.\\n\\n\\x0c', u'207\\n\\nTransactions of the Association for Computational Linguistics, 2 (2014) 207\\u2013218. Action Editor: Alexander Clark.\\n\\nSubmitted 10/2013; Revised 3/2014; Published 4/2014. c(cid:13)2014 Association for Computational Linguistics.\\n\\nGroundedCompositionalSemanticsforFindingandDescribingImageswithSentencesRichardSocher,AndrejKarpathy,QuocV.Le*,ChristopherD.Manning,AndrewY.NgStanfordUniversity,ComputerScienceDepartment,*GoogleInc.richard@socher.org,karpathy@cs.stanford.edu,qvl@google.com,manning@stanford.edu,ang@cs.stanford.eduAbstractPreviousworkonRecursiveNeuralNetworks(RNNs)showsthatthesemodelscanproducecompositionalfeaturevectorsforaccuratelyrepresentingandclassifyingsentencesorim-ages.However,thesentencevectorsofprevi-ousmodelscannotaccuratelyrepresentvisu-allygroundedmeaning.WeintroducetheDT-RNNmodelwhichusesdependencytreestoembedsentencesintoavectorspaceinordertoretrieveimagesthataredescribedbythosesentences.UnlikepreviousRNN-basedmod-elswhichuseconstituencytrees,DT-RNNsnaturallyfocusontheactionandagentsinasentence.Theyarebetterabletoabstractfromthedetailsofwordorderandsyntacticexpression.DT-RNNsoutperformotherre-cursiveandrecurrentneuralnetworks,kernel-izedCCAandabag-of-wordsbaselineonthetasksof\\ufb01ndinganimagethat\\ufb01tsasentencedescriptionandviceversa.Theyalsogivemoresimilarrepresentationstosentencesthatdescribethesameimage.1IntroductionSinglewordvectorspacesarewidelyused(TurneyandPantel,2010)andsuccessfulatclassifyingsin-glewordsandcapturingtheirmeaning(CollobertandWeston,2008;Huangetal.,2012;Mikolovetal.,2013).Sincewordsrarelyappearinisolation,thetaskoflearningcompositionalmeaningrepre-sentationsforlongerphraseshasrecentlyreceivedalotofattention(MitchellandLapata,2010;Socheretal.,2010;Socheretal.,2012;Grefenstetteetal.,2013).Similarly,classifyingwholeimagesintoa\\ufb01xedsetofclassesalsoachievesveryhighperfor-mance(Leetal.,2012;Krizhevskyetal.,2012).However,similartowords,objectsinimagesareof-tenseeninrelationshipswithotherobjectswhicharenotadequatelydescribedbyasinglelabel.Inthiswork,weintroduceamodel,illustratedinFig.1,whichlearnstomapsentencesandimagesintoacommonembeddingspaceinordertobeabletoretrieveonefromtheother.Weassumewordandimagerepresentationsare\\ufb01rstlearnedintheirre-spectivesinglemodalitiesbut\\ufb01nallymappedintoajointlylearnedmultimodalembeddingspace.OurmodelformappingsentencesintothisspaceisbasedonideasfromRecursiveNeuralNetworks(RNNs)(Pollack,1990;Costaetal.,2003;Socheretal.,2011b).However,unlikeallpreviousRNNmodelswhicharebasedonconstituencytrees(CT-RNNs),ourmodelcomputescompositionalvectorrepresentationsinsidedependencytrees.Thecom-positionalvectorscomputedbythisnewdependencytreeRNN(DT-RNN)capturemoreofthemeaningofsentences,wherewede\\ufb01nemeaningintermsofsimilaritytoa\\u201cvisualrepresentation\\u201dofthetextualdescription.DT-RNNinducedvectorrepresenta-tionsofsentencesaremorerobusttochangesinthesyntacticstructureorwordorderthanrelatedmod-elssuchasCT-RNNsorRecurrentNeuralNetworkssincetheynaturallyfocusonasentence\\u2019sactionanditsagents.WeevaluateandcompareDT-RNNinducedrep-resentationsontheirabilitytouseasentencesuchas\\u201cAmanwearingahelmetjumpsonhisbikenearabeach.\\u201dto\\ufb01ndimagesthatshowsuchascene.Thegoalistolearnsentencerepresentationsthatcapture\\x0c208\\n\\nA man wearing a helmet jumps on his bike near a beach.Compositional Sentence VectorsTwo airplanes parked in an airport.A man jumping his downhill bike.Image Vector RepresentationA small child sits on a cement wall near white flower.Multi-Modal RepresentationsFigure1:TheDT-RNNlearnsvectorrepresentationsforsentencesbasedontheirdependencytrees.Welearntomaptheoutputsofconvolutionalneuralnetworksappliedtoimagesintothesamespaceandcanthencomparebothsentencesandimages.Thisallowsustoqueryimageswithasentenceandgivesentencedescriptionstoimages.thevisualscenedescribedandto\\ufb01ndappropriateimagesinthelearned,multi-modalsentence-imagespace.Conversely,whengivenaqueryimage,wewouldliketo\\ufb01ndadescriptionthatgoesbeyondasinglelabelbyprovidingacorrectsentencedescrib-ingit,ataskthathasrecentlygarneredalotofat-tention(Farhadietal.,2010;Ordonezetal.,2011;Kuznetsovaetal.,2012).Weusethedatasetintro-ducedby(Rashtchianetal.,2010)whichconsistsof1000images,eachwith5descriptions.Onalltasks,ourmodeloutperformsbaselinesandrelatedmod-els.2RelatedWorkThepresentedmodelisconnectedtoseveralareasofNLPandvisionresearch,eachwithalargeamountofrelatedworktowhichwecanonlydosomejusticegivenspaceconstraints.SemanticVectorSpacesandTheirComposition-ality.Thedominantapproachinsemanticvec-torspacesusesdistributionalsimilaritiesofsinglewords.Often,co-occurrencestatisticsofawordanditscontextareusedtodescribeeachword(TurneyandPantel,2010;BaroniandLenci,2010),suchastf-idf.Mostofthecompositionalityalgorithmsandrelateddatasetscapturetwo-wordcompositions.Forinstance,(MitchellandLapata,2010)usetwo-wordphrasesandanalyzesimilaritiescomputedbyvectoraddition,multiplicationandothers.Compo-sitionalityisanactive\\ufb01eldofresearchwithmanydifferentmodelsandrepresentationsbeingexplored(Grefenstetteetal.,2013),amongmanyothers.Wecomparetosupervisedcompositionalmodelsthatcanlearntask-speci\\ufb01cvectorrepresentationssuchasconstituencytreerecursiveneuralnetworks(Socheretal.,2011b;Socheretal.,2011a),chainstructuredrecurrentneuralnetworksandotherbaselines.An-otheralternativewouldbetouseCCGtreesasabackboneforvectorcomposition(K.M.Hermann,2013).MultimodalEmbeddings.Multimodalembed-dingmethodsprojectdatafrommultiplesourcessuchassoundandvideo(Ngiametal.,2011)orim-agesandtext.Socheretal.(SocherandFei-Fei,2010)projectwordsandimageregionsintoacom-monspaceusingkernelizedcanonicalcorrelationanalysistoobtainstateoftheartperformanceinan-notationandsegmentation.Similartoourwork,theyuseunsupervisedlargetextcorporatolearnseman-ticwordrepresentations.AmongotherrecentworkisthatbySrivastavaandSalakhutdinov(2012)whodevelopedmultimodalDeepBoltzmannMachines.Similartotheirwork,weusetechniquesfromthebroad\\ufb01eldofdeeplearningtorepresentimagesandwords.Recently,singlewordvectorembeddingshavebeenusedforzeroshotlearning(Socheretal.,2013c).Mappingimagestowordvectorsenabledtheirsystemtoclassifyimagesasdepictingobjectssuchas\\u201dcat\\u201dwithoutseeinganyexamplesofthisclass.RelatedworkhasalsobeenpresentedatNIPS(Socheretal.,2013b;Fromeetal.,2013).Thisworkmoveszero-shotlearningbeyondsinglecategoriesperimageandextendsittounseenphrasesandfulllengthsentences,makinguseofsimilarideasofse-manticspacesgroundedinvisualknowledge.\\x0c209\\n\\nDetailedImageAnnotation.Interactionsbe-tweenimagesandtextsisagrowingresearch\\ufb01eld.Earlyworkinthisareaincludesgeneratingsinglewordsor\\ufb01xedphrasesfromimages(Duyguluetal.,2002;Barnardetal.,2003)orusingcontextualin-formationtoimproverecognition(GuptaandDavis,2008;Torralbaetal.,2010).Apartfromalargebodyofworkonsingleobjectimageclassi\\ufb01cation(Leetal.,2012),thereisalsoworkonattributeclassi\\ufb01cationandothermid-levelelements(Kumaretal.,2009),someofwhichwehopetocapturewithourapproachaswell.Ourworkiscloseinspiritwithrecentworkinde-scribingimageswithmoredetailed,longertextualdescriptions.Inparticular,Yaoetal.(2010)describeimagesusinghierarchicalknowledgeandhumansintheloop.Incontrast,ourworkdoesnotrequirehu-maninteractions.Farhadietal.(2010)andKulkarnietal.(2011),ontheotherhand,useamoreautomaticmethodtoparseimages.Forinstance,theformerap-proachusesasingletripleofobjectsestimatedforanimagetoretrievesentencesfromacollectionwrittentodescribesimilarimages.Itformsrepresentationstodescribe1object,1action,and1scene.Kulkarnietal.(2011)extendstheirmethodtodescribeanim-agewithmultipleobjects.Noneoftheseapproacheshaveusedacompositionalsentencevectorrepre-sentationandtheyrequirespeci\\ufb01clanguagegener-ationtechniquesandsophisticatedinferencemeth-ods.Sinceourmodelisbasedonneuralnetworksin-ferenceisfastandsimple.Kuznetsovaetal.(2012)useaverylargeparallelcorpustoconnectimagesandsentences.FengandLapata(2013)usealargedatasetofcaptionedimagesandexperimentswithbothextractive(search)andabstractive(generation)models.MostrelatedistheveryrecentworkofHodoshetal.(2013).Theytooevaluateusingarankingmea-sure.Inourexperiments,wecomparetokernelizedCanonicalCorrelationAnalysiswhichisthemaintechniqueintheirexperiments.3Dependency-TreeRecursiveNeuralNetworksInthissectionwe\\ufb01rstfocusontheDT-RNNmodelthatcomputescompositionalvectorrepresentationsforphrasesandsentencesofvariablelengthandsyn-tactictype.Insection5theresultingvectorswillthenbecomemultimodalfeaturesbymappingim-agesthatshowwhatthesentencedescribestothesamespaceandlearningboththeimageandsen-tencemappingjointly.Themostcommonwayofbuildingrepresenta-tionsforlongerphrasesfromsinglewordvectorsistosimplylinearlyaveragethewordvectors.Whilethisbag-of-wordsapproachcanyieldreasonableperformanceinsometasks,itgivesallthewordsthesameweightandcannotdistinguishimportantdif-ferencesinsimplevisualdescriptionssuchasThebikecrashedintothestandingcar.vs.Thecarcrashedintothestandingbike..RNNmodels(Pollack,1990;GollerandK\\xa8uchler,1996;Socheretal.,2011b;Socheretal.,2011a)pro-videdanovelwayofcombiningwordvectorsforlongerphrasesthatmovedbeyondsimpleaverag-ing.TheycombinevectorswithanRNNinbinaryconstituencytreeswhichhavepotentiallymanyhid-denlayers.Whiletheinducedvectorrepresentationsworkverywellonmanytasks,theyalsoinevitablycapturealotofsyntacticstructureofthesentence.However,thetaskof\\ufb01ndingimagesfromsentencedescriptionsrequiresustobemoreinvarianttosyn-tacticdifferences.Onesuchexampleareactive-passiveconstructionswhichcancollapsewordssuchas\\u201cby\\u201dinsomeformalisms(deMarneffeetal.,2006),relyinginsteadonthesemanticrelationshipof\\u201cagent\\u201d.Forinstance,Themotherhuggedherchild.andThechildwashuggedbyitsmother.shouldmaptoroughlythesamevisualspace.Cur-rentRecursiveandRecurrentNeuralNetworksdonotexhibitthisbehaviorandevenbagofwordsrep-resentationswouldbein\\ufb02uencedbythewordswasandby.Themodelwedescribebelowfocusesmoreonrecognizingactionsandagentsandhasthepo-tentialtolearnrepresentationsthatareinvarianttoactive-passivedifferences.3.1DT-RNNInputs:WordVectorsandDependencyTreesInorderfortheDT-RNNtocomputeavectorrepre-sentationforanorderedlistofmwords(aphraseorsentence),wemapthesinglewordstoavectorspaceandthenparsethesentence.First,wemapeachwordtoad-dimensionalvec-tor.Weinitializethesewordvectorswiththeun-\\x0c210\\n\\nAmanwearingahelmetjumpsonhisbikenearabeachdetnsubjpartmoddetdobjrootprepposspobjprepdetpobjFigure2:Exampleofafulldependencytreeforalongersentence.TheDT-RNNwillcomputevectorrepresentationsateverywordthatrepresentsthatwordandanarbitrarynumberofchildnodes.The\\ufb01nalrepresentationiscomputedattherootnode,hereattheverbjumps.Notethatmoreimportantactivityandobjectwordsarehigherupinthistreestructure.supervisedmodelofHuangetal.(2012)whichcanlearnsinglewordvectorrepresentationsfrombothlocalandglobalcontexts.Theideaistoconstructaneuralnetworkthatoutputshighscoresforwindowsanddocumentsthatoccurinalargeunlabeledcorpusandlowscoresforwindow-documentpairswhereonewordisreplacedbyarandomword.WhensuchanetworkisoptimizedviagradientdescentthederivativesbackpropagateintoawordembeddingmatrixAwhichstoreswordvectorsascolumns.Inordertopredictcorrectscoresthevectorsinthema-trixcaptureco-occurrencestatistics.Weused=50inallourexperiments.TheembeddingmatrixXisthenusedby\\ufb01ndingthecolumnindexiofeachword:[w]=iandretrievingthecorrespondingcol-umnxwfromX.Henceforth,werepresentaninputsentencesasanorderedlistof(word,vector)pairs:s=((w1,xw1),...,(wm,xwm)).Next,thesequenceofwords(w1,...,wm)isparsedbythedependencyparserofdeMarneffeetal.(2006).Fig.2showsanexample.Wecanrepresentadependencytreedofasentencesasanorderedlistof(child,parent)indices:d(s)={(i,j)},whereeverychildwordinthesequencei=1,...,mispresentandhasanywordj\\u2208{1,...,m}\\u222a{0}asitsparent.Therootwordhasasitsparent0andwenoticethatthesamewordcanbeaparentbetweenzeroandmnumberoftimes.Withoutlossofgenerality,weassumethatthesein-dicesformatreestructure.Tosummarize,theinputtotheDT-RNNforeachsentenceisthepair(s,d):thewordsandtheirvectorsandthedependencytree.3.2ForwardPropagationinDT-RNNsGiventhesetwoinputs,wenowillustratehowtheDT-RNNcomputesparentvectors.Wewillusethefollowingsentenceasarunningexample:Students1ride2bikes3at4night5.Fig.3showsitstreeandcomputedvectorrepresentations.Thedepen-Students                 bikes           nightride at          x1x2x3x4x5h1h2h3h4h5Figure3:ExampleofaDT-RNNtreestructureforcom-putingasentencerepresentationinabottomupfashion.dencytreeforthissentencecanbesummarizedbythefollowingsetof(child,parent)edges:d={(1,2),(2,0),(3,2),(4,2),(5,4)}.TheDT-RNNmodelwillcomputeparentvectorsateachwordthatincludeallthedependent(chil-dren)nodesinabottomupfashionusingacom-positionalityfunctiong\\u03b8whichisparameterizedbyallthemodelparameters\\u03b8.Tothisend,thealgo-rithmsearchesfornodesinatreethathaveeither(i)nochildrenor(ii)whosechildrenhavealreadybeencomputedandthencomputesthecorrespond-ingvector.Inourexample,thewordsx1,x3,x5areleafnodesandhence,wecancomputetheircorrespond-inghiddennodesvia:hc=g\\u03b8(xc)=f(Wvxc)forc=1,3,5,(1)wherewecomputethehiddenvectoratpositioncviaourgeneralcompositionfunctiong\\u03b8.Inthecaseofleafnodes,thiscompositionfunctionbecomessimplyalinearlayer,parameterizedbyWv\\u2208Rn\\xd7d,followedbyanonlinearity.Wecross-validateoverusingnononlinearity(f=id),tanh,sigmoidorrecti\\ufb01edlinearunits(f=max(0,x),butgenerally\\ufb01ndtanhtoperformbest.The\\ufb01nalsentencerepresentationwewanttocom-puteisath2,however,sincewestilldonothaveh4,\\x0c211\\n\\nwecomputethatonenext:h4=g\\u03b8(x4,h5)=f(Wvx4+Wr1h5),(2)whereweusethesameWvasbeforetomapthewordvectorintohiddenspacebutwenowalsohavealinearlayerthattakesasinputh5,theonlychildofthefourthnode.ThematrixWr1\\u2208Rn\\xd7nisusedbecausenode5isthe\\ufb01rstchildnodeontherightsideofnode4.Generally,wehavemultiplematri-cesforcomposingwithhiddenchildvectorsfromtherightandleftsides:Wr\\xb7=(Wr1,...,Wrkr)andWl\\xb7=(Wl1,...,Wlkl).Thenumberofneededma-tricesisdeterminedbythedatabysimply\\ufb01ndingthemaximumnumbersofleftklandrightkrchil-drenanynodehas.Ifattesttimeachildappearedatanevenlargedistance(thisdoesnothappeninourtestset),thecorrespondingmatrixwouldbetheidentitymatrix.Nowthatallchildrenofh2havetheirhiddenvec-tors,wecancomputethe\\ufb01nalsentencerepresenta-tionvia:h2=g\\u03b8(x2,h1,h3,h4)=(3)f(Wvx2+Wl1h1+Wr1h3+Wr2h4).Noticethatthechildrenaremultipliedbymatricesthatdependontheirlocationrelativetothecurrentnode.Anothermodi\\ufb01cationthatimprovesthemeanrankbyapproximately6inimagesearchonthedevsetistoweightnodesbythenumberofwordsunder-neaththemandnormalizebythesumofwordsunderallchildren.Thisencouragestheintuitivedesidera-tumthatnodesdescribinglongerphrasesaremoreimportant.Let\\u2018(i)bethenumberofleafnodes(words)undernodeiandC(i,y)bethesetofchildnodesofnodeiindependencytreey.The\\ufb01nalcom-positionfunctionforanodevectorhibecomes:hi=f\\uf8eb\\uf8ed1\\u2018(i)\\uf8eb\\uf8edWvxi+Xj\\u2208C(i)\\u2018(j)Wpos(i,j)hj\\uf8f6\\uf8f8\\uf8f6\\uf8f8,(4)wherebyde\\ufb01nition\\u2018(i)=1+Pj\\u2208C(i)\\u2018(j)andpos(i,j)istherelativepositionofchildjwithre-specttonodei,e.g.l1orr2inEq.3.3.3SemanticDependencyTreeRNNsAnalternativeistoconditiontheweightmatricesonthesemanticrelationsgivenbythedependencyparser.WeusethecollapsedtreeformalismoftheStanforddependencyparser(deMarneffeetal.,2006).Withsuchasemanticuntyingoftheweights,theDT-RNNmakesbetteruseofthedependencyformalismandcouldgiveactive-passivereversalssimilarsemanticvectorrepresentation.TheequationforthissemanticDT-RNN(SDT-RNN)isthesameastheoneaboveexceptthatthematricesWpos(i,j)arereplacedwithmatricesbasedonthedependencyrelationship.Thereareatotalof141uniquesuchrelationshipsinthedataset.However,mostareveryrare.Forexamplesofsemanticrelationships,seeFig.2andthemodelanalysissection6.7.Thisforwardpropagationcanbeusedforcom-putingcompositionalvectorsandinSec.5wewillexplaintheobjectivefunctioninwhichthesearetrained.3.4ComparisontoPreviousRNNModelsTheDT-RNNhasseveralimportantdifferencestopreviousRNNmodelsofSocheretal.(2011a)and(Socheretal.,2011b;Socheretal.,2011c).TheseconstituencytreeRNNs(CT-RNNs)usethefollow-ingcompositionfunctiontocomputeahiddenpar-entvectorhfromexactlytwochildvectors(c1,c2)inabinarytree:h=f(cid:18)W(cid:20)c1c2(cid:21)(cid:19),whereW\\u2208Rd\\xd72disthemainparametertolearn.ThiscanberewrittentoshowthesimilaritytotheDT-RNNash=f(Wl1c1+Wr1c2).However,thereareseveralimportantdifferences.Note\\ufb01rstthatinpreviousRNNmodelsthepar-entvectorswereofthesamedimensionalitytoberecursivelycompatibleandbeusedasinputtothenextcomposition.Incontrast,ournewmodel\\ufb01rstmapssinglewordsintoahiddenspaceandthenpar-entnodesarecomposedfromthesehiddenvectors.Thisallowsahighercapacityrepresentationwhichisespeciallyhelpfulfornodesthathavemanychil-dren.Secondly,theDT-RNNallowsforn-arynodesinthetree.ThisisanimprovementthatispossibleevenforconstituencytreeCT-RNNsbutithasnotbeenexploredinpreviousmodels.Third,duetocomputingparentnodesincon-stituencytrees,previousmodelshadtheproblemthatwordsthataremergedlastinthetreehavealargerweightorimportanceinthe\\ufb01nalsentencerep-\\x0c212\\n\\nFigure4:Thearchitectureofthevisualmodel.Thismodelhas3sequencesof\\ufb01ltering,poolingandlocalcontrastnormalizationlayers.Thelearnableparametersarethe\\ufb01lteringlayer.The\\ufb01ltersarenotshared,i.e.,thenetworkisnonconvolutional.resentation.Thiscanbeproblematicsincetheseareoftensimplenon-contentwords,suchasaleading\\u2018But,\\u2019.Whilesuchsinglewordscanbeimportantfortaskssuchassentimentanalysis,wearguethatfordescribingvisualscenestheDT-RNNcapturesthemoreimportanteffects:Thedependencytreestruc-turespushthecentralcontentwordssuchasthemainactionorverbanditssubjectandobjecttobemergedlastandhence,byconstruction,the\\ufb01nalsentencerepresentationismorerobusttolessimportantad-jectivalmodi\\ufb01ers,wordorderchanges,etc.Fourth,weallowsomeuntyingofweightsde-pendingoneitherhowfarawayaconstituentisfromthecurrentwordorwhatitssemanticrelationshipis.Nowthatwecancomputecompositionalvectorrepresentationsforsentences,thenextsectionde-scribeshowwerepresentimages.4LearningImageRepresentationswithNeuralNetworksTheimagefeaturesthatweuseinourexperimentsareextractedfromadeepneuralnetwork,replicatedfromtheonedescribedin(Leetal.,2012).Thenet-workwastrainedusingbothunlabeleddata(randomwebimages)andlabeleddatatoclassify22,000cat-egoriesinImageNet(Dengetal.,2009).Wethenusedthefeaturesatthelastlayer,beforetheclassi-\\ufb01er,asthefeaturerepresentationinourexperiments.Thedimensionofthefeaturevectorofthelastlayeris4,096.Thedetailsofthemodelanditstrainingproceduresareasfollows.ThearchitectureofthenetworkcanbeseeninFigure4.Thenetworktakes200x200pixelimagesasinputsandhas9layers.Thelayersconsistofthreesequencesof\\ufb01ltering,poolingandlocalcon-trastnormalization(Jarrettetal.,2009).ThepoolingfunctionisL2poolingofthepreviouslayer(takingthesquareofthe\\ufb01lteringunits,summingthemupinasmallareaintheimage,andtakingthesquare-root).Thelocalcontrastnormalizationtakesinputsinasmallareaofthelowerlayer,subtractsthemeananddividesbythestandarddeviation.Thenetworkwas\\ufb01rsttrainedusinganunsuper-visedobjective:tryingtoreconstructtheinputwhilekeepingtheneuronssparse.Inthisphase,thenet-workwastrainedon20millionimagesrandomlysampledfromtheweb.Weresizedagivenimagesothatitsshortdimensionhas200pixels.Wethencroppeda\\ufb01xedsize200x200pixelimagerightatthecenteroftheresizedimage.Thismeanswemaydis-cardafractionofthelongdimensionoftheimage.Afterunsupervisedtraining,weusedIma-geNet(Dengetal.,2009)toadjustthefeaturesintheentirenetwork.TheImageNetdatasethas22,000categoriesand14millionimages.Thenumberofimagesineachcategoryisequalacrosscategories.The22,000categoriesareextractedfromWordNet.Tospeedupthesupervisedtrainingofthisnet-work,wemadeasimplemodi\\ufb01cationtothealgo-rithmdescribedinLeetal.(2012):addinga\\u201cbottle-neck\\u201dlayerinbetweenthelastlayerandtheclassi-\\ufb01er.toreducethenumberofconnections.Weaddedone\\u201cbottleneck\\u201dlayerwhichhas4,096unitsinbe-tweenthelastlayerofthenetworkandthesoftmaxlayer.Thisnewly-addedlayerisfullyconnectedtothepreviouslayerandhasalinearactivationfunc-tion.Thetotalnumberofconnectionsofthisnet-workisapproximately1.36billion.\\x0c213\\n\\nThenetworkwastrainedagainusingthesuper-visedobjectiveofclassifyingthe22,000classesinImageNet.Mostfeaturesinthenetworksarelocal,whichallowsmodelparallelism.DataparallelismbyasynchronousSGDwasalsoemployedasinLeetal.(2012).Theentiretraining,bothunsupervisedandsupervised,took8daysonalargeclusterofma-chines.Thisnetworkachieves18.3%precision@1onthefullImageNetdataset(ReleaseFall2011).Wewillusethefeaturesatthebottlenecklayerasthefeaturevectorzofanimage.Eachscaledandcroppedimageispresentedtoournetwork.Thenet-workthenperformsafeedforwardcomputationtocomputethevaluesofthebottlenecklayer.Thismeansthateveryimageisrepresentedbya\\ufb01xedlengthvectorof4,096dimensions.Notethatduringtraining,noalignedsentence-imagedatawasusedandtheImageNetclassesdonotfullyintersectwiththewordsusedinourdataset.5MultimodalMappingsTheprevioustwosectionsdescribedhowwecanmapsentencesintoad=50-dimensionalspaceandhowtoextracthighqualityimagefeaturevectorsof4096dimensions.Wenowde\\ufb01neour\\ufb01nalmulti-modalobjectivefunctionforlearningjointimage-sentencerepresentationswiththesemodels.OurtrainingsetconsistsofNimagesandtheirfeaturevectorsziandeachimagehas5sentencedescrip-tionssi1,...,si5forwhichweusetheDT-RNNtocomputevectorrepresentations.SeeFig.5forex-amplesfromthedataset.Fortraining,weuseamax-marginobjectivefunctionwhichintuitivelytrainspairsofcorrectimageandsentencevectorstohavehighinnerproductsandincorrectpairstohavelowinnerproducts.Letvi=WIzibethemappedimagevectorandyij=DTRNN\\u03b8(sij)thecomposedsen-tencevector.Wede\\ufb01neStobethesetofallsentenceindicesandS(i)thesetofsentenceindicescorre-spondingtoimagei.Similarly,Iisthesetofallim-ageindicesandI(j)istheimageindexofsentencej.ThesetPisthesetofallcorrectimage-sentencetrainingpairs(i,j).Therankingcostfunctiontominimizeisthen:J(WI,\\u03b8)=X(i,j)\\u2208PXc\\u2208S\\\\S(i)max(0,\\u2206\\u2212vTiyj+vTiyc)+X(i,j)\\u2208PXc\\u2208I\\\\I(j)max(0,\\u2206\\u2212vTiyj+vTcyj),(5)where\\u03b8arethelanguagecompositionmatrices,andbothsecondsumsareoverothersentencescom-ingfromdifferentimagesandviceversa.Thehyper-parameter\\u2206isthemargin.Themarginisfoundviacrossvalidationonthedevsetandusuallyaround1.The\\ufb01nalobjectivealsoincludestheregulariza-tionterm\\u03bb/left(k\\u03b8k22+kWIkF).Boththevisualmodelandthewordvectorlearningrequireaverylargeamountoftrainingdataandbothhaveahugenumberofparameters.Hence,topreventover\\ufb01tting,weassumetheirweightsare\\ufb01xedandonlytraintheDT-RNNparametersWI.Iflargertrainingcorporabecomeavailableinthefuture,trainingbothjointlybecomesfeasibleandwouldpresentaverypromis-ingdirection.Weuseamodi\\ufb01edversionofAda-Grad(Duchietal.,2011)foroptimizationofbothWIandtheDT-RNNaswellastheotherbaselines(exceptkCCA).Adagradhasachievedgoodperfor-mancepreviouslyinneuralnetworksmodels(Deanetal.,2012;Socheretal.,2013a).Wemodifyitbyresettingallsquaredgradientsumsto1every5epochs.Withbothimagesandsentencesinthesamemultimodalspace,wecaneasilyquerythemodelforsimilarimagesorsentencesby\\ufb01ndingthenearestneighborsintermsofnegativeinnerproducts.AnalternativeobjectivefunctionisbasedonthesquaredlossJ(WI,\\u03b8)=P(i,j)\\u2208Pkvi\\u2212yjk22.Thisrequiresanalternatingminimizationschemethat\\ufb01rsttrainsonlyWI,then\\ufb01xesWIandtrainstheDT-RNNweights\\u03b8andthenrepeatsthisseveraltimes.We\\ufb01ndthattheperformancewiththisob-jectivefunction(pairedwith\\ufb01ndingsimilarimagesusingEuclideandistances)isworseforallmodelsthanthemarginlossofEq.5.InadditionkCCAalsoperformsmuchbetterusinginnerproductsinthemultimodalspace.6ExperimentsWeusethedatasetofRashtchianetal.(2010)whichconsistsof1000images,eachwith5sentences.SeeFig.5forexamples.WeevaluateandcomparetheDT-RNNinthreedifferentexperiments.First,weanalyzehowwellthesentencevectorscapturesimilarityinvisualmeaning.ThenweanalyzeImageSearchwithQuerySentences:toqueryeachmodelwithasen-tenceinorderto\\ufb01ndanimageshowingthatsen-\\x0c214\\n\\n1. A woman and her dog watch the cameraman in their living with wooden floors.2. A woman sitting on the couch while a black faced dog runs across the floor.3. A woman wearing a backpack sits on a couch while a small dog runs on the hardwood floor next to her.4. A women sitting on a sofa while a small Jack Russell walks towards the camera.5. White and black small dog walks toward the camera while woman sits on couch, desk and computer seen     in the background as well as a pillow, teddy bear and moggie toy on the wood floor.1. A man in a cowboy hat check approaches a small red sports car.2. The back and left side of a red Ferrari and two men admiring it.3. The sporty car is admired by passer by.4. Two men next to a red sports car in a parking lot.5. Two men stand beside a red sports car.Figure5:Examplesfromthedatasetofimagesandtheirsentencedescriptions(Rashtchianetal.,2010).Sentencelengthvariesgreatlyanddifferentobjectscanbementioned\\ufb01rst.Hence,modelshavetobeinvarianttowordordering.tence\\u2019svisual\\u2018meaning.\\u2019ThelastexperimentDe-scribingImagesbyFindingSuitableSentencesdoesthereversesearchwherewequerythemodelwithanimageandtryto\\ufb01ndtheclosesttextualdescriptionintheembeddingspace.Inourcomparisontoothermethodswefocusonthosemodelsthatcanalsocompute\\ufb01xed,continu-ousvectorsforsentences.Inparticular,wecomparetotheRNNmodelonconstituencytreesofSocheretal.(2011a),astandardrecurrentneuralnetwork;asimplebag-of-wordsbaselinewhichaveragesthewords.AllmodelsusethewordvectorsprovidedbyHuangetal.(2012)anddonotupdatethemasdis-cussedabove.Modelsaretrainedwiththeircorre-spondinggradientsandbackpropagationtechniques.Astandardrecurrentmodelisusedwherethehiddenvectoratwordindextiscomputedfromthehiddenvectorattheprevioustimestepandthecurrentwordvector:ht=f(Whht\\u22121+Wxxt).Duringtraining,wetakethelasthiddenvectorofthesentencechainandpropagatetheerrorintothat.Itisalsothisvectorthatisusedtorepresentthesentence.Otherpossiblecomparisonsaretotheverydiffer-entmodelsmentionedintherelatedworksection.Thesemodelsusealotmoretask-speci\\ufb01cengineer-ing,suchasrunningobjectdetectorswithboundingboxes,attributeclassi\\ufb01ers,sceneclassi\\ufb01ers,CRFsforcomposingthesentences,etc.Anotherlineofworkuseslargesentence-imagealignedresources(Kuznetsovaetal.,2012),whereaswefocusoneas-ilyobtainabletrainingdataofeachmodalitysepa-ratelyandarathersmallmultimodalcorpus.Inourexperimentswesplitthedatainto800train-ing,100developmentand100testimages.Sincethereare5sentencesdescribingeachimage,wehave4000trainingsentencesand500testingsen-tences.Thedatasethas3020uniquewords,halfofwhichonlyappearonce.Hence,theunsupervised,pre-trainedsemanticwordvectorrepresentationsarecrucial.Wordvectorsarenot\\ufb01netunedduringtrain-ing.Hence,themainparametersaretheDT-RNN\\u2019sWl\\xb7,Wr\\xb7orthesemanticmatricesofwhichthereare141andtheimagemappingWI.ForbothDT-RNNstheweightmatricesareinitializedtoblockidentitymatricesplusGaussiannoise.Wordvectorsandhid-denvectorsaresetolength50.Usingthedevelop-mentsplit,wefound\\u03bb=0.08andthelearningrateofAdaGradto0.0001.Thebestmodelusesamar-ginof\\u2206=3.InspiredbySocherandFei-Fei(2010)andHo-doshetal.(2013)wealsocomparetokernelizedCanonicalCorrelationAnalysis(kCCA).Weusetheaverageofwordvectorsfordescribingsentencesandthesamepowerfulimagevectorsasbefore.WeusethecodeofSocherandFei-Fei(2010).Tech-nically,onecouldcombinetherecentlyintroduceddeepCCAAndrewetal.(2013)andtrainthere-cursiveneuralnetworkarchitectureswiththeCCAobjective.Weleavethistofuturework.Withlin-earkernels,kCCAdoeswellforimagesearchbutisworseforsentenceselfsimilarityanddescribingimageswithsentencesclose-byinembeddingspace.AllothermodelsaretrainedbyreplacingtheDT-RNNfunctioninEq.5.6.1SimilarityofSentencesDescribingtheSameImageInthisexperiment,we\\ufb01rstmapall500sentencesfromthetestsetintothemulti-modalspace.Thenforeachsentence,we\\ufb01ndthenearestneighborsen-\\x0c215\\n\\nSentencesSimilarityforImageModelMeanRankRandom101.1BoW11.8CT-RNN15.8RecurrentNN18.5kCCA10.7DT-RNN11.1SDT-RNN10.5ImageSearchModelMeanRankRandom52.1BoW14.6CT-RNN16.1RecurrentNN19.2kCCA15.9DT-RNN13.6SDT-RNN12.5DescribingImagesModelMeanRankRandom92.1BoW21.1CT-RNN23.9RecurrentNN27.1kCCA18.0DT-RNN19.2SDT-RNN16.9Table1:Left:Comparisonofmethodsforsentencesimilarityjudgments.Lowernumbersarebettersincetheyindicatethatsentencesdescribingthesameimagerankmorehighly(arecloser).Theranksareoutofthe500sentencesinthetestset.Center:Comparisonofmethodsforimagesearchwithquerysentences.Shownistheaveragerankofthesinglecorrectimagethatisbeingdescribed.Right:Averagerankofacorrectsentencedescriptionforaqueryimage.tencesintermsofinnerproducts.Wethensorttheseneighborsandrecordtherankorpositionofthenearestsentencethatdescribesthesameim-age.Ifalltheimageswereveryuniqueandthevi-sualdescriptionsclose-paraphrasesandconsistent,wewouldexpectaverylowrank.However,usuallyahandfulofimagesarequitesimilar(forinstance,therearevariousimagesofairplanes\\ufb02ying,parking,taxiingorwaitingontherunway)andsentencede-scriptionscanvarygreatlyindetailandspeci\\ufb01cityforthesameimage.Table1(left)showstheresults.Wecanseethataveragingthehighqualitywordvectorsalreadycap-turesalotofsimilarity.Thechainstructureofastandardrecurrentneuralnetperformsworstsinceitsrepresentationisdominatedbythelastwordsinthesequencewhichmaynotbeasimportantasear-lierwords.6.2ImageSearchwithQuerySentencesThisexperimentevaluateshowwellwecan\\ufb01ndim-agesthatdisplaythevisualmeaningofagivensen-tence.We\\ufb01rstmapaquerysentenceintothevectorspaceandthen\\ufb01ndimagesinthesamespaceusingsimpleinnerproducts.AsshowninTable1(center),thenewDT-RNNoutperformsallothermodels.6.3DescribingImagesbyFindingSuitableSentencesLastly,werepeattheaboveexperimentsbutwithrolesreversed.Foranimage,wesearchforsuitabletextualdescriptionsagainsimplyby\\ufb01ndingclose-bysentencevectorsinthemulti-modalembeddingspace.Table1(right)showsthattheDT-RNNagainoutperformsrelatedmodels.Fig.2assignedtoim-ImageSearchModelmRankBoW24.7CT-RNN22.2RecurrentNN28.4kCCA13.7DT-RNN13.3SDT-RNN15.8DescribingImagesModelmRankBoW30.7CT-RNN29.4RecurrentNN31.4kCCA38.0DT-RNN26.8SDT-RNN37.5Table2:ResultsofmultimodalrankingwhenmodelsaretrainedwithasquarederrorlossandusingEuclideandis-tanceinthemultimodalspace.Betterperformanceisreachedforallmodelswhentrainedinamax-marginlossandusinginnerproductsasintheprevioustable.ages.Theaveragerankingof25.3foracorrectsen-tencedescriptionisoutof500possiblesentences.Arandomassignmentwouldgiveanaveragerankingof100.6.4Analysis:SquaredErrorLossvs.MarginLossWeanalyzethein\\ufb02uenceofthemultimodallossfunctionontheperformance.Inaddition,wecom-pareusingEuclideandistancesinsteadofinnerprod-ucts.Table2showsthatperformanceisworseforallmodelsinthissetting.6.5Analysis:RecallatnvsMeanRankHodoshetal.(2013)andotherrelatedworkusere-callatnasanevaluationmeasure.Recallatncap-tureshowoftenoneofthetopnclosestvectorswereacorrectimageorsentenceandgivesagoodintu-itionofhowamodelwouldperforminarankingtaskthatpresentsnsuchresultstoauser.Below,wecomparethreecommonlyusedandhighperformingmodels:bagofwords,kCCAandourSDT-RNNon\\x0c216\\n\\nA gray convertible sports car is parked in front of the trees.A close-up view of the headlights of a blue old-fashioned car.Black shiny sports car parked on concrete driveway.Five cows grazing on a patch of grass between two roadways.A jockey rides a brown and white horse in a dirt corral.A young woman is riding a Bay hose in a dirt riding-ring.A white bird pushes a miniature teal shopping cart.A person rides a brown horse.A motocross bike with rider flying through the air.White propeller plane parked in middle of grassy field.The white jet with its landing gear down flies in the blue sky.An elderly woman catches a ride on the back of the bicycle.A green steam train running down the tracks.Steamy locomotive speeding thou the forest.A steam engine comes down a train track near trees.A double decker bus is driving by Big Ben in London.People in an outrigger canoe sail on emerald green water.Two people sailing a small white sail boat.behind a cliff, a boat sails awayTourist move in on Big Ben on a typical overcast London day.A group of people sitting around a table on a porch.A group of four people walking past a giant mushroom.A man and women smiling for the camera in a kitchen.A group of men sitting around a table drinking while a man behind stands pointing.Figure6:ImagesandtheirsentencedescriptionsassignedbytheDT-RNN.ImageSearchModelmRank4R@15R@55R@105BoW14.615.842.260.0kCCA15.916.441.458.0SDT-RNN12.516.446.665.6DescribingImagesBoW21.119.038.057.0kCCA18.021.047.061.0SDT-RNN16.923.045.063.0Table3:Evaluationcomparisonbetweenmeanrankoftheclosestcorrectimageorsentence(lowerisbetter4)withrecallatdifferentthresholds(higherisbetter,5).Withoneexception(R@5,bottomtable),theSDT-RNNoutperformstheothertwomodelsandallothermodelswedidnotincludehere.thisdifferentmetric.Table3showsthatthemea-suresdocorrelatewellandtheSDT-RNNalsoper-formsbestonthemultimodalrankingtaskswhenevaluatedwiththismeasure.6.6ErrorAnalysisInordertounderstandthemainproblemswiththecomposedsentencevectors,weanalyzethesen-tencesthathavetheworstnearestneighborrankbe-tweeneachother.We\\ufb01ndthatthemainfailuremodeoftheSDT-RNNoccurswhenasentencethatshoulddescribethesameimagedoesnotuseaverbbuttheothersentencesofthatimagedoincludeaverb.Forexample,thefollowingsentencepairhasvectorsthatareveryfarapartfromeachothereventhoughtheyaresupposedtodescribethesameimage:1.Ablueandyellowairplane\\ufb02yingstraightdownwhileemittingwhitesmoke2.AirplaneindivepositionGenerally,aslongasbothsentenceseitherhaveaverbordonot,theSDT-RNNismorerobusttodif-ferentsentencelengthsthanbagofwordsrepresen-tations.6.7ModelAnalysis:SemanticCompositionMatricesThebestmodelusescompositionmatricesbasedonsemanticrelationshipsfromthedependencyparser.WegivesomeinsightsintowhatthemodellearnsbylistingthecompositionmatriceswiththelargestFrobeniusnorms.Intuitively,thesematriceshavelearnedlargerweightsthatarebeingmultipliedwiththechildvectorinthetreeandhencethatchildwillhavemoreweightinthe\\ufb01nalcomposedparentvec-tor.IndecreasingorderofFrobeniusnorm,there-lationshipmatricesare:nominalsubject,possessionmodi\\ufb01er(e.g.their),passiveauxiliary,prepositionat,prepositioninfrontof,passiveauxiliary,passivenominalsubject,objectofpreposition,prepositioninandprepositionon.Themodellearnsthatnounsareveryimportantaswellastheirspatialprepositionsandadjectives.7ConclusionWeintroducedanewrecursiveneuralnetworkmodelthatisbasedondependencytrees.Foreval-uation,weusethechallengingtaskofmappingsen-tencesandimagesintoacommonspacefor\\ufb01ndingonefromtheother.Ournewmodeloutperformsbaselinesandothercommonlyusedmodelsthatcancomputecontinuousvectorrepresentationsforsen-tences.Incomparisontorelatedmodels,theDT-RNNismoreinvariantandrobusttosurfacechangessuchaswordorder.\\x0c217\\n\\nReferencesG.Andrew,R.Arora,K.Livescu,andJ.Bilmes.2013.Deepcanonicalcorrelationanalysis.InICML,At-lanta,Georgia.K.Barnard,P.Duygulu,N.deFreitas,D.Forsyth,D.Blei,andM.Jordan.2003.Matchingwordsandpictures.JMLR.M.BaroniandA.Lenci.2010.Distributionalmem-ory:Ageneralframeworkforcorpus-basedsemantics.ComputationalLinguistics,36(4):673\\u2013721.R.CollobertandJ.Weston.2008.Auni\\ufb01edarchi-tecturefornaturallanguageprocessing:deepneuralnetworkswithmultitasklearning.InProceedingsofICML,pages160\\u2013167.F.Costa,P.Frasconi,V.Lombardo,andG.Soda.2003.Towardsincrementalparsingofnaturallanguageusingrecursiveneuralnetworks.AppliedIntelligence.M.deMarneffe,B.MacCartney,andC.D.Manning.2006.Generatingtypeddependencyparsesfromphrasestructureparses.InLREC.J.Dean,G.S.Corrado,R.Monga,K.Chen,M.Devin,Q.V.Le,M.Z.Mao,M.Ranzato,A.Senior,P.Tucker,K.Yang,andA.Y.Ng.2012.Largescaledistributeddeepnetworks.InNIPS.J.Deng,W.Dong,R.Socher,L.-J.Li,K.Li,andL.Fei-Fei.2009.ImageNet:ALarge-ScaleHierarchicalIm-ageDatabase.InCVPR.J.Duchi,E.Hazan,andY.Singer.2011.Adaptivesub-gradientmethodsforonlinelearningandstochasticop-timization.JMLR,12,July.P.Duygulu,K.Barnard,N.deFreitas,andD.Forsyth.2002.Objectrecognitionasmachinetranslation.InECCV.A.Farhadi,M.Hejrati,M.A.Sadeghi,P.Young,C.Rashtchian,J.Hockenmaier,andD.Forsyth.2010.Everypicturetellsastory:Generatingsentencesfromimages.InECCV.Y.FengandM.Lapata.2013.Automaticcaptiongen-erationfornewsimages.IEEETrans.PatternAnal.Mach.Intell.,35.A.Frome,G.Corrado,J.Shlens,S.Bengio,J.Dean,M.Ranzato,andT.Mikolov.2013.Devise:Adeepvisual-semanticembeddingmodel.InNIPS.C.GollerandA.K\\xa8uchler.1996.Learningtask-dependentdistributedrepresentationsbybackpropaga-tionthroughstructure.InProceedingsoftheInterna-tionalConferenceonNeuralNetworks.E.Grefenstette,G.Dinu,Y.-Z.Zhang,M.Sadrzadeh,andM.Baroni.2013.Multi-stepregressionlearningforcompositionaldistributionalsemantics.InIWCS.A.GuptaandL.S.Davis.2008.Beyondnouns:Exploit-ingprepositionsandcomparativeadjectivesforlearn-ingvisualclassi\\ufb01ers.InECCV.M.Hodosh,P.Young,andJ.Hockenmaier.2013.Fram-ingimagedescriptionasarankingtask:Data,mod-elsandevaluationmetrics.J.Artif.Intell.Res.(JAIR),47:853\\u2013899.E.H.Huang,R.Socher,C.D.Manning,andA.Y.Ng.2012.ImprovingWordRepresentationsviaGlobalContextandMultipleWordPrototypes.InACL.K.Jarrett,K.Kavukcuoglu,M.A.Ranzato,andY.Le-Cun.2009.Whatisthebestmulti-stagearchitectureforobjectrecognition?InICCV.P.Blunsom.K.M.Hermann.2013.Theroleofsyntaxinvectorspacemodelsofcompositionalsemantics.InACL.A.Krizhevsky,I.Sutskever,andG.E.Hinton.2012.Imagenetclassi\\ufb01cationwithdeepconvolutionalneuralnetworks.InNIPS.G.Kulkarni,V.Premraj,S.Dhar,S.Li,Y.Choi,A.C.Berg,andT.L.Berg.2011.Babytalk:Understandingandgeneratingimagedescriptions.InCVPR.N.Kumar,A.C.Berg,P.N.Belhumeur,,andS.K.Na-yar.2009.Attributeandsimileclassi\\ufb01ersforfacever-i\\ufb01cation.InICCV.P.Kuznetsova,V.Ordonez,A.C.Berg,T.L.Berg,andYejinChoi.2012.Collectivegenerationofnaturalimagedescriptions.InACL.Q.V.Le,M.A.Ranzato,R.Monga,M.Devin,K.Chen,G.S.Corrado,J.Dean,andA.Y.Ng.2012.Build-inghigh-levelfeaturesusinglargescaleunsupervisedlearning.InICML.T.Mikolov,W.Yih,andG.Zweig.2013.Linguisticregularitiesincontinuousspacewordrepresentations.InHLT-NAACL.J.MitchellandM.Lapata.2010.Compositionindis-tributionalmodelsofsemantics.CognitiveScience,34(8):1388\\u20131429.J.Ngiam,A.Khosla,M.Kim,J.Nam,H.Lee,andA.Y.Ng.2011.Multimodaldeeplearning.InICML.V.Ordonez,G.Kulkarni,andT.L.Berg.2011.Im2text:Describingimagesusing1millioncaptionedpho-tographs.InNIPS.J.B.Pollack.1990.Recursivedistributedrepresenta-tions.Arti\\ufb01cialIntelligence,46,November.C.Rashtchian,P.Young,M.Hodosh,andJ.Hocken-maier.2010.CollectingimageannotationsusingAmazon\\u2019sMechanicalTurk.InWorkshoponCreat-ingSpeechandLanguageDatawithAmazon\\u2019sMTurk.R.SocherandL.Fei-Fei.2010.Connectingmodalities:Semi-supervisedsegmentationandannotationofim-agesusingunalignedtextcorpora.InCVPR.R.Socher,C.D.Manning,andA.Y.Ng.2010.Learningcontinuousphraserepresentationsandsyntacticpars-ingwithrecursiveneuralnetworks.InProceedingsoftheNIPS-2010DeepLearningandUnsupervisedFea-tureLearningWorkshop.\\x0c218\\n\\nR.Socher,E.H.Huang,J.Pennington,A.Y.Ng,andC.D.Manning.2011a.DynamicPoolingandUnfold-ingRecursiveAutoencodersforParaphraseDetection.InNIPS.R.Socher,C.Lin,A.Y.Ng,andC.D.Manning.2011b.ParsingNaturalScenesandNaturalLanguagewithRecursiveNeuralNetworks.InICML.R.Socher,J.Pennington,E.H.Huang,A.Y.Ng,andC.D.Manning.2011c.Semi-SupervisedRecursiveAutoencodersforPredictingSentimentDistributions.InEMNLP.R.Socher,B.Huval,C.D.Manning,andA.Y.Ng.2012.SemanticCompositionalityThroughRecursiveMatrix-VectorSpaces.InEMNLP.R.Socher,J.Bauer,C.D.Manning,andA.Y.Ng.2013a.ParsingWithCompositionalVectorGrammars.InACL.R.Socher,M.Ganjoo,C.D.Manning,andA.Y.Ng.2013b.Zero-ShotLearningThroughCross-ModalTransfer.InNIPS.R.Socher,M.Ganjoo,H.Sridhar,O.Bastani,andA.Y.Ng.C.D.Manningand.2013c.Zero-shotlearn-ingthroughcross-modaltransfer.InProceedingsoftheInternationalConferenceonLearningRepresenta-tions(ICLR,WorkshopTrack).N.SrivastavaandR.Salakhutdinov.2012.Multimodallearningwithdeepboltzmannmachines.InNIPS.A.Torralba,K.P.Murphy,andW.T.Freeman.2010.Usingtheforesttoseethetrees:exploitingcontextforvisualobjectdetectionandlocalization.Communica-tionsoftheACM.P.D.TurneyandP.Pantel.2010.Fromfrequencytomeaning:Vectorspacemodelsofsemantics.JournalofArti\\ufb01cialIntelligenceResearch,37:141\\u2013188.B.Yao,X.Yang,L.Lin,M.W.Lee,andS.-C.Zhu.2010.I2t:imageparsingtotextdescription.IEEEXplore.\\x0c', u'On Deep Multi-View Representation Learning\\n\\nWeiran Wang\\nToyota Technological Institute at Chicago\\n\\nRaman Arora\\nJohns Hopkins University\\n\\nKaren Livescu\\nToyota Technological Institute at Chicago\\n\\nJeff Bilmes\\nUniversity of Washington, Seattle\\n\\nAbstract\\n\\nWe consider learning representations (features)\\nin the setting in which we have access to mul-\\ntiple unlabeled views of the data for representa-\\ntion learning while only one view is available at\\ntest time. Previous work on this problem has pro-\\nposed several techniques based on deep neural\\nnetworks, typically involving either autoencoder-\\nlike networks with a reconstruction objective or\\npaired feedforward networks with a correlation-\\nbased objective. We analyze several techniques\\nbased on prior work, as well as new variants, and\\ncompare them experimentally on visual, speech,\\nand language domains. To our knowledge this\\nis the \\ufb01rst head-to-head comparison of a vari-\\nety of such techniques on multiple tasks. We\\n\\ufb01nd an advantage for correlation-based represen-\\ntation learning, while the best results on most\\ntasks are obtained with our new variant, deep\\ncanonically correlated autoencoders (DCCAE).\\n\\ntest\\n\\ntime.\\n\\n1. Introduction\\nIn many applications, we have access to multiple \\u201cviews\\u201d\\nof data at training time while only one view is avail-\\nable at\\nThe views can be multiple mea-\\nsurement modalities, such as simultaneously recorded\\naudio + video (Kidron et al., 2005; Chaudhuri et al.,\\n2009), audio + articulation (Arora & Livescu, 2013), im-\\nages + text (Hardoon et al., 2004; Socher & Li, 2010;\\nHodosh et al., 2013),\\nin two lan-\\n\\nor parallel\\n\\ntext\\n\\nProceedings of the 32 nd International Conference on Machine\\nLearning, Lille, France, 2015. JMLR: W&CP volume 37. Copy-\\nright 2015 by the author(s).\\n\\nWEIRANWANG@TTIC.EDU\\n\\nARORA@CS.JHU.EDU\\n\\nKLIVESCU@TTIC.EDU\\n\\nBILMES@EE.WASHINGTON.EDU\\n\\nguages (Vinokourov et al., 2003; Haghighi et al., 2008;\\nChandar et al., 2014; Faruqui & Dyer, 2014), but may also\\nbe different information extracted from the same source,\\nsuch as words + context (Pennington et al., 2014) or docu-\\nment text + text of inbound hyperlinks (Bickel & Scheffer,\\n2004). The presence of multiple information sources\\npresents an opportunity to learn better representations (fea-\\ntures) by analyzing multiple views simultaneously. Typical\\napproaches are based on learning a feature transformation\\nof the \\u201cprimary\\u201d view (the one available at test time) that\\ncaptures useful information from the second view using a\\npaired two-view training set. Under certain assumptions,\\ntheoretical results exist showing the advantages of multi-\\nview techniques for downstream tasks (Kakade & Foster,\\n2007; Foster et al., 2009; Chaudhuri et al., 2009).\\n\\n(DNNs),\\n\\ninspired by their\\n\\nSeveral\\nrecently proposed approaches for multi-view\\nrepresentation learning are based on deep neural net-\\nworks\\nin typ-\\nical unsupervised (single-view)\\nfeature learning set-\\ntings (Hinton & Salakhutdinov, 2006). Compared to kernel\\nmethods, DNNs can more easily process large amounts of\\ntraining data and, as a parameteric method, do not require\\nreferring to the training set at test time.\\n\\nsuccess\\n\\nThere are two main training criteria (objectives) that have\\nbeen applied for DNN-based multi-view representation\\nlearning. One is based on autoencoders, where the ob-\\njective is to learn a compact representation that best\\nreconstructs the inputs (Ngiam et al., 2011). The sec-\\nond approach is based on canonical correlation analysis\\n(CCA, Hotelling, 1936), which learns features in two views\\nthat are maximally correlated. CCA and its kernel ex-\\ntension (Lai & Fyfe, 2000; Akaho, 2001; Bach & Jordan,\\n2002; Hardoon et al., 2004) have long been the workhorse\\nfor multi-view feature learning and dimensionality re-\\n\\n\\x0cOn Deep Multi-View Representation Learning\\n\\nReconstructed x Reconstructed y\\n\\np\\n\\nq\\n\\nf\\n\\nx\\n\\n2\\nw\\ne\\ni\\nV\\n\\nU\\n\\nView 1\\n\\nf\\n\\ng\\n\\nV\\n\\nx\\n\\ny\\n\\n(a) SplitAE\\n\\n(b) DCCA\\n\\nReconstructed x\\n\\np\\n\\nReconstructed y\\n\\nq\\n\\n2\\nw\\ne\\ni\\nV\\nU\\n\\nView 1\\ng\\n\\nf\\n\\nV\\n\\nx\\n\\ny\\n(c) DCCAE/CorrAE/DistAE\\n\\nFigure1. Schematic diagram of DNN-based multi-view representation learning models.\\n\\nduction (Vinokourov et al., 2003; Kakade & Foster, 2007;\\nSocher & Li, 2010; Dhillon et al., 2011). Multiple neu-\\nral network based CCA-like models have been pro-\\nposed (Lai & Fyfe, 1999; Hsieh, 2000), but\\nthe full\\nDNN extension of CCA,\\ntermed deep CCA (DCCA,\\nAndrew et al., 2013) has been developed only recently.\\n\\ntest\\n\\ntime.\\n\\nclassi\\ufb01cation/recognition\\n\\n2008; Chaudhuri et al.,\\n(Dhillon et al.,\\n\\nThe contributions of this paper are as follows. We\\ncompare several DNN-based approaches, along with\\nlinear and kernel CCA,\\nin the unsupervised multi-\\nview feature learning setting where the second view\\nis not available at\\nPrior work has shown\\nthe bene\\ufb01t of multi-view methods on tasks such as\\nretrieval (Vinokourov et al., 2003; Hardoon et al., 2004;\\nclustering\\nSocher & Li, 2010; Hodosh et al., 2013),\\n(Blaschko & Lampert,\\n2009)\\n2011;\\nand\\nArora & Livescu, 2013; Ngiam et al., 2011). However, to\\nour knowledge no head-to-head comparison on multiple\\ntasks has previously been done. We address this gap by\\ncomparing approaches based on prior work, as well as\\nnew variants developed here. Empirically, we \\ufb01nd that\\nCCA-based approaches tend to outperform unconstrained\\nreconstruction-based approaches. One of the new methods\\nwe propose, a DNN-based model combining CCA and\\nautoencoder-based terms, is the consistent winner across\\nseveral tasks. To facilitate future work, we release our im-\\nplementations and a new benchmark dataset of simulated\\ntwo-view data based on MNIST.\\n\\n2. DNN-based multiview feature learning\\nNotations\\nIn the multi-view feature learning scenario, we\\nhave access to paired observations from two views, de-\\nnoted (x1, y1), . . . , (xN , yN ), where N is the sample size,\\nxi \\u2208 RDx and yi \\u2208 RDy for i = 1, . . . , N. We also denote\\nthe data matrices for each view by X = [x1, . . . , xN ] and\\nY = [y1, . . . , yN ]. We use bold-face letters, e.g. f, to de-\\nnote mappings implemented by kernel machines or DNNs,\\nwith a corresponding set of learnable parameters, denoted,\\ne.g., Wf . We write the f-projected (view 1) data matrix\\nas f (X) = [f (x1), . . . , f (xN )]. The dimensionality of the\\nprojection (feature) is denoted L.\\n\\nWe now describe the DNN-based multi-view feature\\n\\nlearning algorithms considered here, with corresponding\\nschematic diagrams given in Fig. 1.\\n\\n2.1. Split autoencoders (SplitAE)\\nNgiam et al. (2011) propose to extract shared representa-\\ntions by reconstructing both views from the one view that\\nis available at test time. In this approach, the feature extrac-\\ntion network f is shared while the reconstruction networks\\np and q are separate for each view. We refer to this model\\nas a split autoencoder (SplitAE), shown schematically in\\nFig. 1 (a). The objective of this model is the sum of recon-\\nstruction errors for the two views (we omit the \\u21132 weight\\ndecay term for all models in this section):\\n\\nmin\\n\\nWf ,Wp,Wq\\n\\n1\\nN\\n\\nN\\n\\nXi=1\\n\\n(kxi \\u2212 p(f (xi))k2 + kyi \\u2212 q(f (xi))k2).\\n\\nThe intuition for this model is that the shared representation\\ncan be extracted from a single view, and can be used to re-\\nconstruct all views. 1 The autoencoder loss is the empirical\\nexpectation of the loss incurred at each training sample, and\\nthus stochastic gradient descent (SGD) can be used to op-\\ntimize the objective ef\\ufb01ciently, with the gradient estimated\\nfrom a small minibatch of samples.\\n\\n2.2. Deep canonical correlation analysis (DCCA)\\nAndrew et al. (2013) propose a DNN extension of CCA\\ntermed deep CCA (DCCA; see Fig. 1 (b). In DCCA, two\\nDNNs f and g are used to extract nonlinear features for\\neach view and the canonical correlation between the ex-\\ntracted features f (X) and g(Y) is maximized:\\n\\n(1)\\n\\nmax\\n\\nWf ,Wg,U,V\\n\\n1\\nN\\n\\ntr(cid:0)U\\u22a4f (X)g(Y)\\u22a4V(cid:1)\\n\\nN\\n\\ns.t. U\\u22a4(cid:18) 1\\nV\\u22a4(cid:18) 1\\nu\\u22a4\\ni f (X)g(Y)\\u22a4vj = 0,\\n\\nf (X)f (X)\\u22a4 + rxI(cid:19) U = I,\\ng(Y)g(Y)\\u22a4 + ryI(cid:19) V = I,\\n\\nfor\\n\\ni 6= j,\\n\\nN\\n\\n1The authors also propose a bimodal deep autoencoder com-\\nbining DNN transformed features from both views; this model is\\nmore natural for the multimodal fusion setting where both views\\nare available at test time. Empirically, Ngiam et al. (2011) report\\nthat SplitAE tends to work better in the multi-view setting.\\n\\n\\x0cOn Deep Multi-View Representation Learning\\n\\nwhere U = [u1, . . . , uL] and V = [v1, . . . , vL] are\\nthe CCA directions that project the DNN outputs and\\n(rx, ry) > 0 are regularization parameters for sample\\ncovariance estimation (Bie & Moor, 2003; Hardoon et al.,\\n2004). In DCCA, U\\u22a4f (\\xb7) is the \\ufb01nal projection mapping\\nused for testing. One intuition for CCA-based objectives\\nis that, while it may be dif\\ufb01cult to accurately reconstruct\\none view from the other view, it may be easier, and may be\\nsuf\\ufb01cient, to learn a predictor of a function (or subspace)\\nof the second view. In addition, it should be helpful for the\\nlearned dimensions within each view to be uncorrelated so\\nthat they provide complementary information.\\n\\nOptimization The DCCA objective couples all training\\nsamples through the whitening constraints, so stochastic\\ngradient descent (SGD) cannot be applied in a standard\\nIt has been observed by Wang et al. (2015) that\\nway.\\nDCCA can still be optimized ef\\ufb01ciently as long as the gra-\\ndient is estimated using a suf\\ufb01ciently large minibatch (with\\ngradient formulas given in Andrew et al., 2013).\\nIntu-\\nitively, this approach works because a large minibatch con-\\ntains enough information for estimating the covariances.\\n\\n2.3. Deep canonically correlated autoencoders\\n\\n(DCCAE)\\n\\nInspired by both CCA and reconstruction-based objectives,\\nwe propose a new model that consists of two autoencoders\\nand optimizes the combination of canonical correlation be-\\ntween the learned bottleneck representations and the recon-\\nstruction errors of the autoencoders.\\nIn other words, we\\noptimize the following objective\\n\\nWf ,W\\n\\ng,Wp,Wq,U,V\\n\\n1\\nN\\n\\nmin\\n\\n\\u2212\\n\\ntr(cid:0)U\\u22a4f (X)g(Y)\\u22a4V(cid:1)\\n(kxi \\u2212 p(f (xi))k2 + kyi \\u2212 q(g(yi))k2)\\n\\n(2)\\n\\n+\\n\\n\\u03bb\\nN\\n\\nN\\n\\nXi=1\\n\\ns.t.\\n\\nthe same constraints in (1),\\n\\nwhere \\u03bb > 0 is a trade-off parameter. Alternatively, this\\napproach can be seen as adding an autoencoder regulariza-\\ntion term to DCCA. We call this approach deep canonically\\ncorrelated autoencoders (DCCAE). Similarly to DCCA, we\\napply stochastic optimization to the DCCAE objective; the\\nstochastic gradient is the sum of the gradient for the au-\\ntoencoder term and the gradient for the DCCA term.\\nInterpretations CCA maximizes the mutual informa-\\ntion between the projected views for certain distributions\\n(Borga, 2001), while training an autoencoder to minimize\\nreconstruction error amounts to maximizing a lower bound\\non the mutual information between inputs and learned fea-\\ntures (Vincent et al., 2010). The DCCAE objective offers\\na trade-off between the information captured in the (in-\\nput, feature) mapping within each view on the one hand,\\nand the information in the (feature, feature) relationship\\n\\nacross views on the other. Intuitively, this is the same prin-\\nciple as the information bottleneck method (Tishby et al.,\\n1999), and indeed, in the case of Gaussian variables, the\\ninformation bottleneck method \\ufb01nds the same subspaces as\\nCCA (Chechik et al., 2005).\\n\\n2.4. Correlated autoencoders (CorrAE)\\nIn the next model, we replace the CCA term in the DC-\\nCAE objective with the sum of the scalar correlations be-\\ntween the pairs of learned dimensions across views, which\\nis an alternative measure of agreement between views. In\\nother words, the feature dimensions within each view are\\nnot constrained to be uncorrelated with each other. This\\nmodel is intended to test how important the original CCA\\nconstraint is. We call this model correlated autoencoders\\n(CorrAE), shown in Fig. 1 (c). Its objective can be equiva-\\nlently written in a constrained form as\\n\\nWf ,W\\n\\ng,Wp,Wq,U,V\\n\\n1\\nN\\n\\nmin\\n\\nN\\n\\n\\u2212\\n\\ntr(cid:0)U\\u22a4f (X)g(Y)\\u22a4V(cid:1)\\n(kxi \\u2212 p(f (xi))k2 + kyi \\u2212 q(g(yi))k2)\\n\\n+\\n\\n\\u03bb\\nN\\n\\ns.t. u\\u22a4\\n\\nXi=1\\ni f (X)f (X)\\u22a4ui = v\\u22a4\\n\\ni g(Y)g(Y)\\u22a4vi = N, 1 \\u2264 i \\u2264 L.\\n\\n(3)\\n\\nwhere \\u03bb > 0 is a trade-off parameter. It is clear that the\\nconstraint set in (3) is a relaxed version of that of (2). We\\nwill demonstrate that this difference results in a large per-\\nformance gap. We apply the same optimization strategy of\\nDCCAE to CorrAE.\\n\\nCorrAE is similar to the model of Chandar et al. (2014),\\nwho try to learn vectorial word representations using par-\\nallel corpora from two languages. They use DNNs in each\\nview (language) to predict the bag-of-words representation\\nof the input sentences, or that of the paired sentences from\\nthe other view, while encouraging the learned bottleneck\\nlayer representations to be highly correlated.\\n\\n2.5. Minimum-distance autoencoders (DistAE)\\nThe CCA objective can be seen as minimzing the dis-\\ntance between the learned projections of the two views,\\nwhile satisfying the whitening constraints for the projec-\\ntions (Hardoon et al., 2004). The constraints complicate\\nthe optimization of CCA-based objectives, as pointed out\\nabove. This observation motivates us to consider additional\\nobjectives that decompose into sums over training exam-\\nples, while maintaining the intuition of the CCA objective\\nas a reconstruction error between two mappings. Here we\\nconsider a variant we refer to as minimum-distance autoen-\\ncoders (DistAE) that optimizes the following objective:\\n\\nmin\\n\\nWf ,Wg,Wp,Wq\\n\\n1\\nN\\n\\nN\\n\\nXi=1\\n\\nkf (xi) \\u2212 g(yi)k2\\n\\nkf (xi)k2 + kg(yi)k2\\n\\n+\\n\\n\\u03bb\\nN\\n\\nN\\n\\nXi=1\\n\\n(kxi \\u2212 p(f (xi))k2 + kyi \\u2212 q(g(yi))k2)\\n\\n(4)\\n\\n\\x0cOn Deep Multi-View Representation Learning\\n\\nwhich is a weighted combination of reconstruction errors\\nof two autoencoders and the average discrepancy between\\nthe projected sample pairs. The denominator of the discrep-\\nancy term is used to keep the optimization from improving\\nthe objective by simply scaling down the projections (al-\\nthough they can never become identically zero due to the\\nreconstruction terms). This objective is unconstrained and\\nis the expectation of loss incurred at each training sample,\\nso normal SGD applies using small minibatches.\\n\\n3. Related work\\nWe focus on related work based on feed-forward\\nneural networks and the kernel extension of CCA.\\nThere has also been work using deep Boltzmann ma-\\n(Srivastava & Salakhutdinov, 2014; Sohn et al.,\\nchines\\n2014), where several layers of restricted Boltzmann ma-\\nchines (RBM) are stacked to represent each view, with an\\nadditional top layer that provides the joint representation.\\nThese are probabilistic graphical models, for which the\\nmaximum likelihood objective is intractable and the train-\\ning procedures are more complex. Although probabilistic\\nmodels have some advantages (e.g., dealing with missing\\nvalues and generating samples in a natural way), DNN-\\nbased models are tractable and ef\\ufb01cient to train.\\n\\n3.1. DNN feature learning using CCA-like objectives\\nThere have been several approaches to multi-view repre-\\nsentation learning using neural networks with an objective\\nsimilar to that of CCA. Under the assumption that the two\\nviews share a common cause (e.g., depth is a common\\ncause for adjacent patches of images), Becker & Hinton\\n(1992) maximize a sample-based estimate of mutual infor-\\nmation between the common signal and the average outputs\\nof neural networks for the two views. It is less straightfor-\\nward, however, to develop sample-based estimates of mu-\\ntual information in higher dimensions.\\n\\nLai & Fyfe (1999) propose to optimize the correlation\\n(rather than canonical correlation) between the outputs of\\nnetworks for each view, subject to scale constraints on each\\noutput dimension.\\nInstead of directly solving this con-\\nstrained formulation, the authors apply Lagrangian relax-\\nation and solve the resulting unconstrained objective using\\nSGD. The objective in this work is different from that of\\nCCA, as there are no constraints that the learned dimen-\\nsions within each view be uncorrelated. Hsieh (2000) pro-\\nposes a neural network based model involving three mod-\\nules: one module for extracting a pair of maximally cor-\\nrelated one-dimensional features for the two views; and a\\nsecond and third module for reconstructing the original in-\\nputs of the two views from the learned features.\\nIn this\\nmodel, the feature dimensions can be learned one after an-\\nother, each learned using as input the reconstruction resid-\\nual from previous dimensions. The three modules are each\\n\\ntrained separately, so there is no uni\\ufb01ed objective.\\n\\nKim et al. (2012) propose an algorithm that \\ufb01rst uses deep\\nbelief networks and the autoencoder objective to extract\\nfeatures for two languages independently, and then applies\\nlinear CCA to the learned features (activations at the bot-\\ntleneck layer of the autoencoders) to learn the \\ufb01nal repre-\\nsentation. In this two-step approach, the DNN weight pa-\\nrameters are not updated to optimize the CCA objective.\\n\\ni=1\\n\\nated on the training set, i.e., U\\u22a4f (\\xb7) = PN\\n\\n3.2. Kernel CCA\\nAnother nonlinear extension of CCA is kernel CCA\\n(KCCA, Lai & Fyfe, 2000; Akaho, 2001; Melzer et al.,\\n2001; Bach & Jordan, 2002; Hardoon et al., 2004). KCCA\\ncorresponds to using (potentially in\\ufb01nite-dimensional)\\nfeature mappings induced by positive-de\\ufb01nite kernels\\nkx(\\xb7, \\xb7), ky(\\xb7, \\xb7) (e.g., Gaussian RBF kernels k(a, b) =\\ne\\u2212ka\\u2212bk2/2s2 where s is the kernel width), and learning\\na linear CCA on them with linear projections (U, V).\\nFrom the representer theorem of reproducing kernel Hilbert\\nspaces (Sch\\xa8olkopf & Smola, 2001), the \\ufb01nal projection can\\nbe written as linear combinations of kernel functions evalu-\\n\\u03b1ikx(x, xi)\\nwhere \\u03b1i \\u2208 RL, i = 1, . . . , N; one can then work with\\nthe kernel matrices and directly solve for the linear coef-\\ni=1. KCCA involves an N \\xd7 N eigenvalue\\n\\ufb01cients {\\u03b1i}N\\nproblem and so is challenging in both memory (storing the\\nkernel matrices) and time (solving the eigenvalue system\\nna\\xa8\\u0131vely costs O(N 3)). To alleviate these issues, various\\nkernel approximation techniques have been proposed, such\\nas random Fourier features (Lopez-Paz et al., 2014) and\\nthe Nystr\\xa8om approximation (Williams & Seeger, 2001). In\\nrandom Fourier features, we randomly sample M Dx/Dy-\\ndimensional vectors from a Gaussian distribution and map\\nthe original inputs to RM by computing the dot product\\nwith the random samples followed by an elementwise co-\\nsine; the inner products between transformed samples ap-\\nproximate kernel similarities between original inputs.\\nIn\\nthe Nystr\\xa8om approximation, we randomly select M train-\\ning samples and construct the M \\xd7 M kernel matrix for\\nthese samples, and use its eigen-decomposition to obtain\\na rank-M approximation of the full kernel matrix. Both\\ntechniques produce rank-M approximations of the kernel\\nmatrices with computational complexity O(M 3 + M 2N );\\nbut random Fourier features are data independent and more\\nef\\ufb01cient to generate. Other approximation techniques such\\nas incomplete Cholesky decomposition (Bach & Jordan,\\n2002), partial Gram-Schmidt (Hardoon et al., 2004), and\\nincremental SVD (Arora & Livescu, 2012) have also been\\nproposed. However, for very large training sets, as in\\nsome of our tasks below, it remains dif\\ufb01cult and costly\\nto approximate KCCA well. Although iterative algorithms\\nhave recently been introduced for very large CCA problems\\n(Lu & Foster, 2014), they are aimed at sparse matrices and\\ndo not have a natural out-of-sample extension.\\n\\n\\x0cOn Deep Multi-View Representation Learning\\n\\nTable1.Performance of several representation learning methods\\non the test set of noisy MNIST digits. Performance measures\\nare clustering accuracy (ACC), normalized mutual information\\n(NMI) of clustering, and classi\\ufb01cation error rates of a linear SVM\\non the projections. The selected feature dimensionality L is given\\nin parentheses. Results are averaged over 5 random seeds.\\n\\nMethod\\nBaseline\\nCCA (L = 10)\\nSplitAE (L = 10)\\nCorrAE (L = 10)\\nDistAE (L = 20)\\nFKCCA(L = 10)\\nNKCCA(L = 10)\\nDCCA (L = 10)\\nDCCAE (L = 10)\\n\\nACC (%) NMI (%) Error (%)\\n\\n47.0\\n72.9\\n64.0\\n65.5\\n53.5\\n94.7\\n95.1\\n97.0\\n97.5\\n\\n50.6\\n56.0\\n69.0\\n67.2\\n60.2\\n87.3\\n88.3\\n92.0\\n93.4\\n\\n13.1\\n19.6\\n11.9\\n12.9\\n16.0\\n5.1\\n4.5\\n2.9\\n2.2\\n\\nbe able to extract features that disregard the noise. We mea-\\nsure the class separation in the learned feature spaces by\\nclustering the projected view 1 inputs into 10 clusters and\\nevaluating how well the clusters agree with ground-truth\\nlabels. We use spectral clustering (Ng et al., 2002) so as\\nto account for possibly non-convex cluster shapes. Speci\\ufb01-\\ncally, we \\ufb01rst build a k-nearest-neighbor graph on the pro-\\njected view 1 tuning/test samples with a binary weighting\\nscheme (edges connecting neighboring samples have a con-\\nstant weight of 1), then embed these samples in R10 using\\neigenvectors of the normalized graph Laplacian, and \\ufb01nally\\nrun K-means in the embedding to obtain a hard partition of\\nthe samples. In the last step, K-means is run 20 times with\\nrandom initialization and the run with the best K-means\\nobjective is used. The size of the neighborhood graph k is\\nselected from {5, 10, 20, 30, 50} using the tuning set. We\\nmeasure clustering performance with two criteria, cluster-\\ning accuracy (ACC) and normalized mutual information\\n(NMI) (Cai et al., 2005).\\n\\nEach algorithm has hyperparameters that are selected us-\\ning the tuning set. The \\ufb01nal dimensionality L is selected\\nfrom {5, 10, 20, 30, 50}. For CCA, the regularization pa-\\nrameters rx/ry are selected via grid search. For KC-\\nCAs, we \\ufb01x rx/ry at a small positive value of 10\\u22124 (as\\nsuggested by Lopez-Paz et al. (2014), FKCCA is robust\\nto rx/ry), do grid search for the Gaussian kernel width\\nfor each view at rank M = 5, 000, and then test with\\nM = 20, 000. For DNN-based models, feature mappings\\n(f , g) are implemented by networks of 3 hidden layers,\\neach of 1, 024 sigmoid units, and a linear output layer of\\nL units; reconstruction mappings (p, q) are implemented\\nby networks of 3 hidden layers, each of 1, 024 sigmoid\\nunits, and an output layer of 784 sigmoid units. We \\ufb01x\\nrx = ry = 10\\u22124 for DCCA and DCCAE. For Spli-\\ntAE/CorrAE/DCCAE/DistAE we select the trade-off pa-\\nrameter \\u03bb via grid search. The two networks (f , p) are\\n\\nFigure2.Selection of view 1 images (left) and their correspond-\\ning view 2 images (right) from our noisy MNIST dataset.\\n\\n4. Experiments\\nWe compare the following methods in the multi-view learn-\\ning setting, focusing on several downstream tasks: noisy\\ndigit image classi\\ufb01cation, speech recognition, and word\\npair semantic similarity.\\nDNN-based models, including SplitAE, CorrAE, DCCA,\\nDCCAE, and DistAE.\\nLinear CCA (CCA), corresponding to DCCA with only a\\nlinear network without hidden layers for both views.\\nKernel CCA approximations. Exact KCCA is intractable\\nfor our tasks; we instead implement two kernel approxima-\\ntion techniques, using Gaussian RBF kernels. The \\ufb01rst im-\\nplementation, denoted FKCCA, uses random Fourier fea-\\ntures (Lopez-Paz et al., 2014) and the second implemen-\\ntation, denoted NKCCA, uses the Nystr\\xa8om approxima-\\ntion (Williams & Seeger, 2001). As described in Sec. 3.2,\\nin FKCCA/NKCCA we transform the original inputs to\\nan M-dimensional feature space where the inner prod-\\nucts between samples approximate the kernel similarities\\n(Yang et al., 2012). We apply linear CCA to the trans-\\nformed inputs to obtain the approximate KCCA solution.\\n\\n4.1. Noisy MNIST digits\\nIn this task, we generate two-view data using the MNIST\\ndataset (LeCun et al., 1998), which consists of 28 \\xd7 28\\ngrayscale digit images, with 60K/10K images for train-\\ning/testing. We generate a more challenging version of\\nthe dataset as follows (see Fig. 2 for examples). We \\ufb01rst\\nrescale the pixel values to [0, 1]. We then randomly rotate\\nthe images at angles uniformly sampled from [\\u2212\\u03c0/4, \\u03c0/4]\\nand the resulting images are used as view 1 inputs. For\\neach view 1 image, we randomly select an image of the\\nsame identity (0-9) from the original dataset, add indepen-\\ndent random noise uniformly sampled from [0, 1] to each\\npixel, and truncate the pixel \\ufb01nal values to [0, 1] to obtain\\nthe corresponding view 2 sample. The original training set\\nis further split into training/tuning sets of size 50K/10K.\\n\\nSince, given the digit identity, observing a view 2 image\\ndoes not provide any information about the corresponding\\nview 1 image, a good multi-view learning algorithm should\\n\\n\\x0cOn Deep Multi-View Representation Learning\\n\\n(a) Inputs\\n\\n(b) LLE\\n\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n0\\n\\n(c) SplitAE\\n\\n(d) CorrAE\\n\\n(e) DistAE\\n\\n(f) CCA\\n\\n(g) FKCCA\\n\\n(h) NKCCA\\n\\n(i) DCCA\\n\\n(j) DCCAE\\n\\nFigure3. t-SNE embedding of the projected test set of noisy\\nMNIST digits using different methods. Each sample is denoted\\nby a marker located at its coordinates of embedding and color\\ncoded by its label. Neither the feature learning algorithms nor\\nt-SNE use the class information.\\n\\npre-trained in a layerwise manner using restricted Boltz-\\nmann machines (Hinton & Salakhutdinov, 2006) and sim-\\nilarly for (g, q) with inputs from the corresponding view.\\nFor DNN-based models, we use SGD for optimization with\\nminibatch size, learning rate and momentum tuned on the\\ntuning set. A small weight decay parameter of 10\\u22124 is used\\nfor all layers. We monitor the objective on the tuning set\\nfor early stopping. For each algorithm, we select the model\\n\\nwith best ACC on the tuning set, and report its results on\\nthe test set. The ACC and NMI results (in percentage) for\\neach algorithm are given in Table 1. As a baseline, we also\\ncluster the original 784-dimensional view 1 images.\\n\\nAll of the multi-view feature learning algorithms achieve\\nsome improvement over the baseline. The nonlinear CCA\\nalgorithms perform similarly to each other and signi\\ufb01cantly\\nbetter than SplitAE/CorrAE/DistAE. We also qualitatively\\ninvestigate the features by embedding the projected fea-\\ntures in 2D using t-SNE (van der Maaten & Hinton, 2008);\\nthe resulting visualizations are given in Fig. 3. Overall, the\\nclass separation in the visualizations qualitatively agrees\\nwith the relative clustering performances in Table 1.\\n\\nIn the embedding of input images (Fig. 3 (a)), samples of\\neach digit form an approximately one dimensional, stripe-\\nshaped manifold, and the degree of freedom along each\\nmanifold corresponds roughly to the variation in rotation\\nangle (see supplementary material for embedding with im-\\nages). This degree of freedom does not change the identity\\nof the image, which is common to both views. Projections\\nby SplitAE/CorrAE/DistAE do achieve somewhat better\\nseparation for some classes, but the unwanted rotation vari-\\nation is still prominent in the embeddings. On the other\\nhand, without using any label information and with only\\npaired noisy images, the nonlinear CCA algorithms man-\\nage to map digits of the same identity to similar locations\\nwhile supressing the rotational variation and separating im-\\nages of different identities (linear CCA also approximates\\nthe same behavior, but fails to separate the classes, pre-\\nsumably because the input variations are too complex to be\\ncaptured by linear mappings). Overall, DCCAE gives the\\ncleanest embedding, with different digits pushed far apart.\\n\\nThe different behaviour of CCA-based methods from Spli-\\ntAE/CorrAE/DistAE suggests two things. First, when the\\ninputs are noisy, reconstructing the inputs faithfully may\\nstill lead to unwanted degrees of freedom in the features\\n(DCCAE tends to select quite small trade-off parameter\\n\\u03bb = 10\\u22123 or 10\\u22122, further supporting that it is not nec-\\nessary to minimize reconstruction error).\\nthe\\nhard CCA constraints, which enforce uncorrelatedness be-\\ntween different feature dimensions, appear essential; these\\nconstraints are the difference between DCCAE and Cor-\\nrAE/DistAE. However, the constraints without the multi-\\nview objective are insuf\\ufb01cient. To see this, we also vi-\\nsualize a 10-dimensional locally linear embedding (LLE,\\nRoweis & Saul, 2000) of the test images in Fig. 3 (b). LLE\\nsatis\\ufb01es the same un-correlatedness constraints as in CCA-\\nbased methods, but without access to the second view, it\\ndoes not separate the classes as nicely.\\n\\nSecond,\\n\\nIn view of the embeddings in Fig. 3, one would expect\\nthat a simple linear classi\\ufb01er can achieve high accuracy on\\nDCCA/DCCAE projections. We train one-versus-one lin-\\n\\n\\x0cOn Deep Multi-View Representation Learning\\n\\near SVMs (Chang & Lin, 2011) on the projected training\\nset (now using the ground truth labels), and test on the pro-\\njected test set, while using the projected tuning set for se-\\nlecting the SVM hyperparameter (the penalty parameter for\\nhinge loss). Test error rates on the optimal embedding of\\neach algorithm (with highest ACC) are provided in Table 1\\n(last column). These error rates agree with the clustering\\nresults. Multi-view feature learning makes classi\\ufb01cation\\nmuch easier on this task: Instead of using a heavily non-\\nlinear classi\\ufb01er on the original inputs, a very simple linear\\nclassi\\ufb01er that can be trained ef\\ufb01ciently on low-dimensional\\nprojections already achieves high accuracy.\\n\\n4.2. Acoustic-articulatory data for speech recognition\\nWe next experiment with the Wisconsin X-Ray Micro-\\nBeam (XRMB) corpus (Westbury, 1994) of simultaneously\\nrecorded speech and articulatory measurements from 47\\nAmerican English speakers. Multi-view feature learning\\nvia CCA/KCCA has previously been shown to improve\\nphonetic recognition performance when tested on audio\\nalone (Arora & Livescu, 2013; Wang et al., 2015).\\n\\nWe follow the setup of Wang et al. (2015) and use learned\\nfeatures for speaker-independent phonetic recognition. In-\\nputs to multi-view feature learning are acoustic features\\n(39D features consisting of mel frequency cepstral coef\\ufb01-\\ncients (MFCCs) and their \\ufb01rst and second derivatives) and\\narticulatory features (horizontal/vertical displacement of 8\\npellets attached to several parts of the vocal tract) concate-\\nnated over a 7-frame window around each frame, giving\\n273D acoustic inputs and 112D articulatory inputs for each\\nview.\\n\\nWe split\\nthe XRMB speakers into disjoint sets of\\n35/8/2/2 speakers for feature learning/recognizer train-\\ning/tuning/testing. The 35 speakers for feature learning\\nare \\ufb01xed; the remaining 12 are used in a 6-fold experi-\\nment (recognizer training on 4 2-speaker folds, tuning on\\n1 fold, and testing on the last fold). Each speaker has\\nroughly 50K frames, giving 1.43M multi-view training\\nframes. We remove the per-speaker mean and variance\\nof the articulatory measurements for each training speaker.\\nAll of the learned feature types are used in a \\u201ctandem\\u201d ap-\\nproach (Hermansky et al., 2000), i.e., they are appended\\nto the original 39D features and used in a standard hid-\\nden Markov model (HMM)-based recognizer with Gauss-\\nian mixture observation distributions. The baseline recog-\\nnizer uses the original MFCC features. The recognizer has\\none 3-state left-to-right HMM per phone and the same lan-\\nguage model as in Wang et al. (2015)\\n\\nFor each fold, we select the hyperparameters based on\\nrecognition accuracy on the tuning set. As before, models\\nbased on neural networks are trained via SGD, with no pre-\\ntraining and with optimization parameters tuned by grid\\n\\nTable2.Mean and standard deviations of PERs over 6 folds ob-\\ntained by each algorithm on the XRMB test speakers.\\n\\nMethod Mean (std) PER (%)\\nBaseline\\nCCA\\nSplitAE\\nCorrAE\\nDistAE\\nFKCCA\\nNKCCA\\nDCCA\\nDCCAE\\n\\n34.8 (4.5)\\n26.7 (5.0)\\n29.0 (4.7)\\n30.6 (4.8)\\n33.2 (4.7)\\n26.0 (4.4)\\n26.6 (4.2)\\n24.8 (4.4)\\n24.5 (3.9)\\n\\nsearch. A small weight decay parameter of 5\\xd710\\u22124 is used\\nfor all layers. For each algorithm, the dimensionality L is\\ntuned over {30, 50, 70}. For DNN-based models, we use\\nhidden layers of 1 500 ReLUs. For DCCA, we tune the net-\\nwork depths (up to 3 nonlinear hidden layers) and \\ufb01nd that\\nin the best-performing architecture, f has 3 hidden ReLU\\nlayers followed by a linear output layer while g has only a\\nlinear output layer. For SplitAE/CorrAE/DistAE/DCCAE,\\nthe same encoder architecture as that of DCCA performs\\nbest, and we set the decoders to have symmetric architec-\\ntures to the encoders (except for SplitAE which does not\\nhave an encoder g and its decoder q is linear). We \\ufb01x\\nrx = ry = 10\\u22124 for DCCAE (and FKCCA/NKCCA). The\\ntrade-off parameter \\u03bb is tuned for each algorithm by grid\\nsearch.\\n\\nFor FKCCA, we \\ufb01nd it important to use a large number\\nof random features M to get a competitive result, consis-\\ntent with the \\ufb01ndings of Huang et al. (2014) when using\\nrandom Fourier features for speech data. We tune kernel\\nwidths at M = 5, 000 with FKCCA, and test FKCCA with\\nM = 30, 000 (the largest M we could afford to obtain an\\nexact SVD solution on a workstation with 32G main mem-\\nory); we are not able to obtain results for NKCCA with\\nM = 30, 000 in 48 hours with our implementation, so we\\nreport its test performance at M = 20, 000 with the optimal\\nFKCCA hyper-parameters. Notice that FKCCA has about\\n14.6 million parameters (random Gaussian samples + pro-\\njection matrices), which is more than the number of weight\\nparameters in the largest DCCA model, so it is slower than\\nDCCA for testing (cost of obtaining test features is linear\\nin the number of parameters for both KCCA and DNNs).\\n\\nPhone error rates (PERs) obtained by different feature\\nlearning algorithms are given in Table 2. We see the same\\npattern as on MNIST: nonlinear CCA-based algorithms\\noutperform SplitAE/CorrAE/DistAE. Since the recognizer\\nnow is a nonlinear mapping (HMM), the performance of\\nthe linear CCA features is highly competitive. Again, DC-\\nCAE selects a relatively small \\u03bb = 0.01, indicating that the\\ncanonical correlation term is more important.\\n\\n\\x0cOn Deep Multi-View Representation Learning\\n\\n4.3. Multilingual data for word embeddings\\nIn this task, we learn a vectorial representation of Eng-\\nlish words from pairs of English-German word embed-\\ndings. We follow the setup of Faruqui & Dyer (2014) and\\nLu et al. (2015), and use as inputs 640-dimensional mono-\\nlingual word vectors trained via latent semantic analysis\\non the WMT 2011 monolingual news corpora and use\\nthe same 36K English-German word pairs for multi-view\\nlearning. The learned mappings are applied to the origi-\\nnal English word embeddings (180K words) and the pro-\\njections are used for evaluation. We evaluate on the bi-\\ngram similarity dataset of Mitchell & Lapata (2010), using\\nthe adjective-noun (AN) and verb-object (VN) subsets, and\\ntuning and test splits (of size 649/1,972) for each subset (we\\nexclude the noun-noun subset as it is observed by Lu et al.\\n(2015) that the NN human annotations often re\\ufb02ect \\u201ctop-\\nical\\u201d rather than \\u201cfunctional\\u201d similarity). We simply add\\nthe projections of the two words in each bigram to obtain\\nan L-dimensional representation of the bigram, as done in\\nprior work (Blacoe & Lapata, 2012; Lu et al., 2015). We\\ncompute the cosine similarity between the two vectors of\\neach bigram pair, order the pairs by similarity, and report\\nthe Spearman\\u2019s correlation (\\u03c1) between the model\\u2019s rank-\\ning and human rankings.\\n\\nWe \\ufb01x the feature dimensionality at L = 384; other hyper-\\nparameters are tuned as in previous experiments. DNN-\\nbased models use ReLU hidden layers of width 1, 280. A\\nsmall weight decay parameter of 10\\u22124 is used for all lay-\\ners. We use two ReLU hidden layers for encoders (f and\\ng), and try both linear and nonlinear networks with two\\nhidden layers for decoders (p and q). FKCCA/NKCCA\\nare tested with M = 20, 000 using kernel widths tuned at\\nM = 4, 000. We \\ufb01x rx = ry = 10\\u22124 for nonlinear CCAs.\\nFor each algorithm, we select the model with the highest\\nSpearman\\u2019s correlation on the 649 tuning bigram pairs, and\\nwe report its performance on the 1,972 test pairs in Table 3\\n(our baseline and DCCA results are different from that of\\nLu et al. (2015) due to a different normalization and better\\ntuning). Unlike MNIST and XRMB, it is important for the\\nfeatures to reconstruct the input monolingual word embed-\\ndings well, as can be seen from the superior performance of\\nSplitAE over FKCCA/NKCCA/DCCA. This implies there\\nis useful information in the original inputs that is not cor-\\nrelated across views. However, DCCAE still performs the\\nbest on the AN task, in this case using a relatively large \\u03bb.\\n\\n5. Discussion\\nWe have explored several approaches in the space of\\nDNN-based mutli-view representation learning. We have\\nfound that on several tasks, CCA-based models outperform\\nautoencoder-based models (SplitAE) and models based\\non between-view squared distance (DistAE) or correlation\\n\\nTable3.Spearman\\u2019s correlation (\\u03c1) for bigram similarities.\\n\\nAN\\nMethod\\n45.0\\nBaseline\\n46.6\\nCCA\\n47.0\\nSplitAE\\n43.0\\nCorrAE\\nDistAE\\n43.6\\nFKCCA 46.4\\nNKCCA 44.3\\nDCCA\\n48.5\\n49.1\\nDCCAE\\n\\nVN Avg.\\n42.1\\n39.1\\n42.2\\n37.7\\n45.0\\n46.0\\n42.0\\n42.5\\n41.5\\n39.4\\n44.7\\n42.9\\n41.9\\n39.5\\n42.5\\n45.5\\n46.2\\n43.2\\n\\n(CorrAE) instead of canonical correlation. The best overall\\nperformer is a new DCCA extension introduced here, deep\\ncanonically correlated autoencoders (DCCAE).\\n\\nIn light of the empirical results, it is interesting to consider\\nagain the main features of each type of objective and cor-\\nresponding constraints. Autoencoder-based approaches are\\nbased on the idea that the learned features should be able\\nto accurately reconstruct the inputs (in the case of multi-\\nview learning, the inputs in both views). The CCA objec-\\ntive, on the other hand, focuses on how well each view\\u2019s\\nrepresentation predicts the other\\u2019s, ignoring the ability to\\nreconstruct each view. CCA is expected to perform well\\nwhen the two views are uncorrelated given the class la-\\nbel (Chaudhuri et al., 2009). The noisy MNIST dataset\\nused here simulates exactly this scenario, and indeed this\\nis the task where CCA outperforms other objectives by the\\nlargest margins. Even in the other tasks, however, there is\\noften no signi\\ufb01cant advantage to being able to reconstruct\\nthe inputs faithfully.\\n\\nThe constraints in the various methods also have an\\nimportant effect. The performance difference between\\nDCCA and CorrAE demonstrates that uncorrelatedness\\nbetween learned dimensions is important. On the other\\nhand,\\nthe stronger DCCA constraint may still not be\\nsuf\\ufb01ciently strong; an even better constraint may be to\\nrequire the learned dimensions to be independent (or\\napproximately so), and this is an interesting avenue\\nfor future work. Another future direction is to com-\\npare DNN-based models with models based on deep\\nBoltzmann machines (Srivastava & Salakhutdinov, 2014;\\nSohn et al., 2014) and noise-constrastive learning criteria\\n(Gutmann & Hyv\\xa8arinen, 2012).\\n\\nAcknowledgments\\nThis research was supported by NSF grant IIS-1321015.\\nThe opinions expressed in this work are those of the au-\\nthors and do not necessarily re\\ufb02ect the views of the funding\\nagency. The Tesla K40 GPUs used for this research were\\ndonated by NVIDIA Corporation. We thank Geoff Hinton\\nfor helpful discussion.\\n\\n\\x0cOn Deep Multi-View Representation Learning\\n\\nReferences\\nAkaho, Shotaro. A kernel method for canonical correlation\\nanalysis. In Proceedings of the International Meeting of\\nthe Psychometric Society (IMPS2001), 2001.\\n\\nAndrew, Galen, Arora, Raman, Bilmes, Jeff, and Livescu,\\nKaren. Deep canonical correlation analysis. In ICML,\\npp. 1247\\u20131255, 2013.\\n\\nArora, Raman and Livescu, Karen. Kernel CCA for\\nmulti-view learning of acoustic features using articula-\\ntory measurements. In Symposium on Machine Learning\\nin Speech and Language Processing (MLSLP), 2012.\\n\\nArora, Raman and Livescu, Karen. Multi-view CCA-based\\nacoustic features for phonetic recognition across speak-\\ners and domains. In ICASSP, 2013.\\n\\nBach, Francis R. and Jordan, Michael I. Kernel indepen-\\ndent component analysis. Journal of Machine Learning\\nResearch, 3:1\\u201348, 2002.\\n\\nBecker, Suzanna and Hinton, Geoffrey E. Self-organizing\\nneural network that discovers surfaces in random-dot\\nstereograms. Nature, 355:161\\u2013163, 1992.\\n\\nBickel, Steffen and Scheffer, Tobias. Multi-view cluster-\\nIn Proc. of the 4th IEEE Int. Conf. Data Mining\\n\\ning.\\n(ICDM\\u201904), pp. 19\\u201326, 2004.\\n\\nBie, Tijl De and Moor, Bart De. On the regularization of\\nInt. Sympos. ICA and\\n\\ncanonical correlation analysis.\\nBSS, 2003.\\n\\nBlacoe, William and Lapata, Mirella. A comparison of\\nvector-based representations for semantic composition.\\nIn EMNLP, pp. 546\\u2013556, 2012.\\n\\nBlaschko, Mathew B. and Lampert, Christoph H. Correla-\\n\\ntional spectral clustering. In CVPR, pp. 1\\u20138, 2008.\\n\\nBorga, Magnus. Canonical correlation: A tutorial. 2001.\\n\\nCai, Deng, He, Xiaofei, and Han, Jiawei. Document clus-\\ntering using Locality Preserving Indexing. IEEE Trans.\\nKnowledge and Data Engineering, 17(12):1624\\u20131637,\\n2005.\\n\\nChandar, Sarath, Lauly, Stanislas, Larochelle, Hugo,\\nKhapra, Mitesh M., Ravindran, Balaraman, Raykar,\\nVikas, and Saha, Amrita. An autoencoder approach to\\nlearning bilingual word representations.\\nIn NIPS, pp.\\n1853\\u20131861, 2014.\\n\\nChaudhuri, Kamalika, Kakade, Sham M., Livescu, Karen,\\nand Sridharan, Karthik. Multi-view clustering via canon-\\nical correlation analysis. In ICML, pp. 129\\u2013136, 2009.\\n\\nChechik, Gal, Globerson, Amir, Tishby, Naftali, and\\nWeiss, Yair. Information bottleneck for Gaussian vari-\\nables. Journal of Machine Learning Research, 6:165\\u2013\\n188, 2005.\\n\\nDhillon, Paramveer, Foster, Dean, and Ungar, Lyle. Multi-\\nview learning of word embeddings via CCA. In NIPS,\\npp. 199\\u2013207, 2011.\\n\\nFaruqui, Manaal and Dyer, Chris. Improving vector space\\nword representations using multilingual correlation. In\\nProceedings of European Chapter of the Association for\\nComputational Linguistics, 2014.\\n\\nFoster, Dean P., Johnson, Rie, Kakade, Sham M., and\\nZhang, Tong. Multi-view dimensionality reduction via\\ncanonical correlation analysis. Technical report, 2009.\\n\\nGutmann, Michael and Hyv\\xa8arinen, Aapo.\\n\\nNoise-\\ncontrastive estimation of unnormalized statistical mod-\\nels, with applications to natural image statistics. Journal\\nof Machine Learning Research, 13:307\\u2013361, 2012.\\n\\nHaghighi, Aria, Liang, Percy, Berg-Kirkpatrick, Taylor,\\nand Klein, Dan. Learning bilingual lexicons from mono-\\nlingual corpora. In Proceedings of the Annual Meeting\\nof the Association for Computational Linguistics (ACL\\n2008), pp. 771\\u2013779, 2008.\\n\\nHardoon, David R., Szedmak, Sandor, and Shawe-Taylor,\\nJohn. Canonical correlation analysis: An overview with\\napplication to learning methods. Neural Computation,\\n16(12):2639\\u20132664, 2004.\\n\\nHermansky, Hynek, Ellis, Daniel P. W., and Sharma, San-\\ngita. Tandem connectionist feature extraction for con-\\nventional HMM systems.\\nIn ICASSP, pp. 1635\\u20131638,\\n2000.\\n\\nHinton, G. E. and Salakhutdinov, R. R. Reducing the di-\\nmensionality of data with neural networks. Science, 313\\n(5786):504\\u2013507, 2006.\\n\\nHodosh, Micah, Young, Peter, and Hockenmaier, Julia.\\nFraming image description as a ranking task: Data, mod-\\nels and evaluation metrics. Journal of Arti\\ufb01cial Intelli-\\ngence Research, 47:853\\u2013899, 2013.\\n\\nHotelling, Harold. Relations between two sets of variates.\\n\\nBiometrika, 28(3/4):321\\u2013377, 1936.\\n\\nChang, Chih-Chung and Lin, Chih-Jen. LIBSVM: A li-\\nbrary for support vector machines. ACM Trans. Intelli-\\ngent Systems and Technology, 2(3):27, 2011.\\n\\nHsieh, W. W. Nonlinear canonical correlation analysis by\\nneural networks. Neural Networks, 13(10):1095\\u20131105,\\n2000.\\n\\n\\x0cOn Deep Multi-View Representation Learning\\n\\nHuang, Po-Sen, Avron, Haim, Sainath, Tara, Sindhwani,\\nVikas, and Ramabhadran, Bhuvana. Kernel methods\\nmatch deep neural networks on TIMIT: Scalable learn-\\ning in high-dimensional random Fourier spaces.\\nIn\\nICASSP, pp. 205\\u2013209, 2014.\\n\\nKakade, Sham M. and Foster, Dean P. Multi-view regres-\\nIn COLT, pp.\\n\\nsion via canonical correlation analysis.\\n82\\u201396, 2007.\\n\\nKidron, Einat, Schechner, Yoav Y., and Elad, Michael. Pix-\\n\\nels that sound. In CVPR, pp. 88\\u201395, 2005.\\n\\nKim, Jungi, Nam, Jinseok, and Gurevych, Iryna. Learning\\nsemantics with deep belief network for cross-language\\ninformation retrieval. In COLING, pp. 579\\u2013588, 2012.\\n\\nLai, Pei Ling and Fyfe, Colin. A neural implementation of\\ncanonical correlation analysis. Neural Networks, 12(10):\\n1391\\u20131397, 1999.\\n\\nLai, Pei Ling and Fyfe, Colin. Kernel and nonlinear canon-\\nical correlation analysis. Int. J. Neural Syst., 10(5):365\\u2013\\n377, 2000.\\n\\nLeCun, Yann, Bottou, L\\xb4eon, Bengio, Yoshua, and Haffner,\\nPatrick. Gradient-based learning applied to document\\nrecognition. Proc. IEEE, 86(11):2278\\u20132324, 1998.\\n\\nLopez-Paz, David, Sra, Suvrit, Smola, Alex, Ghahramani,\\nZoubin, and Schoelkopf, Bernhard. Randomized nonlin-\\near component analysis. In ICML, pp. 1359\\u20131367, 2014.\\n\\nLu, Ang, Wang, Weiran, Bansal, Mohit, Gimpel, Kevin,\\nand Livescu, Karen. Deep multilingual correlation for\\nimproved word embeddings. In NAACL-HLT, 2015.\\n\\nLu, Yichao and Foster, Dean P. Large scale canonical cor-\\nIn NIPS,\\n\\nrelation analysis with iterative least squares.\\npp. 91\\u201399, 2014.\\n\\nMelzer, Thomas, Reiter, Michael, and Bischof, Horst. Non-\\nlinear feature extraction using generalized canonical cor-\\nrelation analysis. In Proc. of the 11th Int. Conf. Arti\\ufb01cial\\nNeural Networks (ICANN\\u201901), pp. 353\\u2013360, 2001.\\n\\nMitchell, Jeff and Lapata, Mirella. Composition in distri-\\nbutional models of semantics. Cognitive Science, 34(8):\\n1388\\u20131429, 2010.\\n\\nNg, Andrew Y., Jordan, Michael I., and Weiss, Yair. On\\nspectral clustering: Analysis and an algorithm. In NIPS,\\npp. 849\\u2013856, 2002.\\n\\nNgiam, Jiquan, Khosla, Aditya, Kim, Mingyu, Nam,\\nJuhan, Lee, Honglak, and Ng, Andrew. Multimodal deep\\nlearning. In ICML, pp. 689\\u2013696, 2011.\\n\\nPennington,\\n\\nJeffrey, Socher, Richard, and Manning,\\nChristopher D. GloVe: Global vectors for word repre-\\nsentation. In EMNLP, 2014.\\n\\nRoweis, Sam T. and Saul, Lawrence K. Nonlinear dimen-\\nsionality reduction by locally linear embedding. Science,\\n290(5500):2323\\u20132326, 2000.\\n\\nSch\\xa8olkopf, Bernhard and Smola, Alexander J. Learning\\nwith Kernels. Support Vector Machines, Regularization,\\nOptimization, and Beyond. MIT Press, 2001.\\n\\nSocher, Richard and Li, Fei-Fei. Connecting modalities:\\nSemi-supervised segmentation and annotation of images\\nusing unaligned text corpora.\\nIn CVPR, pp. 966\\u2013973,\\n2010.\\n\\nSohn, Kihyuk, Shang, Wenling, and Lee, Honglak.\\n\\nIm-\\nproved multimodal deep learning with variation of infor-\\nmation. In NIPS, pp. 2141\\u20132149, 2014.\\n\\nSrivastava, Nitish and Salakhutdinov, Ruslan. Multimodal\\nlearning with deep boltzmann machines. Journal of Ma-\\nchine Learning Research, 15:2949\\u20132980, 2014.\\n\\nTishby, Naftali, Pereira, Fernando, and Bialek, William.\\nThe information bottleneck method. In Proc. 37th An-\\nnual Allerton Conference on Communication, Control,\\nand Computing, pp. 368\\u2013377, 1999.\\n\\nvan der Maaten, Laurens J. P. and Hinton, Geoffrey E. Vi-\\nsualizing data using t-SNE. Journal of Machine Learn-\\ning Research, 9:2579\\u20132605, 2008.\\n\\nVincent, Pascal, Larochelle, Hugo, Lajoie, Isabelle, Ben-\\ngio, Yoshua, and Manzagol, Pierre-Antoine. Stacked de-\\nnoising autoencoders: Learning useful representations in\\na deep network with a local denoising criterion. Journal\\nof Machine Learning Research, 11:3371\\u20133408, 2010.\\n\\nVinokourov, Alexei, Cristianini, Nello, and Shawe-Taylor,\\nJohn.\\nInferring a semantic representation of text via\\ncross-language correlation analysis. In NIPS, pp. 1497\\u2013\\n1504, 2003.\\n\\nWang, Weiran, Arora, Raman, Livescu, Karen, and Bilmes,\\nJeff. Unsupervised learning of acoustic features via deep\\ncanonical correlation analysis. In ICASSP, 2015.\\n\\nWestbury, John R. X-Ray Microbeam Speech Production\\n\\nDatabase User\\u2019s Handbook Version 1.0, 1994.\\n\\nWilliams, Christopher K. I. and Seeger, Matthias. Using\\nIn\\n\\nthe Nystr\\xa8om method to speed up kernel machines.\\nNIPS, pp. 682\\u2013688, 2001.\\n\\nYang, Tianbao, Li, Yu-Feng, Mahdavi, Mehrdad, Jin,\\nRong, and Zhou, Zhi-Hua. Nystr\\xa8om method vs random\\nFourier features: A theoretical and empirical compari-\\nson. In NIPS, pp. 476\\u2013484, 2012.\\n\\n\\x0c', u'Deep Learning:\\nMethods and Applications\\n\\nLi Deng\\nMicrosoft Research\\nOne Microsoft Way\\nRedmond, WA 98052; USA\\ndeng@microsoft.com\\nDong Yu\\nMicrosoft Research\\nOne Microsoft Way\\nRedmond, WA 98052; USA\\nDong.Yu@microsoft.com\\n\\nBoston \\u2014 Delft\\n\\nFull text available at: http://dx.doi.org/10.1561/2000000039\\x0cFoundations and Trends R(cid:1) in Signal Processing\\n\\nPublished, sold and distributed by:\\nnow Publishers Inc.\\nPO Box 1024\\nHanover, MA 02339\\nUnited States\\nTel. +1-781-985-4510\\nwww.nowpublishers.com\\nsales@nowpublishers.com\\nOutside North America:\\nnow Publishers Inc.\\nPO Box 179\\n2600 AD Delft\\nThe Netherlands\\nTel. +31-6-51115274\\nThe preferred citation for this publication is\\nL. Deng and D. Yu. Deep Learning: Methods and Applications. Foundations and\\nTrends R(cid:1) in Signal Processing, vol. 7, nos. 3\\u20134, pp. 197\\u2013387, 2013.\\nThis Foundations and Trends R(cid:1) issue was typeset in LATEX using a class \\ufb01le designed\\nby Neal Parikh. Printed on acid-free paper.\\nISBN: 978-1-60198-815-7\\nc(cid:1) 2014 L. Deng and D. Yu\\n\\nAll rights reserved. No part of this publication may be reproduced, stored in a retrieval\\nsystem, or transmitted in any form or by any means, mechanical, photocopying, recording\\nor otherwise, without prior written permission of the publishers.\\nPhotocopying. In the USA: This journal is registered at the Copyright Clearance Cen-\\nter, Inc., 222 Rosewood Drive, Danvers, MA 01923. Authorization to photocopy items for\\ninternal or personal use, or the internal or personal use of speci\\ufb01c clients, is granted by\\nnow Publishers Inc for users registered with the Copyright Clearance Center (CCC). The\\n\\u2018services\\u2019 for users can be found on the internet at: www.copyright.com\\nFor those organizations that have been granted a photocopy license, a separate system\\nof payment has been arranged. Authorization does not extend to other kinds of copy-\\ning, such as that for general distribution, for advertising or promotional purposes, for\\ncreating new collective works, or for resale. In the rest of the world: Permission to pho-\\ntocopy must be obtained from the copyright owner. Please apply to now Publishers Inc.,\\nPO Box 1024, Hanover, MA 02339, USA; Tel. +1 781 871 0245; www.nowpublishers.com;\\nsales@nowpublishers.com\\nnow Publishers Inc. has an exclusive license to publish this material worldwide. Permission\\nto use this content must be obtained from the copyright license holder. Please apply to\\nnow Publishers, PO Box 179, 2600 AD Delft, The Netherlands, www.nowpublishers.com;\\ne-mail: sales@nowpublishers.com\\n\\nFull text available at: http://dx.doi.org/10.1561/2000000039\\x0cFoundations and Trends R(cid:1) in Signal Processing\\n\\nVolume 7, Issues 3\\u20134, 2013\\n\\nEditorial Board\\n\\nEditor-in-Chief\\n\\nYonina Eldar\\nTechnion - Israel Institute of Technology\\nIsrael\\nEditors\\n\\nRobert M. Gray\\nFounding Editor-in-Chief\\nStanford University\\nPao-Chi Chang\\nNCU, Taiwan\\nPamela Cosman\\nUC San Diego\\nMichelle E\\ufb00ros\\nCaltech\\nYariv Ephraim\\nGMU\\nAlfonso Farina\\nSelex ES\\nSadaoki Furui\\nTokyo Tech\\nGeorgios Giannakis\\nUniversity of Minnesota\\nVivek Goyal\\nBoston University\\nSinan Gunturk\\nCourant Institute\\nChristine Guillemot\\nINRIA\\nRobert W. Heath, Jr.\\nUT Austin\\n\\nSheila Hemami\\nCornell University\\nLina Karam\\nArizona State U\\nNick Kingsbury\\nUniversity of Cambridge\\nAlex Kot\\nNTU, Singapore\\nJelena Kovacevic\\nCMU\\nGeert Leus\\nTU Delft\\nJia Li\\nPenn State\\nHenrique Malvar\\nMicrosoft Research\\nB.S. Manjunath\\nUC Santa Barbara\\nUrbashi Mitra\\nUSC\\nBj\\xf6rn Ottersten\\nKTH Stockholm\\nThrasos Pappas\\nNorthwestern University\\n\\nVincent Poor\\nPrinceton University\\nAnna Scaglione\\nUC Davis\\nMihaela van der Shaar\\nUCLA\\nNicholas D. Sidiropoulos\\nTU Crete\\nMichael Unser\\nEPFL\\nP. P. Vaidyanathan\\nCaltech\\nAmi Wiesel\\nHebrew U\\nMin Wu\\nUniversity of Maryland\\nJosiane Zerubia\\nINRIA\\n\\nFull text available at: http://dx.doi.org/10.1561/2000000039\\x0cEditorial Scope\\n\\nTopics\\nFoundations and Trends R(cid:1) in Signal Processing publishes survey and\\ntutorial articles in the following topics:\\n\\nprocessing\\n\\nprocessing\\n\\n\\u2022 Adaptive signal processing\\n\\u2022 Audio signal processing\\n\\u2022 Biological and biomedical signal\\n\\u2022 Complexity in signal processing\\n\\u2022 Digital signal processing\\n\\u2022 Distributed and network signal\\n\\u2022 Image and video processing\\n\\u2022 Linear and nonlinear \\ufb01ltering\\n\\u2022 Multidimensional signal\\n\\u2022 Multimodal signal processing\\n\\u2022 Multirate signal processing\\n\\u2022 Multiresolution signal processing\\n\\u2022 Nonlinear signal processing\\n\\u2022 Randomized algorithms in signal\\n\\u2022 Sensor and multiple source signal\\n\\nprocessing\\n\\nprocessing\\n\\nprocessing, source separation\\n\\n\\u2022 Signal decompositions, subband\\nand transform methods, sparse\\nrepresentations\\n\\n\\u2022 Signal processing for\\n\\ncommunications\\n\\n\\u2022 Signal processing for security and\\nforensic analysis, biometric signal\\nprocessing\\n\\n\\u2022 Signal quantization, sampling,\\nanalog-to-digital conversion,\\ncoding and compression\\n\\u2022 Signal reconstruction,\\n\\ndigital-to-analog conversion,\\nenhancement, decoding and\\ninverse problems\\n\\n\\u2022 Speech/audio/image/video\\n\\n\\u2022 Speech and spoken language\\n\\ncompression\\n\\nprocessing\\n\\n\\u2022 Statistical/machine learning\\n\\u2022 Statistical signal processing\\n\\nInformation for Librarians\\nFoundations and Trends R(cid:1) in Signal Processing, 2013, Volume 7, 4 issues. ISSN\\npaper version 1932-8346. ISSN online version 1932-8354. Also available as a\\ncombined paper and online subscription.\\n\\nFull text available at: http://dx.doi.org/10.1561/2000000039\\x0cFoundations and Trends R(cid:1) in Signal Processing\\nVol. 7, Nos. 3\\u20134 (2013) 197\\u2013387\\nc(cid:1) 2014 L. Deng and D. Yu\\nDOI: 10.1561/2000000039\\n\\nDeep Learning: Methods and Applications\\n\\nLi Deng\\n\\nMicrosoft Research\\nOne Microsoft Way\\n\\nRedmond, WA 98052; USA\\n\\ndeng@microsoft.com\\n\\nDong Yu\\n\\nMicrosoft Research\\nOne Microsoft Way\\n\\nRedmond, WA 98052; USA\\nDong.Yu@microsoft.com\\n\\nFull text available at: http://dx.doi.org/10.1561/2000000039\\x0cContents\\n\\nEndorsement\\n\\n1 Introduction\\n\\n1.1 De\\ufb01nitions and background . . . . . . . . . . . . . . . . .\\n. . . . . . . . . . . . . .\\n1.2 Organization of this monograph\\n\\n2 Some Historical Context of Deep Learning\\n\\n3 Three Classes of Deep Learning Networks\\n\\n3.1 A three-way categorization . . . . . . . . . . . . . . . . .\\n3.2 Deep networks for unsupervised or generative learning . . .\\n3.3 Deep networks for supervised learning . . . . . . . . . . .\\n3.4 Hybrid deep networks . . . . . . . . . . . . . . . . . . . .\\n\\n4 Deep Autoencoders \\u2014 Unsupervised Learning\\n\\nIntroduction . . . . . . . . . . . . . . . . . . . . . . . . .\\n4.1\\n4.2 Use of deep autoencoders to extract speech features\\n. . .\\n4.3 Stacked denoising autoencoders . . . . . . . . . . . . . . .\\n4.4 Transforming autoencoders . . . . . . . . . . . . . . . . .\\n\\n5 Pre-Trained Deep Neural Networks \\u2014 A Hybrid\\n\\n5.1 Restricted Boltzmann machines . . . . . . . . . . . . . . .\\n\\n1\\n\\n3\\n3\\n7\\n\\n10\\n\\n19\\n19\\n21\\n28\\n31\\n\\n35\\n35\\n36\\n40\\n44\\n\\n46\\n46\\n\\nii\\n\\nFull text available at: http://dx.doi.org/10.1561/2000000039\\x0c5.2 Unsupervised layer-wise pre-training . . . . . . . . . . . .\\n. . . . . . . . . . . . . . .\\n5.3\\n\\nInterfacing DNNs with HMMs\\n\\n6 Deep Stacking Networks and Variants \\u2014\\n\\nSupervised Learning\\n6.1\\nIntroduction . . . . . . . . . . . . . . . . . . . . . . . . .\\n6.2 A basic architecture of the deep stacking network . . . . .\\n6.3 A method for learning the DSN weights\\n. . . . . . . . . .\\n6.4 The tensor deep stacking network . . . . . . . . . . . . . .\\n6.5 The Kernelized deep stacking network . . . . . . . . . . .\\n\\n7 Selected Applications in Speech and Audio Processing\\n\\n7.1 Acoustic modeling for speech recognition . . . . . . . . . .\\n7.2 Speech synthesis . . . . . . . . . . . . . . . . . . . . . . .\\n7.3 Audio and music processing . . . . . . . . . . . . . . . . .\\n\\niii\\n\\n50\\n53\\n\\n55\\n55\\n57\\n59\\n60\\n62\\n\\n67\\n67\\n91\\n93\\n\\n8 Selected Applications in Language\\n\\n97\\nModeling and Natural Language Processing\\n8.1 Language modeling . . . . . . . . . . . . . . . . . . . . .\\n98\\n8.2 Natural language processing . . . . . . . . . . . . . . . . . 104\\n\\n9 Selected Applications in Information Retrieval\\n113\\n9.1 A brief introduction to information retrieval\\n. . . . . . . . 113\\n9.2 SHDA for document indexing and retrieval . . . . . . . . . 115\\n9.3 DSSM for document retrieval . . . . . . . . . . . . . . . . 116\\n9.4 Use of deep stacking networks for information retrieval\\n. . 122\\n\\n10 Selected Applications in Object Recognition\\n\\nand Computer Vision\\n125\\n10.1 Unsupervised or generative feature learning . . . . . . . . 126\\n10.2 Supervised feature learning and classi\\ufb01cation . . . . . . . . 129\\n\\n11 Selected Applications in Multimodal\\n\\nand Multi-task Learning\\n136\\n11.1 Multi-modalities: Text and image . . . . . . . . . . . . . . 137\\n. . . . . . . . . . . . 141\\n11.2 Multi-modalities: Speech and image\\n11.3 Multi-task learning within the speech, NLP or image\\n. . . 144\\n\\nFull text available at: http://dx.doi.org/10.1561/2000000039\\x0civ\\n\\n12 Conclusion\\n\\nReferences\\n\\n148\\n\\n154\\n\\nFull text available at: http://dx.doi.org/10.1561/2000000039\\x0cEndorsement\\n\\n\\u201cIn the past few years, deep learning has rapidly evolved into the\\nde-facto approach for acoustic modeling in automatic speech recogni-\\ntion (ASR), showing tremendous improvement in accuracy, robustness,\\nand cross-language generalizability over conventional approaches. This\\ntimely book is written by the pioneers of deep learning innovations\\nand applications to ASR, who, as early as 2010, \\ufb01rst succeeded in large\\nvocabulary speech recognition using deep learning. This was accom-\\nplished using a special form of the deep neural net, developed by the\\nauthors, perfectly \\ufb01t for fast decoding as required by industrial deploy-\\nment of ASR technology. In addition to recounting this remarkable\\nadvance which ignited the industry-scale adoption of deep learning in\\nASR, this book also provides an overview of a sweeping range of up-\\nto-date deep learning methodologies and its application to a variety of\\nsignal and information processing tasks, including not only ASR but\\nalso computer vision, language modeling, text processing, multimodal\\nlearning, and information retrieval. This is the \\ufb01rst and the most valu-\\nable book for \\u201cdeep and wide learning\\u201d of deep learning, not to be\\nmissed by anyone who wants to know the breath taking impact of deep\\nlearning in many facets of information processing, especially ASR, all\\nof vital importance to our modern technological society.\\u201d\\n\\n\\u2014 Sadaoki Furui, President of Toyota Technological Institute at\\nChicago, and Professor at the Tokyo Institute of Technology\\n\\nFull text available at: http://dx.doi.org/10.1561/2000000039\\x0cAbstract\\n\\nThis monograph provides an overview of general deep learning method-\\nology and its applications to a variety of signal and information pro-\\ncessing tasks. The application areas are chosen with the following three\\ncriteria in mind: (1) expertise or knowledge of the authors; (2) the\\napplication areas that have already been transformed by the successful\\nuse of deep learning technology, such as speech recognition and com-\\nputer vision; and (3) the application areas that have the potential to be\\nimpacted signi\\ufb01cantly by deep learning and that have been experienc-\\ning research growth, including natural language and text processing,\\ninformation retrieval, and multimodal information processing empow-\\nered by multi-task deep learning.\\n\\nL. Deng and D. Yu. Deep Learning: Methods and Applications. Foundations and\\nTrends R(cid:1) in Signal Processing, vol. 7, nos. 3\\u20134, pp. 197\\u2013387, 2013.\\nDOI: 10.1561/2000000039.\\n\\nFull text available at: http://dx.doi.org/10.1561/2000000039\\x0c1\\n\\nIntroduction\\n\\n1.1 De\\ufb01nitions and background\\n\\nSince 2006, deep structured learning, or more commonly called deep\\nlearning or hierarchical learning, has emerged as a new area of machine\\nlearning research [20, 163]. During the past several years, the techniques\\ndeveloped from deep learning research have already been impacting\\na wide range of signal and information processing work within the\\ntraditional and the new, widened scopes including key aspects of\\nmachine learning and arti\\ufb01cial intelligence; see overview articles in\\n[7, 20, 24, 77, 94, 161, 412], and also the media coverage of this progress\\nin [6, 237]. A series of workshops, tutorials, and special issues or con-\\nference special sessions in recent years have been devoted exclusively\\nto deep learning and its applications to various signal and information\\nprocessing areas. These include:\\n\\n\\u2022 2008 NIPS Deep Learning Workshop;\\n\\u2022 2009 NIPS Workshop on Deep Learning for Speech Recognition\\n\\nand Related Applications;\\n\\n\\u2022 2009 ICML Workshop on Learning Feature Hierarchies;\\n\\n3\\n\\nFull text available at: http://dx.doi.org/10.1561/2000000039\\x0c4\\n\\nIntroduction\\n\\n\\u2022 2011 ICML Workshop on Learning Architectures, Representa-\\ntions, and Optimization for Speech and Visual Information Pro-\\ncessing;\\n\\n\\u2022 2012 ICASSP Tutorial on Deep Learning for Signal and Informa-\\n\\ntion Processing;\\n\\n\\u2022 2012 ICML Workshop on Representation Learning;\\n\\u2022 2012 Special Section on Deep Learning for Speech and Language\\nProcessing in IEEE Transactions on Audio, Speech, and Lan-\\nguage Processing (T-ASLP, January);\\n\\n\\u2022 2010, 2011, and 2012 NIPS Workshops on Deep Learning and\\n\\nUnsupervised Feature Learning;\\n\\n\\u2022 2013 NIPS Workshops on Deep Learning and on Output Repre-\\n\\nsentation Learning;\\n\\n\\u2022 2013 Special Issue on Learning Deep Architectures in IEEE\\nTransactions on Pattern Analysis and Machine Intelligence\\n(T-PAMI, September).\\n\\n\\u2022 2013 International Conference on Learning Representations;\\n\\u2022 2013 ICML Workshop on Representation Learning Challenges;\\n\\u2022 2013 ICML Workshop on Deep Learning for Audio, Speech, and\\n\\nLanguage Processing;\\n\\n\\u2022 2013 ICASSP Special Session on New Types of Deep Neural Net-\\nwork Learning for Speech Recognition and Related Applications.\\n\\nThe authors have been actively involved in deep learning research and\\nin organizing or providing several of the above events, tutorials, and\\neditorials. In particular, they gave tutorials and invited lectures on\\nthis topic at various places. Part of this monograph is based on their\\ntutorials and lecture material.\\n\\nBefore embarking on describing details of deep learning, let\\u2019s pro-\\nvide necessary de\\ufb01nitions. Deep learning has various closely related\\nde\\ufb01nitions or high-level descriptions:\\n\\n\\u2022 De\\ufb01nition 1: A class of machine learning techniques that\\nexploit many layers of non-linear information processing for\\n\\nFull text available at: http://dx.doi.org/10.1561/2000000039\\x0c1.1. De\\ufb01nitions and background\\n\\n5\\n\\nsupervised or unsupervised feature extraction and transforma-\\ntion, and for pattern analysis and classi\\ufb01cation.\\n\\u2022 De\\ufb01nition 2: \\u201cA sub-\\ufb01eld within machine learning that is based\\non algorithms for learning multiple levels of representation in\\norder to model complex relationships among data. Higher-level\\nfeatures and concepts are thus de\\ufb01ned in terms of lower-level\\nones, and such a hierarchy of features is called a deep architec-\\nture. Most of these models are based on unsupervised learning of\\nrepresentations.\\u201d (Wikipedia on \\u201cDeep Learning\\u201d around March\\n2012.)\\n\\u2022 De\\ufb01nition 3: \\u201cA sub-\\ufb01eld of machine learning that is based\\non learning several levels of representations, corresponding to a\\nhierarchy of features or factors or concepts, where higher-level\\nconcepts are de\\ufb01ned from lower-level ones, and the same lower-\\nlevel concepts can help to de\\ufb01ne many higher-level concepts. Deep\\nlearning is part of a broader family of machine learning methods\\nbased on learning representations. An observation (e.g., an image)\\ncan be represented in many ways (e.g., a vector of pixels), but\\nsome representations make it easier to learn tasks of interest (e.g.,\\nis this the image of a human face?) from examples, and research\\nin this area attempts to de\\ufb01ne what makes better representations\\nand how to learn them.\\u201d (Wikipedia on \\u201cDeep Learning\\u201d around\\nFebruary 2013.)\\n\\u2022 De\\ufb01nition 4: \\u201cDeep learning is a set of algorithms in machine\\nlearning that attempt to learn in multiple levels, correspond-\\ning to di\\ufb00erent levels of abstraction. It typically uses arti\\ufb01cial\\nneural networks. The levels in these learned statistical models\\ncorrespond to distinct levels of concepts, where higher-level con-\\ncepts are de\\ufb01ned from lower-level ones, and the same lower-\\nlevel concepts can help to de\\ufb01ne many higher-level concepts.\\u201d\\nSee Wikipedia http://en.wikipedia.org/wiki/Deep_learning on\\n\\u201cDeep Learning\\u201d as of this most recent update in October 2013.\\n\\u2022 De\\ufb01nition 5: \\u201cDeep Learning is a new area of Machine Learning\\nresearch, which has been introduced with the objective of moving\\nMachine Learning closer to one of its original goals: Arti\\ufb01cial\\n\\nFull text available at: http://dx.doi.org/10.1561/2000000039\\x0c6\\n\\nIntroduction\\n\\nIntelligence. Deep Learning is about learning multiple levels of\\nrepresentation and abstraction that help to make sense of data\\nsuch as images, sound, and text.\\u201d See https://github.com/lisa-\\nlab/DeepLearningTutorials\\n\\nNote that the deep learning that we discuss in this monograph is\\nabout learning with deep architectures for signal and information pro-\\ncessing. It is not about deep understanding of the signal or infor-\\nmation, although in many cases they may be related. It should also\\nbe distinguished from the overloaded term in educational psychology:\\n\\u201cDeep learning describes an approach to learning that is character-\\nized by active engagement, intrinsic motivation, and a personal search\\nfor meaning.\\u201d http://www.blackwellreference.com/public/tocnode?id=\\ng9781405161251_chunk_g97814051612516_ss1-1\\n\\nCommon among the various high-level descriptions of deep learning\\nabove are two key aspects: (1) models consisting of multiple layers\\nor stages of nonlinear information processing; and (2) methods for\\nsupervised or unsupervised learning of\\nfeature representation at\\nsuccessively higher, more abstract layers. Deep learning is in the\\nintersections among the research areas of neural networks, arti\\ufb01cial\\nintelligence, graphical modeling, optimization, pattern recognition,\\nand signal processing. Three important reasons for the popularity\\nof deep learning today are the drastically increased chip processing\\nabilities (e.g., general-purpose graphical processing units or GPGPUs),\\nthe signi\\ufb01cantly increased size of data used for training, and the recent\\nadvances\\nin machine learning and signal/information processing\\nresearch. These advances have enabled the deep learning methods\\nto e\\ufb00ectively exploit complex, compositional nonlinear functions, to\\nlearn distributed and hierarchical feature representations, and to make\\ne\\ufb00ective use of both labeled and unlabeled data.\\n\\nActive researchers in this area include those at University of\\nToronto, New York University, University of Montreal, Stanford\\nUniversity, Microsoft Research (since 2009), Google (since about\\n2011), IBM Research (since about 2011), Baidu (since 2012), Facebook\\n(since 2013), UC-Berkeley, UC-Irvine,\\nIDSIA, University\\nCollege London, University of Michigan, Massachusetts Institute of\\n\\nIDIAP,\\n\\nFull text available at: http://dx.doi.org/10.1561/2000000039\\x0c1.2. Organization of this monograph\\n\\n7\\n\\nTechnology, University of Washington, and numerous other places; see\\nhttp://deeplearning.net/deep-learning-research-groups-and-labs/\\nfor\\na more detailed list. These researchers have demonstrated empirical\\nsuccesses of deep learning in diverse applications of computer vision,\\nphonetic recognition, voice search, conversational speech recognition,\\nspeech and image feature coding, semantic utterance classi\\ufb01ca-\\ntion, natural language understanding, hand-writing recognition, audio\\nprocessing, information retrieval, robotics, and even in the analysis of\\nmolecules that may lead to discovery of new drugs as reported recently\\nby [237].\\n\\nIn addition to the reference list provided at the end of this mono-\\ngraph, which may be outdated not long after the publication of this\\nmonograph, there are a number of excellent and frequently updated\\nreading lists, tutorials, software, and video lectures online at:\\n\\n\\u2022 http://deeplearning.net/reading-list/\\n\\u2022 http://u\\ufb02dl.stanford.edu/wiki/index.php/\\n\\nUFLDL_Recommended_Readings\\n\\n\\u2022 http://www.cs.toronto.edu/\\u223chinton/\\n\\u2022 http://deeplearning.net/tutorial/\\n\\u2022 http://u\\ufb02dl.stanford.edu/wiki/index.php/UFLDL_Tutorial\\n\\n1.2 Organization of this monograph\\n\\nThe rest of the monograph is organized as follows:\\n\\nIn Section 2, we provide a brief historical account of deep learning,\\nmainly from the perspective of how speech recognition technology has\\nbeen hugely impacted by deep learning, and how the revolution got\\nstarted and has gained and sustained immense momentum.\\n\\nIn Section 3, a three-way categorization scheme for a majority of\\nthe work in deep learning is developed. They include unsupervised,\\nsupervised, and hybrid deep learning networks, where in the latter cat-\\negory unsupervised learning (or pre-training) is exploited to assist the\\nsubsequent stage of supervised learning when the \\ufb01nal tasks pertain to\\nclassi\\ufb01cation. The supervised and hybrid deep networks often have the\\n\\nFull text available at: http://dx.doi.org/10.1561/2000000039\\x0c8\\n\\nIntroduction\\n\\nsame type of architectures or the structures in the deep networks, but\\nthe unsupervised deep networks tend to have di\\ufb00erent architectures\\nfrom the others.\\n\\nSections 4\\u20136 are devoted, respectively, to three popular types of\\ndeep architectures, one from each of the classes in the three-way cat-\\negorization scheme reviewed in Section 3. In Section 4, we discuss\\nin detail deep autoencoders as a prominent example of the unsuper-\\nvised deep learning networks. No class labels are used in the learning,\\nalthough supervised learning methods such as back-propagation are\\ncleverly exploited when the input signal itself, instead of any label\\ninformation of interest to possible classi\\ufb01cation tasks, is treated as the\\n\\u201csupervision\\u201d signal.\\n\\nIn Section 5, as a major example in the hybrid deep network cate-\\ngory, we present in detail the deep neural networks with unsupervised\\nand largely generative pre-training to boost the e\\ufb00ectiveness of super-\\nvised training. This bene\\ufb01t is found critical when the training data\\nare limited and no other appropriate regularization approaches (i.e.,\\ndropout) are exploited. The particular pre-training method based on\\nrestricted Boltzmann machines and the related deep belief networks\\ndescribed in this section has been historically signi\\ufb01cant as it ignited\\nthe intense interest in the early applications of deep learning to speech\\nrecognition and other information processing tasks. In addition to this\\nretrospective review, subsequent development and di\\ufb00erent paths from\\nthe more recent perspective are discussed.\\n\\nIn Section 6, the basic deep stacking networks and their several\\nextensions are discussed in detail, which exemplify the discrimina-\\ntive, supervised deep learning networks in the three-way classi\\ufb01cation\\nscheme. This group of deep networks operate in many ways that are\\ndistinct from the deep neural networks. Most notably, they use target\\nlabels in constructing each of many layers or modules in the overall\\ndeep networks. Assumptions made about part of the networks, such as\\nlinear output units in each of the modules, simplify the learning algo-\\nrithms and enable a much wider variety of network architectures to\\nbe constructed and learned than the networks discussed in Sections 4\\nand 5.\\n\\nFull text available at: http://dx.doi.org/10.1561/2000000039\\x0c1.2. Organization of this monograph\\n\\n9\\n\\nIn Sections 7\\u201311, we select a set of typical and successful applica-\\ntions of deep learning in diverse areas of signal and information process-\\ning. In Section 7, we review the applications of deep learning to speech\\nrecognition, speech synthesis, and audio processing. Subsections sur-\\nrounding the main subject of speech recognition are created based on\\nseveral prominent themes on the topic in the literature.\\n\\nIn Section 8, we present recent results of applying deep learning to\\nlanguage modeling and natural language processing, where we highlight\\nthe key recent development in embedding symbolic entities such as\\nwords into low-dimensional, continuous-valued vectors.\\n\\nSection 9 is devoted to selected applications of deep learning to\\n\\ninformation retrieval including web search.\\n\\nIn Section 10, we cover selected applications of deep learning to\\nimage object recognition in computer vision. The section is divided to\\ntwo main classes of deep learning approaches: (1) unsupervised feature\\nlearning, and (2) supervised learning for end-to-end and joint feature\\nlearning and classi\\ufb01cation.\\n\\nSelected applications to multi-modal processing and multi-task\\nlearning are reviewed in Section 11, divided into three categories\\naccording to the nature of the multi-modal data as inputs to the deep\\nlearning systems. For single-modality data of speech, text, or image,\\na number of recent multi-task learning studies based on deep learning\\nmethods are reviewed in the literature.\\n\\nFinally, conclusions are given in Section 12 to summarize the mono-\\n\\ngraph and to discuss future challenges and directions.\\n\\nThis short monograph contains the material expanded from two\\ntutorials that the authors gave, one at APSIPA in October 2011 and\\nthe other at ICASSP in March 2012. Substantial updates have been\\nmade based on the literature up to January 2014 (including the mate-\\nrials presented at NIPS-2013 and at IEEE-ASRU-2013 both held in\\nDecember of 2013), focusing on practical aspects in the fast develop-\\nment of deep learning research and technology during the interim years.\\n\\nFull text available at: http://dx.doi.org/10.1561/2000000039\\x0cReferences\\n\\n[1] O. Abdel-Hamid, L. Deng, and D. Yu. Exploring convolutional neural\\nnetwork structures and optimization for speech recognition. Proceedings\\nof Interspeech, 2013.\\n\\n[2] O. Abdel-Hamid, L. Deng, D. Yu, and H. Jiang. Deep segmental neural\\n\\nnetworks for speech recognition. In Proceedings of Interspeech. 2013.\\n\\n[3] O. Abdel-Hamid, A. Mohamed, H. Jiang, and G. Penn. Applying convo-\\nlutional neural networks concepts to hybrid NN-HMM model for speech\\nrecognition.\\nIn Proceedings of International Conference on Acoustics\\nSpeech and Signal Processing (ICASSP). 2012.\\n\\n[4] A. Acero, L. Deng, T. Kristjansson, and J. Zhang. HMM adaptation\\nusing vector taylor series for noisy speech recognition. In Proceedings\\nof Interspeech. 2000.\\n\\n[5] G. Alain and Y. Bengio. What regularized autoencoders learn from the\\ndata generating distribution. In Proceedings of International Conference\\non Learning Representations (ICLR). 2013.\\n\\n[6] G. Anthes. Deep learning comes of age. Communications of the Asso-\\n\\nciation for Computing Machinery (ACM), 56(6):13\\u201315, June 2013.\\n\\n[7] I. Arel, C. Rose, and T. Karnowski. Deep machine learning \\u2014 a new\\nfrontier in arti\\ufb01cial intelligence. IEEE Computational Intelligence Mag-\\nazine, 5:13\\u201318, November 2010.\\n\\n154\\n\\nFull text available at: http://dx.doi.org/10.1561/2000000039\\x0cReferences\\n\\n155\\n\\n[8] E. Arisoy, T. Sainath, B. Kingsbury, and B. Ramabhadran. Deep neural\\nnetwork language models. In Proceedings of the Joint Human Language\\nTechnology Conference and the North American Chapter of the Associ-\\nation of Computational Linguistics (HLT-NAACL) Workshop. 2012.\\n\\n[9] O. Aslan, H. Cheng, D. Schuurmans, and X. Zhang. Convex two-layer\\nIn Proceedings of Neural Information Processing Systems\\n\\nmodeling.\\n(NIPS). 2013.\\n\\n[10] J. Ba and B. Frey. Adaptive dropout for training deep neural networks.\\nIn Proceedings of Neural Information Processing Systems (NIPS). 2013.\\n[11] J. Baker, L. Deng, J. Glass, S. Khudanpur, C.-H. Lee, N. Morgan, and\\nD. O\\u2019Shaughnessy. Research developments and directions in speech\\nrecognition and understanding.\\nIEEE Signal Processing Magazine,\\n26(3):75\\u201380, May 2009.\\n\\n[12] J. Baker, L. Deng, J. Glass, S. Khudanpur, C.-H. Lee, N. Morgan, and\\nD. O\\u2019Shaughnessy. Updated MINS report on speech recognition and\\nunderstanding. IEEE Signal Processing Magazine, 26(4), July 2009.\\n\\n[13] P. Baldi and P. Sadowski. Understanding dropout. In Proceedings of\\n\\nNeural Information Processing Systems (NIPS). 2013.\\n\\n[14] E. Battenberg, E. Schmidt, and J. Bello.\\n\\nDeep learning for\\nmusic,\\nInternational Conference on Acoustics\\nSpeech and Signal Processing (ICASSP) (http://www.icassp2014.org/\\nspecial_sections.html#ss8), 2014.\\n\\nsession at\\n\\nspecial\\n\\n[15] E. Batternberg and D. Wessel. Analyzing drum patterns using condi-\\ntional deep belief networks. In Proceedings of International Symposium\\non Music Information Retrieval (ISMIR). 2012.\\n\\n[16] P. Bell, P. Swietojanski, and S. Renals. Multi-level adaptive networks\\nin tandem and hybrid ASR systems.\\nIn Proceedings of International\\nConference on Acoustics Speech and Signal Processing (ICASSP). 2013.\\n[17] Y. Bengio. Arti\\ufb01cial neural networks and their application to sequence\\nrecognition. Ph.D. Thesis, McGill University, Montreal, Canada, 1991.\\n[18] Y. Bengio. New distributed probabilistic language models. Technical\\n\\nReport, University of Montreal, 2002.\\n\\n[19] Y. Bengio. Neural net language models. Scholarpedia, 3, 2008.\\n[20] Y. Bengio. Learning deep architectures for AI.\\n\\nin Foundations and\\n\\nTrends in Machine Learning, 2(1):1\\u2013127, 2009.\\n\\nFull text available at: http://dx.doi.org/10.1561/2000000039\\x0c156\\n\\nReferences\\n\\n[21] Y. Bengio. Deep learning of representations for unsupervised and trans-\\nfer learning. Journal of Machine Learning Research Workshop and Con-\\nference Proceedings, 27:17\\u201337, 2012.\\n\\n[22] Y. Bengio. Deep learning of representations: Looking forward. In Sta-\\n\\ntistical Language and Speech Processing, pages 1\\u201337. Springer, 2013.\\n\\n[23] Y. Bengio, N. Boulanger, and R. Pascanu. Advances in optimizing recur-\\nrent networks. In Proceedings of International Conference on Acoustics\\nSpeech and Signal Processing (ICASSP). 2013.\\n\\n[24] Y. Bengio, A. Courville, and P. Vincent. Representation learning: A\\nreview and new perspectives. IEEE Transactions on Pattern Analysis\\nand Machine Intelligence (PAMI), 38:1798\\u20131828, 2013.\\n\\n[25] Y. Bengio, R. De Mori, G. Flammia, and R. Kompe. Global optimiza-\\ntion of a neural network-hidden markov model hybrid. IEEE Transac-\\ntions on Neural Networks, 3:252\\u2013259, 1992.\\n\\n[26] Y. Bengio, R. Ducharme, P. Vincent, and C. Jauvin. A neural proba-\\nbilistic language model. In Proceedings of Neural Information Processing\\nSystems (NIPS). 2000.\\n\\n[27] Y. Bengio, R. Ducharme, P. Vincent, and C. Jauvin. A neural proba-\\nbilistic language model. Journal of Machine Learning Research, 3:1137\\u2013\\n1155, 2003.\\n\\n[28] Y. Bengio, P. Lamblin, D. Popovici, and H. Larochelle. Greedy layer-\\nwise training of deep networks. In Proceedings of Neural Information\\nProcessing Systems (NIPS). 2006.\\n\\n[29] Y. Bengio, P. Simard, and P. Frasconi. Learning long-term dependen-\\nIEEE Transactions on Neural\\n\\ncies with gradient descent is di\\ufb03cult.\\nNetworks, 5:157\\u2013166, 1994.\\n\\n[30] Y. Bengio, E. Thibodeau-Laufer, and J. Yosinski. Deep generative\\nstochastic networks trainable by backprop.\\narXiv 1306:1091, 2013.\\nalso accepted to appear in Proceedings of International Conference on\\nMachine Learning (ICML), 2014.\\n\\n[31] Y. Bengio, L. Yao, G. Alain, and P. Vincent. Generalized denoising\\nautoencoders as generative models. In Proceedings of Neural Informa-\\ntion Processing Systems (NIPS). 2013.\\n\\n[32] J. Bergstra and Y. Bengio. Random search for hyper-parameter opti-\\n\\nmization. Journal on Machine Learning Research, 3:281\\u2013305, 2012.\\n\\nFull text available at: http://dx.doi.org/10.1561/2000000039\\x0cReferences\\n\\n157\\n\\n[33] A. Biem, S. Katagiri, E. McDermott, and B. Juang. An application\\nof discriminative feature extraction to \\ufb01lter-bank-based speech recog-\\nnition. IEEE Transactions on Speech and Audio Processing, 9:96\\u2013110,\\n2001.\\n\\n[34] J. Bilmes. Dynamic graphical models. IEEE Signal Processing Maga-\\n\\nzine, 33:29\\u201342, 2010.\\n\\n[35] J. Bilmes and C. Bartels. Graphical model architectures for speech\\n\\nrecognition. IEEE Signal Processing Magazine, 22:89\\u2013100, 2005.\\n\\n[36] A. Bordes, X. Glorot, J. Weston, and Y. Bengio. A semantic matching\\nenergy function for learning with multi-relational data \\u2014 application\\nto word-sense disambiguation. Machine Learning, May 2013.\\n\\n[37] A. Bordes, J. Weston, R. Collobert, and Y. Bengio. Learning structured\\nembeddings of knowledge bases. In Proceedings of Association for the\\nAdvancement of Arti\\ufb01cial Intelligence (AAAI). 2011.\\n\\n[38] L. Bottou. From machine learning to machine reasoning: An essay.\\n\\nJournal of Machine Learning Research, 14:3207\\u20133260, 2013.\\n\\n[39] L. Bottou and Y. LeCun. Large scale online learning. In Proceedings of\\n\\nNeural Information Processing Systems (NIPS). 2004.\\n\\n[40] N. Boulanger-Lewandowski, Y. Bengio, and P. Vincent. Modeling\\nTemporal dependencies in high-dimensional sequences: Application to\\npolyphonic music generation and transcription. In Proceedings of Inter-\\nnational Conference on Machine Learning (ICML). 2012.\\n\\n[41] N. Boulanger-Lewandowski, Y. Bengio, and P. Vincent. Audio chord\\nrecognition with recurrent neural networks. In Proceedings of Interna-\\ntional Symposium on Music Information Retrieval (ISMIR). 2013.\\n\\n[42] H. Bourlard and N. Morgan. Connectionist Speech Recognition: A\\n\\nHybrid Approach. Kluwer, Norwell, MA, 1993.\\n\\n[43] J. Bouvrie. Hierarchical learning: Theory with applications in speech\\n\\nand vision. Ph.D. thesis, MIT, 2009.\\n\\n[44] L. Breiman. Stacked regression. Machine Learning, 24:49\\u201364, 1996.\\n[45] J. Bridle, L. Deng, J. Picone, H. Richards, J. Ma, T. Kamm, M. Schus-\\nter, S. Pike, and R. Reagan. An investigation of segmental hidden\\ndynamic models of speech coarticulation for automatic speech recogni-\\ntion. Final Report for 1998 Workshop on Language Engineering, CLSP,\\nJohns Hopkins, 1998.\\n\\n[46] P. Cardinal, P. Dumouchel, and G. Boulianne. Large vocabulary speech\\nIEEE Transactions on Audio,\\n\\nrecognition on parallel architectures.\\nSpeech, and Language Processing, 21(11):2290\\u20132300, November 2013.\\n\\nFull text available at: http://dx.doi.org/10.1561/2000000039\\x0c158\\n\\nReferences\\n\\n[47] R. Caruana. Multitask learning. Machine Learning, 28:41\\u201375, 1997.\\n[48] J. Chen and L. Deng. A primal-dual method for training recurrent\\nneural networks constrained by the echo-state property. In Proceedings\\nof International Conference on Learning Representations. April 2014.\\n[49] X. Chen, A. Eversole, G. Li, D. Yu, and F. Seide. Pipelined back-\\npropagation for context-dependent deep neural networks. In Proceedings\\nof Interspeech. 2012.\\n\\n[50] R. Chengalvarayan and L. Deng. Hmm-based speech recognition using\\nstate-dependent, discriminatively derived transforms on Mel-warped\\nDFT features.\\nIEEE Transactions on Speech and Audio Processing,\\npages 243\\u2013256, 1997.\\n\\n[51] R. Chengalvarayan and L. Deng. Use of generalized dynamic feature\\nparameters for speech recognition. IEEE Transactions on Speech and\\nAudio Processing, pages 232\\u2013242, 1997a.\\n\\n[52] R. Chengalvarayan and L. Deng. Speech trajectory discrimination using\\nthe minimum classi\\ufb01cation error learning. IEEE Transactions on Speech\\nand Audio Processing, 6(6):505\\u2013515, 1998.\\n\\n[53] Y. Cho and L. Saul. Kernel methods for deep learning. In Proceedings of\\nNeural Information Processing Systems (NIPS), pages 342\\u2013350. 2009.\\n[54] D. Ciresan, A. Giusti, L. Gambardella, and J. Schmidhuber. Deep neural\\nnetworks segment neuronal membranes in electron microscopy images.\\nIn Proceedings of Neural Information Processing Systems (NIPS). 2012.\\n[55] D. Ciresan, U. Meier, L. Gambardella, and J. Schmidhuber. Deep, big,\\nsimple neural nets for handwritten digit recognition. Neural Computa-\\ntion, December 2010.\\n\\n[56] D. Ciresan, U. Meier, J. Masci, and J. Schmidhuber. A committee of\\nneural networks for tra\\ufb03c sign classi\\ufb01cation. In Proceedings of Interna-\\ntional Joint Conference on Neural Networks (IJCNN). 2011.\\n\\n[57] D. Ciresan, U. Meier, and J. Schmidhuber. Multi-column deep neural\\nnetworks for image classi\\ufb01cation. In Proceedings of Computer Vision\\nand Pattern Recognition (CVPR). 2012.\\n\\n[58] D. C. Ciresan, U. Meier, and J. Schmidhuber. Transfer learning for Latin\\nand Chinese characters with deep neural networks. In Proceedings of\\nInternational Joint Conference on Neural Networks (IJCNN). 2012.\\n\\n[59] A. Coates, B. Huval, T. Wang, D. Wu, A. Ng, and B. Catanzaro. Deep\\nlearning with COTS HPC. In Proceedings of International Conference\\non Machine Learning (ICML). 2013.\\n\\nFull text available at: http://dx.doi.org/10.1561/2000000039\\x0cReferences\\n\\n159\\n\\n[60] W. Cohen and R. V. de Carvalho. Stacked sequential learning.\\n\\nIn\\nProceedings of International Joint Conference on Arti\\ufb01cial Intelligence\\n(IJCAI), pages 671\\u2013676. 2005.\\n\\n[61] R. Collobert. Deep learning for e\\ufb03cient discriminative parsing.\\n\\nIn\\nProceedings of Arti\\ufb01cial Intelligence and Statistics (AISTATS). 2011.\\n[62] R. Collobert and J. Weston. A uni\\ufb01ed architecture for natural language\\nprocessing: Deep neural networks with multitask learning. In Proceed-\\nings of International Conference on Machine Learning (ICML). 2008.\\n[63] R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu, and\\nP. Kuksa. Natural language processing (almost) from scratch. Journal\\non Machine Learning Research, 12:2493\\u20132537, 2011.\\n\\n[64] G. Dahl, M. Ranzato, A. Mohamed, and G. Hinton. Phone recognition\\nwith the mean-covariance restricted boltzmann machine. In Proceedings\\nof Neural Information Processing Systems (NIPS), volume 23, pages\\n469\\u2013477. 2010.\\n\\n[65] G. Dahl, T. Sainath, and G. Hinton. Improving deep neural networks\\nfor LVCSR using recti\\ufb01ed linear units and dropout.\\nIn Proceedings\\nof International Conference on Acoustics Speech and Signal Processing\\n(ICASSP). 2013.\\n\\n[66] G. Dahl, J. Stokes, L. Deng, and D. Yu. Large-scale malware classi\\ufb01-\\ncation using random projections and neural networks. In Proceedings\\nof International Conference on Acoustics Speech and Signal Processing\\n(ICASSP). 2013.\\n\\n[67] G. Dahl, D. Yu, L. Deng, and A. Acero. Context-dependent DBN-\\nHMMs in large vocabulary continuous speech recognition. In Proceed-\\nings of International Conference on Acoustics Speech and Signal Pro-\\ncessing (ICASSP). 2011.\\n\\n[68] G. Dahl, D. Yu, L. Deng, and A. Acero. Context-dependent, pre-trained\\ndeep neural networks for large vocabulary speech recognition. IEEE\\nTransactions on Audio, Speech, & Language Processing, 20(1):30\\u201342,\\nJanuary 2012.\\n\\n[69] J. Dean, G. Corrado, R. Monga, K. Chen, M. Devin, Q. Le, M. Mao,\\nM. Ranzato, A. Senior, P. Tucker, K. Yang, and A. Ng. Large scale\\ndistributed deep networks. In Proceedings of Neural Information Pro-\\ncessing Systems (NIPS). 2012.\\n\\n[70] K. Demuynck and F. Triefenbach. Porting concepts from DNNs back\\nIn Proceedings of the Automatic Speech Recognition and\\n\\nto GMMs.\\nUnderstanding Workshop (ASRU). 2013.\\n\\nFull text available at: http://dx.doi.org/10.1561/2000000039\\x0c160\\n\\nReferences\\n\\n[71] L. Deng. A generalized hidden Markov model with state-conditioned\\nSignal Processing,\\n\\ntrend functions of time for the speech signal.\\n27(1):65\\u201378, 1992.\\n\\n[72] L. Deng. A stochastic model of speech incorporating hierarchical nonsta-\\ntionarity. IEEE Transactions on Speech and Audio Processing, 1(4):471\\u2013\\n475, 1993.\\n\\n[73] L. Deng. A dynamic, feature-based approach to the interface between\\nphonology and phonetics for speech modeling and recognition. Speech\\nCommunication, 24(4):299\\u2013323, 1998.\\n\\n[74] L. Deng. Computational models for speech production.\\n\\nIn Compu-\\ntational Models of Speech Pattern Processing, pages 199\\u2013213. Springer\\nVerlag, 1999.\\n\\n[75] L. Deng. Switching dynamic system models for speech articulation and\\nacoustics. In Mathematical Foundations of Speech and Language Pro-\\ncessing, pages 115\\u2013134. Springer-Verlag, New York, 2003.\\n\\n[76] L. Deng. Dynamic Speech Models \\u2014 Theory, Algorithm, and Applica-\\n\\ntion. Morgan & Claypool, December 2006.\\n\\n[77] L. Deng. An overview of deep-structured learning for information pro-\\ncessing. In Proceedings of Asian-Paci\\ufb01c Signal & Information Process-\\ning Annual Summit and Conference (APSIPA-ASC). October 2011.\\n\\n[78] L. Deng. The MNIST database of handwritten digit images for machine\\nlearning research. IEEE Signal Processing Magazine, 29(6), November\\n2012.\\n\\n[79] L. Deng. Design and learning of output representations for speech recog-\\nnition. In Neural Information Processing Systems (NIPS) Workshop on\\nLearning Output Representations. December 2013.\\n\\n[80] L. Deng. A tutorial survey of architectures, algorithms, and applications\\nfor deep learning.\\nIn Asian-Paci\\ufb01c Signal & Information Processing\\nAssociation Transactions on Signal and Information Processing. 2013.\\n[81] L. Deng, O. Abdel-Hamid, and D. Yu. A deep convolutional neural\\nnetwork using heterogeneous pooling for trading acoustic invariance\\nwith phonetic confusion.\\nIn Proceedings of International Conference\\non Acoustics Speech and Signal Processing (ICASSP). 2013.\\n\\n[82] L. Deng, A. Acero, L. Jiang, J. Droppo, and X. Huang. High perfor-\\nmance robust speech recognition using stereo training data.\\nIn Pro-\\nceedings of International Conference on Acoustics Speech and Signal\\nProcessing (ICASSP). 2001.\\n\\nFull text available at: http://dx.doi.org/10.1561/2000000039\\x0cReferences\\n\\n161\\n\\n[83] L. Deng and M. Aksmanovic. Speaker-independent phonetic classi\\ufb01-\\ncation using hidden markov models with state-conditioned mixtures of\\ntrend functions. IEEE Transactions on Speech and Audio Processing,\\n5:319\\u2013324, 1997.\\n\\n[84] L. Deng, M. Aksmanovic, D. Sun, and J. Wu. Speech recognition using\\nhidden Markov models with polynomial regression functions as nonsta-\\ntionary states.\\nIEEE Transactions on Speech and Audio Processing,\\n2(4):507\\u2013520, 1994.\\n\\n[85] L. Deng and J. Chen. Sequence classi\\ufb01cation using the high-level fea-\\ntures extracted from deep neural networks. In Proceedings of Interna-\\ntional Conference on Acoustics Speech and Signal Processing (ICASSP).\\n2014.\\n\\n[86] L. Deng and K. Erler. Structural design of a hidden Markov model\\nbased speech recognizer using multi-valued phonetic features: Compar-\\nison with segmental speech units. Journal of the Acoustical Society of\\nAmerica, 92(6):3058\\u20133067, 1992.\\n\\n[87] L. Deng, K. Hassanein, and M. Elmasry. Analysis of correlation struc-\\nture for a neural predictive model with application to speech recognition.\\nNeural Networks, 7(2):331\\u2013339, 1994.\\n\\n[88] L. Deng, X. He, and J. Gao. Deep stacking networks for informa-\\ntion retrieval. In Proceedings of International Conference on Acoustics\\nSpeech and Signal Processing (ICASSP). 2013c.\\n\\n[89] L. Deng, G. Hinton, and B. Kingsbury. New types of deep neural\\nnetwork learning for speech recognition and related applications: An\\noverview.\\nIn Proceedings of International Conference on Acoustics\\nSpeech and Signal Processing (ICASSP). 2013b.\\n\\n[90] L. Deng and X. D. Huang. Challenges in adopting speech recognition.\\nCommunications of the Association for Computing Machinery (ACM),\\n47(1):11\\u201313, January 2004.\\n\\n[91] L. Deng, B. Hutchinson, and D. Yu. Parallel training of deep stacking\\n\\nnetworks. In Proceedings of Interspeech. 2012b.\\n\\n[92] L. Deng, M. Lennig, V. Gupta, F. Seitz, P. Mermelstein, and P. Kenny.\\nPhonemic hidden Markov models with continuous mixture output den-\\nsities for large vocabulary word recognition.\\nIEEE Transactions on\\nSignal Processing, 39(7):1677\\u20131681, 1991.\\n\\n[93] L. Deng, M. Lennig, F. Seitz, and P. Mermelstein. Large vocabulary\\nword recognition using context-dependent allophonic hidden Markov\\nmodels. Computer Speech and Language, 4(4):345\\u2013357, 1990.\\n\\nFull text available at: http://dx.doi.org/10.1561/2000000039\\x0c162\\n\\nReferences\\n\\n[94] L. Deng, J. Li, K. Huang, Yao, D. Yu, F. Seide, M. Seltzer, G. Zweig,\\nX. He, J. Williams, Y. Gong, and A. Acero. Recent advances in deep\\nlearning for speech research at Microsoft.\\nIn Proceedings of Interna-\\ntional Conference on Acoustics Speech and Signal Processing (ICASSP).\\n2013a.\\n\\n[95] L. Deng and X. Li. Machine learning paradigms in speech recogni-\\ntion: An overview. IEEE Transactions on Audio, Speech, & Language,\\n21:1060\\u20131089, May 2013.\\n\\n[96] L. Deng and J. Ma. Spontaneous speech recognition using a statistical\\ncoarticulatory model for the vocal tract resonance dynamics. Journal\\nof the Acoustical Society America, 108:3036\\u20133048, 2000.\\n\\n[97] L. Deng and D. O\\u2019Shaughnessy. Speech Processing \\u2014 A Dynamic and\\n\\nOptimization-Oriented Approach. Marcel Dekker, 2003.\\n\\n[98] L. Deng, G. Ramsay, and D. Sun. Production models as a structural\\nbasis for automatic speech recognition. Speech Communication, 33(2\\u2013\\n3):93\\u2013111, August 1997.\\n\\n[99] L. Deng and H. Sameti. Transitional speech units and their represen-\\ntation by regressive Markov states: Applications to speech recognition.\\nIEEE Transactions on speech and audio processing, 4(4):301\\u2013306, July\\n1996.\\n\\n[100] L. Deng, M. Seltzer, D. Yu, A. Acero, A. Mohamed, and G. Hinton.\\nIn\\n\\nBinary coding of speech spectrograms using a deep autoencoder.\\nProceedings of Interspeech. 2010.\\n\\n[101] L. Deng and D. Sun. A statistical approach to automatic speech\\nrecognition using the atomic speech units constructed from overlap-\\nping articulatory features. Journal of the Acoustical Society of America,\\n85(5):2702\\u20132719, 1994.\\n\\n[102] L. Deng, G. Tur, X. He, and D. Hakkani-Tur. Use of kernel deep convex\\nnetworks and end-to-end learning for spoken language understanding.\\nIn Proceedings of IEEE Workshop on Spoken Language Technologies.\\nDecember 2012.\\n\\n[103] L. Deng, K. Wang, A. Acero, H. W. Hon, J. Droppo, C. Boulis, Y. Wang,\\nD. Jacoby, M. Mahajan, C. Chelba, and X. Huang. Distributed speech\\nprocessing in mipad\\u2019s multimodal user interface. IEEE Transactions on\\nSpeech and Audio Processing, 10(8):605\\u2013619, 2002.\\n\\n[104] L. Deng, J. Wu, J. Droppo, and A. Acero. Dynamic compensation of\\nHMM variances using the feature enhancement uncertainty computed\\nfrom a parametric model of speech distortion. IEEE Transactions on\\nSpeech and Audio Processing, 13(3):412\\u2013421, 2005.\\n\\nFull text available at: http://dx.doi.org/10.1561/2000000039\\x0cReferences\\n\\n163\\n\\n[105] L. Deng and D. Yu. Use of di\\ufb00erential cepstra as acoustic features\\nin hidden trajectory modeling for phonetic recognition. In Proceedings\\nof International Conference on Acoustics Speech and Signal Processing\\n(ICASSP). 2007.\\n\\n[106] L. Deng and D. Yu. Deep convex network: A scalable architecture for\\n\\nspeech pattern classi\\ufb01cation. In Proceedings of Interspeech. 2011.\\n\\n[107] L. Deng, D. Yu, and A. Acero. A bidirectional target \\ufb01ltering model of\\nspeech coarticulation: Two-stage implementation for phonetic recogni-\\ntion. IEEE Transactions on Audio and Speech Processing, 14(1):256\\u2013\\n265, January 2006.\\n\\n[108] L. Deng, D. Yu, and A. Acero. Structured speech modeling.\\n\\nIEEE\\nTransactions on Audio, Speech and Language Processing, 14(5):1492\\u2013\\n1504, September 2006.\\n\\n[109] L. Deng, D. Yu, and G. Hinton. Deep learning for speech recognition and\\nrelated applications. Neural Information Processing Systems (NIPS)\\nWorkshop, 2009.\\n\\n[110] L. Deng, D. Yu, and J. Platt. Scalable stacking and learning for build-\\ning deep architectures. In Proceedings of International Conference on\\nAcoustics Speech and Signal Processing (ICASSP). 2012a.\\n\\n[111] T. Deselaers, S. Hasan, O. Bender, and H. Ney. A deep learning\\napproach to machine transliteration. In Proceedings of 4th Workshop on\\nStatistical Machine Translation, pages 233\\u2013241. Athens, Greece, March\\n2009.\\n\\n[112] A. Diez. Automatic language recognition using deep neural networks.\\n\\nThesis, Universidad Autonoma de Madrid, SPAIN, September 2013.\\n\\n[113] P. Dognin and V. Goel. Combining stochastic average gradient and\\nhessian-free optimization for sequence training of deep neural networks.\\nIn Proceedings of the Automatic Speech Recognition and Understanding\\nWorkshop (ASRU). 2013.\\n\\n[114] D. Erhan, Y. Bengio, A. Courvelle, P. Manzagol, P. Vencent, and S. Ben-\\ngio. Why does unsupervised pre-training help deep learning? Journal\\non Machine Learning Research, pages 201\\u2013208, 2010.\\n\\n[115] R. Fernandez, A. Rendel, B. Ramabhadran, and R. Hoory. F0 contour\\nprediction with a deep belief network-gaussian process hybrid model. In\\nProceedings of International Conference on Acoustics Speech and Signal\\nProcessing (ICASSP), pages 6885\\u20136889. 2013.\\n\\n[116] S. Fine, Y. Singer, and N. Tishby. The hierarchical hidden Markov\\nmodel: Analysis and applications. Machine Learning, 32:41\\u201362, 1998.\\n\\nFull text available at: http://dx.doi.org/10.1561/2000000039\\x0c164\\n\\nReferences\\n\\n[117] A. Frome, G. Corrado, J. Shlens, S. Bengio, J. Dean, M. Ranzato, and\\nT. Mikolov. Devise: A deep visual-semantic embedding model. In Pro-\\nceedings of Neural Information Processing Systems (NIPS). 2013.\\n\\n[118] Q. Fu, X. He, and L. Deng. Phone-discriminating minimum classi\\ufb01ca-\\ntion error (p-mce) training for phonetic recognition. In Proceedings of\\nInterspeech. 2007.\\n\\n[119] M. Gales. Model-based approaches to handling uncertainty. In Robust\\nSpeech Recognition of Uncertain or Missing Data: Theory and Applica-\\ntion, pages 101\\u2013125. Springer, 2011.\\n\\n[120] J. Gao, X. He, and J.-Y. Nie. Clickthrough-based translation models\\nfor web search: From word models to phrase models. In Proceedings of\\nConference on Information and Knowledge Management (CIKM). 2010.\\n[121] J. Gao, X. He, W. Yih, and L. Deng. Learning semantic representations\\nfor the phrase translation model.\\nIn Proceedings of Neural Informa-\\ntion Processing Systems (NIPS) Workshop on Deep Learning. December\\n2013.\\n\\n[122] J. Gao, X. He, W. Yih, and L. Deng. Learning semantic representations\\nfor the phrase translation model. MSR-TR-2013-88, September 2013.\\n[123] J. Gao, X. He, W. Yih, and L. Deng. Learning continuous phrase rep-\\nresentations for translation modeling. In Proceedings of Association for\\nComputational Linguistics (ACL). 2014.\\n\\n[124] J. Gao, K. Toutanova, and W.-T. Yih. Clickthrough-based latent seman-\\ntic models for web search. In Proceedings of Special Interest Group on\\nInformation Retrieval (SIGIR). 2011.\\n\\n[125] R. Gens and P. Domingo. Discriminative learning of sum-product net-\\n\\nworks. Neural Information Processing Systems (NIPS), 2012.\\n\\n[126] D. George. How the brain might work: A hierarchical and temporal\\nmodel for learning and recognition. Ph.D. thesis, Stanford University,\\n2008.\\n\\n[127] M. Gibson and T. Hain. Error approximation and minimum phone error\\nacoustic model estimation. IEEE Transactions on Audio, Speech, and\\nLanguage Processing, 18(6):1269\\u20131279, August 2010.\\n\\n[128] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature\\nhierarchies for accurate object detection and semantic segmentation.\\narXiv:1311.2524v1, 2013.\\n\\n[129] X. Glorot and Y. Bengio. Understanding the di\\ufb03culty of training deep\\nfeed-forward neural networks. In Proceedings of Arti\\ufb01cial Intelligence\\nand Statistics (AISTATS). 2010.\\n\\nFull text available at: http://dx.doi.org/10.1561/2000000039\\x0cReferences\\n\\n165\\n\\n[130] X. Glorot, A. Bordes, and Y. Bengio. Deep sparse recti\\ufb01er neural\\nnetworks. In Proceedings of Arti\\ufb01cial Intelligence and Statistics (AIS-\\nTATS). April 2011.\\n\\n[131] I. Goodfellow, M. Mirza, A. Courville, and Y. Bengio. Multi-prediction\\ndeep boltzmann machines. In Proceedings of Neural Information Pro-\\ncessing Systems (NIPS). 2013.\\n\\n[132] E. Grais, M. Sen, and H. Erdogan. Deep neural networks for single\\n\\nchannel source separation. arXiv:1311.2746v1, 2013.\\n\\n[133] A. Graves. Sequence transduction with recurrent neural networks. Rep-\\nresentation Learning Workshop, International Conference on Machine\\nLearning (ICML), 2012.\\n\\n[134] A. Graves, S. Fernandez, F. Gomez, and J. Schmidhuber. Connection-\\nist temporal classi\\ufb01cation: Labeling unsegmented sequence data with\\nrecurrent neural networks. In Proceedings of International Conference\\non Machine Learning (ICML). 2006.\\n\\n[135] A. Graves, N. Jaitly, and A. Mohamed. Hybrid speech recognition with\\ndeep bidirectional LSTM. In Proceedings of the Automatic Speech Recog-\\nnition and Understanding Workshop (ASRU). 2013.\\n\\n[136] A. Graves, A. Mohamed, and G. Hinton. Speech recognition with deep\\nrecurrent neural networks. In Proceedings of International Conference\\non Acoustics Speech and Signal Processing (ICASSP). 2013.\\n\\n[137] F. Grezl and P. Fousek. Optimizing bottle-neck features for LVCSR. In\\nProceedings of International Conference on Acoustics Speech and Signal\\nProcessing (ICASSP). 2008.\\n\\n[138] C. Gulcehre, K. Cho, R. Pascanu, and Y. Bengio.\\n\\nLearned-\\nnorm pooling for deep feedforward and recurrent neural networks.\\nhttp://arxiv.org/abs/1311.1780, 2014.\\n\\n[139] M. Gutmann and A. Hyvarinen. Noise-contrastive estimation of unnor-\\nmalized statistical models, with applications to natural image statistics.\\nJournal of Machine Learning Research, 13:307\\u2013361, 2012.\\n\\n[140] T. Hain, L. Burget, J. Dines, P. Garner, F. Grezl, A. Hannani, M. Hui-\\njbregts, M. Kara\\ufb01at, M. Lincoln, and V. Wan. Transcribing meetings\\nwith the AMIDA systems. IEEE Transactions on Audio, Speech, and\\nLanguage Processing, 20:486\\u2013498, 2012.\\n\\n[141] P. Hamel and D. Eck. Learning features from music audio with deep\\nbelief networks. In Proceedings of International Symposium on Music\\nInformation Retrieval (ISMIR). 2010.\\n\\nFull text available at: http://dx.doi.org/10.1561/2000000039\\x0c166\\n\\nReferences\\n\\n[142] G. Hawkins, S. Ahmad, and D. Dubinsky. Hierarchical temporal mem-\\nory including HTM cortical learning algorithms. Numenta Technical\\nReport, December 10 2010.\\n\\n[143] J. Hawkins and S. Blakeslee. On Intelligence: How a New Understanding\\nof the Brain will lead to the Creation of Truly Intelligent Machines.\\nTimes Books, New York, 2004.\\n\\n[144] X. He and L. Deng. Speech recognition, machine translation, and speech\\ntranslation \\u2014 a unifying discriminative framework. IEEE Signal Pro-\\ncessing Magazine, 28, November 2011.\\n\\n[145] X. He and L. Deng. Optimization in speech-centric information process-\\ning: Criteria and techniques. In Proceedings of International Conference\\non Acoustics Speech and Signal Processing (ICASSP). 2012.\\n\\n[146] X. He and L. Deng.\\n\\nSpeech-centric information processing: An\\n\\noptimization-oriented approach. In Proceedings of the IEEE. 2013.\\n\\n[147] X. He, L. Deng, and W. Chou. Discriminative learning in sequential pat-\\ntern recognition \\u2014 a unifying review for optimization-oriented speech\\nrecognition. IEEE Signal Processing Magazine, 25:14\\u201336, 2008.\\n\\n[148] G. Heigold, H. Ney, P. Lehnen, T. Gass, and R. Schluter. Equivalence of\\ngenerative and log-liner models. IEEE Transactions on Audio, Speech,\\nand Language Processing, 19(5):1138\\u20131148, February 2011.\\n\\n[149] G. Heigold, H. Ney, and R. Schluter. Investigations on an EM-style opti-\\nmization algorithm for discriminative training of HMMs. IEEE Trans-\\nactions on Audio, Speech, and Language Processing, 21(12):2616\\u20132626,\\nDecember 2013.\\n\\n[150] G. Heigold, V. Vanhoucke, A. Senior, P. Nguyen, M. Ranzato, M. Devin,\\nand J. Dean. Multilingual acoustic models using distributed deep neu-\\nral networks. In Proceedings of International Conference on Acoustics\\nSpeech and Signal Processing (ICASSP). 2013.\\n\\n[151] I. Heintz, E. Fosler-Lussier, and C. Brew. Discriminative input stream\\ncombination for conditional random \\ufb01eld phone recognition.\\nIEEE\\nTransactions on Audio, Speech, and Language Processing, 17(8):1533\\u2013\\n1546, November 2009.\\n\\n[152] M. Henderson, B. Thomson, and S. Young. Deep neural network\\napproach for the dialog state tracking challenge. In Proceedings of Spe-\\ncial Interest Group on Disclosure and Dialogue (SIGDIAL). 2013.\\n\\n[153] M. Hermans and B. Schrauwen. Training and analysing deep recur-\\nrent neural networks. In Proceedings of Neural Information Processing\\nSystems (NIPS). 2013.\\n\\nFull text available at: http://dx.doi.org/10.1561/2000000039\\x0cReferences\\n\\n167\\n\\n[154] H. Hermansky, D. Ellis, and S. Sharma. Tandem connectionist feature\\nextraction for conventional HMM systems. In Proceedings of Interna-\\ntional Conference on Acoustics Speech and Signal Processing (ICASSP).\\n2000.\\n\\n[155] Y. Hifny and S. Renals. Speech recognition using augmented conditional\\nIEEE Transactions on Audio, Speech, and Language\\n\\nrandom \\ufb01elds.\\nProcessing, 17(2):354\\u2013365, February 2009.\\n\\n[156] G. Hinton. Mapping part-whole hierarchies into connectionist networks.\\n\\nArti\\ufb01cial Intelligence, 46:47\\u201375, 1990.\\n\\n[157] G. Hinton. Preface to the special issue on connectionist symbol pro-\\n\\ncessing. Arti\\ufb01cial Intelligence, 46:1\\u20134, 1990.\\n\\n[158] G. Hinton. The ups and downs of Hebb synapses. Canadian Psychology,\\n\\n44:10\\u201313, 2003.\\n\\n[159] G. Hinton. A practical guide to training restricted boltzmann machines.\\n\\nUTML Tech Report 2010-003, Univ. Toronto, August 2010.\\n\\n[160] G. Hinton. A better way to learn features. Communications of the\\nAssociation for Computing Machinery (ACM), 54(10), October 2011.\\n\\n[161] G. Hinton, L. Deng, D. Yu, G. Dahl, A. Mohamed, N. Jaitly, A. Senior,\\nV. Vanhoucke, P. Nguyen, T. Sainath, and B. Kingsbury. Deep neu-\\nral networks for acoustic modeling in speech recognition. IEEE Signal\\nProcessing Magazine, 29(6):82\\u201397, November 2012.\\n\\n[162] G. Hinton, A. Krizhevsky, and S. Wang. Transforming autoencoders. In\\nProceedings of International Conference on Arti\\ufb01cial Neural Networks.\\n2011.\\n\\n[163] G. Hinton, S. Osindero, and Y. Teh. A fast learning algorithm for deep\\n\\nbelief nets. Neural Computation, 18:1527\\u20131554, 2006.\\n\\n[164] G. Hinton and R. Salakhutdinov. Reducing the dimensionality of data\\n\\nwith neural networks. Science, 313(5786):504\\u2013507, July 2006.\\n\\n[165] G. Hinton and R. Salakhutdinov. Discovering binary codes for docu-\\nments by learning deep generative models. Topics in Cognitive Science,\\npages 1\\u201318, 2010.\\n\\n[166] G. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and R. Salakhut-\\ndinov. Improving neural networks by preventing co-adaptation of fea-\\nture detectors. arXiv: 1207.0580v1, 2012.\\n\\n[167] S. Hochreiter.\\n\\nUntersuchungen zu dynamischen neuronalen net-\\nzen. Diploma thesis, Institut fur Informatik, Technische Universitat\\nMunchen, 1991.\\n\\nFull text available at: http://dx.doi.org/10.1561/2000000039\\x0c168\\n\\nReferences\\n\\n[168] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural\\n\\nComputation, 9:1735\\u20131780, 1997.\\n\\n[169] E. Huang, R. Socher, C. Manning, and A. Ng. Improving word represen-\\ntations via global context and multiple word prototypes. In Proceedings\\nof Association for Computational Linguistics (ACL). 2012.\\n\\n[170] J. Huang, J. Li, L. Deng, and D. Yu. Cross-language knowledge transfer\\nusing multilingual deep neural networks with shared hidden layers. In\\nProceedings of International Conference on Acoustics Speech and Signal\\nProcessing (ICASSP). 2013.\\n\\n[171] P. Huang, L. Deng, M. Hasegawa-Johnson, and X. He. Random fea-\\ntures for kernel deep convex network. In Proceedings of International\\nConference on Acoustics Speech and Signal Processing (ICASSP). 2013.\\n[172] P. Huang, X. He, J. Gao, L. Deng, A. Acero, and L. Heck. Learning\\ndeep structured semantic models for web search using clickthrough data.\\nAssociation for Computing Machinery (ACM) International Conference\\nInformation and Knowledge Management (CIKM), 2013.\\n\\n[173] P. Huang, K. Kumar, C. Liu, Y. Gong, and L. Deng. Predicting speech\\nrecognition con\\ufb01dence using deep learning with word identity and score\\nfeatures. In Proceedings of International Conference on Acoustics Speech\\nand Signal Processing (ICASSP). 2013.\\n\\n[174] S. Huang and S. Renals. Hierarchical bayesian language models for\\nconversational speech recognition. IEEE Transactions on Audio, Speech,\\nand Language Processing, 18(8):1941\\u20131954, November 2010.\\n\\n[175] X. Huang, A. Acero, C. Chelba, L. Deng, J. Droppo, D. Duchene,\\nJ. Goodman, and H. Hon. Mipad: A multimodal interaction proto-\\ntype. In Proceedings of International Conference on Acoustics Speech\\nand Signal Processing (ICASSP). 2001.\\n\\n[176] Y. Huang, D. Yu, Y. Gong, and C. Liu. Semi-supervised GMM and DNN\\nacoustic model training with multi-system combination and con\\ufb01dence\\nre-calibration. In Proceedings of Interspeech, pages 2360\\u20132364. 2013.\\n\\n[177] E. Humphrey and J. Bello. Rethinking automatic chord recognition\\nIn Proceedings of International\\n\\nwith convolutional neural networks.\\nConference on Machine Learning and Application (ICMLA). 2012a.\\n\\n[178] E. Humphrey, J. Bello, and Y. LeCun. Moving beyond feature design:\\nDeep architectures and automatic feature learning in music informat-\\nics. In Proceedings of International Symposium on Music Information\\nRetrieval (ISMIR). 2012.\\n\\nFull text available at: http://dx.doi.org/10.1561/2000000039\\x0cReferences\\n\\n169\\n\\n[179] E. Humphrey, J. Bello, and Y. LeCun. Feature learning and deep archi-\\ntectures: New directions for music informatics. Journal of Intelligent\\nInformation Systems, 2013.\\n\\n[180] B. Hutchinson, L. Deng, and D. Yu. A deep architecture with bilinear\\nmodeling of hidden representations: Applications to phonetic recogni-\\ntion. In Proceedings of International Conference on Acoustics Speech\\nand Signal Processing (ICASSP). 2012.\\n\\n[181] B. Hutchinson, L. Deng, and D. Yu. Tensor deep stacking net-\\nworks. IEEE Transactions on Pattern Analysis and Machine Intelli-\\ngence, 35:1944\\u20131957, 2013.\\n\\n[182] D. Imseng, P. Motlicek, P. Garner, and H. Bourlard. Impact of deep\\nMLP architecture on di\\ufb00erent modeling techniques for under-resourced\\nspeech recognition. In Proceedings of the Automatic Speech Recognition\\nand Understanding Workshop (ASRU). 2013.\\n\\n[183] N. Jaitly and G. Hinton. Learning a better representation of speech\\nsound waves using restricted boltzmann machines.\\nIn Proceedings of\\nInternational Conference on Acoustics Speech and Signal Processing\\n(ICASSP). 2011.\\n\\n[184] N. Jaitly, P. Nguyen, and V. Vanhoucke. Application of pre-trained deep\\nneural networks to large vocabulary speech recognition. In Proceedings\\nof Interspeech. 2012.\\n\\n[185] K. Jarrett, K. Kavukcuoglu, and Y. LeCun. What is the best multi-\\nstage architecture for object recognition? In Proceedings of International\\nConference on Computer Vision, pages 2146\\u20132153. 2009.\\n\\n[186] H. Jiang and X. Li. Parameter estimation of statistical models using\\nconvex optimization: An advanced method of discriminative training\\nfor speech and language processing. IEEE Signal Processing Magazine,\\n27(3):115\\u2013127, 2010.\\n\\n[187] B. Juang, S. Levinson, and M. Sondhi. Maximum likelihood estimation\\nfor multivariate mixture observations of Markov chains. IEEE Trans-\\nactions on Information Theory, 32:307\\u2013309, 1986.\\n\\n[188] B.-H. Juang, W. Chou, and C.-H. Lee. Minimum classi\\ufb01cation error\\nIEEE Transactions On Speech\\n\\nrate methods for speech recognition.\\nand Audio Processing, 5:257\\u2013265, 1997.\\n\\n[189] S. Kahou et al. Combining modality speci\\ufb01c deep neural networks for\\nemotion recognition in video. In Proceedings of International Conference\\non Multimodal Interaction (ICMI). 2013.\\n\\nFull text available at: http://dx.doi.org/10.1561/2000000039\\x0c170\\n\\nReferences\\n\\n[190] S. Kang, X. Qian, and H. Meng. Multi-distribution deep belief network\\nfor speech synthesis.\\nIn Proceedings of International Conference on\\nAcoustics Speech and Signal Processing (ICASSP), pages 8012\\u20138016.\\n2013.\\n\\n[191] Y. Kashiwagi, D. Saito, N. Minematsu, and K. Hirose. Discriminative\\npiecewise linear transformation based on deep learning for noise robust\\nautomatic speech recognition. In Proceedings of the Automatic Speech\\nRecognition and Understanding Workshop (ASRU). 2013.\\n\\n[192] K. Kavukcuoglu, P. Sermanet, Y. Boureau, K. Gregor, M. Mathieu,\\nand Y. LeCun. Learning convolutional feature hierarchies for visual\\nrecognition. In Proceedings of Neural Information Processing Systems\\n(NIPS). 2010.\\n\\n[193] H. Ketabdar and H. Bourlard. Enhanced phone posteriors for improving\\nspeech recognition systems. IEEE Transactions on Audio, Speech, and\\nLanguage Processing, 18(6):1094\\u20131106, August 2010.\\n\\n[194] B. Kingsbury. Lattice-based optimization of sequence classi\\ufb01cation cri-\\nteria for neural-network acoustic modeling. In Proceedings of Interna-\\ntional Conference on Acoustics Speech and Signal Processing (ICASSP).\\n2009.\\n\\n[195] B. Kingsbury, T. Sainath, and H. Soltau. Scalable minimum bayes\\nrisk training of deep neural network acoustic models using distributed\\nhessian-free optimization. In Proceedings of Interspeech. 2012.\\n\\n[196] R. Kiros, R. Zemel, and R. Salakhutdinov. Multimodal neural lan-\\nguage models. In Proceedings of Neural Information Processing Systems\\n(NIPS) Deep Learning Workshop. 2013.\\n\\n[197] T. Ko and B. Mak. Eigentriphones for context-dependent acoustic mod-\\neling. IEEE Transactions on Audio, Speech, and Language Processing,\\n21(6):1285\\u20131294, 2013.\\n\\n[198] A. Krizhevsky, I. Sutskever, and G. Hinton. Imagenet classi\\ufb01cation with\\ndeep convolutional neural networks. In Proceedings of Neural Informa-\\ntion Processing Systems (NIPS). 2012.\\n\\n[199] Y. Kubo, T. Hori, and A. Nakamura. Integrating deep neural networks\\ninto structural classi\\ufb01cation approach based on weighted \\ufb01nite-state\\ntransducers. In Proceedings of Interspeech. 2012.\\n\\n[200] R. Kurzweil. How to Create a Mind. Viking Books, December 2012.\\n[201] P. Lal and S. King. Cross-lingual automatic speech recognition using\\ntandem features. IEEE Transactions on Audio, Speech, and Language\\nProcessing, 21(12):2506\\u20132515, December 2013.\\n\\nFull text available at: http://dx.doi.org/10.1561/2000000039\\x0cReferences\\n\\n171\\n\\n[202] K. Lang, A. Waibel, and G. Hinton. A time-delay neural network archi-\\ntecture for isolated word recognition. Neural Networks, 3(1):23\\u201343, 1990.\\n[203] H. Larochelle and Y. Bengio. Classi\\ufb01cation using discriminative\\nIn Proceedings of International Con-\\n\\nrestricted boltzmann machines.\\nference on Machine Learning (ICML). 2008.\\n\\n[204] D. Le and P. Mower. Emotion recognition from spontaneous speech\\nusing hidden markov models with deep belief networks.\\nIn Proceed-\\nings of the Automatic Speech Recognition and Understanding Workshop\\n(ASRU). 2013.\\n\\n[205] H. Le, A. Allauzen, G. Wisniewski, and F. Yvon. Training continuous\\nspace language models: Some practical issues. In Proceedings of Empiri-\\ncal Methods in Natural Language Processing (EMNLP), pages 778\\u2013788.\\n2010.\\n\\n[206] H. Le, I. Oparin, A. Allauzen, J. Gauvain, and F. Yvon. Structured\\noutput layer neural network language model. In Proceedings of Interna-\\ntional Conference on Acoustics Speech and Signal Processing (ICASSP).\\n2011.\\n\\n[207] H. Le, I. Oparin, A. Allauzen, J.-L. Gauvain, and F. Yvon. Struc-\\ntured output layer neural network language models for speech recogni-\\ntion. IEEE Transactions on Audio, Speech, and Language Processing,\\n21(1):197\\u2013206, January 2013.\\n\\n[208] Q. Le, J. Ngiam, A. Coates, A. Lahiri, B. Prochnow, and A. Ng. On\\noptimization methods for deep learning. In Proceedings of International\\nConference on Machine Learning (ICML). 2011.\\n\\n[209] Q. Le, M. Ranzato, R. Monga, M. Devin, G. Corrado, K. Chen, J. Dean,\\nand A. Ng. Building high-level features using large scale unsupervised\\nlearning. In Proceedings of International Conference on Machine Learn-\\ning (ICML). 2012.\\n\\n[210] Y. LeCun. Learning invariant feature hierarchies.\\n\\nIn Proceedings of\\n\\nEuropean Conference on Computer Vision (ECCV). 2012.\\n\\n[211] Y. LeCun and Y. Bengio. Convolutional networks for images, speech,\\nand time series.\\nIn M. Arbib, editor, The Handbook of Brain The-\\nory and Neural Networks, pages 255\\u2013258. MIT Press, Cambridge, Mas-\\nsachusetts, 1995.\\n\\n[212] Y. LeCun, L. Bottou, Y. Bengio, and P. Ha\\ufb00ner. Gradient-based learn-\\ning applied to document recognition. Proceedings of the IEEE, 86:2278\\u2013\\n2324, 1998.\\n\\nFull text available at: http://dx.doi.org/10.1561/2000000039\\x0c172\\n\\nReferences\\n\\n[213] Y. LeCun, S. Chopra, M. Ranzato, and F. Huang. Energy-based models\\nin document recognition and computer vision. In Proceedings of Inter-\\nnational Conference on Document Analysis and Recognition (ICDAR).\\n2007.\\n\\n[214] C.-H. Lee. From knowledge-ignorant to knowledge-rich modeling: A new\\nspeech research paradigm for next-generation automatic speech recog-\\nnition. In Proceedings of International Conference on Spoken Language\\nProcessing (ICSLP), pages 109\\u2013111. 2004.\\n\\n[215] H. Lee, R. Grosse, R. Ranganath, and A. Ng. Convolutional deep belief\\nnetworks for scalable unsupervised learning of hierarchical representa-\\ntions. In Proceedings of International Conference on Machine Learning\\n(ICML). 2009.\\n\\n[216] H. Lee, R. Grosse, R. Ranganath, and A. Ng. Unsupervised learning\\nof hierarchical representations with convolutional deep belief networks.\\nCommunications of the Association for Computing Machinery (ACM),\\n54(10):95\\u2013103, October 2011.\\n\\n[217] H. Lee, Y. Largman, P. Pham, and A. Ng. Unsupervised feature learning\\nIn\\n\\nfor audio classi\\ufb01cation using convolutional deep belief networks.\\nProceedings of Neural Information Processing Systems (NIPS). 2010.\\n\\n[218] P. Lena, K. Nagata, and P. Baldi. Deep spatiotemporal architectures\\nand learning for protein structure prediction. In Proceedings of Neural\\nInformation Processing Systems (NIPS). 2012.\\n\\n[219] S. Levine. Exploring deep and recurrent architectures for optimal con-\\n\\ntrol. arXiv:1311.1761v1.\\n\\n[220] J. Li, L. Deng, Y. Gong, and R. Haeb-Umbach. An overview of\\nnoise-robust automatic speech recognition. IEEE/Association for Com-\\nputing Machinery (ACM) Transactions on Audio, Speech, and Language\\nProcessing, pages 1\\u201333, 2014.\\n\\n[221] J. Li, D. Yu, J. Huang, and Y. Gong.\\n\\nImproving wideband speech\\nrecognition using mixed-bandwidth training data in CD-DNN-HMM.\\nIn Proceedings of IEEE Spoken Language Technology (SLT). 2012.\\n\\n[222] L. Li, Y. Zhao, D. Jiang, and Y. Zhang etc. Hybrid deep neural network\\u2013\\nhidden markov model (DNN-HMM) based speech emotion recognition.\\nIn Proceedings Conference on A\\ufb00ective Computing and Intelligent Inter-\\naction (ACII), pages 312\\u2013317. September 2013.\\n\\n[223] H. Liao. Speaker adaptation of context dependent deep neural net-\\nworks. In Proceedings of International Conference on Acoustics Speech\\nand Signal Processing (ICASSP). 2013.\\n\\nFull text available at: http://dx.doi.org/10.1561/2000000039\\x0cReferences\\n\\n173\\n\\n[224] H. Liao, E. McDermott, and A. Senior. Large scale deep neural network\\nacoustic modeling with semi-supervised training data for youtube video\\ntranscription. In Proceedings of the Automatic Speech Recognition and\\nUnderstanding Workshop (ASRU). 2013.\\n\\n[225] H. Lin, L. Deng, D. Yu, Y. Gong, A. Acero, and C.-H. Lee. A study on\\nmultilingual acoustic modeling for large vocabulary ASR. In Proceedings\\nof International Conference on Acoustics Speech and Signal Processing\\n(ICASSP). 2009.\\n\\n[226] Y. Lin, F. Lv, S. Zhu, M. Yang, T. Cour, K. Yu, L. Cao, and T. Huang.\\nLarge-scale image classi\\ufb01cation: Fast feature extraction and SVM train-\\ning.\\nIn Proceedings of Computer Vision and Pattern Recognition\\n(CVPR). 2011.\\n\\n[227] Z. Ling, L. Deng, and D. Yu. Modeling spectral envelopes using\\nrestricted boltzmann machines and deep belief networks for statisti-\\ncal parametric speech synthesis. IEEE Transactions on Audio Speech\\nLanguage Processing, 21(10):2129\\u20132139, 2013.\\n\\n[228] Z. Ling, L. Deng, and D. Yu. Modeling spectral envelopes using\\nrestricted boltzmann machines for statistical parametric speech synthe-\\nsis. In International Conference on Acoustics Speech and Signal Pro-\\ncessing (ICASSP), pages 7825\\u20137829. 2013.\\n\\n[229] Z. Ling, K. Richmond, and J. Yamagishi. Articulatory control of HMM-\\nbased parametric speech synthesis using feature-space-switched multi-\\nple regression.\\nIEEE Transactions on Audio, Speech, and Language\\nProcessing, 21, January 2013.\\n\\n[230] L. Lu, K. Chin, A. Ghoshal, and S. Renals. Joint uncertainty decoding\\nfor noise robust subspace gaussian mixture models. IEEE Transactions\\non Audio, Speech, and Language Processing, 21(9):1791\\u20131804, 2013.\\n\\n[231] J. Ma and L. Deng. A path-stack algorithm for optimizing dynamic\\nregimes in a statistical hidden dynamical model of speech. Computer,\\nSpeech and Language, 2000.\\n\\n[232] J. Ma and L. Deng. E\\ufb03cient decoding strategies for conversational\\nspeech recognition using a constrained nonlinear state-space model.\\nIEEE Transactions on Speech and Audio Processing, 11(6):590\\u2013602,\\n2003.\\n\\n[233] J. Ma and L. Deng. Target-directed mixture dynamic models for spon-\\ntaneous speech recognition. IEEE Transactions on Speech and Audio\\nProcessing, 12(1):47\\u201358, 2004.\\n\\nFull text available at: http://dx.doi.org/10.1561/2000000039\\x0c174\\n\\nReferences\\n\\n[234] A. Maas, A. Hannun, and A. Ng. Recti\\ufb01er nonlinearities improve neural\\nnetwork acoustic models. International Conference on Machine Learn-\\ning (ICML) Workshop on Deep Learning for Audio, Speech, and Lan-\\nguage Processing, 2013.\\n\\n[235] A. Maas, Q. Le, T. O\\u2019Neil, O. Vinyals, P. Nguyen, and P. Ng. Recurrent\\nneural networks for noise reduction in robust ASR. In Proceedings of\\nInterspeech. 2012.\\n\\n[236] C. Manning, P. Raghavan, and H. Sch\\xfctze. Introduction to Information\\n\\nRetrieval. Cambridge University Press, 2009.\\n\\n[237] J. Marko\\ufb00. Scientists see promise in deep-learning programs. New York\\n\\nTimes, November 24 2012.\\n\\n[238] J. Martens. Deep learning with hessian-free optimization. In Proceedings\\n\\nof International Conference on Machine Learning (ICML). 2010.\\n\\n[239] J. Martens and I. Sutskever. Learning recurrent neural networks with\\nhessian-free optimization. In Proceedings of International Conference\\non Machine Learning (ICML). 2011.\\n\\n[240] D. McAllester. A PAC-bayesian tutorial with a dropout bound. ArX-\\n\\nive1307.2118, July 2013.\\n\\n[241] I. McGraw, I. Badr, and J. R. Glass. Learning lexicons from speech\\nusing a pronunciation mixture model. IEEE Transactions on Audio,\\nSpeech, and Language Processing, 21(2):357,366, February 2013.\\n\\n[242] G. Mesnil, X. He, L. Deng, and Y. Bengio. Investigation of recurrent-\\nneural-network architectures and learning methods for spoken language\\nunderstanding. In Proceedings of Interspeech. 2013.\\n\\n[243] Y. Miao and F. Metze. Improving low-resource CD-DNN-HMM using\\ndropout and multilingual DNN training. In Proceedings of Interspeech.\\n2013.\\n\\n[244] Y. Miao, S. Rawat, and F. Metze. Deep maxout networks for low\\nIn Proceedings of the Automatic Speech\\n\\nresource speech recognition.\\nRecognition and Understanding Workshop (ASRU). 2013.\\n\\n[245] T. Mikolov. Statistical language models based on neural networks.\\n\\nPh.D. thesis, Brno University of Technology, 2012.\\n\\n[246] T. Mikolov, K. Chen, G. Corrado, and J. Dean. E\\ufb03cient estimation of\\nword representations in vector space. In Proceedings of International\\nConference on Learning Representations (ICLR). 2013.\\n\\nFull text available at: http://dx.doi.org/10.1561/2000000039\\x0cReferences\\n\\n175\\n\\n[247] T. Mikolov, A. Deoras, D. Povey, L. Burget, and J. Cernocky. Strategies\\nfor training large scale neural network language models. In Proceedings\\nof the IEEE Automatic Speech Recognition and Understanding Work-\\nshop (ASRU). 2011.\\n\\n[248] T. Mikolov, M. Kara\\ufb01at, L. Burget, J. Cernocky, and S. Khudanpur.\\nRecurrent neural network based language model.\\nIn Proceedings of\\nInternational Conference on Acoustics Speech and Signal Processing\\n(ICASSP), pages 1045\\u20131048. 2010.\\n\\n[249] T. Mikolov, Q. Le, and I. Sutskever. Exploiting similarities among lan-\\n\\nguages for machine translation. arXiv:1309.4168v1, 2013.\\n\\n[250] T. Mikolov, I. Sutskever, K. Chen, G. Corrado, and J. Dean. Distributed\\nIn\\n\\nrepresentations of words and phrases and their compositionality.\\nProceedings of Neural Information Processing Systems (NIPS). 2013.\\n\\n[251] Y. Minami, E. McDermott, A. Nakamura, and S. Katagiri. A recogni-\\ntion method with parametric trajectory synthesized using direct rela-\\ntions between static and dynamic feature vector time series. In Pro-\\nceedings of International Conference on Acoustics Speech and Signal\\nProcessing (ICASSP), pages 957\\u2013960. 2002.\\n\\n[252] A. Mnih and G. Hinton. Three new graphical models for statistical lan-\\nguage modeling. In Proceedings of International Conference on Machine\\nLearning (ICML), pages 641\\u2013648. 2007.\\n\\n[253] A. Mnih and G. Hinton. A scalable hierarchical distributed lan-\\nguage model. In Proceedings of Neural Information Processing Systems\\n(NIPS), pages 1081\\u20131088. 2008.\\n\\n[254] A. Mnih and K. Kavukcuoglu. Learning word embeddings e\\ufb03ciently\\nwith noise-contrastive estimation. In Proceedings of Neural Information\\nProcessing Systems (NIPS). 2013.\\n\\n[255] A. Mnih and W.-T. Teh. A fast and simple algorithm for training\\nneural probabilistic language models. In Proceedings of International\\nConference on Machine Learning (ICML), pages 1751\\u20131758. 2012.\\n\\n[256] V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wier-\\nstra, and M. Riedmiller. Playing arari with deep reinforcement learning.\\nNeural Information Processing Systems (NIPS) Deep Learning Work-\\nshop, 2013. also arXiv:1312.5602v1.\\n\\n[257] A. Mohamed, G. Dahl, and G. Hinton. Deep belief networks for phone\\nrecognition. In Proceedings of Neural Information Processing Systems\\n(NIPS) Workshop Deep Learning for Speech Recognition and Related\\nApplications. 2009.\\n\\nFull text available at: http://dx.doi.org/10.1561/2000000039\\x0c176\\n\\nReferences\\n\\n[258] A. Mohamed, G. Dahl, and G. Hinton. Acoustic modeling using deep\\nbelief networks. IEEE Transactions on Audio, Speech, & Language Pro-\\ncessing, 20(1), January 2012.\\n\\n[259] A. Mohamed, G. Hinton, and G. Penn. Understanding how deep belief\\nnetworks perform acoustic modelling. In Proceedings of International\\nConference on Acoustics Speech and Signal Processing (ICASSP). 2012.\\n[260] A. Mohamed, D. Yu, and L. Deng. Investigation of full-sequence train-\\nIn Proceedings of\\n\\ning of deep belief networks for speech recognition.\\nInterspeech. 2010.\\n\\n[261] N. Morgan. Deep and wide: Multiple layers in automatic speech recog-\\nnition. IEEE Transactions on Audio, Speech, & Language Processing,\\n20(1), January 2012.\\n\\n[262] N. Morgan, Q. Zhu, A. Stolcke, K. Sonmez, S. Sivadas, T. Shinozaki,\\nM. Ostendorf, P. Jain, H. Hermansky, D. Ellis, G. Doddington, B. Chen,\\nO. Cretin, H. Bourlard, and M. Athineos. Pushing the envelope \\u2014 aside\\n[speech recognition].\\nIEEE Signal Processing Magazine, 22(5):81\\u201388,\\nSeptember 2005.\\n\\n[263] F. Morin and Y. Bengio. Hierarchical probabilistic neural network lan-\\nIn Proceedings of Arti\\ufb01cial Intelligence and Statistics\\n\\nguage models.\\n(AISTATS). 2005.\\n\\n[264] K. Murphy. Machine Learning \\u2014 A Probabilistic Perspective. The MIT\\n\\nPress, 2012.\\n\\n[265] V. Nair and G. Hinton. 3-d object recognition with deep belief nets. In\\n\\nProceedings of Neural Information Processing Systems (NIPS). 2009.\\n\\n[266] T. Nakashika, R. Takashima, T. Takiguchi, and Y. Ariki. Voice conver-\\nsion in high-order eigen space using deep belief nets. In Proceedings of\\nInterspeech. 2013.\\n\\n[267] H. Ney. Speech translation: Coupling of recognition and translation. In\\nProceedings of International Conference on Acoustics Speech and Signal\\nProcessing (ICASSP). 1999.\\n\\n[268] J. Ngiam, Z. Chen, P. Koh, and A. Ng. Learning deep energy models. In\\nProceedings of International Conference on Machine Learning (ICML).\\n2011.\\n\\n[269] J. Ngiam, A. Khosla, M. Kim, J. Nam, H. Lee, and A. Ng. Multimodal\\ndeep learning. In Proceedings of International Conference on Machine\\nLearning (ICML). 2011.\\n\\nFull text available at: http://dx.doi.org/10.1561/2000000039\\x0cReferences\\n\\n177\\n\\n[270] M. Norouzi, T. Mikolov, S. Bengio, J. Shlens, A. Frome, G. Corrado,\\nand J. Dean. Zero-shot learning by convex combination of semantic\\nembeddings. arXiv:1312.5650v2, 2013.\\n\\n[271] N. Oliver, A. Garg, and E. Horvitz. Layered representations for learning\\nand inferring o\\ufb03ce activity from multiple sensory channels. Computer\\nVision and Image Understanding, 96:163\\u2013180, 2004.\\n\\n[272] B. Olshausen. Can \\u2018deep learning\\u2019 o\\ufb00er deep insights about visual rep-\\nresentation? Neural Information Processing Systems (NIPS) Workshop\\non Deep Learning and Unsupervised Feature Learning, 2012.\\n\\n[273] M. Ostendorf. Moving beyond the \\u2018beads-on-a-string\\u2019 model of speech.\\nIn Proceedings of the Automatic Speech Recognition and Understanding\\nWorkshop (ASRU). 1999.\\n\\n[274] M. Ostendorf, V. Digalakis, and O. Kimball. From HMMs to segment\\nmodels: A uni\\ufb01ed view of stochastic modeling for speech recognition.\\nIEEE Transactions on Speech and Audio Processing, 4(5), September\\n1996.\\n\\n[275] L. Oudre, C. Fevotte, and Y. Grenier. Probabilistic template-based\\nchord recognition. IEEE Transactions on Audio, Speech, and Language\\nProcessing, 19(8):2249\\u20132259, November 2011.\\n\\n[276] H. Palangi, L. Deng, and R. Ward. Learning input and recurrent weight\\nmatrices in echo state networks. Neural Information Processing Systems\\n(NIPS) Deep Learning Workshop, December 2013.\\n\\n[277] H. Palangi, R. Ward, and L. Deng. Using deep stacking network to\\nimprove structured compressive sensing with multiple measurement vec-\\ntors. In Proceedings of International Conference on Acoustics Speech\\nand Signal Processing (ICASSP). 2013.\\n\\n[278] G. Papandreou, A. Katsamanis, V. Pitsikalis, and P. Maragos. Adap-\\ntive multimodal fusion by uncertainty compensation with application to\\naudiovisual speech recognition. IEEE Transactions on Audio, Speech,\\nand Language Processing, 17:423\\u2013435, 2009.\\n\\n[279] R. Pascanu, C. Gulcehre, K. Cho, and Y. Bengio. How to construct deep\\nrecurrent neural networks. In Proceedings of International Conference\\non Learning Representations (ICLR). 2014.\\n\\n[280] R. Pascanu, T. Mikolov, and Y. Bengio. On the di\\ufb03culty of training\\nrecurrent neural networks. In Proceedings of International Conference\\non Machine Learning (ICML). 2013.\\n\\n[281] J. Peng, L. Bo, and J. Xu. Conditional neural \\ufb01elds. In Proceedings of\\n\\nNeural Information Processing Systems (NIPS). 2009.\\n\\nFull text available at: http://dx.doi.org/10.1561/2000000039\\x0c178\\n\\nReferences\\n\\n[282] P. Picone, S. Pike, R. Regan, T. Kamm, J. bridle, L. Deng, Z. Ma,\\nH. Richards, and M. Schuster.\\nInitial evaluation of hidden dynamic\\nmodels on conversational speech. In Proceedings of International Con-\\nference on Acoustics Speech and Signal Processing (ICASSP). 1999.\\n\\n[283] J. Pinto, S. Garimella, M. Magimai-Doss, H. Hermansky, and\\nH. Bourlard. Analysis of MLP-based hierarchical phone posterior prob-\\nability estimators. IEEE Transactions on Audio, Speech, and Language\\nProcessing, 19(2), February 2011.\\n\\n[284] C. Plahl, T. Sainath, B. Ramabhadran, and D. Nahamoo. Improved\\npre-training of deep belief networks using sparse encoding symmet-\\nric machines. In Proceedings of International Conference on Acoustics\\nSpeech and Signal Processing (ICASSP). 2012.\\n\\n[285] C. Plahl, R. Schl\\xfcter, and H. Ney. Hierarchical bottleneck features for\\n\\nLVCSR. In Proceedings of Interspeech. 2010.\\n\\n[286] T. Plate. Holographic reduced representations. IEEE Transactions on\\n\\nNeural Networks, 6(3):623\\u2013641, May 1995.\\n\\n[287] T. Poggio. How the brain might work: The role of information and\\nlearning in understanding and replicating intelligence. In G. Jacovitt,\\nA. Pettorossi, R. Consolo, and V. Senni, editors, Information: Science\\nand Technology for the New Century, pages 45\\u201361. Lateran University\\nPress, 2007.\\n\\n[288] J. Pollack. Recursive distributed representations. Arti\\ufb01cial Intelligence,\\n\\n46:77\\u2013105, 1990.\\n\\n[289] H. Poon and P. Domingos. Sum-product networks: A new deep archi-\\ntecture. In Proceedings of Uncertainty in Arti\\ufb01cial Intelligence. 2011.\\n[290] D. Povey and P. Woodland. Minimum phone error and I-smoothing\\nfor improved discriminative training.\\nIn Proceedings of International\\nConference on Acoustics Speech and Signal Processing (ICASSP). 2002.\\n[291] R. Prabhavalkar and E. Fosler-Lussier. Backpropagation training for\\nmultilayer conditional random \\ufb01eld based phone recognition. In Pro-\\nceedings of International Conference on Acoustics Speech and Signal\\nProcessing (ICASSP). 2010.\\n\\n[292] A. Prince and P. Smolensky. Optimality: From neural networks to uni-\\n\\nversal grammar. Science, 275:1604\\u20131610, 1997.\\n\\n[293] L. Rabiner. A tutorial on hidden markov models and selected applica-\\ntions in speech recognition. In Proceedings of the IEEE, pages 257\\u2013286.\\n1989.\\n\\nFull text available at: http://dx.doi.org/10.1561/2000000039\\x0cReferences\\n\\n179\\n\\n[294] M. Ranzato, Y. Boureau, and Y. LeCun. Sparse feature learning for\\ndeep belief networks. In Proceedings of Neural Information Processing\\nSystems (NIPS). 2007.\\n\\n[295] M. Ranzato, S. Chopra, Y. LeCun, and F.-J. Huang. Energy-based\\nmodels in document recognition and computer vision.\\nIn Proceed-\\nings of International Conference on Document Analysis and Recognition\\n(ICDAR). 2007.\\n\\n[296] M. Ranzato and G. Hinton. Modeling pixel means and covariances using\\nfactorized third-order boltzmann machines. In Proceedings of Computer\\nVision and Pattern Recognition (CVPR). 2010.\\n\\n[297] M. Ranzato, C. Poultney, S. Chopra, and Y. LeCun. E\\ufb03cient learning\\nof sparse representations with an energy-based model. In Proceedings\\nof Neural Information Processing Systems (NIPS). 2006.\\n\\n[298] M. Ranzato, J. Susskind, V. Mnih, and G. Hinton. On deep generative\\nmodels with applications to recognition. In Proceedings of Computer\\nVision and Pattern Recognition (CVPR). 2011.\\n\\n[299] C. Rathinavalu and L. Deng. Construction of state-dependent dynamic\\nparameters by maximum likelihood: Applications to speech recognition.\\nSignal Processing, 55(2):149\\u2013165, 1997.\\n\\n[300] S. Rennie, K. Fouset, and P. Dognin. Factorial hidden restricted boltz-\\nmann machines for noise robust speech recognition.\\nIn Proceedings\\nof International Conference on Acoustics Speech and Signal Processing\\n(ICASSP). 2012.\\n\\n[301] S. Rennie, H. Hershey, and P. Olsen. Single-channel multi-talker speech\\nrecognition \\u2014 graphical modeling approaches. IEEE Signal Processing\\nMagazine, 33:66\\u201380, 2010.\\n\\n[302] M. Riedmiller and H. Braun. A direct adaptive method for faster back-\\nIn Proceedings of the\\n\\npropagation learning: The RPROP algorithm.\\nIEEE International Conference on Neural Networks. 1993.\\n\\n[303] S. Rifai, P. Vincent, X. Muller, X. Glorot, and Y. Bengio. Contractive\\nautoencoders: Explicit invariance during feature extraction. In Proceed-\\nings of International Conference on Machine Learning (ICML), pages\\n833\\u2013840. 2011.\\n\\n[304] A. Robinson. An application of recurrent nets to phone probability\\nestimation. IEEE Transactions on Neural Networks, 5:298\\u2013305, 1994.\\n[305] T. Sainath, L. Horesh, B. Kingsbury, A. Aravkin, and B. Ramabhad-\\nran. Accelerating hessian-free optimization for deep neural networks by\\nimplicit pre-conditioning and sampling. arXiv: 1309.1508v3, 2013.\\n\\nFull text available at: http://dx.doi.org/10.1561/2000000039\\x0c180\\n\\nReferences\\n\\n[306] T. Sainath, B. Kingsbury, A. Mohamed, G. Dahl, G. Saon, H. Soltau,\\nT. Beran, A. Aravkin, and B. Ramabhadran.\\nImprovements to deep\\nconvolutional neural networks for LVCSR. In Proceedings of the Auto-\\nmatic Speech Recognition and Understanding Workshop (ASRU). 2013.\\n[307] T. Sainath, B. Kingsbury, A. Mohamed, and B. Ramabhadran. Learn-\\ning \\ufb01lter banks within a deep neural network framework. In Proceed-\\nings of The Automatic Speech Recognition and Understanding Workshop\\n(ASRU). 2013.\\n\\n[308] T. Sainath, B. Kingsbury, and B. Ramabhadran. Autoencoder bottle-\\nneck features using deep belief networks. In Proceedings of International\\nConference on Acoustics Speech and Signal Processing (ICASSP). 2012.\\n[309] T. Sainath, B. Kingsbury, B. Ramabhadran, P. Novak, and\\nA. Mohamed. Making deep belief networks e\\ufb00ective for large vocab-\\nulary continuous speech recognition. In Proceedings of the Automatic\\nSpeech Recognition and Understanding Workshop (ASRU). 2011.\\n\\n[310] T. Sainath, B. Kingsbury, V. Sindhwani, E. Arisoy, and B. Ramabhad-\\nran. Low-rank matrix factorization for deep neural network training\\nwith high-dimensional output targets. In Proceedings of International\\nConference on Acoustics Speech and Signal Processing (ICASSP). 2013.\\n[311] T. Sainath, B. Kingsbury, H. Soltau, and B. Ramabhadran. Optimiza-\\ntion techniques to improve training speed of deep neural networks for\\nlarge speech tasks. IEEE Transactions on Audio, Speech, and Language\\nProcessing, 21(11):2267\\u20132276, November 2013.\\n\\n[312] T. Sainath, A. Mohamed, B. Kingsbury, and B. Ramabhadran. Con-\\nvolutional neural networks for LVCSR. In Proceedings of International\\nConference on Acoustics Speech and Signal Processing (ICASSP). 2013.\\n[313] T. Sainath, B. Ramabhadran, M. Picheny, D. Nahamoo, and\\nD. Kanevsky. Exemplar-based sparse representation features: From\\nTIMIT to LVCSR. IEEE Transactions on Speech and Audio Processing,\\nNovember 2011.\\n\\n[314] R. Salakhutdinov and G. Hinton. Semantic hashing. In Proceedings of\\nSpecial Interest Group on Information Retrieval (SIGIR) Workshop on\\nInformation Retrieval and Applications of Graphical Models. 2007.\\n\\n[315] R. Salakhutdinov and G. Hinton. Deep boltzmann machines. In Pro-\\n\\nceedings of Arti\\ufb01cial Intelligence and Statistics (AISTATS). 2009.\\n\\n[316] R. Salakhutdinov and G. Hinton. A better way to pretrain deep boltz-\\nmann machines. In Proceedings of Neural Information Processing Sys-\\ntems (NIPS). 2012.\\n\\nFull text available at: http://dx.doi.org/10.1561/2000000039\\x0cReferences\\n\\n181\\n\\n[317] G. Saon, H. Soltau, D. Nahamoo, and M. Picheny. Speaker adaptation\\nof neural network acoustic models using i-vectors. In Proceedings of the\\nAutomatic Speech Recognition and Understanding Workshop (ASRU).\\n2013.\\n\\n[318] R. Sarikaya, G. Hinton, and B. Ramabhadran. Deep belief nets for nat-\\nural language call-routing. In Proceedings of International Conference\\non Acoustics Speech and Signal Processing (ICASSP), pages 5680\\u20135683.\\n2011.\\n\\n[319] E. Schmidt and Y. Kim. Learning emotion-based acoustic features with\\ndeep belief networks. In Proceedings IEEE of Signal Processing to Audio\\nand Acoustics. 2011.\\n\\n[320] H. Schwenk. Continuous space translation models for phrase-based sta-\\ntistical machine translation. In Proceedings of Computional Linguistics.\\n2012.\\n\\n[321] H. Schwenk, A. Rousseau, and A. Mohammed. Large, pruned or contin-\\nuous space language models on a gpu for statistical machine translation.\\nIn Proceedings of the Joint Human Language Technology Conference\\nand the North American Chapter of the Association of Computational\\nLinguistics (HLT-NAACL) 2012 Workshop on the future of language\\nmodeling for Human Language Technology (HLT), pages 11\\u201319.\\n\\n[322] F. Seide, H. Fu, J. Droppo, G. Li, and D. Yu. On parallelizability of\\nstochastic gradient descent for speech DNNs. In Proceedings of Interna-\\ntional Conference on Acoustics Speech and Signal Processing (ICASSP).\\n2014.\\n\\n[323] F. Seide, G. Li, X. Chen, and D. Yu. Feature engineering in context-\\ndependent deep neural networks for conversational speech transcription.\\nIn Proceedings of the Automatic Speech Recognition and Understanding\\nWorkshop (ASRU), pages 24\\u201329. 2011.\\n\\n[324] F. Seide, G. Li, and D. Yu. Conversational speech transcription using\\ncontext-dependent deep neural networks. In Proceedings of Interspeech,\\npages 437\\u2013440. 2011.\\n\\n[325] M. Seltzer, D. Yu, and E. Wang. An investigation of deep neural net-\\nworks for noise robust speech recognition. In Proceedings of Interna-\\ntional Conference on Acoustics Speech and Signal Processing (ICASSP).\\n2013.\\n\\n[326] M. Shannon, H. Zen, and W. Byrne. Autoregressive models for statisti-\\ncal parametric speech synthesis. IEEE Transactions on Audio, Speech,\\nLanguage Processing, 21(3):587\\u2013597, 2013.\\n\\nFull text available at: http://dx.doi.org/10.1561/2000000039\\x0c182\\n\\nReferences\\n\\n[327] H. Sheikhzadeh and L. Deng. Waveform-based speech recognition using\\nhidden \\ufb01lter models: Parameter selection and sensitivity to power nor-\\nmalization.\\nIEEE Transactions on on Speech and Audio Processing\\n(ICASSP), 2:80\\u201391, 1994.\\n\\n[328] Y. Shen, X. He, J. Gao, L. Deng, and G. Mesnil. Learning semantic\\nrepresentations using convolutional neural networks for web search. In\\nProceedings World Wide Web. 2014.\\n\\n[329] K. Simonyan, A. Vedaldi, and A. Zisserman. Deep \\ufb01sher networks for\\nlarge-scale image classi\\ufb01cation. In Proceedings of Neural Information\\nProcessing Systems (NIPS). 2013.\\n\\n[330] M. Siniscalchi, J. Li, and C. Lee. Hermitian polynomial for speaker\\nadaptation of connectionist speech recognition systems. IEEE Trans-\\nactions on Audio, Speech, and Language Processing, 21(10):2152\\u20132161,\\n2013a.\\n\\n[331] M. Siniscalchi, T. Svendsen, and C.-H. Lee. A bottom-up modular\\nsearch approach to large vocabulary continuous speech recognition.\\nIEEE Transactions on Audio, Speech, Language Processing, 21, 2013.\\n[332] M. Siniscalchi, D. Yu, L. Deng, and C.-H. Lee. Exploiting deep neu-\\nral networks for detection-based speech recognition. Neurocomputing,\\n106:148\\u2013157, 2013.\\n\\n[333] M. Siniscalchi, D. Yu, L. Deng, and C.-H. Lee. Speech recognition using\\nIEEE Signal\\n\\nlong-span temporal patterns in a deep network model.\\nProcessing Letters, 20(3):201\\u2013204, March 2013.\\n\\n[334] G. Sivaram and H. Hermansky.\\n\\nphoneme recognition.\\nguage Processing, 20(1), January 2012.\\n\\nSparse multilayer perceptrons for\\nIEEE Transactions on Audio, Speech, & Lan-\\n\\n[335] P. Smolensky. Tensor product variable binding and the representation\\nof symbolic structures in connectionist systems. Arti\\ufb01cial Intelligence,\\n46:159\\u2013216, 1990.\\n\\n[336] P. Smolensky and G. Legendre. The Harmonic Mind \\u2014 From Neu-\\nral Computation to Optimality-Theoretic Grammar. The MIT Press,\\nCambridge, MA, 2006.\\n\\n[337] J. Snoek, H. Larochelle, and R. Adams. Practical bayesian optimization\\nof machine learning algorithms. In Proceedings of Neural Information\\nProcessing Systems (NIPS). 2012.\\n\\n[338] R. Socher. New directions in deep learning: Structured models, tasks,\\nand datasets. Neural Information Processing Systems (NIPS) Workshop\\non Deep Learning and Unsupervised Feature Learning, 2012.\\n\\nFull text available at: http://dx.doi.org/10.1561/2000000039\\x0cReferences\\n\\n183\\n\\n[339] R. Socher, Y. Bengio, and C. Manning. Deep learning for NLP.\\nTutorial at Association of Computational Logistics (ACL), 2012, and\\nNorth American Chapter of the Association of Computational Linguis-\\ntics (NAACL), 2013. http://www.socher.org/index.php/DeepLearning\\nTutorial.\\n\\n[340] R. Socher, D. Chen, C. Manning, and A. Ng. Reasoning with neural\\ntensor networks for knowledge base completion. In Proceedings of Neural\\nInformation Processing Systems (NIPS). 2013.\\n\\n[341] R. Socher and L. Fei-Fei. Connecting modalities: Semi-supervised\\nsegmentation and annotation of images using unaligned text corpora.\\nIn Proceedings of Computer Vision and Pattern Recognition (CVPR).\\n2010.\\n\\n[342] R. Socher, M. Ganjoo, H. Sridhar, O. Bastani, C. Manning, and A. Ng.\\nZero-shot learning through cross-modal transfer. In Proceedings of Neu-\\nral Information Processing Systems (NIPS). 2013b.\\n\\n[343] R. Socher, Q. Le, C. Manning, and A. Ng. Grounded compositional\\nsemantics for \\ufb01nding and describing images with sentences. Neu-\\nral Information Processing Systems (NIPS) Deep Learning Workshop,\\n2013c.\\n\\n[344] R. Socher, C. Lin, A. Ng, and C. Manning. Parsing natural scenes\\nand natural language with recursive neural networks. In Proceedings of\\nInternational Conference on Machine Learning (ICML). 2011.\\n\\n[345] R. Socher, J. Pennington, E. Huang, A. Ng, and C. Manning. Dynamic\\npooling and unfolding recursive autoencoders for paraphrase detection.\\nIn Proceedings of Neural Information Processing Systems (NIPS). 2011.\\n[346] R. Socher, J. Pennington, E. Huang, A. Ng, and C. Manning. Semi-\\nsupervised recursive autoencoders for predicting sentiment distribu-\\ntions. In Proceedings of Empirical Methods in Natural Language Pro-\\ncessing (EMNLP). 2011.\\n\\n[347] R. Socher, A. Perelygin, J. Wu, J. Chuang, C. Manning, A. Ng, and\\nC. Potts. Recursive deep models for semantic compositionality over a\\nsentiment treebank.\\nIn Proceedings of Empirical Methods in Natural\\nLanguage Processing (EMNLP). 2013.\\n\\n[348] N. Srivastava and R. Salakhutdinov. Multimodal learning with deep\\nboltzmann machines. In Proceedings of Neural Information Processing\\nSystems (NIPS). 2012.\\n\\nFull text available at: http://dx.doi.org/10.1561/2000000039\\x0c184\\n\\nReferences\\n\\n[349] N. Srivastava and R. Salakhutdinov. Discriminative transfer learning\\nwith tree-based priors. In Proceedings of Neural Information Processing\\nSystems (NIPS). 2013.\\n\\n[350] R. Srivastava, J. Masci, S. Kazerounian, F. Gomez, and J. Schmidhuber.\\nCompete to compute. In Proceedings of Neural Information Processing\\nSystems (NIPS). 2013.\\n\\n[351] T. Stafylakis, P. Kenny, M. Senoussaoui, and P. Dumouchel. Prelimi-\\nnary investigation of boltzmann machine classi\\ufb01ers for speaker recogni-\\ntion. In Proceedings of Odyssey, pages 109\\u2013116. 2012.\\n\\n[352] V. Stoyanov, A. Ropson, and J. Eisner. Empirical risk minimization of\\ngraphical model parameters given approximate inference, decoding, and\\nmodel structure. In Proceedings of Arti\\ufb01cial Intelligence and Statistics\\n(AISTATS). 2011.\\n\\n[353] H. Su, G. Li, D. Yu, and F. Seide. Error back propagation for sequence\\ntraining of context-dependent deep networks for conversational speech\\ntranscription. In Proceedings of International Conference on Acoustics\\nSpeech and Signal Processing (ICASSP). 2013.\\n\\n[354] A. Subramanya, L. Deng, Z. Liu, and Z. Zhang. Multi-sensory speech\\nprocessing: Incorporating automatically extracted hidden dynamic\\ninformation. In Proceedings of IEEE International Conference on Mul-\\ntimedia & Expo (ICME). Amsterdam, July 2005.\\n\\n[355] J. Sun and L. Deng. An overlapping-feature based phonological model\\nincorporating linguistic constraints: Applications to speech recognition.\\nJournal on Acoustical Society of America, 111(2):1086\\u20131101, 2002.\\n\\n[356] I. Sutskever. Training recurrent neural networks. Ph.D. Thesis, Univer-\\n\\nsity of Toronto, 2013.\\n\\n[357] I. Sutskever, J. Martens, and G. Hinton. Generating text with recurrent\\nneural networks. In Proceedings of International Conference on Machine\\nLearning (ICML). 2011.\\n\\n[358] Y. Tang and C. Eliasmith. Deep networks for robust visual recogni-\\ntion. In Proceedings of International Conference on Machine Learning\\n(ICML). 2010.\\n\\n[359] Y. Tang and R. Salakhutdinov. Learning Stochastic Feedforward Neural\\n\\nNetworks. NIPS, 2013.\\n\\n[360] A. Tarralba, R. Fergus, and Y. Weiss. Small codes and large image\\ndatabases for recognition. In Proceedings of Computer Vision and Pat-\\ntern Recognition (CVPR). 2008.\\n\\nFull text available at: http://dx.doi.org/10.1561/2000000039\\x0cReferences\\n\\n185\\n\\n[361] G. Taylor, G. E. Hinton, and S. Roweis. Modeling human motion using\\nbinary latent variables. In Proceedings of Neural Information Processing\\nSystems (NIPS). 2007.\\n\\n[362] S. Thomas, M. Seltzer, K. Church, and H. Hermansky. Deep neural\\nnetwork features and semi-supervised training for low resource speech\\nrecognition. In Proceedings of Interspeech. 2013.\\n\\n[363] T. Tieleman. Training restricted boltzmann machines using approx-\\nIn Proceedings of International\\n\\nimations to the likelihood gradient.\\nConference on Machine Learning (ICML). 2008.\\n\\n[364] K. Tokuda, Y. Nankaku, T. Toda, H. Zen, H. Yamagishi, and K. Oura.\\nSpeech synthesis based on hidden markov models. Proceedings of the\\nIEEE, 101(5):1234\\u20131252, 2013.\\n\\n[365] F. Triefenbach, A. Jalalvand, K. Demuynck, and J.-P. Martens. Acoustic\\nIEEE Transactions on Audio,\\n\\nmodeling with hierarchical reservoirs.\\nSpeech, and Language Processing, 21(11):2439\\u20132450, November 2013.\\n\\n[366] G. Tur, L. Deng, D. Hakkani-T\\xfcr, and X. He. Towards deep under-\\nstanding: Deep convex networks for semantic utterance classi\\ufb01cation. In\\nProceedings of International Conference on Acoustics Speech and Signal\\nProcessing (ICASSP). 2012.\\n\\n[367] J. Turian, L. Ratinov, and Y. Bengio. Word representations: A simple\\nIn Proceedings of\\n\\nand general method for semi-supervised learning.\\nAssociation for Computational Linguistics (ACL). 2010.\\n\\n[368] Z. T\\xfcske, M. Sundermeyer, R. Schl\\xfcter, and H. Ney. Context-dependent\\nMLPs for LVCSR: TANDEM, hybrid or both? In Proceedings of Inter-\\nspeech. 2012.\\n\\n[369] B. Uria, S. Renals, and K. Richmond. A deep neural network for\\nacoustic-articulatory speech inversion. Neural Information Processing\\nSystems (NIPS) Workshop on Deep Learning and Unsupervised Feature\\nLearning, 2011.\\n\\n[370] R. van Dalen and M. Gales. Extended VTS for noise-robust speech\\nrecognition. IEEE Transactions on Audio, Speech, and Language Pro-\\ncessing, 19(4):733\\u2013743, 2011.\\n\\n[371] A. van den Oord, S. Dieleman, and B. Schrauwen. Deep content-based\\nmusic recommendation. In Proceedings of Neural Information Process-\\ning Systems (NIPS). 2013.\\n\\n[372] V. Vasilakakis, S. Cumani, and P. Laface. Speaker recognition by means\\nIn Proceedings of Biometric Technologies in\\n\\nof deep belief networks.\\nForensic Science. 2013.\\n\\nFull text available at: http://dx.doi.org/10.1561/2000000039\\x0c186\\n\\nReferences\\n\\n[373] K. Vesely, A. Ghoshal, L. Burget, and D. Povey. Sequence-discriminative\\ntraining of deep neural networks. In Proceedings of Interspeech. 2013.\\n[374] K. Vesely, M. Hannemann, and L. Burget. Semi-supervised training of\\ndeep neural networks. In Proceedings of the Automatic Speech Recogni-\\ntion and Understanding Workshop (ASRU). 2013.\\n\\n[375] P. Vincent. A connection between score matching and denoising autoen-\\n\\ncoder. Neural Computation, 23(7):1661\\u20131674, 2011.\\n\\n[376] P. Vincent, H. Larochelle, I. Lajoie, Y. Bengio, and P. Manzagol.\\nStacked denoising autoencoders: Learning useful representations in a\\ndeep network with a local denoising criterion. Journal of Machine\\nLearning Research, 11:3371\\u20133408, 2010.\\n\\n[377] O. Vinyals, Y. Jia, L. Deng, and T. Darrell. Learning with recursive\\nperceptual representations. In Proceedings of Neural Information Pro-\\ncessing Systems (NIPS). 2012.\\n\\n[378] O. Vinyals and D. Povey. Krylov subspace descent for deep learning. In\\nProceedings of Arti\\ufb01cial Intelligence and Statistics (AISTATS). 2012.\\n[379] O. Vinyals and S. Ravuri. Comparing multilayer perceptron to deep\\nbelief network tandem features for robust ASR.\\nIn Proceedings of\\nInternational Conference on Acoustics Speech and Signal Processing\\n(ICASSP). 2011.\\n\\n[380] O. Vinyals, S. Ravuri, and D. Povey. Revisiting recurrent neural net-\\nworks for robust ASR. In Proceedings of International Conference on\\nAcoustics Speech and Signal Processing (ICASSP). 2012.\\n\\n[381] S. Wager, S. Wang, and P. Liang. Dropout training as adaptive reg-\\nularization. In Proceedings of Neural Information Processing Systems\\n(NIPS). 2013.\\n\\n[382] A. Waibel, T. Hanazawa, G. Hinton, K. Shikano, and K. Lang. Phoneme\\nrecognition using time-delay neural networks. IEEE Transactions on\\nAcoustical Speech, and Signal Processing, 37:328\\u2013339, 1989.\\n\\n[383] G. Wang and K. Sim. Context-dependent modelling of deep neural\\nnetwork using logistic regression. In Proceedings of the Automatic Speech\\nRecognition and Understanding Workshop (ASRU). 2013.\\n\\n[384] G. Wang and K. Sim. Regression-based context-dependent modeling\\nof deep neural networks for speech recognition. IEEE/Association for\\nComputing Machinery (ACM) Transactions on Audio, Speech, and Lan-\\nguage Processing, 2014.\\n\\nFull text available at: http://dx.doi.org/10.1561/2000000039\\x0cReferences\\n\\n187\\n\\n[385] D. Warde-Farley, I. Goodfellow, A. Courville, and Y. Bengi. An empir-\\nical analysis of dropout in piecewise linear networks. In Proceedings of\\nInternational Conference on Learning Representations (ICLR). 2014.\\n\\n[386] M. Welling, M. Rosen-Zvi, and G. Hinton. Exponential family harmo-\\nniums with an application to information retrieval. In Proceedings of\\nNeural Information Processing Systems (NIPS). 2005.\\n\\n[387] C. Weng, D. Yu, M. Seltzer, and J. Droppo. Single-channel mixed speech\\nrecognition using deep neural networks. In Proceedings of International\\nConference on Acoustics Speech and Signal Processing (ICASSP). 2014.\\n[388] J. Weston, S. Bengio, and N. Usunier. Large scale image annotation:\\nLearning to rank with joint word-image embeddings. Machine Learning,\\n81(1):21\\u201335, 2010.\\n\\n[389] J. Weston, S. Bengio, and N. Usunier. Wsabie: Scaling up to large\\nIn Proceedings of International Joint\\n\\nvocabulary image annotation.\\nConference on Arti\\ufb01cial Intelligence (IJCAI). 2011.\\n\\n[390] S. Wiesler, J. Li, and J. Xue. Investigations on hessian-free optimization\\nfor cross-entropy training of deep neural networks. In Proceedings of\\nInterspeech. 2013.\\n\\n[391] M. Wohlmayr, M. Stark, and F. Pernkopf. A probabilistic interac-\\ntion model for multi-pitch tracking with factorial hidden markov model.\\nIEEE Transactions on Audio, Speech, and Language Processing, 19(4),\\nMay 2011.\\n\\n[392] D. Wolpert. Stacked generalization. Neural Networks, 5(2):241\\u2013259,\\n\\n1992.\\n\\n[393] S. J. Wright, D. Kanevsky, L. Deng, X. He, G. Heigold, and H. Li.\\nOptimization algorithms and applications for speech and language pro-\\ncessing. IEEE Transactions on Audio, Speech, and Language Processing,\\n21(11):2231\\u20132243, November 2013.\\n\\n[394] L. Xiao and L. Deng. A geometric perspective of large-margin training\\nof gaussian models. IEEE Signal Processing Magazine, 27(6):118\\u2013123,\\nNovember 2010.\\n\\n[395] X. Xie and S. Seung. Equivalence of backpropagation and contrastive\\nhebbian learning in a layered network. Neural computation, 15:441\\u2013454,\\n2003.\\n\\n[396] Y. Xu, J. Du, L. Dai, and C. Lee. An experimental study on speech\\nenhancement based on deep neural networks. IEEE Signal Processing\\nLetters, 21(1):65\\u201368, 2014.\\n\\nFull text available at: http://dx.doi.org/10.1561/2000000039\\x0c188\\n\\nReferences\\n\\n[397] J. Xue, J. Li, and Y. Gong. Restructuring of deep neural network\\nacoustic models with singular value decomposition. In Proceedings of\\nInterspeech. 2013.\\n\\n[398] S. Yamin, L. Deng, Y. Wang, and A. Acero. An integrative and discrimi-\\nnative technique for spoken utterance classi\\ufb01cation. IEEE Transactions\\non Audio, Speech, and Language Processing, 16:1207\\u20131214, 2008.\\n\\n[399] Z. Yan, Q. Huo, and J. Xu. A scalable approach to using DNN-derived\\nfeatures in GMM-HMM based acoustic modeling for LVCSR. In Pro-\\nceedings of Interspeech. 2013.\\n\\n[400] D. Yang and S. Furui. Combining a two-step CRF model and a joint\\nsource-channel model for machine transliteration.\\nIn Proceedings of\\nAssociation for Computational Linguistics (ACL), pages 275\\u2013280. 2010.\\n[401] K. Yao, D. Yu, L. Deng, and Y. Gong. A fast maximum likelihood non-\\nlinear feature transformation method for GMM-HMM speaker adapta-\\ntion. Neurocomputing, 2013a.\\n\\n[402] K. Yao, D. Yu, F. Seide, H. Su, L. Deng, and Y. Gong. Adaptation of\\ncontext-dependent deep neural networks for automatic speech recogni-\\ntion. In Proceedings of International Conference on Acoustics Speech\\nand Signal Processing (ICASSP). 2012.\\n\\n[403] K. Yao, G. Zweig, M. Hwang, Y. Shi, and D. Yu. Recurrent neural\\nIn Proceedings of Interspeech.\\n\\nnetworks for language understanding.\\n2013.\\n\\n[404] T. Yoshioka and T. Nakatani. Noise model transfer: Novel approach to\\nrobustness against nonstationary noise. IEEE Transactions on Audio,\\nSpeech, and Language Processing, 21(10):2182\\u20132192, 2013.\\n\\n[405] T. Yoshioka, A. Ragni, and M. Gales.\\n\\nInvestigation of unsupervised\\nadaptation of DNN acoustic models with \\ufb01lter bank input.\\nIn Pro-\\nceedings of International Conference on Acoustics Speech and Signal\\nProcessing (ICASSP). 2013.\\n\\n[406] L. Younes. On the convergence of markovian stochastic algorithms with\\nrapidly decreasing ergodicity rates. Stochastics and Stochastic Reports,\\n65(3):177\\u2013228, 1999.\\n\\n[407] D. Yu, X. Chen, and L. Deng. Factorized deep neural networks for adap-\\ntive speech recognition. International Workshop on Statistical Machine\\nLearning for Speech Processing, March 2012b.\\n\\n[408] D. Yu, D. Deng, and S. Wang. Learning in the deep-structured con-\\nditional random \\ufb01elds. Neural Information Processing Systems (NIPS)\\n2009 Workshop on Deep Learning for Speech Recognition and Related\\nApplications, 2009.\\n\\nFull text available at: http://dx.doi.org/10.1561/2000000039\\x0cReferences\\n\\n189\\n\\n[409] D. Yu and L. Deng. Solving nonlinear estimation problems using splines.\\n\\nIEEE Signal Processing Magazine, 26(4):86\\u201390, July 2009.\\n\\n[410] D. Yu and L. Deng. Deep-structured hidden conditional random \\ufb01elds\\nfor phonetic recognition. In Proceedings of Interspeech. September 2010.\\n[411] D. Yu and L. Deng. Accelerated parallelizable neural networks learning\\nalgorithms for speech recognition. In Proceedings of Interspeech. 2011.\\n[412] D. Yu and L. Deng. Deep learning and its applications to signal and\\ninformation processing. IEEE Signal Processing Magazine, pages 145\\u2013\\n154, January 2011.\\n\\n[413] D. Yu and L. Deng. E\\ufb03cient and e\\ufb00ective algorithms for training single-\\nhidden-layer neural networks. Pattern Recognition Letters, 33:554\\u2013558,\\n2012.\\n\\n[414] D. Yu, L. Deng, and G. E. Dahl. Roles of pre-training and \\ufb01ne-tuning in\\ncontext-dependent DBN-HMMs for real-world speech recognition. Neu-\\nral Information Processing Systems (NIPS) 2010 Workshop on Deep\\nLearning and Unsupervised Feature Learning, December 2010.\\n\\n[415] D. Yu, L. Deng, J. Droppo, J. Wu, Y. Gong, and A. Acero. Robust\\nspeech recognition using cepstral minimum-mean-square-error noise\\nsuppressor. IEEE Transactions on Audio, Speech, and Language Pro-\\ncessing, 16(5), July 2008.\\n\\n[416] D. Yu, L. Deng, Y. Gong, and A. Acero. A novel framework and training\\nalgorithm for variable-parameter hidden markov models. IEEE Trans-\\nactions on Audio, Speech and Language Processing, 17(7):1348\\u20131360,\\n2009.\\n\\n[417] D. Yu, L. Deng, X. He, and A. Acero. Large-margin minimum clas-\\nsi\\ufb01cation error training: A theoretical risk minimization perspective.\\nComputer Speech and Language, 22(4):415\\u2013429, October 2008.\\n\\n[418] D. Yu, L. Deng, X. He, and X. Acero. Large-margin minimum classi-\\n\\ufb01cation error training for large-scale speech recognition tasks. In Pro-\\nceedings of International Conference on Acoustics Speech and Signal\\nProcessing (ICASSP). 2007.\\n\\n[419] D. Yu, L. Deng, G. Li, and F. Seide. Discriminative pretraining of deep\\n\\nneural networks. U.S. Patent Filing, November 2011.\\n\\n[420] D. Yu, L. Deng, P. Liu, J. Wu, Y. Gong, and A. Acero. Cross-lingual\\nspeech recognition under runtime resource constraints. In Proceedings\\nof International Conference on Acoustics Speech and Signal Processing\\n(ICASSP). 2009b.\\n\\nFull text available at: http://dx.doi.org/10.1561/2000000039\\x0c190\\n\\nReferences\\n\\n[421] D. Yu, L. Deng, and F. Seide. Large vocabulary speech recognition using\\n\\ndeep tensor neural networks. In Proceedings of Interspeech. 2012c.\\n\\n[422] D. Yu, L. Deng, and F. Seide. The deep tensor neural network with\\napplications to large vocabulary speech recognition. IEEE Transactions\\non Audio, Speech, and Language Processing, 21(2):388\\u2013396, 2013.\\n\\n[423] D. Yu, J.-Y. Li, and L. Deng. Calibration of con\\ufb01dence measures in\\nspeech recognition. IEEE Transactions on Audio, Speech and Language,\\n19:2461\\u20132473, 2010.\\n\\n[424] D. Yu, F. Seide, G. Li, and L. Deng. Exploiting sparseness in deep\\nneural networks for large vocabulary speech recognition. In Proceedings\\nof International Conference on Acoustics Speech and Signal Processing\\n(ICASSP). 2012.\\n\\n[425] D. Yu and M. Seltzer. Improved bottleneck features using pre-trained\\n\\ndeep neural networks. In Proceedings of Interspeech. 2011.\\n\\n[426] D. Yu, M. Seltzer, J. Li, J.-T. Huang, and F. Seide. Feature learning in\\ndeep neural networks \\u2014 studies on speech recognition. In Proceedings\\nof International Conference on Learning Representations (ICLR). 2013.\\n[427] D. Yu, S. Siniscalchi, L. Deng, and C. Lee. Boosting attribute and\\nphone estimation accuracies with deep neural networks for detection-\\nbased speech recognition. In Proceedings of International Conference\\non Acoustics Speech and Signal Processing (ICASSP). 2012.\\n\\n[428] D. Yu, S. Wang, and L. Deng. Sequential labeling using deep-structured\\nconditional random \\ufb01elds. Journal of Selected Topics in Signal Process-\\ning, 4:965\\u2013973, 2010.\\n\\n[429] D. Yu, S. Wang, Z. Karam, and L. Deng. Language recognition using\\ndeep-structured conditional random \\ufb01elds. In Proceedings of Interna-\\ntional Conference on Acoustics Speech and Signal Processing (ICASSP),\\npages 5030\\u20135033. 2010.\\n\\n[430] D. Yu, K. Yao, H. Su, G. Li, and F. Seide. KL-divergence regularized\\ndeep neural network adaptation for improved large vocabulary speech\\nrecognition.\\nIn Proceedings of International Conference on Acoustics\\nSpeech and Signal Processing (ICASSP). 2013.\\n\\n[431] K. Yu, M. Gales, and P. Woodland. Unsupervised adaptation with dis-\\ncriminative mapping transforms. IEEE Transactions on Audio, Speech,\\nand Language Processing, 17(4):714\\u2013723, 2009.\\n\\n[432] K. Yu, Y. Lin, and H. La\\ufb00erty. Learning image representations from\\nthe pixel level via hierarchical sparse coding. In Proceedings Computer\\nVision and Pattern Recognition (CVPR). 2011.\\n\\nFull text available at: http://dx.doi.org/10.1561/2000000039\\x0cReferences\\n\\n191\\n\\n[433] F. Zamora-Mart\\xednez, M. Castro-Bleda, and S. Espa\\xf1a-Boquera. Fast\\nevaluation of connectionist language models. International Conference\\non Arti\\ufb01cial Neural Networks, pages 144\\u2013151, 2009.\\n\\n[434] M. Zeiler. Hierarchical convolutional deep learning in computer vision.\\n\\nPh.D. Thesis, New York University, January 2014.\\n\\n[435] M. Zeiler and R. Fergus. Stochastic pooling for regularization of deep\\nconvolutional neural networks. In Proceedings of International Confer-\\nence on Learning Representations (ICLR). 2013.\\n\\n[436] M. Zeiler and R. Fergus. Visualizing and understanding convolutional\\n\\nnetworks. arXiv:1311.2901, pages 1\\u201311, 2013.\\n\\n[437] M. Zeiler, G. Taylor, and R. Fergus. Adaptive deconvolutional networks\\nfor mid and high level feature learning. In Proceedings of International\\nConference on Computer vision (ICCV). 2011.\\n\\n[438] H. Zen, M. Gales, J. F. Nankaku, and Y. K. Tokuda. Product of\\nexperts for statistical parametric speech synthesis. IEEE Transactions\\non Audio, Speech, and Language Processing, 20(3):794\\u2013805, March 2012.\\n[439] H. Zen, Y. Nankaku, and K. Tokuda. Continuous stochastic feature\\nIEEE Transactions on Audio,\\n\\nmapping based on trajectory HMMs.\\nSpeech, and Language Processings, 19(2):417\\u2013430, February 2011.\\n\\n[440] H. Zen, A. Senior, and M. Schuster. Statistical parametric speech syn-\\nthesis using deep neural networks. In Proceedings of International Con-\\nference on Acoustics Speech and Signal Processing (ICASSP), pages\\n7962\\u20137966. 2013.\\n\\n[441] X. Zhang, J. Trmal, D. Povey, and S. Khudanpur.\\n\\nImproving deep\\nneural network acoustic models using generalized maxout networks. In\\nProceedings of International Conference on Acoustics Speech and Signal\\nProcessing (ICASSP). 2014.\\n\\n[442] X. Zhang and J. Wu. Deep belief networks based voice activity detec-\\ntion. IEEE Transactions on Audio, Speech, and Language Processing,\\n21(4):697\\u2013710, 2013.\\n\\n[443] Z. Zhang, Z. Liu, M. Sinclair, A. Acero, L. Deng, J. Droppo, X. Huang,\\nand Y. Zheng. Multi-sensory microphones for robust speech detection,\\nenhancement and recognition. In Proceedings of International Confer-\\nence on Acoustics Speech and Signal Processing (ICASSP). 2004.\\n\\n[444] Y. Zhao and B. Juang. Nonlinear compensation using the gauss-newton\\nIEEE Transactions on\\n\\nmethod for noise-robust speech recognition.\\nAudio, Speech, and Language Processing, 20(8):2191\\u20132206, 2012.\\n\\nFull text available at: http://dx.doi.org/10.1561/2000000039\\x0c192\\n\\nReferences\\n\\n[445] W. Zou, R. Socher, D. Cer, and C. Manning. Bilingual word embed-\\ndings for phrase-based machine translation. In Proceedings of Empirical\\nMethods in Natural Language Processing (EMNLP). 2013.\\n\\n[446] G. Zweig and P. Nguyen. A segmental CRF approach to large vocab-\\nulary continuous speech recognition. In Proceedings of the Automatic\\nSpeech Recognition and Understanding Workshop (ASRU). 2009.\\n\\nFull text available at: http://dx.doi.org/10.1561/2000000039\\x0c', u'Multimodal Deep Learning\\n\\nJiquan Ngiam1\\nAditya Khosla1\\nMingyu Kim1\\nJuhan Nam1\\nHonglak Lee2\\nAndrew Y. Ng1\\n1 Computer Science Department, Stanford University, Stanford, CA 94305, USA\\n2 Computer Science and Engineering Division, University of Michigan, Ann Arbor, MI 48109, USA\\n\\njngiam@cs.stanford.edu\\naditya86@cs.stanford.edu\\nminkyu89@cs.stanford.edu\\njuhan@ccrma.stanford.edu\\nhonglak@eecs.umich.edu\\nang@cs.stanford.edu\\n\\nAbstract\\n\\nDeep networks have been successfully applied\\nto unsupervised feature learning for single\\nmodalities (e.g., text, images or audio).\\nIn\\nthis work, we propose a novel application of\\ndeep networks to learn features over multiple\\nmodalities. We present a series of tasks for\\nmultimodal learning and show how to train\\ndeep networks that learn features to address\\nthese tasks.\\nIn particular, we demonstrate\\ncross modality feature learning, where better\\nfeatures for one modality (e.g., video) can be\\nlearned if multiple modalities (e.g., audio and\\nvideo) are present at feature learning time.\\nFurthermore, we show how to learn a shared\\nrepresentation between modalities and evalu-\\nate it on a unique task, where the classi\\ufb01er is\\ntrained with audio-only data but tested with\\nvideo-only data and vice-versa. Our mod-\\nels are validated on the CUAVE and AVLet-\\nters datasets on audio-visual speech classi\\ufb01-\\ncation, demonstrating best published visual\\nspeech classi\\ufb01cation on AVLetters and e\\ufb00ec-\\ntive shared representation learning.\\n\\n1. Introduction\\n\\nIn speech recognition, humans are known to inte-\\ngrate audio-visual information in order to understand\\nspeech. This was \\ufb01rst exempli\\ufb01ed in the McGurk ef-\\nfect (McGurk & MacDonald, 1976) where a visual /ga/\\nwith a voiced /ba/ is perceived as /da/ by most sub-\\njects. In particular, the visual modality provides infor-\\n\\nAppearing in Proceedings of the 28 th International Con-\\nference on Machine Learning, Bellevue, WA, USA, 2011.\\nCopyright 2011 by the author(s)/owner(s).\\n\\nmation on the place of articulation and muscle move-\\nments (Summer\\ufb01eld, 1992) which can often help to dis-\\nambiguate between speech with similar acoustics (e.g.,\\nthe unvoiced consonants /p/ and /k/ ).\\n\\nMultimodal\\nlearning involves relating information\\nfrom multiple sources. For example, images and 3-d\\ndepth scans are correlated at \\ufb01rst-order as depth dis-\\ncontinuities often manifest as strong edges in images.\\nConversely, audio and visual data for speech recogni-\\ntion have correlations at a \\u201cmid-level\\u201d, as phonemes\\nand visemes (lip pose and motions); it can be di\\ufb03cult\\nto relate raw pixels to audio waveforms or spectro-\\ngrams.\\n\\nIn this paper, we are interested in modeling \\u201cmid-\\nlevel\\u201d relationships, thus we choose to use audio-visual\\nspeech classi\\ufb01cation to validate our methods. In par-\\nticular, we focus on learning representations for speech\\naudio which are coupled with videos of the lips.\\n\\nWe will consider the learning settings shown in Figure\\n1. The overall task can be divided into three phases\\n\\u2013 feature learning, supervised training, and testing.\\nA simple linear classi\\ufb01er is used for supervised train-\\ning and testing to examine di\\ufb00erent feature learning\\nmodels with multimodal data. In particular, we con-\\nsider three learning settings \\u2013 multimodal fusion, cross\\nmodality learning, and shared representation learning.\\n\\nIn the multimodal fusion setting, data from all modal-\\nities is available at all phases; this represents the typ-\\nical setting considered in most prior work in audio-\\nvisual speech recognition (Potamianos et al., 2004). In\\ncross modality learning, data from multiple modalities\\nis available only during feature learning; during the\\nsupervised training and testing phase, only data from\\na single modality is provided. For this setting, the aim\\nis to learn better single modality representations given\\nunlabeled data from multiple modalities. Last, we con-\\n\\n\\x0cMultimodal Deep Learning\\n\\nsider a shared representation learning setting, which is\\nunique in that di\\ufb00erent modalities are presented for su-\\npervised training and testing. This setting allows us\\nto evaluate if the feature representations can capture\\ncorrelations across di\\ufb00erent modalities. Speci\\ufb01cally,\\nstudying this setting allows us to assess whether the\\nlearned representations are modality-invariant.\\n\\nIn the following sections, we \\ufb01rst describe the build-\\ning blocks of our model. We then present di\\ufb00erent\\nmultimodal learning models leading to a deep network\\nthat is able to perform the various multimodal learn-\\ning tasks. Finally, we report experimental results and\\nconclude.\\n\\n2. Background\\n\\nRecent work on deep learning (Hinton & Salakhut-\\ndinov, 2006; Salakhutdinov & Hinton, 2009) has ex-\\namined how deep sigmoidal networks can be trained\\nto produce useful representations for handwritten dig-\\nits and text. The key idea is to use greedy layer-wise\\ntraining with Restricted Boltzmann Machines (RBMs)\\nfollowed by \\ufb01ne-tuning. We use an extension of RBMs\\nwith sparsity (Lee et al., 2007), which have been shown\\nto learn meaningful features for digits and natural im-\\nages. In the next section, we review the sparse RBM,\\nwhich is used as a layer-wise building block for our\\nmodels.\\n\\n2.1. Sparse restricted Boltzmann machines\\n\\nThe RBM is an undirected graphical model with hid-\\nden variables (h) and visible variables (v) (Figure 2a).\\nThere are symmetric connections between the hidden\\nand visible variables (Wi,j), but no connections within\\nhidden variables or visible variables. The model de-\\n\\ufb01nes a probability distribution over h, v (Equation 1).\\nThis particular con\\ufb01guration makes it easy to compute\\nthe conditional probability distributions, when v or h\\nis \\ufb01xed (Equation 2).\\n\\n(cid:16)\\n\\n\\u2212 log P (v, h) \\u221d E(v, h) =\\n\\n1\\n\\n2\\u03c32 vT v \\u2212 1\\n\\u03c32\\np(hj|v) = sigmoid(\\n\\n(cid:17)\\n\\n(1)\\n\\n(2)\\n\\ncT v + bT h + hT W v\\n\\n1\\n\\u03c32 (bj + wT\\n\\nj v))\\n\\nThis formulation models the visible variables as real-\\nvalued units and the hidden variables as binary units.1\\nAs it is intractable to compute the gradient of the\\nlog-likelihood term, we learn the parameters of the\\n\\n1We use Gaussian visible units for the RBM that is\\nconnected to the input data. When training the deeper\\nlayers, we use binary visible units.\\n\\nFigure 1: Multimodal Learning settings where A+V\\nrefers to Audio and Video.\\n\\nmodel (wi,j, bj, ci) using contrastive divergence (Hin-\\nton, 2002).\\n\\npenalty of the form \\u03bb(cid:80)\\n\\nfor sparsity (Lee et al.,\\nTo regularize the model\\n2007), we encourage each hidden unit to have a pre-\\ndetermined expected activation using a regularization\\nk=1 E[hj|vk]))2,\\nwhere {v1, ..., vm} is the training set and \\u03c1 determines\\nthe sparsity of the hidden unit activations.\\n\\nm ((cid:80)m\\n\\nj(\\u03c1 \\u2212 1\\n\\n3. Learning architectures\\n\\nIn this section, we describe our models for the task of\\naudio-visual bimodal feature learning, where the au-\\ndio and visual input to the model are contiguous audio\\n(spectrogram) and video frames. To motivate our deep\\nautoencoder (Hinton & Salakhutdinov, 2006) model,\\nwe \\ufb01rst describe several simple models and their draw-\\nbacks.\\n\\nOne of the most straightforward approaches to feature\\nlearning is to train a RBM model separately for au-\\ndio and video (Figure 2a,b). After learning the RBM,\\nthe posteriors of the hidden variables given the visible\\nvariables (Equation 2) can then be used as a new repre-\\nsentation for the data. We use this model as a baseline\\nto compare the results of our multimodal models, as\\nwell as for pre-training the deep networks.\\n\\nTo train a multimodal model, a direct approach is to\\ntrain a RBM over the concatenated audio and video\\ndata (Figure 2c). While this approach jointly mod-\\nels the distribution of the audio and video data, it is\\nlimited as a shallow model. In particular, since the cor-\\nrelations between the audio and video data are highly\\nnon-linear, it is hard for a RBM to learn these corre-\\nlations and form multimodal representations. In prac-\\ntice, we found that learning a shallow bimodal RBM\\nresults in hidden units that have strong connections to\\nvariables from individual modality but few units that\\nconnect across the modalities.\\n\\nFeatureLearningSupervised TrainingTestingClassic Deep LearningAudioAudioAudioVideoVideoVideoMultimodal FusionA + VA + VA + VCrossModality LearningA + VVideoVideoA + VAudioAudioShared Representation LearningA + VAudioVideoA + VVideoAudio\\x0cMultimodal Deep Learning\\n\\n(a) Audio RBM\\n\\n(b) Video RBM\\n\\n(c) Shallow Bimodal RBM\\n\\n(d) Bimodal DBN\\nFigure 2: RBM Pretraining Models. We train RBMs for (a) audio and (b) video separately as\\na baseline. The shallow model (c) is limited and we \\ufb01nd that this model is unable to capture\\ncorrelations across the modalities. The bimodal deep belief network (DBN) model (d) is trained\\nin a greedy layer-wise fashion by \\ufb01rst training models (a) & (b). We later \\u201cunroll\\u201d the deep\\nmodel (d) to train the deep autoencoder models presented in Figure 3.\\n\\n(a) Video-Only Deep Autoencoder\\n\\n(b) Bimodal Deep Autoencoder\\n\\nFigure 3: Deep Autoencoder Models. A \\u201cvideo-only\\u201d model is shown in (a) where the model\\nlearns to reconstruct both modalities given only video as the input. A similar model can be\\ndrawn for the \\u201caudio-only\\u201d setting. We train the (b) bimodal deep autoencoder in a denoising\\nfashion, using an augmented dataset with examples that require the network to reconstruct both\\nmodalities given only one. Both models are pre-trained using sparse RBMs (Figure 2d). Since\\nwe use a sigmoid transfer function in the deep network, we can initialize the network using the\\nconditional probability distributions p(h|v) and p(v|h) of the learned RBM.\\n\\nTherefore, we consider greedily training a RBM over\\nthe pre-trained layers for each modality, as motivated\\nby deep learning methods (Figure 2d).2 In particular,\\nthe posteriors (Equation 2) of the \\ufb01rst layer hidden\\nvariables are used as the training data for the new\\nlayer. By representing the data through learned \\ufb01rst\\nlayer representations, it can be easier for the model to\\nlearn higher-order correlations across modalities. In-\\nformally, the \\ufb01rst layer representations correspond to\\nphonemes and visemes and the second layer models the\\nrelationships between them. Figure 4 shows visualiza-\\ntions of learned features from our models including\\nexamples of visual bases corresponding to visemes.\\n\\nHowever, there are still two issues with the above mul-\\ntimodal models. First, there is no explicit objective for\\nthe models to discover correlations across the modali-\\n\\n2It is possible to instead learn a large RBM as the \\ufb01rst\\nlayer that connects to both modalities. However, since a\\nsingle layer RBM tends to learn unimodal units, it is much\\nmore e\\ufb03cient to learn separate models for each modality.\\n\\nties; it is possible for the model to \\ufb01nd representations\\nsuch that some hidden units are tuned only for au-\\ndio while others are tuned only for video. Second, the\\nmodels are clumsy to use in a cross modality learn-\\ning setting where only one modality is present during\\nsupervised training and testing. With only a single\\nmodality present, one would need to integrate out the\\nunobserved visible variables to perform inference.\\n\\nThus, we propose a deep autoencoder that resolves\\nboth issues. We \\ufb01rst consider the cross modality learn-\\ning setting where both modalities are present during\\nfeature learning but only a single modality is used for\\nsupervised training and testing. The deep autoencoder\\n(Figure 3a) is trained to reconstruct both modalities\\nwhen given only video data and thus discovers corre-\\nlations across the modalities. Analogous to Hinton\\n& Salakhutdinov (2006), we initialize the deep au-\\ntoencoder with the bimodal DBN weights (Figure 2d)\\nbased on Equation 2, discarding any weights that are\\nno longer present. The middle layer can be used as the\\n\\n......Hidden UnitsAudio Input......Hidden UnitsVideo Input\\u2026......Audio InputShared Representation...Video Input............Audio InputVideo InputDeep Hidden Layer........................Video InputSharedRepresentationAudio ReconstructionVideo Reconstruction...........................Audio InputVideo InputSharedRepresentationAudio ReconstructionVideo Reconstruction\\x0cMultimodal Deep Learning\\n\\n4. Experiments and Results\\n\\nWe evaluate our methods on audio-visual speech clas-\\nsi\\ufb01cation of isolated letters and digits. The sparseness\\nparameter \\u03c1 was chosen using cross-validation, while\\nall other parameters (including hidden layer size and\\nweight regularization) were kept \\ufb01xed.3\\n\\n4.1. Data Preprocessing\\n\\nWe represent the audio signal using its spectrogram4\\nwith temporal derivatives, resulting in a 483 dimension\\nvector which was reduced to 100 dimensions with PCA\\nwhitening. 10 contiguous audio frames were used as\\nthe input to our models.\\n\\nFor the video, we preprocessed the frames so as to\\nextract only the region-of-interest (ROI) encompass-\\ning the mouth.5 Each mouth ROI was rescaled to\\n60 \\xd7 80 pixels and further reduced to 32 dimensions,6\\nusing PCA whitening. Temporal derivatives over the\\nreduced vector were also used. We used 4 contiguous\\nvideo frames for input since this had approximately\\nthe same duration as 10 audio frames.\\n\\nFor both modalities, we also performed feature mean\\nnormalization over time (Potamianos et al., 2004),\\nakin to removing the DC component from each ex-\\nample. We also note that adding temporal derivatives\\nto the representations has been widely used in the lit-\\nerature as it helps to model dynamic speech informa-\\ntion (Potamianos et al., 2004; Zhao & Barnard, 2009).\\nThe temporal derivatives were computed using a nor-\\nmalized linear slope so that the dynamic range of the\\nderivative features is comparable to the original signal.\\n\\n4.2. Datasets and Task\\n\\nSince only unlabeled data was required for unsuper-\\nvised feature learning, we combined diverse datasets\\n(as listed below) to learn features. AVLetters and\\nCUAVE were further used for supervised classi\\ufb01cation.\\nWe ensured that no test data was used for unsuper-\\nvised feature learning. All deep autoencoder models\\nwere trained with all available unlabeled audio and\\nvideo data.\\n\\n3We cross-validated \\u03c1 over {0.01, 0.03, 0.05, 0.07}. The\\n\\ufb01rst layer features were 4x overcomplete for video (1536\\nunits) and 1.5x overcomplete for audio (1500 units). The\\nsecond layer was 1.5x the size of the combined \\ufb01rst layers\\n(4554 units).\\n\\n4Each spectrogram frame (161 frequency bins) had a\\n\\n20ms window with 10ms overlaps.\\n\\n5We used an o\\ufb00-the-shelf object detector (Dalal &\\nTriggs, 2005) with median \\ufb01ltering over time to extract\\nthe mouth regions.\\n\\n6Similar to (Duchnowski et al., 1994) we found that 32\\n\\ndimensions were su\\ufb03cient and performed well.\\n\\nFigure 4: Visualization of learned representations.\\nThese \\ufb01gures correspond to two deep hidden units,\\nwhere we visualize the most strongly connected\\n\\ufb01rst layer features. The units are presented in\\naudio-visual pairs (we have found it generally di\\ufb03-\\ncult to interpret the connection between the pair).\\nThe visual bases captured lip motions and articula-\\ntions, including di\\ufb00erent mouth articulations, open-\\ning and closing of the mouth, exposing teeth.\\n\\nnew feature representation. This model can be viewed\\nas an instance of multitask learning (Caruana, 1997).\\n\\nWe use the deep autoencoder (Figure 3a) models in\\nsettings where only a single modality is present at su-\\npervised training and testing. On the other hand,\\nwhen multiple modalities are available for the task\\n(e.g., multimodal fusion), it is less clear how to use the\\nmodel as one would need to train a deep autoencoder\\nfor each modality. One straightforward solution is to\\ntrain the networks such that the decoding weights are\\ntied. However, such an approach does not scale well \\u2013\\nif we were to allow any combination of modalities to\\nbe present or absent at test time, we will need to train\\nan exponential number of models.\\n\\nInspired by denoising autoencoders (Vincent et al.,\\n2008), we propose training the bimodal deep au-\\ntoencoder (Figure 3b) using an augmented but noisy\\ndataset with additional examples that have only a\\nsingle-modality as input.\\nIn practice, we add exam-\\nples that have zero values for one of the input modali-\\nties (e.g., video) and original values for the other input\\nmodality (e.g., audio), but still require the network to\\nreconstruct both modalities (audio and video). Thus,\\none-third of the training data has only video for input,\\nwhile another one-third of the data has only audio, and\\nthe last one-third of the data has both audio and video.\\n\\nDue to initialization using sparse RBMs, we \\ufb01nd that\\nthe hidden units have low expected activation, even\\nafter the deep autoencoder training. Therefore, when\\none of the input modalities is set to zero, the \\ufb01rst\\nlayer representations are also close to zero.\\nIn this\\ncase, we are essentially training a modality-speci\\ufb01c\\ndeep autoencoder network (Figure 3a). E\\ufb00ectively, the\\nmethod learns a model which is robust to inputs where\\na modality is absent.\\n\\n\\x0cMultimodal Deep Learning\\n\\nCUAVE (Patterson et al., 2002). 36 speakers saying\\nthe digits 0 to 9. We used the normal portion of the\\ndataset which contained frontal facing speakers saying\\neach digit 5 times. We evaluated digit classi\\ufb01cation on\\nthe CUAVE dataset in a speaker independent setting.\\nAs there has not been a \\ufb01xed protocol for evaluation on\\nthis dataset, we chose to use odd-numbered speakers\\nfor the test set and even-numbered speakers for the\\ntraining set.\\n\\nAVLetters (Matthews et al., 2002). 10 speakers say-\\ning the letters A to Z, three times each. The dataset\\nprovided pre-extracted lip regions of 60 \\xd7 80 pixels.\\nAs the raw audio was not available for this dataset,\\nwe used it for evaluation on a visual-only lipreading\\ntask (Section 4.3). We report results on the third-test\\nsettings used by Zhao & Barnard (2009) and Matthews\\net al. (2002) for comparisons.\\n\\nAVLetters2 (Cox et al., 2008). 5 speakers saying the\\nletters A to Z, seven times each. This is a new high-\\nde\\ufb01nition version of the AVLetters dataset. We used\\nthis dataset for unsupervised training only.\\n\\nStanford Dataset. 23 volunteers spoke the digits 0\\nto 9, letters A to Z and selected sentences from the\\nTIMIT dataset. We collected this data in a similar\\nfashion to the CUAVE dataset and used it for unsu-\\npervised training only.\\n\\nTIMIT (Fisher et al., 1986). We used this dataset for\\nunsupervised audio feature pre-training.\\n\\nWe note that in all datasets there is variability in the\\nlips in terms of appearance, orientation and size. For\\neach audio-video clip, features were extracted from\\noverlapping sequences of frames. Since examples had\\nvarying durations, we divided each example into S\\nequal slices and performed average-pooling over each\\nslice. The features from all slices were subsequently\\nconcatenated together. Speci\\ufb01cally, we combined fea-\\ntures using S = 1 and S = 3 to form our \\ufb01nal feature\\nrepresentation for classi\\ufb01cation with a linear SVM.\\n\\n4.3. Cross Modality Learning\\nIn the cross modality learning experiments, we eval-\\nuate if we can learn better representations for one\\nmodality (e.g., video) when given multiple modalities\\n(e.g., audio and video) during feature learning.\\n\\nOn the AVLetters dataset (Table 1a), our deep au-\\ntoencoder models show a signi\\ufb01cant improvement over\\nhand-engineered features from prior work. The video-\\nonly deep autoencoder performed the best on the\\ndataset, obtaining a classi\\ufb01cation accuracy of 64.4%,\\noutperforming the best previous published results.\\n\\nOn the CUAVE dataset (Table 1b), there is an im-\\nprovement by learning video features with both video\\n\\nand audio compared to learning features with only\\nvideo data (although not performing as well as state-\\nof-the-art).\\nIn our models, we chose to use a very\\nsimple front-end that only extracts bounding boxes,\\nwithout any correction for orientation or perspective\\nchanges.\\nIn contrast, recent AAM models (Papan-\\ndreou et al., 2009) are trained to accurately track the\\nspeaker\\u2019s face and further register the face with a mean\\nface template, canceling shape deformations. Combin-\\ning these sophisticated visual front-ends with our fea-\\ntures has the potential to do even better.\\n\\nTable 1: Classi\\ufb01cation performance for visual speech\\nclassi\\ufb01cation on (a) AVLetters and (b) CUAVE. Deep\\nautoencoders perform the best and show e\\ufb00ective\\ncross modality learning. Where indicated, the error\\nbars show the variation (\\xb1 2 s.d.) due to random ini-\\ntialization. \\xa7Results are on continuous speech recog-\\nnition performance, though we note that the normal\\nportion of CUAVE has speakers saying isolated dig-\\nits. \\u2020These models use a visual front-end system that\\nis signi\\ufb01cantly more complicated than ours and a dif-\\nferent train/test split.\\n\\nFeature Representation\\nBaseline Preprocessed Video\\nRBM Video (Figure 2b)\\nVideo-Only Deep Autoencoder\\n(Figure 3a)\\nBimodal Deep Autoencoder\\n(Figure 3b)\\nMultiscale Spatial Analysis\\n(Matthews et al., 2002)\\nLocal Binary Pattern\\n(Zhao & Barnard, 2009)\\n\\n(a) AVLetters\\n\\nFeature Representation\\nBaseline Preprocessed Video\\nRBM Video (Figure 2b)\\nVideo-Only Deep Autoencoder\\n(Figure 3a)\\nBimodal Deep Autoencoder\\n(Figure 3b)\\nDiscrete Cosine Transform\\n(Gurban & Thiran, 2009)\\nActive Appearence Model\\n(Papandreou et al., 2007)\\nActive Appearence Model\\n(Pitsikalis et al., 2006)\\nFused Holistic+Patch\\n(Lucey & Sridharan, 2006)\\nVisemic AAM\\n(Papandreou et al., 2009)\\n\\n(b) CUAVE Video\\n\\n46.2%\\n\\nAccuracy\\n54.2%\\xb13.3%\\n64.4%\\xb12.4%\\n\\n59.2%\\n\\n44.6%\\n\\n58.85%\\n\\n58.5%\\n\\nAccuracy\\n65.4%\\xb10.6%\\n68.7%\\xb11.8%\\n\\n66.7%\\n\\n64% \\u2020\\xa7\\n75.7% \\u2020\\n68.7% \\u2020\\n77.08% \\u2020\\n83% \\u2020\\xa7\\n\\n\\x0cMultimodal Deep Learning\\n\\nTable 2: Digit classi\\ufb01cation performance for bimodal speech classi\\ufb01cation on CUAVE, under clean and\\nnoisy conditions. We added white Gaussian noise to the original audio signal at 0 dB SNR. The error bars\\nre\\ufb02ect the variation (\\xb1 2 s.d.) of the results due to the random noise added to the audio data. We compare\\nperformance of the Bimodal Deep Autoencoder model with the best audio features (Audio RBM) and the\\nbest video features (Video-only Deep Autoencoder).\\n\\nFeature Representation\\n(a) Audio RBM (Figure 2a)\\n(b) Video-only Deep Autoencoder (Figure 3a)\\n(c) Bimodal Deep Autoencoder (Figure 3b)\\n(d) Bimodal + Audio RBM\\n(e) Video-only Deep AE + Audio-RBM\\n\\nAccuracy\\n\\n(Clean Audio)\\n\\n95.8%\\n68.7%\\n90.0%\\n94.4%\\n87.0%\\n\\nAccuracy\\n(Noisy Audio)\\n75.8% \\xb1 2.0%\\n77.3% \\xb1 1.4%\\n82.2% \\xb1 1.2%\\n76.6% \\xb1 0.8%\\n\\n68.7%\\n\\nThese video classi\\ufb01cation results show that the deep\\nautoencoders achieve cross modality learning by dis-\\ncovering better video representations when given ad-\\nditional audio data.\\nIn particular, even though the\\nAVLetters dataset did not have any audio data, we\\nwere able to improve performance by learning better\\nvideo features using other additional unlabeled audio\\nand video data.\\n\\nmultimodal features that go beyond simply concate-\\nnating the audio and visual features, we propose com-\\nbining the audio features with our multimodal features\\n(Table 2d). When the best audio features are con-\\ncatenated with the bimodal features, it outperforms\\nthe other feature combinations. This shows that the\\nlearned multimodal features are better able to comple-\\nment the audio features.\\n\\nHowever, the bimodal deep autoencoder did not per-\\nform as well as the video-only deep autoencoder: while\\nthe video-only autoencoder learns only video features\\n(which are also good for audio reconstruction), the bi-\\nmodal autoencoder learns audio-only, video-only and\\ninvariant features. As such, the feature set learned by\\nthe bimodal autoencoder might not be optimal when\\nthe task at hand has only visual input.\\n\\nWe also note that cross modality learning for audio\\ndid not improve classi\\ufb01cation results compared to us-\\ning audio RBM features; audio features are highly dis-\\ncriminative for speech classi\\ufb01cation, adding video in-\\nformation can sometimes hurt performance.\\n\\n4.4. Multimodal Fusion Results\\nWhile using audio information alone performs reason-\\nably well for speech recognition, fusing audio and video\\ninformation can substantially improve performance,\\nespecially when the audio is degraded with noise (Gur-\\nban & Thiran, 2009; Papandreou et al., 2007; Pitsikalis\\net al., 2006; Papandreou et al., 2009). In particular, it\\nis common to \\ufb01nd that audio features perform well on\\ntheir own and concatenating video features can some-\\ntimes hurt performance. Hence, we evaluate our mod-\\nels in both clean and noisy audio settings.\\n\\nThe video modality complements the audio modality\\nby providing information such as place of articulation,\\nwhich can help distinguish between similar sounding\\nspeech. However, when one simply concatenates audio\\nand visual features (Table 2e), it is often the case that\\nperformance is worse as compared to using only audio\\nfeatures (Table 2a). Since our models are able to learn\\n\\n4.5. McGurk e\\ufb00ect\\n\\nTable 3: McGurk E\\ufb00ect\\n\\nAudio / Visual\\nSetting\\nVisual /ga/, Audio /ga/\\nVisual /ba/, Audio /ba/\\nVisual /ga/, Audio /ba/\\n\\n/ba/\\n\\nModel prediction\\n/da/\\n/ga/\\n82.6% 2.2% 15.2%\\n4.4% 89.1% 6.5%\\n28.3% 13.0% 58.7%\\n\\nThe McGurk e\\ufb00ect (McGurk & MacDonald, 1976)\\nrefers to an audio-visual perception phenomenon\\nwhere a visual /ga/ with a audio /ba/ is perceived\\nas /da/ by most subjects. Since our model learns a\\nmultimodal representation, it would be interesting to\\nobserve if the model is able to replicate a similar e\\ufb00ect.\\n\\nWe obtained data from 23 volunteers speaking 5 rep-\\netitions of /ga/, /ba/ and /da/. The bimodal deep\\nautoencoder features7 were used to train a linear SVM\\non this 3-way classi\\ufb01cation task. The model was tested\\non three conditions that simulate the McGurk e\\ufb00ect.\\nWhen the visual and audio data matched at test time,\\nthe model was able to predict the correct class /ba/\\nand /ga/ with an accuracy of 82.6% and 89.1% re-\\nspectively. On the other hand, when a visual /ga/\\nwith a voiced /ba/ was mixed at test time, the model\\nwas most likely to predict /da/, even though /da/ nei-\\nther appears in the visual nor audio inputs, consistent\\nwith the McGurk e\\ufb00ect on people. The same e\\ufb00ect\\nwas not observed with the bimodal DBN (Figure 2d)\\nor with concatenating audio and video RBM features.\\n\\n7The /ga/, /ba/ and /da/ data was not used for train-\\n\\ning the bimodal deep autoencoder.\\n\\n\\x0cMultimodal Deep Learning\\n\\n4.6. Shared Representation Learning\\n\\nTable 4: Shared representation learning on CUAVE.\\nChance performance is at 10%.\\n\\nTrain/Test\\n\\nMethod\\nRaw-CCA\\n\\nAudio/Video\\n\\nRBM-CCA Features\\n\\nBimodal Deep AE\\n\\nRaw-CCA\\n\\nVideo/Audio\\n\\nRBM-CCA Features\\n\\nBimodal Deep AE\\n\\nAccuracy\\n\\n41.9%\\n57.3%\\n30.7%\\n42.9%\\n91.7%\\n24.3%\\n\\nFigure 5: \\u201cHearing to see\\u201d setting (train on audio, test\\non video) for evaluating shared representations.\\n\\nIn this experiment, we propose a novel setting which\\nexamines if a shared representation can be learned over\\naudio and video speech data, During supervised train-\\ning, the algorithm is provided data solely from one\\nmodality (e.g., audio) and later tested only on the\\nother modality (e.g., video), as shown in Figure 5. In\\nessence, we are telling the supervised learner how the\\ndigits \\u201c1\\u201d, \\u201c2\\u201d, etc. sound, while asking it to distin-\\nguish them based on how they are visually spoken \\u2013\\n\\u201chearing to see\\u201d. If we are able to capture the corre-\\nlations across the modalities in our shared representa-\\ntion, the model will perform this task well.\\n\\nOne approach to learning a shared representation is to\\n\\ufb01nd transformations for the modalities that maximize\\ncorrelations.\\nIn particular, we suggest using canoni-\\ncal correlation analysis (CCA) (Hardoon et al., 2004),\\nwhich \\ufb01nds linear transformations of audio and video\\n8 Learning\\ndata, to form a shared representation.\\na CCA shared representation on raw data results in\\nsurprisingly good performance (Table 4: Raw-CCA).\\nHowever, learning the CCA representation on the \\ufb01rst\\nlayer features (i.e., Audio RBM and Video RBM fea-\\ntures) results in signi\\ufb01cantly better performance, com-\\nparable to using the original modalities for supervised\\nclassi\\ufb01cation (Table 4: RBM-CCA Features). This is\\nparticularly surprising since testing on audio performs\\n\\n8Given audio data a and video data v, CCA \\ufb01nds ma-\\ntrices P and Q such that P a and Qv have maximum cor-\\nrelations.\\n\\nbetter than testing on video, even when the model was\\ntrained on video data. These results show that cap-\\nturing relationships across the modalities require at\\nleast a single non-linear stage to be successful. When\\ngood features have been learned from both modalities,\\na linear model can be well suited to capture the rela-\\ntionships. However, it is important to note that CCA,\\na linear transformation, does not help in other tasks\\nsuch as cross-modality learning.\\n\\nWe further used this task to examine whether the fea-\\ntures from the bimodal deep autoencoder captures cor-\\nrelations across the modalities. 9 While the bimodal\\ndeep autoencoder model does not perform as well as\\nCCA, the results show that our learned representations\\nare partially invariant to the input modality.\\n\\n4.7. Additional Control Experiments\\n\\nThe video-only deep autoencoder has audio as a train-\\ning cue and multiple hidden layers (Figure 3a). We\\n\\ufb01rst considered removing audio as a cue by training\\na similar deep autoencoder that did not reconstruct\\naudio data; the performance decreased by 7.7% on\\nCUAVE and 14.3% on AVLetters. Next, we trained\\na video-only shallow autoencoder with a single hidden\\nlayer to reconstruct both audio and video10; the per-\\nformance decreased by 2.1% on CUAVE and 5.0% on\\nAVLetters. Hence, both audio as a cue and depth were\\nimportant ingredients for the video-only deep autoen-\\ncoder to perform well.\\n\\nWe also compared the performance of using the bi-\\nmodal DBN without training it as an autoencoder. In\\ncases where only one modality was present, we used the\\nsame approach as the bimodal deep autoencoder, set-\\nting the absent modality to zero.11 The bimodal DBN\\nperformed worse in the cross-modality and shared rep-\\nresentation tasks and did not show the McGurk ef-\\nfect. It performed comparably on the multimodal fu-\\nsion task. 12\\n\\n9For the bimodal deep autoencoder, we set the value of\\nthe absent modality to zero when computing the shared\\nrepresentation, which is consistent with the feature learn-\\ning phase.\\n\\n10The single hidden layer takes video as input and re-\\n\\nconstructs both audio and video.\\n\\n11We also tried alternating Gibbs sampling to obtain the\\n\\nposterior, but the results were worse.\\n\\n12For the video-only setting, the bimodal DBN per-\\nformed 4.9% worse on the CUAVE dataset and 5.0% worse\\non the AVLetters dataset. It was at chance on the \\u201chearing\\nto see\\u201d task and obtained 28.1% on \\u201cseeing to hear\\u201d.\\n\\nVideoSupervisedTestingAudioSharedRepresentationVideoLinear ClassifierTrainingTestingAudioShared Representation\\x0cMultimodal Deep Learning\\n\\n5. Related Work\\nWhile we present special cases of neural networks for\\nmultimodal learning, we note that prior work on audio-\\nvisual speech recognition (Duchnowski et al., 1994;\\nYuhas et al., 1989; Meier et al., 1996; Bregler & Konig,\\n1994) has also explored the use of neural networks.\\nYuhas et al. (1989) trained a neural network to pre-\\ndict the auditory signal given the visual input. They\\nshowed improved performance in a noisy setting when\\nthey combined the predicted auditory signal (from the\\nnetwork using visual input) with a noisy auditory sig-\\nnal. Duchnowski et al. (1994) and Meier et al. (1996)\\ntrained separate networks to model phonemes and\\nvisemes and combined the predictions at a phonetic\\nlayer to predict the spoken phoneme.\\n\\nIn contrast to these approaches, we use the hidden\\nunits to build a new representation of the data. Fur-\\nthermore, we do not model phonemes or visemes,\\nwhich require expensive labeling e\\ufb00orts. Finally, we\\nbuild deep bimodal representations by modeling the\\ncorrelations across the learned shallow representations.\\n\\n6. Discussion\\nHand-engineering task-speci\\ufb01c features is often di\\ufb03-\\ncult and time consuming. For example, it is not im-\\nmediately clear what the appropriate features should\\nbe for lipreading (visual-only data). This di\\ufb03culty is\\nmore pronounced with multimodal data as the features\\nhave to relate multiple data sources. In this work, we\\nshowed how deep learning can be applied to this chal-\\nlenging task for discovering multimodal features.\\n\\nAcknowledgments\\n\\nWe thank Clemson University for providing the CUAVE\\ndataset and University of Surrey for providing the AVLet-\\nters2 dataset. We also thank Quoc Le, Andrew Saxe, An-\\ndrew Maas, and Adam Coates for insightful discussions,\\nand the anonymous reviewers for helpful comments. This\\nwork is supported by the DARPA Deep Learning program\\nunder contract number FA8650-10-C-7020.\\n\\nReferences\\n\\nBregler, C. and Konig, Y. \\u201dEigenlips\\u201d for robust speech\\n\\nrecognition. In ICASSP, 1994.\\n\\nCaruana, R. Multitask learning. Machine Learning, 28(1):\\n\\n41\\u201375, 1997.\\n\\nCox, S., Harvey, R., Lan, Y., and Newman, J. The chal-\\nlenge of multispeaker lip-reading. In International Con-\\nference on Auditory-Visual Speech Processing, 2008.\\n\\nDalal, N. and Triggs, B. Histograms of Oriented Gradients\\n\\nfor Human Detection. In CVPR, 2005.\\n\\nDuchnowski, P., Meier, U., and Waibel, A. See me, hear\\nme: Integrating automatic speech recognition and lip-\\nreading. In ICSLP, pp. 547\\u2013550, 1994.\\n\\nFisher, W., Doddington, G., and Marshall, Goudie. The\\n\\nDARPA speech recognition research database: Speci\\ufb01-\\ncation and status. In DARPA Speech Recognition Work-\\nshop, pp. 249\\u2013249, 1986.\\n\\nGurban, M. and Thiran, J.P. Information theoretic feature\\nIEEE\\n\\nextraction for audio-visual speech recognition.\\nTrans. on Sig. Proc., 57(12):4765\\u20134776, 2009.\\n\\nHardoon, David R., Szedmak, Sandor R., and Shawe-\\ntaylor, John R. Canonical correlation analysis: An\\noverview with application to learning methods. Neural\\nComputation, 16:2639\\u20132664, 2004.\\n\\nHinton, G. Training products of experts by minimizing\\n\\ncontrastive divergence. Neural Computation, 2002.\\n\\nHinton, G. and Salakhutdinov, R. Reducing the dimension-\\nality of data with neural networks. Science, 313(5786):\\n504\\u2013507, 2006.\\n\\nLee, H., Ekanadham, C., and Ng, A. Sparse deep belief net\\n\\nmodel for visual area V2. In NIPS, 2007.\\n\\nLucey, P. and Sridharan, S. Patch-based representation\\nIn HCSNet Workshop on the Use of\\n\\nof visual speech.\\nVision in Human-Computer Interaction, 2006.\\n\\nMatthews, I., Cootes, T.F., Bangham, J.A., and Cox, S.\\nExtraction of visual features for lipreading. PAMI, 24:\\n198 \\u2013213, 2002.\\n\\nMcGurk, H. and MacDonald, J. Hearing lips and seeing\\n\\nvoices. Nature, 264(5588):746\\u2013748, 1976.\\n\\nMeier, U., H\\xa8urst, W., and Duchnowski, P. Adaptive Bi-\\nmodal Sensor Fusion For Automatic Speechreading. In\\nICASSP, pp. 833\\u2013836, 1996.\\n\\nPapandreou, G., Katsamanis, A., Pitsikalis, V., and Mara-\\ngos, P. Multimodal fusion and learning with uncertain\\nfeatures applied to audiovisual speech recognition.\\nIn\\nMMSP, pp. 264\\u2013267, 2007.\\n\\nPapandreou, G., Katsamanis, A., Pitsikalis, V., and Mara-\\ngos, P. Adaptive multimodal fusion by uncertainty com-\\npensation with application to audiovisual speech recog-\\nnition. IEEE TASLP, 17(3):423\\u2013435, 2009.\\n\\nPatterson, E., Gurbuz, S., Tufekci, Z., and Gowdy, J.\\nCUAVE: A new audio-visual database for multimodal\\nhuman-computer interface research. 2:2017\\u20132020, 2002.\\nPitsikalis, V., Katsamanis, A., Papandreou, G., and Mara-\\ngos, P. Adaptive multimodal fusion by uncertainty com-\\npensation. In ICSLP, pp. 2458\\u20132461, 2006.\\n\\nPotamianos, G., Neti, C., Luettin, J., and Matthews,\\nI. Audio-visual automatic speech recognition: An\\noverview. In Issues in Visual and Audio-Visual Speech\\nProcessing. MIT Press, 2004.\\n\\nSalakhutdinov, R. and Hinton, G. Semantic hashing. IJAR,\\n\\n50(7):969\\u2013978, 2009.\\n\\nSummer\\ufb01eld, Q. Lipreading and audio-visual speech per-\\n\\nception. Trans. R. Soc. Lond., pp. 71\\u201378, 1992.\\n\\nVincent, P., Larochelle, H., Bengio, Y., and Manzagol, P.A.\\nExtracting and composing robust features with denois-\\ning autoencoders. In ICML, pp. 1096\\u20131103. ACM, 2008.\\nYuhas, B. P., Goldstein, M. H., and Sejnowski, T. J. Inte-\\ngration of acoustic and visual speech signals using neural\\nnetworks. IEEE Comm. Magazine, pp. 65 \\u201371, 1989.\\n\\nZhao, G. and Barnard, M. Lipreading with local spatiotem-\\nporal descriptors. IEEE Transactions on Multimedia, 11\\n(7):1254\\u20131265, 2009.\\n\\n\\x0c', u'IdeaWall: Improving Creative Collaboration\\n\\nthrough Combinatorial Visual Stimuli\\n\\nYang Shi\\u2217\\n\\nCentral South University\\nshiyang1230@gmail.com\\n\\nJohn Chen\\n\\nUniversity of California, Davis\\n\\njhochen@ucdavis.edu\\n\\nYang Wang\\n\\nUniversity of California, Davis\\n\\nywang@ucdavis.edu\\n\\nXiaoyao Xu\\u2217\\n\\nZhejiang University\\n\\nhsu.xiaoyao@gmail.com\\n\\nYe Qi\\u2217\\n\\nZhejiang University\\n\\nye.charlotte.qi@gmail.com\\n\\nKwan-Liu Ma\\n\\nUniversity of California, Davis\\n\\nma@cs.ucdavis.edu\\n\\nABSTRACT\\nWith the recent advances in computer-supported cooperative\\nwork systems and increasing popularization of speech-based\\ninterfaces, groupware attempting to emulate a knowledgeable\\nparticipant in a collaborative environment is bound to become\\na reality in the near future. In this paper, we present IdeaWall,\\na real-time system that continuously extracts essential informa-\\ntion from a verbal discussion and augments that information\\nwith web-search materials. IdeaWall provides combinatorial\\nvisual stimuli to the participants to facilitate their creative\\nprocess. We develop three cognitive strategies, from which a\\nprototype application with three display modes was designed,\\nimplemented, and evaluated. The results of the user study\\nwith twelve groups show that IdeaWall effectively presents\\nvisual cues to facilitate verbal creative collaboration for idea\\ngeneration and sets the stage for future research on intelligent\\nsystems that assist collaborative work.\\n\\nACM Classi\\ufb01cation Keywords\\nH.5.3. Information Interfaces and Presentation (e.g. HCI):\\nGroup and Organization Interfaces\\n\\nAuthor Keywords\\nVerbal Collaboration; Brainstorming; Visual Cues;\\nGroupware.\\n\\nINTRODUCTION\\nBrainstorming is a form of collaboration by which efforts are\\nmade towards problem solving. A list of ideas spontaneously\\ncontributed by the participants is gathered over the course [28].\\nIn the phase of ideation, collecting, navigating and communi-\\ncating information play important roles for creative thinking\\n[31]. Extensive research efforts in computer-supported co-\\noperative work (CSCW) have been made toward designing\\ngroupware systems that help engage participants in a joint\\n\\n*The work was done while the authors are in residence in UC Davis.\\n\\nPermission to make digital or hard copies of all or part of this work for personal or\\nclassroom use is granted without fee provided that copies are not made or distributed\\nfor pro\\ufb01t or commercial advantage and that copies bear this notice and the full citation\\non the \\ufb01rst page. Copyrights for components of this work owned by others than ACM\\nmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,\\nto post on servers or to redistribute to lists, requires prior speci\\ufb01c permission and/or a\\nfee. Request permissions from permissions@acm.org.\\nCSCW \\u201917, February 25-March 01, 2017, Portland, OR, USA\\n\\xa9 2017 ACM. ISBN 978-1-4503-4335-0/17/03. . . $15.00\\nDOI: http://dx.doi.org/10.1145/2998181.2998208\\n\\ntask by increasing interpersonal awareness or coordinating\\ninformation sharing during collaborative activities (e.g., [12,\\n21]).\\nThe success of brainstorming can be evaluated through the\\nfactors of meeting process constructs and meeting outcomes\\n[7]. Meeting process refers to communication and teamwork\\namong participants that originate from group dynamics [19].\\nAs an effective augmentation of collaboration, visualization\\nhas a proven track record of providing bene\\ufb01cial impacts on\\ndiscourse in online communications (e.g., [33, 32]), which\\nillustrates individual participation to facilitate social interac-\\ntion. Although the participant-centered method presents an\\noverview of the individual activities, relevant information ex-\\nchanged during the conversation is absent. Therefore, we\\napply a content-centered lens to visualize the semantics of\\nbrainstorming content with a purpose of improving the partici-\\npants\\u2019 awareness of their progress.\\nMeeting outcome, on the other hand, refers to the quanti\\ufb01able\\nresults regarding ef\\ufb01ciency. One approach shown to effec-\\ntively improve brainstorming result is presenting conversation-\\nbased stimuli (e.g., text [1] or image [34]). This technique\\nenhances the ideation phase with cognitive strategies. To ex-\\npand existing techniques, our study tests the effect of different\\ncombinations of visual cues on brainstorming\\u2019s success.\\n\\nFigure 1. A concept art of IdeaWall.\\n\\n\\x0cThe illustrative scenario, as shown in Figure 1, depicts how a\\nvisualization may contribute to communication and productiv-\\nity improvement. The visualization which conveys a variety\\nof information to brainstorming participants can inspire them\\nto explore new ideas. These inspirations break the ice when\\nthe team experiences a creative slump and maintain the idea\\ngeneration cycle.\\nIn this paper, we present IdeaWall, a real-time system that\\nsupports brainstorming by providing visual cues derived from\\nan on-going conversation. We \\ufb01rst develop a set of cognitive\\nstrategies based on human cognition mechanisms: Semantic\\nRe\\ufb01nement, Pictorial Activation, and Associative Grouping.\\nSemantic Re\\ufb01nement captures keywords from the conversa-\\ntion. By handling information that short-term memory is no\\nlonger storing, it minimizes cognitive load and redirects focus\\ntowards creative thinking. Pictorial Activation uses cogni-\\ntive stimulation, attaching information-dense visual media to\\nkeywords. The activation of a visual stimulus in a cognitive\\nnetwork can propagate to connected nodes, leading to the\\ngeneration of new ideas. Associative Grouping structures re-\\nlated concepts into clusters according to associative theory.\\nThrough an effective organization of ideas, thoughts can be\\neasily discussed, evaluated, and merged. We then apply the\\nabove strategies to the construction of a proof-of-concept ap-\\nplication that leverages speech recognition (SR), information\\nextraction (IE) and Natural Language Processing (NLP). A\\npilot study was \\ufb01rst conducted to collect preliminary feedback\\non visual interface design alternatives. The \\ufb01nal system em-\\nploys three display modes, namely, Keyword Matrix, Caption\\nMatrix, and Dynamic Cells, which correspond to the three\\ncognitive strategies, respectively.\\nTo evaluate the effectiveness of IdeaWall, a user study with\\n24 participants was conducted. Users became accustomed to\\nthe visualization system and quickly integrated it into their\\nidea generation process. Our study results show that instantly\\npresenting visual cues as a collaborative aid leads to a bet-\\nter conversational \\ufb02ow during brainstorming. The qualitative\\nfeedback suggests that the majority of users preferred our sys-\\ntem which acts as a source of inspiration. We discuss various\\ndesign implications on developing real-time systems to pro-\\nvide a better means of capturing and displaying collaborative\\npractices.\\n\\nRELATED WORK\\n\\nFacilitating Conversation with Visualizations\\nConversation visualization highlights salient information and\\nhelps participants understand the social structure of the dis-\\ncussion [10]. For example, PeopleGarden [37] reveals the\\nsocial structure of a conversation by visualizing patterns which\\nexhibit the arrival of new participants and progression of activ-\\nities. Conversation clock [2] displays individual contribution\\nusing participant audio input augmented with interaction infor-\\nmation and conversation attributes. While much research has\\ngone into participant-centered visualizations, others have used\\na content-centered lens. Speci\\ufb01cally, Conversation Clusters\\n[3] visually summarizes conversations by showing clusters of\\nwords grouped by topics and the evolution of topics over time.\\n\\nOur work focuses on visualizing semantics of a conversation\\nby recording ideas out of brainstorming.\\n\\nGenerating Better Ideas using Content-Driven Media\\nWhen fostering idea generation, a stage of problem solving and\\ndecision making [27], multiple input sources can help elicit\\nprofound thought. With appropriate technology assistance,\\nexternal stimuli may be used to enhance performance. Inspi-\\nrationWall [1] displays concepts which relate to the conversa-\\ntion, enriching ongoing idea generation. Similarly, Nguyen\\net al. [26] introduces a system that uses ranking recommen-\\ndations to generate personalized topic suggestions during a\\nconversation. While some have used textual cues, others have\\nemphasized on presenting pictorial stimuli. For example, Idea-\\nExpander [34] improves upon existing group brainstorming\\nparadigms by adding a picture channel to a chat window. After\\nthe conversation\\u2019s semantic content is analyzed, it is shown on\\nthe visual channel along with related images. Lewis et al. [20]\\napply digitally embedded image stimuli which affect users.\\nTheir results suggest that positive emotional priming improves\\nthe quality of ideas. IdeaWall augments idea generation with\\nrelated pictorial stimuli and effective concept organizations to\\nimprove creative collaboration.\\n\\nMaintaining Group Awareness in the Workspace\\nA well-designed shared workspace is essential to collaboration.\\nPresenting information on a display maintains group aware-\\nness of the current task and widens the breadth of discussion\\nwith additional perspectives while reducing communication\\nambiguities and information distortion [18, 9]. Dynamo [15]\\nsupports meetings in locations such as outdoor venues by\\nexchanging and distributing live information among devices\\nand screens. A multi-surface collaboration space, WeSpace\\n[36], allows simultaneous visual exploration coalesced from\\nmultiple data sources. Otmar et al. [14] design an electronic\\nbrainstorming system which uses two different interactive sur-\\nfaces; a tabletop surface that facilitates writing and group\\ncohesion and a wall display that provides an overview to main-\\ntain context awareness. IdeaWall uses a touch screen to help\\nusers interactively explore ideas in terms of words or images.\\n\\nDESIGN\\nOur goal is to design a real-time visualization system that stim-\\nulates collaborative creative meetings. Grounded in human\\ncognition theories, three cognitive strategies were developed\\nto guide the design process. Corresponding layouts were made\\nto evaluate the effect of the strategies on how visual stimuli\\naffect users\\u2019 responses.\\n\\nCognitive Strategies\\nAfter re\\ufb02ecting on information assimilation and extensive\\nexamination of background literature, we come to three cog-\\nnitive strategies that can be used to facilitate idea generation:\\nSemantic Re\\ufb01nement, Pictorial Activation, and Associative\\nGrouping.\\n\\nSemantic Re\\ufb01nement - Capturing Keywords for Review\\nThe Semantic Re\\ufb01nement strategy captures and displays key-\\nwords extracted from the conversations during a meeting. As\\nreported by the 3M Study [24] as well as Mosvick and Nelson\\n\\n\\x0c[25], up to \\ufb01fty percent of the time spent in a meeting was\\nwasted due to information being lost or distorted, suboptimal\\ndecision making, and meeting mismanagement. The reason\\nis that humans have limited capability for short-term mem-\\norization of information chunks [23]. A typical technique\\nfor minimizing cognitive load is externalization [16]. Seman-\\ntic Re\\ufb01nement utilizes the participants\\u2019 spatial memory and\\nrecords the \\ufb02ow of keywords on a medium accessible to the\\nentire group. Each keyword can be assigned with a degree of\\nsigni\\ufb01cance, correspond to its meaning, frequency, and sur-\\nrounding context. Semantic Re\\ufb01nement reduces the cognitive\\nload on users\\u2019 short-term memory, and then redirects the focus\\ntowards creative tasks such as association and re\\ufb02ection.\\n\\nPictorial Activation - Stimulating Knowledge Using Pictures\\nThe Pictorial Activation strategy provides content-based image\\nstimuli to the problem-solvers. According to Nijstad, Stroebe\\n[27], and Wang [35], the concept of cognitive stimulation sug-\\ngests that using pictures as stimuli constitutes a strategy for\\npresenting multiple concepts at once, through the simultane-\\nous delivery of visual components. A single picture contains\\nmultiple sources of stimuli, including color, shape, context,\\nand more [35]. Each stimulus acts as a new entry node into\\nthe cognitive network [30]. A node in the cognitive network\\nrepresents a perceptual concept. The activation of an entry\\nnode will propagate to other connected nodes in the cognitive\\nnetwork, and lead to the generation of new ideas. By rep-\\nresenting a conversation using pictures, Pictorial Activation\\nallows the provision of additional information-dense stimuli,\\nthen leads thinkers to new and alternative explorations of their\\nknowledge.\\n\\nAssociative Grouping - Structural Organization of Concepts\\nThe Associative Grouping strategy structures related concepts\\ninto clusters. It is derived from the concept of categorization,\\nthrough which ideas and objects are recognized, differentiated,\\nand understood [5]. According to Mednick\\u2019s [22] associative\\ntheory, creating new connections between previously unrelated\\nconcepts and bringing associative elements into ideational\\ncontiguity increases the speed and chance of arriving at a cre-\\native solution. For example, in brainstormings, one common\\nmethod of developing ideas is to aggregate similar concepts\\nsuch as circling related ideas or putting ideas written on post-\\nits next to each other. Thoughts can then be easily discussed,\\n\\nevaluated, and merged. Associative Grouping adds meaning-\\nful structure to otherwise scattered information, producing a\\nmore systematic and organic construction of ideas.\\n\\nDesign Approach\\nTo demonstrate the feasibility of cognitive strategies, we\\nequipped the real-time meeting visualization system with three\\ndisplay modes, namely: Keywords Matrix, Captioned Matrix,\\nand Dynamic Cells. The layout design follows perceptual\\nprinciples drawn from a class of theories known as the Gestalt\\nlaws of grouping. By utilizing the grouping methods while\\nmaintaining a straightforward aesthetic, the design improves\\nreadability and minimizes distraction. The design principles\\nare as follows:\\nDP1 Each display mode addresses features of one cognitive\\n\\nstrategy.\\n\\nDP2 Each display mode incrementally builds up from previ-\\n\\nous ones; consistency is maintained across all modes.\\n\\nDP3 Each display mode contains intuitive and minimalistic\\n\\nvisual components.\\n\\nKeywords Matrix - A Grid of Signi\\ufb01cant Phrases\\nThe Keywords Matrix mode addresses the Semantic Re\\ufb01ne-\\nment strategy by presenting keywords extracted from conver-\\nsational content, as shown in Figure 2. Keywords are arranged\\nin a 3\\xd77 matrix. The layout is symmetric about the x-axis\\nand y-axis. When a new keyword is captured, it is placed in\\nthe central position. Replaced words are moved to one of the\\neight adjacent positions in clockwise order, and then pushed\\ntoward outermost columns. If no space available in that col-\\numn, the keyword with the lowest signi\\ufb01cance is discarded\\nand replaced. The font size of a keyword indicates its signi\\ufb01-\\ncance. As a result, the more important keywords are visually\\nstriking and persist for a longer duration. These design deci-\\nsions are made based on Semantic Re\\ufb01nement strategy that the\\ndisplayed keywords help users maintain their train of thought\\nand externalize cognitive load.\\n\\nCaptioned Matrix - Pictures Attached to Keywords\\nThe Captioned Matrix mode supplements Keywords Matrix\\nmode with Pictorial Activation through the attachment of\\nrelated pictures to conversationally-retrieved keywords, as\\nshown in Figure 3. We keep the 3\\xd77 matrix layout and use\\n\\nFigure 2. A Snapshot of Keywords Matrix Mode\\n\\nFigure 3. A Snapshot of Captioned Matrix Mode\\n\\n\\x0ca rectangular cell for each unit in the matrix. Same as the\\nprevious mode, the most recently added cell is displayed in\\nthe center, and other cells are updated following the same rule\\nas the Keywords Matrix mode. All cells are of identical size\\nwith the keywords as captions and associated pictures as visual\\ncontents. The font size of the caption is used to indicate the\\nsigni\\ufb01cance of the keywords. Each cell has up to ten picture\\ncandidates with a green arrow indicator showing if there are\\nadditional pictures to explore. Users could interact with the\\nvisualization to cycle through pictures for given keywords and\\nare encouraged to do so to bene\\ufb01t from the Pictorial Activation\\nstrategy. Through the introduction of pictorial stimuli, this\\nmode allows users to investigate additional resources beyond\\nwhat was said during conversations.\\n\\nDynamic Cells - Clustering Structure Based on Association\\nThe Dynamic Cells mode combines the three strategies of\\nSemantic Re\\ufb01nement, Pictorial Activation, and Associative\\nGrouping by displaying keywords with related pictures and\\nrepresenting relations amongst them through clustering, as\\nshown in Figure 4. We apply a force-directed layout to utilize\\nits grouping features. Clusters of cells are organized according\\nto their semantic similarity. Large clusters or those with a\\nhigh degree of similarity to other clusters gravitate toward\\nthe center. If an incoming cell is associated with an existing\\none, it would be placed within the same cluster using the\\nUlam spiral method. Otherwise, it forms a new cluster on its\\nown. Up to 21 cells are presented on display for maintaining\\nconsistency. When the number of cells exceeds the maximum\\namount, the oldest cluster fades out of view. We measure\\neach group\\u2019s age by recording the time of its most recent\\naddition. By grouping keywords based on similarity, this mode\\nhelps organize ideas within existing clusters and prompts new\\ncategories of thoughts.\\n\\nImplementation\\nThe architecture of IdeaWall contains four backend compo-\\nnents: Speech Recognition, Keyphrase Extraction, Image Re-\\ntrieval, and Similarity Calculation (see Figure 5).\\nSpeech Recognition The speech recognition component of\\nour prototype system records participants\\u2019 speech, generates a\\nvocal input stream, and returns with a text output stream.\\n\\nWe conducted tests on the state-of-the-art speech recognition\\nservices: Bing Speech API1, CMU Sphinx2, and Google Web\\nSpeech API3. We chose Google Web Speech API because it\\ndemonstrates good performance in the following facets: fast\\nrecognition speed with low delays where latency ranges be-\\ntween 50ms to 100ms; ef\\ufb01cient message communication with\\nthe keyphrase extraction component; and high recognition ac-\\ncuracy when used with the dominant local dialect (i.e., in the\\noptimal environment with no background noise, the recogni-\\ntion rate for users with the Standard American accent were\\nabove 93.5%).\\nKeyphrase Extraction Keyphrase extraction performed by\\nIdeaWall is conducted using the RAKE algorithm [29]. After\\nremoving stop words and punctuation, RAKE chunks the re-\\nmaining tokens in the sentence according to Part-of-Speech\\ntags, leaving only nouns as keyphrase candidates. Then, it\\nbuilds a word co-occurrence graph, scores its signi\\ufb01cance\\ndetermined by the quotient of its word degree and word fre-\\nquency, with emphasis on words predominantly appearing in\\nlonger phrases.\\nRAKE stands out against other complex models (e.g., SVM\\nclassi\\ufb01cation in [34]) in terms of speed, which causes nearly\\nzero delays while maintaining decent accuracy, especially for\\nlengthy input sentences. Besides, RAKE requires few or no\\ntraining sets. This feature enables generating acceptable re-\\nsults even at the beginning of a test case when the accumulated\\ntextual input is not suf\\ufb01cient.\\nImage Retrieval To implement the back-end image retrieval\\nservice, which is initiated after search terms have been de-\\ntermined, we adopted the Custom Search API from Google4.\\nThis API provides comprehensive and precise results within an\\nacceptable period (i.e., around 300ms for any given request).\\nWe also customized the search \\ufb01lter to optimize the applicabil-\\nity of each image to the topic (e.g., set emphasized sites and\\nrestrict results from certain \\ufb01elds).\\nWhen compared to using a self-built local dataset as in [34],\\nour real-time retrieval component can respond to a variety of\\nrequests. By instantly collecting and presenting information,\\nit allows exploration of a much larger database and increases\\nof user engagement.\\n\\n1https://www.microsoft.com/cognitive-services/en-us/speech-api\\n2http://cmusphinx.sourceforge.net\\n3https://dvcs.w3.org/hg/speech-api/raw-\\ufb01le/tip/speechapi.html\\n4https://developers.google.com/custom-search/\\n\\nFigure 4. A Snapshot of Dynamic Cells Mode\\n\\nFigure 5. System Architecture\\n\\n\\x0cSimilarity Calculation IdeaWall calculates the similarity be-\\ntween two key phrases using the normalized cosine distance\\nbetween word2vec5 word vector representations trained from\\nthe English Wikipedia corpora in advance. Once a new key\\nphrase is detected, its most similar counterpart among existing\\nkey phrases is computed, and an edge is de\\ufb01ned based on the\\nsimilarity of this pair.\\nWord2vec is a neural network model that can capture most of\\nthe regularities in the training dataset. It maintains better com-\\nputational ef\\ufb01ciency in comparison with other distributional\\nrepresentations (e.g., ESA model in [3]).\\n\\nEVALUATION\\nThe evaluation process began with a small, limited pilot study\\ndesigned to provide early-stage feedback. It was then followed\\nby a formal user study which determines the effectiveness of\\nthe presented visualizations.\\n\\nPilot Study\\nIn the pilot study, IdeaWall\\u2019s functionality was tested to collect\\npreliminary results in preparation for the main user study. A\\ntotal of 10 participants aged 20 to 28 took part in \\ufb01ve pairs. We\\nexplored a few design alternatives in the pilot study. For exam-\\nple, the proper number of items to be shown in display modes\\nwere tested (e.g., 1\\xd711 matrix, 3\\xd77 matrix, 5\\xd75 matrix). We\\ndecided upon the layout based on user requirements to en-\\nsure they feel neither overwhelmed by the amount of text nor\\nwanting for more information. In addition, participants were\\npresented with a word cloud in early design phase. Through\\nour observations, participants indicated a preference for addi-\\ntional pictorial information, which is more eye-catching and\\nvisually rich. Based on the design principles and participants\\u2019\\nfeedback, we reduced the added complexity between display\\nmodes to conduct a fair comparative study.\\n\\nUser Study\\nAfter modifying the visual design, we conducted a laboratory\\nstudy to investigate how IdeaWall facilitates collaborative cre-\\native meetings. We posed three research questions that we\\naimed to answer:\\n5https://radimrehurek.com/gensim/models/word2vec\\n\\nFigure 6. Verbal Collaboration Assistant in Action\\n\\nType\\n\\nCondition\\n\\nTask\\n\\nDescription\\n\\nTable 1. Factors used in the study\\nID\\nM0 No visual cues\\nM1 Display mode: Keywords Matrix\\nM2 Display mode: Captioned Matrix\\nM3 Display mode: Dynamic Cells\\nB\\nN\\nC\\nQ\\n\\nList unique uses for Bricks\\nList unique uses for Newspapers\\nList unique uses for Coffee mugs\\nList unique uses for Quarters\\n\\nRQ1 How does the system in\\ufb02uence brainstorming?\\n\\nRQ2 How well do the three display modes of the visualization\\n\\nperform?\\n\\nRQ3 How do participants integrate the system into their dis-\\n\\ncussion?\\n\\nParticipants\\nWe recruited 24 participants (8 females) aged 18 to 31 (mean\\n24). We paired them into 12 groups (participants sometimes\\nknew each other before the user study). We used pairs in order\\nto optimize team synergy, duration of speaking time, and user\\nengagement. All participants are \\ufb02uent American English\\nspeakers with experience conducting brainstorming meetings.\\nParticipants included college students, researchers, and profes-\\nsionals from various backgrounds such as computer science,\\npsychology, electrical engineering, biology, neurology, and\\nmusic.\\n\\nApparatus\\nOur user study was conducted using an iMac with 2560\\xd71440\\ndisplay connected to a 27-inch touch screen. The iMac was\\nused as the backend server for speech recognition, data pro-\\ncessing, and monitoring logs generated during the experiment.\\nThe participants were able to interact with the touchscreen\\nand explore image candidates when desired. The brainstorm-\\ning processes were recorded using a camera and on-screen\\ninteractions were recorded using a screen capture tool for later\\nreferences. A studio condenser microphone was used to record\\nthe conversation (see Figure 6).\\n\\nMethodology and Tasks\\nWe conducted a within-subject study and employed three dis-\\nplay modes of IdeaWall (i.e., Keywords Matrix, Captioned\\nMatrix, and Dynamic Cells) as experimental conditions. The\\nthree experimental conditions were counterbalanced using a\\nLatin Square while a control condition with no visualization\\nwas always presented \\ufb01rst. In each of the four conditions, par-\\nticipants were instructed to conduct a \\ufb01ve-minute Guilford\\u2019s\\nAlternative Use Task [13]. Guilford\\u2019s task has been used by\\nprevious brainstorming studies (e.g., [20]) as an activity gen-\\neralizable to other creative meetings. The order of the tasks\\nwith different topics was also counterbalanced to minimize\\nlearning effect (see Table 1).\\n\\n\\x0cType\\n\\ne Group Dynamics\\n\\nTable 2. Measures used in the study\\nID\\n\\u2206Lull\\n#Lull\\n\\nDescription\\nTotal duration of lulls in conversation\\nQuantity of lulls in conversation\\n\\nc\\nn\\na\\nm\\nr\\no\\nf\\nr\\ne\\nP\\n\\ne\\nc\\nn\\ne\\ni\\nr\\ne\\np\\nx\\nE\\n\\nSystem Engagement \\u2206Stare Total duration spent looking at screen\\n\\nCreative Output\\n\\nUsefulness\\n\\nEnjoyability\\n\\n#Click Quantity of clicks on screen\\n#Idea\\n\\u223cIdea\\nQ1\\nQ2\\nQ3\\nQ4\\nQ5\\n\\nQuantity of ideas\\nSimilarity of ideas\\nThe keywords capture the essence of the ideas.\\nThe images are stimulating.\\nThe organization of the keywords are helpful.\\nOverall, the visual cues are helpful.\\nGenerally, we had a productive discussion.\\n\\nProcedure\\nThe participants were brought to the laboratory and given a\\nbrief introduction to the purpose of IdeaWall. They were in-\\nstructed about the task and provided with the four conventional\\nbrainstorming rules [28] before the study: 1) the more ideas,\\nthe better; 2) the wilder ideas, the better; 3) improving or com-\\nbining ideas are better; and 4) do not be critical. Each group\\nreceived instructions (adapted from [13]) at the beginning of\\nthe session to complete a modi\\ufb01ed Guilford\\u2019s Alternative Use\\nTasks, as follows:\\nIn this task, your goal is to think of as many unique and\\nunusual uses for a common object. For example, using a pa-\\nperclip as an earring is an unusual and unique use. However,\\nusing a paperclip to bind papers is not unique or unusual. Try\\nto think of as many unique and unusual uses as possible.\\nEach group \\ufb01rst tested the control condition M0 to warm up\\nand became accustomed to the task, and then completed the\\nremaining three experimental conditions, M1, M2, and M3.\\nBefore each task, participants were given a short introduction\\nof the visual cues that will be shown on the screen and the\\ninteraction types that were supported in this display mode. Par-\\nticipants were free to inspect or interact with the visualization\\nwhen they wished. To allow comparison between different\\nconditions, a questionnaire was administered when users com-\\npleted four conditions at the end of the study.\\n\\nMeasures\\nWe evaluated our prototype based on measurements from two\\nperspectives: performance and experience.\\nPerformance We analyzed two quantitative aspects: meet-\\ning process attributes (e.g., teamwork) and meeting outcomes\\n(e.g., ef\\ufb01ciency) [7]. Two external coders were asked to review\\nvideo recordings of the participants\\u2019 conversations and system\\ninteractions. First, the coders gauged the meeting process\\nbetween users by recording the verbal facet of group dynamics\\nand the physical facet of system engagement. In particular,\\nwe de\\ufb01ne group dynamics as the degree of participation, in\\nwhich silent lulls is used as the main measure. While lulls\\ngive group members time to formulate ideas, they also mark\\na lack of motivation and energy [17]. In our user study, we\\n\\ndetermined that lulls often coincide with low creative energy\\nin a group. Extended silence from both members indicates\\nthat they are having dif\\ufb01culty collaborating. Afterward, the\\ntwo coders measured the overall quality of the users\\u2019 results\\nin creative output. (see Performance section in Table 2)\\nExperience To qualitatively evaluate user experience, we de-\\nsigned a questionnaire consisting of \\ufb01ve questions. Responses\\nto the questionnaire were provided on a 7-likert scale (1 =\\nstrongly disagree; 7 = strongly agree). Questions Q1 to Q3\\nwere intended to examine the usefulness of each display mode,\\nwhereas Q4 and Q5 focused on the users\\u2019 overall enjoyability\\n(see Experience section in Table 2).\\n\\nRESULTS AND ANALYSIS\\nWe next report the results of our study and interpret them to\\nanswer RQ1-3 listed earlier.\\n\\nRQ1: How does the system in\\ufb02uence brainstorming?\\nImprovements in Conversational Engagement We assessed\\nthe total duration of conversational lulls as \\u2206Lull, and their\\nnumbers with #Lull in the conversation across all conditions.\\nThis is used as an indicator of group dynamics. One-way\\nANOVA tests suggest that there exist signi\\ufb01cant differences\\nin \\u2206Lull (F3,44=6.165, p<0.005) and #Lull (F3,44=11.104,\\np<0.0001). Post-hoc Tukey test further reveals that M1 (\\u2206Lull:\\n\\xb5 = 25.36, \\u03c3 = 31.02, #Lull: \\xb5 = 2.64, \\u03c3 = 2.46), M2 (\\u2206Lull:\\n\\xb5 = 16.82, \\u03c3 = 39.75, #Lull: \\xb5 = 1.09, \\u03c3 = 2.02), and M3\\n(\\u2206Lull: \\xb5 = 7.09, \\u03c3 = 14.04, #Lull: \\xb5 = 1.00, \\u03c3 = 1.48) sig-\\nni\\ufb01cantly outperform M0 (\\u2206Lull: \\xb5 = 60.91, \\u03c3 = 34.33, #Lull:\\n\\xb5 = 5.91, \\u03c3 = 2.91). These \\ufb01ndings show that IdeaWall is\\ncapable of reducing the duration and amount of lulls during a\\nmeeting. The use of visual cues result in a marked improve-\\nment in collaboration dynamics.\\nBene\\ufb01ts of System Interaction To further investigate how the\\neffect of system interaction on brainstorming dynamics, we\\nanalyzed the total duration spent staring at screen \\u2206Stare and\\nquantity of clicks #Click on screen. We found a signi\\ufb01cant neg-\\native correlation between \\u2206Stare and #Lull in display modes\\nwhich contain images (M2: R = -0.679, p<0.05, M3: R =\\n-0.671, p<0.05), suggesting that interacting with IdeaWall can\\n\\n\\x0cfacilitate group dynamics. When conveying information, pic-\\ntorial stimuli act as an effective complement to the text, im-\\nproving cognitive performance.\\nWe looked at quantities of ideas #Idea and their similarity\\n\\u223cIdea to analyze creative performance (\\u223cIdea between each\\npair of ideas was computed using the method described in\\nSimilarity Calculation section). We observed no signi\\ufb01cant\\nenhancement within the creative dimension, implying that our\\nsystem may not be able to alter users\\u2019 inherent creative abilities\\nsigni\\ufb01cantly. The reason may be that by tasking the users\\nwith developing unusual uses for familiar items, we are not\\nfully exploring the users\\u2019 creativity. As one of the participants\\nsuggested, \\u201cI\\u2019d like to try more dif\\ufb01cult and challenging topics\\u201d\\n(P13). Thus, we suggest that an alternative scenario of using\\nIdeaWall can be improvising uses for unfamiliar objects.\\n\\nRQ2: How well do the three display modes perform?\\nKeywords Tracking Aids Review and Recall A majority of\\nthe participants found keywords helpful (see Figure 7). \\u201cNot\\nall the [captured] keywords made sense, but a lot of them\\nwere helpful. . . You can look back at words when you are stuck.\\nIt helps when relating and connecting ideas\\u201d, stated by P6.\\n\\u201cI didn\\u2019t have to stop to recall what we came up with, they\\nwere already there\\u201d (P23). Some of the participants suggested\\nadding interesting features, \\u201cit might work better if you had\\nclusters and showed connections among those [keywords]\\u201d\\n(P19).\\nImages Inspire Thought and Reduce Ambiguity Many partic-\\nipants commented that the retrieved images were \\u201cinspiring\\u201d.\\n\\nFigure 7. Responses to \\ufb01ve evaluation questions illustrated in a stacked\\nbar chart.\\n\\n\\u201cWatching them [pop up] while brainstorming is fun\\u201d (P2, P14,\\nP17). \\u201cI like colors, so it\\u2019s helpful to me. Color works well.\\nI think it\\u2019s easy to associate an object with the color after\\nyou see this\\u201d (P17). In addition, participants occasionally re-\\nfocused their conversation by using the visual cues. \\u201cOh, I\\nthought you were referring to. . . Yes, that would make sense\\u201d\\n(P7). \\u201cYeah, that\\u2019s what I was talking about, look. . . \\u201d (P20).\\nOrganized Thoughts Improve Cue\\u2019s Effectiveness Clustering\\nstructure is described as an \\u201cinteresting\\u201d feature to assist par-\\nticipants\\u2019 idea generation process. \\u201cSmall clumps in level three\\noffered ideas\\u201d (P2). \\u201cWhen I was trying to come up with ideas,\\nI moved one cluster to another\\u201d (P4). \\u201cWhen two keywords\\nare combined, it helps trigger new ideas\\u201d (P10). Participants\\nthought the layout reduces mental strain. \\u201cIt takes time if I had\\nto sort [the information] by myself, this [automatic sorting]\\nallows me to think about new ideas\\u201d (P9). \\u201cSometimes [the\\ncaptioned pictures] moved around too much, that could be\\ndistracting. But it helps by shuf\\ufb02ing ideas, and it\\u2019s interesting\\nto the eye\\u201d (P5).\\nWhen compared to other modes, Dynamic Cell mode received\\nmore feedback regarding potential changes. \\u201cMode three high-\\nlights related images and ideas. There is room for more fea-\\ntures like you could make the cluster selectable, make the\\nkeywords indicate high-level ideas\\u201d (P3). \\u201cI wish I could\\ndelete useless clusters or create a new one by myself \\u201d (P2).\\n\\u201cYou may consider using the existing clusters to generate new\\nblocks. That could be very interesting. It gives ideas even\\nthough I didn\\u2019t say a word about it\\u201d (P14).\\n\\nRQ3: How do participants integrate the system into their\\ndiscussion?\\nImages Augment Text Cues Most groups expressed a prefer-\\nence for pictures over text. As remarked by P3, P14, P19, and\\nP22: \\u201cPictures convey more\\u201d. When asking about the order\\nof preference when comparing three display modes, a major-\\nity of the participants indicated that they prefer the modes\\nwith images. \\u201cI de\\ufb01nitely like level two and three the best,\\nthey\\u2019re more eye-catching, though level one could be helpful\\nsomehow\\u201d (P1). Participants also suggested various design\\nalternatives, such as \\u201cbigger images\\u201d (P3, P7, P16), \\u201cmore\\npicture candidates\\u201d (P13), and \\u201cself-cycling pictures\\u201d (P5,\\nP9).\\nMismatched Cues may Give Unexpected Bene\\ufb01ts Several\\nparticipants suggested that even mismatched keywords some-\\ntimes provide bene\\ufb01ts. \\u201cIt\\u2019s interesting that the computer\\npicked up words we didn\\u2019t say. . . Keywords that are incorrect\\ngive new ideas\\u201d (P5, P11). Participants noted the bene\\ufb01ts of\\nthe mismatched pictures, as such pictures could deliver unan-\\nticipated but helpful information and then prompt alternative\\ndiscussions. \\u201cIt\\u2019s actually more helpful when images didn\\u2019t\\nmatch [the keyword]\\u201d (P1, P6, P18). P16 added: \\u201cI found\\nunrelated words add confusion but photos don\\u2019t. . . I\\u2019d prefer\\nmore variance in photos, like for \\u2018cat\\u2019, show me some cat toys\\ninstead of three photos of ordinary cats\\u201d.\\nVarious Uses Based on Collaborative Style Groups used the\\nvisualization system differently based on their communication\\n\\n\\x0cand collaboration styles. For example, P6 from group 3 re-\\nmarked that \\u201cwe were so good at this that I didn\\u2019t really look at\\nthe screen\\u201d, while P12 from group 6 stated, \\u201cmy partner didn\\u2019t\\nsay much. It was a lot harder to come up with ideas on my own.\\nI had to look at the screen for inspiration now and then. It\\u2019s\\nextremely helpful when you\\u2019re running out of ideas.\\u201d Partici-\\npants tended to have very different ratings and opinions about\\ntheir user experience based on the groups\\u2019 performance. How\\ndifferent group styles affected their collaboration is discussed\\nlater on.\\n\\nDISCUSSION\\nIn this section, we discuss design implications to help set\\nthe stage for future development of real-time collaborative\\nsystems.\\n\\nMismatched Queries and Idea Generation\\nWe observed mismatched queries in both keywords and im-\\nages due to inaccuracy inherent within the speech recognition\\nand information extraction components of IdeaWall. These\\ninaccurate queries interfere with users\\u2019 thought processes and\\nresult in distraction. The accuracy of queries could be more\\ncritical for tasks that involve organizational logic and pattern\\nrecognition. For example, if participants are told to \\u201cdeter-\\nmine the most effective use of bricks\\u201d instead of \\u201cthink of as\\nmany uses of bricks as possible\\u201d, the system should maintain\\nhigh accuracy to keep track of the items that in\\ufb02uence the\\ndecision-making process. This way, participants can propose\\nrelated ideas more quickly, as the distraction is minimized by\\nproviding more focused cues.\\nAlthough inaccuracy poses undesirable factors, we found that\\nusers are able to draw inspiration from some mismatched cues\\nand use them to achieve improvements. For example, during a\\nuser study session where the task was brainstorming unique\\nuses for bricks, the system captured \\u201cbreak\\u201d when one of the\\nparticipants said \\u201cbrick\\u201d. This inaccurate result caught the\\nparticipants\\u2019 attention, and they immediately came up with\\nthe idea to \\u201cbreak bricks to make sculptures\\u201d. In this case,\\ndiscrepancies between the spoken and captured words result in\\nthe activation of the train of thought. Similarly, when a picture\\ndiffers from the spoken word, it inspires users to explore new\\nconcepts along that track.\\nMismatched results can be leveraged to generate a wider vari-\\nety of cues by promoting lateral thinking [8]. Instead of con-\\ntinuing to move in familiar directions, lateral thinking takes\\noff laterally to a new and innovative direction using random\\ninputs (e.g., word, picture, and sound). We suggest consider-\\ning lateral stimulus, which can range from similar words in\\npronunciation to synonyms or antonyms. For example, when\\nthe keyword \\u201ccat\\u201d is captured, rather than presenting informa-\\ntion on mundane cats, a lateral approach would present results\\nderived from varying classi\\ufb01cations, such as \\u201ccartoon cat\\u201d,\\n\\u201cCatwoman\\u201d, or even \\u201cdog\\u201d. These loosely related concepts\\nreap additional bene\\ufb01ts by delivering unanticipated but helpful\\ninformation from a unique perspective.\\n\\nUse Context to Mark Areas of Interest\\nTo further improve the idea generation process, multiple con-\\ntextual dimensions can be attached to each of the ideas gener-\\n\\nated in brainstorming. Contextual data of the raw audio input\\ncan be used to depict interesting conversational features, such\\nas emotional states and volumes (e.g., [6]). For example, a\\nparticipant might be excited when expressing his ideas in a\\nloud voice. The context suggests that such ideas of interest are\\nvaluable to explore and review in post-meeting session.\\nLog data that shows participants\\u2019 interaction with the system\\ncan also be collected to track areas of interest (e.g., [4]). Users\\nmay exhibit certain tendencies regarding different stimuli. In\\nour case, some participants actively cycled through pictures\\nfor clues while others tried to re-organize idea cluster. These\\nengagement patterns can be found through the analysis of\\nmouse click and eye movement data. Visualization of log data\\nhelps determine the components (e.g., text, image) that attract\\nmore attention and achieve greater effects, thus providing new\\ninsights into the effectiveness of IdeaWall.\\n\\nIdentify Individual Contribution\\nThe current design of IdeaWall focuses on analyzing the se-\\nmantics of brainstorming content, its functionality could be\\nexpanded by using various participant-related information to\\norganize visual cues. For example, methods revealing indi-\\nvidual participation such as speaker recognition and opinion\\nextraction can be used to better understand the ef\\ufb01cacy of\\nmeetings. Differentiation between speakers in a conversation\\ncould alleviate distraction by separating clusters based on in-\\ndividuals. This speaker-based clustering organizes thoughts\\nin a way that augments depiction of individual characteristics\\nand supports re\\ufb02ection on group productivity.\\nOther mentioned venues for enhancement include adding argu-\\nmentative dimension. For example, within a topic discussion,\\nthere may be a con\\ufb02ict variation, as some people disagree\\nwith others. It would be bene\\ufb01cial to capture this argumenta-\\ntive divide in ideas, that is, whether all participants agreed on\\nthe same decision, whether there were two sides discussing\\nover a divide, or whether the topic was a free-for-all of opin-\\nions. Further studies could be conducted to compare different\\nclustering methods based on group criteria. By highlighting\\nthought process in each camp, IdeaWall acts as a research tool\\nto assess how different participants contribute to certain ideas\\nor design decisions.\\n\\nMake Passive Displays Participate\\nThe design of interactive systems should consider the man-\\nifestation of user behavior. In our deployment of IdeaWall,\\ncollaborations between participants pose different forms de-\\npendent upon group chemistry and dynamics. Among the\\ndifferent styles of teams, groups with long idle times between\\nideas relied heavily on the visual support interface to receive\\ninspiration. On the other hand, highly productive pairs with\\nbetter rapport preferred to focus on the ongoing conversation.\\nThey used the visualization if they got stuck or ran out of\\nideas. For both collaboration styles, the system proved most\\neffective when conversations reach a lull. In addition, partici-\\npants\\u2019 roles play a large part in determining how they interact\\nwith the system. Participants who were listeners were able to\\ncomplete additional tasks. They may retrieve clues with the\\npicture cycling feature of the visualization while directing the\\n\\n\\x0cspeaker\\u2019s attention toward valuable leads. The active speakers\\nwere less likely to use the interface until they had completed\\ntheir current thought.\\nAn advanced real-time intelligent system could engage in a\\nconversation as a knowledgeable participant; it gives different\\nresponses based on speci\\ufb01c situations, whether active, reactive,\\nor passive. For example, it suggests a new direction to explore\\nwhen a collaborative discussion reaches a deadlock. It gives\\nfeedback and suggestions accordingly when other participants\\nare presenting their thoughts. Through the use of situation-\\nbased strategies, further development of such systems could\\nbe designed to present selected cues based on the perceived\\nparticipants\\u2019 intentions (e.g., by mining a broad knowledge\\nbase of human behavior [11]).\\n\\nLimitations and Future Work\\nThere were several limitations discovered in our study that\\nleave room for future improvement. By adopting group brain-\\nstorming solutions for Guilford\\u2019s Alternative Uses Tasks, we\\nchose to task users with objectives that could be ful\\ufb01lled imme-\\ndiately. Meanwhile, the brainstorming was conducted using\\npairs in order to optimize team synergy and user engagement.\\nHowever, evaluating Guilford\\u2019s task on pairs with limited sam-\\nple size makes it dif\\ufb01cult to generalize the \\ufb01ndings to an open\\nbrainstorm of a group. We suggest evaluating the tool in a\\nmore realistic context, such as that of the major academic\\ndiscussions or design agency meetings. Developing an ideal\\nscenario would allow us to draw stronger conclusions regard-\\ning the effectiveness of our system for larger, more complex\\nmeetings.\\nFurther, the design of clustering in Dynamic Cell uses a force-\\ndirected layout. According to qualitative feedback, this dy-\\nnamic organization is novel but could also be distracting due\\nto its progressive movement. It is not clear if other designs\\nmight achieve better or worse results. For example, making\\nuse of Treemapping, Sunburst, and Sankey diagrams could\\nvisualize the information in more static ways. We suggest\\nfuture studies focus on conducting a comparison between the\\ndifferent clustering layouts to obtain a deeper understanding\\nof the visual forms most helpful in supporting collaborative\\ncreative activities.\\n\\nCONCLUSION\\nThis paper presents IdeaWall, a groupware system which com-\\nprehends human conversation and provides helpful visual aids.\\nThe state-of-the-art speech recognition and information ex-\\ntraction technologies were used to develop a proof-of-concept\\napplication. Using design strategies grounded in mechanisms\\nof human cognition, the content-centered visualization with\\nnovel layouts act as an effective tool for improving verbal\\ngroup discussions.\\nThrough a laboratory study, our work was found to bestow sev-\\neral bene\\ufb01ts: a majority of users preferred our system with in-\\nstant visual feedback; pictorial cues were considered to be far\\nmore helpful than purely textual cues; and structuring informa-\\ntion organically positively affected creative performance. Our\\nstudy results indicate that IdeaWall provides a better means\\n\\nof capturing and displaying collaborative practices using visu-\\nalization techniques. Our work also suggests areas for more\\nstudies on converting multi-party conversation data into ef-\\nfective visual aids to help inspire idea generation, identify\\nindividual contribution, and re\\ufb02ect on group productivity.\\n\\nACKNOWLEDGMENTS\\nThis research is supported in part by the U.S. National Science\\nFoundation via grants NSF DRL-1323214, NSF IIS-1528203\\nand NSF IIS-1320229, the U.S. Department of Energy through\\ngrant DE-FC02-12ER26072, and National Natural Science\\nFoundation of China under grant No.61402540, No.61672538.\\nWe thank Yaoxue Zhang, Fangfang Zhou, Ying Zhao from\\nCentral South University and Yi-Ling Chen from University\\nof California, Davis for their assistance. We also thank Justin\\nMa for creating the concept art.\\n\\nREFERENCES\\n1. Salvatore Andolina, Khalil Klouche, Diogo Cabral,\\n\\nTuukka Ruotsalo, and Giulio Jacucci. 2015.\\nInspirationWall: Supporting Idea Generation Through\\nAutomatic Information Exploration. In Proceedings of\\nthe ACM SIGCHI Conference on Creativity and\\nCognition. 103\\u2013106.\\n\\n2. Tony Bergstrom and Karrie Karahalios. 2007.\\n\\nConversation Clock: Visualizing audio patterns in\\nco-located groups. In Proceedings of the 40th Annual\\nHawaii International Conference on System Sciences.\\nIEEE, 78\\u201378.\\n\\n3. Tony Bergstrom and Karrie Karahalios. 2009.\\n\\nConversation clusters: grouping conversation topics\\nthrough human-computer dialog. In Proceedings of the\\nACM SIGCHI Conference on Human Factors in\\nComputing Systems. 2349\\u20132352.\\n\\n4. Tanja Blascheck, Markus John, Kuno Kurzhals, Steffen\\nKoch, and Thomas Ertl. 2016. VA2: A Visual Analytics\\nApproach for Evaluating Visual Analytics Applications.\\nIEEE transactions on visualization and computer\\ngraphics 22, 1 (2016), 61\\u201370.\\n\\n5. Henri Cohen and Claire Lefebvre. 2005. Handbook of\\n\\ncategorization in cognitive science. Elsevier.\\n\\n6. Andrew J Cowell, Michelle L Gregory, Joe Bruce,\\n\\nJereme Haack, Doug Love, Stuart Rose, and Adrienne H\\nAndrew. 2006. Understanding the dynamics of\\ncollaborative multi-party discourse. Information\\nVisualization 5, 4 (2006), 250\\u2013259.\\n\\n7. Robert Davison. 1997. An instrument for measuring\\nmeeting success. Information & Management 32, 4\\n(1997), 163\\u2013176.\\n\\n8. Edward De Bono. 2010. Lateral thinking: a textbook of\\n\\ncreativity. Penguin UK.\\n\\n9. Joan Morris DiMicco, Anna Pandolfo, and Walter Bender.\\n\\n2004. In\\ufb02uencing group participation with a shared\\ndisplay. In Proceedings of the ACM conference on\\nComputer supported cooperative work. 614\\u2013623.\\n\\n\\x0c10. Judith Donath, Karrie Karahalios, and Fernanda Viegas.\\n\\n1999. Visualizing conversation. Journal of\\nComputer-Mediated Communication 4, 4 (1999).\\n\\n11. Ethan Fast, William McGrath, Pranav Rajpurkar, and\\n\\nMichael S Bernstein. 2016. Augur: Mining Human\\nBehaviors from Fiction to Power Interactive Systems. In\\nProceedings of the ACM SIGCHI Conference on Human\\nFactors in Computing Systems. 237\\u2013247.\\n\\n12. Saul Greenberg and Michael Rounding. 2001. The\\n\\nnoti\\ufb01cation collage: posting information to public and\\npersonal displays. In Proceedings of the ACM SIGCHI\\nConference on Human factors in computing systems.\\n514\\u2013521.\\n\\n13. Joy Paul Guilford. 1967. The nature of human\\n\\nintelligence. McGraw-Hill.\\n\\n14. Otmar Hilliges, Lucia Terrenghi, Sebastian Boring, David\\n\\nKim, Hendrik Richter, and Andreas Butz. 2007.\\nDesigning for collaborative creative problem solving. In\\nProceedings of the ACM SIGCHI Conference on\\nCreativity & Cognition. 137\\u2013146.\\n\\n15. Shahram Izadi, Harry Brignull, Tom Rodden, Yvonne\\nRogers, and Mia Underwood. 2003. Dynamo: a public\\ninteractive surface supporting the cooperative sharing and\\nexchange of media. In Proceedings of the ACM\\nsymposium on User interface software and technology.\\n159\\u2013168.\\n\\n16. David Kirsh. 1995. The intelligent use of space. Arti\\ufb01cial\\n\\nintelligence 73, 1 (1995), 31\\u201368.\\n\\n17. Jeffrey A Kottler and Matt Englar-Carlson. 2009.\\n\\nLearning group leadership: An experiential approach.\\nSage, Chapter Understanding Group Dynamics and\\nSystems, 76\\u201379.\\n\\n18. Robert E Kraut, Darren Gergle, and Susan R Fussell.\\n2002. The use of visual information in shared visual\\nspaces: Informing the development of virtual co-presence.\\nIn Proceedings of the ACM Conference on Computer\\nsupported cooperative work. 31\\u201340.\\n\\n19. Daniel Levi. 2016. Group dynamics for teams. Sage\\n\\nPublications.\\n\\n20. Sheena Lewis, Mira Dontcheva, and Elizabeth Gerber.\\n\\n2011. Affective computational priming and creativity. In\\nProceedings of the ACM SIGCHI Conference on Human\\nFactors in Computing Systems. 735\\u2013744.\\n\\n21. Joseph F McCarthy, Ben Congleton, and F Maxwell\\n\\nHarper. 2008. The context, content & community collage:\\nsharing personal digital media in the physical workplace.\\nIn Proceedings of the ACM Conference on Computer\\nsupported cooperative work. 97\\u2013106.\\n\\n22. Sarnoff Mednick. 1962. The associative basis of the\\n\\ncreative process. Psychological review 69, 3 (1962), 220.\\n23. George A Miller. 1956. The magical number seven, plus\\nor minus two: some limits on our capacity for processing\\ninformation. Psychological review 63, 2 (1956), 81.\\n\\n24. Peter R Monge, Charles McSween, and JoAnne Wyer.\\n\\n1989. A pro\\ufb01le of meetings in corporate America: Results\\nof the 3M meeting effectiveness study. Annenberg School\\nof Communications, University of Southern California.\\n\\n25. Roger K Mosvick and Robert B Nelson. 1987. We\\u2019ve got\\nto start meeting like this: a guide to successful business\\nmeeting management. Scott Foresman Trade.\\n\\n26. Tien T Nguyen, Duyen T Nguyen, Shamsi T Iqbal, and\\n\\nEyal Ofek. 2015. The Known Stranger: Supporting\\nConversations between Strangers with Personalized Topic\\nSuggestions. In Proceedings of the ACM SIGCHI\\nConference on Human Factors in Computing Systems.\\n555\\u2013564.\\n\\n27. Bernard A Nijstad and Wolfgang Stroebe. 2006. How the\\n\\ngroup affects the mind: A cognitive model of idea\\ngeneration in groups. Personality and social psychology\\nreview 10, 3 (2006), 186\\u2013213.\\n\\n28. Alex F Osborn. 1953. Applied imagination. Scribner\\u2019s.\\n29. Stuart Rose, Dave Engel, Nick Cramer, and Wendy\\nCowley. 2010. Automatic keyword extraction from\\nindividual documents. Text Mining (2010), 1\\u201320.\\n30. Eric L Santanen, Robert O Briggs, and Gert-Jan\\n\\nDe Vreede. 2000. The cognitive network model of\\ncreativity: a new causal model of creativity and a new\\nbrainstorming technique. In Proceedings of the Hawaii\\nInternational Conference on System Sciences. IEEE.\\n\\n31. Ben Shneiderman. 2007. Creativity support tools:\\n\\nAccelerating discovery and innovation. Commun. ACM\\n50, 12 (2007), 20\\u201332.\\n\\n32. Annie Tat and M Sheelagh T Carpendale. 2002.\\n\\nVisualising human dialog. In Proceedings of\\nInternational Conference on Information Visualisation.\\nIEEE, 16\\u201321.\\n\\n33. Fernanda B Vi\\xe9gas and Judith S Donath. 1999. Chat\\n\\ncircles. In Proceedings of the ACM SIGCHI Conference\\non Human Factors in Computing Systems. 9\\u201316.\\n\\n34. Hao-Chuan Wang, Dan Cosley, and Susan R Fussell.\\n2010. Idea Expander: Supporting group brainstorming\\nwith conversationally triggered visual thinking stimuli. In\\nProceedings of the ACM conference on Computer\\nsupported cooperative work. 103\\u2013106.\\n\\n35. Hao-Chuan Wang, Susan R Fussell, and Dan Cosley.\\n2011. From diversity to creativity: Stimulating group\\nbrainstorming with cultural differences and\\nconversationally-retrieved pictures. In Proceedings of the\\nACM conference on Computer supported cooperative\\nwork. 265\\u2013274.\\n\\n36. Daniel Wigdor, Hao Jiang, Clifton Forlines, Michelle\\n\\nBorkin, and Chia Shen. 2009. WeSpace: the design\\ndevelopment and deployment of a walk-up and share\\nmulti-surface visual collaboration system. In Proceedings\\nof the ACM SIGCHI Conference on Human Factors in\\nComputing Systems. 1237\\u20131246.\\n\\n37. Rebecca Xiong and Judith Donath. 1999. PeopleGarden:\\n\\ncreating data portraits for users. In Proceedings of the\\nACM symposium on User interface software and\\ntechnology. 37\\u201344.\\n\\n\\x0c', u'6\\n1\\n0\\n2\\n\\n \\nt\\nc\\nO\\n2\\n1\\n\\n \\n\\n \\n \\n]\\nL\\nC\\n.\\ns\\nc\\n[\\n \\n \\n\\n1\\nv\\n5\\n8\\n5\\n3\\n0\\n\\n.\\n\\n0\\n1\\n6\\n1\\n:\\nv\\ni\\nX\\nr\\na\\n\\nA Paradigm for Situated and Goal-Driven Language Learning\\n\\nJon Gauthier1,2\\njon@gauthiers.net\\n\\nIgor Mordatch1,3\\n\\nmordatch@openai.com\\n\\n1OpenAI\\n\\n2Stanford NLP Group 3UC Berkeley\\n\\nA distinguishing property of human intelligence is the ability to \\ufb02exibly use language in\\norder to communicate complex ideas with other humans in a variety of contexts. Research\\nin natural language dialogue should focus on designing communicative agents which can\\nintegrate themselves into these contexts and productively collaborate with humans.\\n\\nIn this abstract, we propose a general situated language learning paradigm which is\\ndesigned to bring about robust language agents able to cooperate productively with humans.\\nThis dialogue paradigm is built on a utilitarian de\\ufb01nition of language understanding.\\nLanguage is one of multiple tools which an agent may use to accomplish goals in its\\nenvironment. We say an agent \\u201cunderstands\\u201d language only when it is able to use language\\nproductively to accomplish these goals. Under this de\\ufb01nition, an agent\\u2019s communication\\nsuccess reduces to its success on tasks within its environment.\\n\\nThis setup contrasts with many conventional natural language tasks, which maximize\\nlinguistic objectives derived from static datasets. Such applications often make the mistake\\nof reifying language as an end in itself. The tasks prioritize an isolated measure of linguistic\\nintelligence (often one of linguistic competence, in the sense of Chomsky (1965)), rather\\nthan measuring a model\\u2019s e\\ufb00ectiveness in real-world scenarios.1 Our utilitarian de\\ufb01nition\\nis motivated by recent successes in reinforcement learning methods. In a reinforcement\\nlearning setting, agents maximize success metrics on real-world tasks, without requiring\\ndirect supervision of linguistic behavior.\\n\\nThe environment\\n\\nWe propose an end-to-end learning environment with multiple language-enabled agents,\\neach with the capacity to de\\ufb01ne their own internal goals and plans to reach those goals.\\nEach agent may also have di\\ufb00erent capacities to observe or act in this environment. Their\\ngoals are grounded non-linguistic objectives: for example, to reach a desired location,\\nmanipulate objects in the environment, or transmit a piece of information.2 (We address\\nthe issue of non-linguistic grounding at length in the next section.) Some \\ufb01xed-language\\nagents in the environment speak an existing conventional language (e.g. English), and\\nother learning agents are tasked with jointly learning this language while also solving other\\ngoals in the environment. The agents are assigned di\\ufb03cult (possibly distinct) tasks which\\n\\n1Our motivation here is similar to that of Dagan et al. (2006), who recognized the negative e\\ufb00ects of a community\\nfragmented across di\\ufb00erent isolated application-speci\\ufb01c tasks, and suggested the uni\\ufb01ed task of recognizing textual\\nentailment (RTE) as a solution.\\n\\n2A related line of work in evolutionary linguistics constructs a similar language learning scenario entirely without\\n\\ufb01xed-language agents (Smith et al., 2003; Steels, 2012; Kirby et al., 2014). All of the agents in these environments\\nconstruct a novel language simultaneously to accomplish some shared task. This is an interesting separate line\\nof research, but ultimately a separate task from the understanding and acquisition problems discussed in this\\nabstract.\\n\\n\\x0crequire them to cooperate by communicating via their language channel.3\\n\\nIn the simplest instance of the paradigm above, a single \\ufb01xed-language parent agent\\ninteracts with a learning child agent. The parent cooperates with the child, but only\\nwhen prompted through language. As a result, the child acquires the language of the\\nparent in order to accomplish its task. This speci\\ufb01c scenario is designed to train single\\ncommunicative agents which can accomplish tasks in their environments with the help of\\nhuman or simulated counterparts.\\n\\nIt is possible to include \\ufb01xed-language agents in these environments without requiring\\nhuman involvement. We have developed several instantiations of this paradigm in which\\nthe \\ufb01xed-language agents speak some simpli\\ufb01ed hard-coded English or an arti\\ufb01cial language\\n(e.g. a programming language or query language). Near the limit of arti\\ufb01cial complexity,\\nagents might take advantage of the Internet to access world knowledge and synthesize\\nresponses. As our algorithms for learning in these environments improve, however, we\\nexpect that involving human agents in the environment would be an extremely e\\ufb00ective\\nway to train language-enabled arti\\ufb01cial agents.\\n\\nEnvironment grounding\\n\\nOur language-learning agents crucially need to be grounded in a world which is not only\\nlinguistic. This grounding may be physical \\u2014 for example, agents which are embodied\\nin the real world \\u2014 or virtual. This grounding is what allows us to evaluate the agents\\nin some way that does not prioritize language as the only objective. Grounding imposes\\nadditional responsibilities on the agent, such as competently sensing and acting in the\\nworld, but this should be seen as an opportunity rather than a problem. As we discuss in\\nthe following paragraphs, a grounded agent may use its experience in its environment to\\nbuild better models of language, and likewise use its language to better reason about its\\nenvironment.\\n\\nThis grounded environment is designed to bring about agents with comprehensive predic-\\ntive models of the world, which combine linguistic knowledge with more general intelligent\\nbehavior. By design, we do not separate the activity of language model construction\\nfrom many other intelligent predictive activities \\u2014 whether physical (predicting physical\\nbehavior of objects), psychological (modeling the beliefs and intentions of other agents), or\\nsocial (understanding group membership and group-level action) (Lake et al., 2016; Clark,\\n2013). While early instantiations of this environment will limit the complexity necessary\\nfor the agents to model, we expect all of these di\\ufb00erent factors to eventually be relevant in\\na comprehensive learning environment.\\n\\nMikolov et al. (2015) make a similar argument for grounding, but arrive at an environment\\nin which perception and action is mediated only through a linguistic channel. While this\\ntextual interface certainly has the potential to simplify the problem, we believe there are\\nseveral issues with this setup.\\n\\nTextual interfaces impose a lossy representation of an agent\\u2019s actual environment (Brooks,\\n\\n3 Much of the recent novel work in dialogue-based learning (Fleischman and Roy, 2005; Vogel et al., 2014;\\nWang et al., 2016; Weston, 2016) and multi-agent communication (Lazaridou et al., 2016; Andreas and Klein,\\n2016; Foerster et al., 2016; Sukhbaatar et al., 2016) can be \\ufb01t into the paradigm described so far. This paper is\\nconcerned with designing a general paradigm for language-learning which contains these experiments, and picking\\nout properties of the learning environment which are important for future research.\\n\\n\\x0c1991), and necessarily bias the focus of an agent\\u2019s observations. Consider the following\\ntext-only interaction in the style of Mikolov et al. (2015) between a learning agent A and\\nomniscient agent B, situated in some simulated physical world:\\n\\nB: You are carrying a box of eggs and need to set them down.\\nA: Is there a table nearby?\\nB: There is a table to your left.\\nA: I put the box on the table.\\nB: The box slides off of the table and the eggs break open.\\n[The table is missing a leg and tilted when the box was laid down.]\\n\\nThe agent could have avoided this disaster if it had queried to \\ufb01nd out that the table was\\nmissing a leg before placing weight on it. But we cannot expect an agent to exhaustively\\nquery its environment via text in general. (Should it also ask whether the table is made of\\nsolid material, or whether the table is on \\ufb01re?) By contrast, a lossless visual observation\\ndoes not remove possibly relevant information about the properties of tables, and it is up\\nto the agent to learn what to attend to in such an environment and how to interpret these\\nvisual percepts.\\n\\nFor environments involving pre-programmed \\ufb01xed-language agents, generating textual\\nenvironment descriptions also places a signi\\ufb01cant implementational burden. It is relatively\\neasy to generate physical motion and visual renderings of three-legged tables or toppling\\nevents, but much more di\\ufb03cult to describe these concepts in natural language.\\n\\nOf course, even if we claim that text data is not enough, we must settle for some level of\\nabstraction. A similar argument to the one above could be made for including sound-wave\\nspeech data in the agent\\u2019s observations rather than only providing text and visual data.\\nWhile this argument is likely also correct, we have to settle for a level of representation\\nwhich is computationally tractable. Given that we want to make near-term progress on\\nlanguage comprehension and production, it seems reasonable to work with messages in text\\nrepresentation for the time being. It is likewise tractable to work in a virtual environment\\nwith simulated physics and visual inputs.\\n\\nConclusion\\n\\nIn this abstract, we outlined a paradigm for grounded and goal-driven language learning\\nin arti\\ufb01cial agents. The paradigm is centered around a utilitarian de\\ufb01nition of language\\nunderstanding, which equates language understanding with the ability to cooperate with\\nother language users in real-world environments. This position demotes language from\\nits position as a separate task to be solved to one of several communicative tools agents\\nmight use to accomplish their real-world goals.\\n\\nWhile this paradigm does already capture a small amount of recent work in dialogue, on\\nthe whole it has not received the focus it deserves in the research communities of natural\\nlanguage processing and machine learning. We hope this paper brings focus to the task of\\nsituated language learning as a way forward for research in natural language dialogue.\\n\\nAcknowledgments\\n\\nThe authors wish to thank their colleagues at OpenAI and Stanford for their useful\\ncomments and critiques.\\n\\n\\x0cReferences\\n\\nJacob Andreas and Dan Klein. Reasoning About Pragmatics with Neural Listeners and\\nSpeakers. arXiv:1604.00562 [cs], April 2016. URL http://arxiv.org/abs/1604.00562.\\narXiv: 1604.00562.\\n\\nRodney Brooks. Intelligence without representation. Arti\\ufb01cial Intelligence, 47:139\\u2013159,\\n\\n1991.\\n\\nNoam Chomsky. Aspects of the Theory of Syntax. M.I.T. Press, 1965.\\nAndy Clark. Whatever next? predictive brains, situated agents, and the future of cognitive\\nscience. Behavioral and Brain Sciences, 36(03):181\\u2013204, 5 2013.\\nISSN 1469-1825.\\ndoi: 10.1017/S0140525X12000477. URL http://journals.cambridge.org/article_\\nS0140525X12000477.\\n\\nIdo Dagan, Oren Glickman, and Bernardo Magnini. The pascal recognising textual\\nentailment challenge. In Proceedings of the First International Conference on Machine\\nLearning Challenges: Evaluating Predictive Uncertainty Visual Object Classi\\ufb01cation, and\\nRecognizing Textual Entailment, MLCW\\u201905, pages 177\\u2013190, Berlin, Heidelberg, 2006.\\nSpringer-Verlag. ISBN 3-540-33427-0, 978-3-540-33427-9. doi: 10.1007/11736790 9. URL\\nhttp://dx.doi.org/10.1007/11736790_9.\\n\\nMichael Fleischman and Deb Roy. Proceedings of the Ninth Conference on Computational\\nNatural Language Learning (CoNLL-2005), chapter Intentional Context in Situated\\nNatural Language Learning, pages 104\\u2013111. Association for Computational Linguistics,\\n2005. URL http://aclweb.org/anthology/W05-0614.\\n\\nJakob N. Foerster, Yannis M. Assael, Nando de Freitas, and Shimon Whiteson. Learn-\\ning to Communicate to Solve Riddles with Deep Distributed Recurrent Q-Networks.\\narXiv:1602.02672 [cs], February 2016. URL http://arxiv.org/abs/1602.02672.\\narXiv: 1602.02672.\\n\\nSimon Kirby, Tom Gri\\ufb03ths, and Kenny Smith. Iterated learning and the evolution of\\nlanguage. Current Opinion in Neurobiology, 28:108 \\u2013 114, 2014. ISSN 0959-4388. doi:\\nhttp://dx.doi.org/10.1016/j.conb.2014.07.014. URL http://www.sciencedirect.com/\\nscience/article/pii/S0959438814001421. SI: Communication and language.\\n\\nBrenden M. Lake, Tomer D. Ullman, Joshua B. Tenenbaum, and Samuel J. Gershman.\\nBuilding machines that learn and think like people. CoRR, abs/1604.00289, 2016. URL\\nhttp://arxiv.org/abs/1604.00289.\\n\\nAngeliki Lazaridou, Nghia The Pham, and Marco Baroni. Towards Multi-Agent\\nCommunication-Based Language Learning. arXiv:1605.07133 [cs], May 2016. URL\\nhttp://arxiv.org/abs/1605.07133. arXiv: 1605.07133.\\n\\nTomas Mikolov, Armand Joulin, and Marco Baroni. A Roadmap towards Machine\\nIntelligence. arXiv:1511.08130 [cs], November 2015. URL http://arxiv.org/abs/\\n1511.08130. arXiv: 1511.08130.\\n\\nKenneth Smith, Simon Kirby, and Henry Brighton. Iterated learning: A framework for\\nthe emergence of language. Arti\\ufb01cial Life, 9(4):371\\u2013386, 10 2003. ISSN 1064-5462. doi:\\n10.1162/106454603322694825. doi: 10.1162/106454603322694825.\\n\\n\\x0cLuc Steels. Grounding language through evolutionary language games.\\n\\nIn Language\\nGrounding in Robots, pages 1\\u201322. Springer, 2012. URL http://link.springer.com/\\nchapter/10.1007/978-1-4614-3064-3_1.\\n\\nSainbayar Sukhbaatar, Arthur Szlam, and Rob Fergus. Learning Multiagent Communication\\nwith Backpropagation. arXiv:1605.07736 [cs], May 2016. URL http://arxiv.org/\\nabs/1605.07736. arXiv: 1605.07736.\\n\\nAdam Vogel, Andr\\xb4es G\\xb4omez Emilsson, Michael C. Frank, Dan Jurafsky, and Christopher\\nPotts. Learning to reason pragmatically with cognitive limitations. In Proceedings of the\\n36th Annual Meeting of the Cognitive Science Society, pages 3055\\u20133060, Wheat Ridge,\\nCO, July 2014. Cognitive Science Society.\\n\\nSida I. Wang, Percy Liang, and Christopher D. Manning. Learning language games\\nthrough interaction. In Proceedings of the 54th Annual Meeting of the Association for\\nComputational Linguistics (Volume 1: Long Papers), pages 2368\\u20132378, Berlin, Germany,\\nAugust 2016. Association for Computational Linguistics. URL http://www.aclweb.\\norg/anthology/P16-1224.\\n\\nJason Weston. Dialog-based language learning. arXiv preprint arXiv:1604.06045, 2016.\\n\\n\\x0c', u' Studies in Second Language Acquisition ,  2010 ,  32 , 303\\u2013 334 . \\ndoi:10.1017/S0272263109990532\\n\\n                             LEARNERS\\u2019 PROCESSING, UPTAKE, \\nAND RETENTION OF CORRECTIVE \\n\\nFEEDBACK ON WRITING \\n\\n Case Studies \\n\\n       Neomy     Storch          and     Gillian     Wigglesworth       \\n   University of Melbourne  \\n\\n         The  literature  on  corrective  feedback  (CF)  that  second  language \\nwriters receive in response to their grammatical and lexical errors is \\nplagued by controversies and con\\ufb02 icting \\ufb01 ndings about the merits of \\nfeedback. Although more recent studies suggest that CF is valuable \\n(e.g., Bitchener,  2008 ; Sheen,  2007 ), it is still not clear whether direct \\nor indirect feedback is the most effective, or why. This study explored \\nthe ef\\ufb01 cacy of two different forms of CF. The investigation focused on \\nthe nature of the learners\\u2019 engagement with the feedback received to \\ngain a better understanding of why some feedback is taken up and \\nretained  and  some  is  not.  The  study  was  composed  of  three  ses-\\nsions. In session 1, learners worked in pairs to compose a text based \\non a graphic prompt. Feedback was provided either in the form of \\nreformulations  (direct  feedback)  or  editing  symbols  (indirect  feed-\\nback). In session 2 (day 5), the learners reviewed the feedback they \\nreceived and rewrote their text. All pair talk was audio-recorded. In \\nsession 3 (day 28), each of the learners composed a text individually \\nusing the same prompt as in session 1. The texts produced by the \\npairs  after  feedback  were  analyzed  for  evidence  of  uptake  of  the \\nfeedback given and texts produced individually in session 3 for evi-\\ndence of retention. The learners\\u2019 transcribed pair talk proved a very \\nrich source of data that showed not only how learners processed the \\n\\n  This project was supported by an Australian Research Council Grant DPO450422.  \\n\\n   Address correspondence to: Neomy Storch, School of Languages & Linguistics, The \\n\\nUniversity of Melbourne, Victoria, 3010, Australia; e-mail:  neomys@unimelb.edu.au . \\n\\n\\xa9 Cambridge University Press, 2010 0272-2631/10 $15.00\\n\\n303\\n\\n\\x0c304\\n\\nNeomy Storch and Gillian Wigglesworth\\n\\nfeedback received but also their attitudes toward the feedback and \\ntheir beliefs about language conventions and use. Closer analysis of \\nfour case study pairs suggests that uptake and retention may be af-\\nfected by a host of linguistic and affective factors, including the type \\nof  errors  the  learners  make  in  their  writing  and,  more  importantly, \\nlearners\\u2019  attitudes,  beliefs,  and  goals.  The  \\ufb01 ndings  suggest  that, \\nalthough often ignored in research on CF, these affective factors play \\nan important role in uptake and retention of feedback.      \\n\\n In second language (L2) writing classes, teachers generally give correc-\\ntive feedback (CF) on their learners\\u2019 writing, particularly on errors in \\ngrammar and lexis. The underlying assumption for giving feedback is \\nthat  it  will  help  learners  to  notice  their  errors  and,  subsequently,  to \\nproduce  the  correct  forms.  However,  although  some  recent  studies \\n(e.g., Bitchener,  2008 ; Bitchener & Knoch,  2008 ; Sheen,  2007 ) show that \\ntargeted CF can be effective, extensive reviews of the available empir-\\nical research (e.g., F. Hyland & K. Hyland, 2006; K. Hyland & F. Hyland, \\n2006; Goldstein,  2004 ,  2005 ) conclude that the \\ufb01 ndings about the ef\\ufb01 -\\ncacy  of  CF  are  mixed  and  thus  inconclusive  (see  also  the  debate  be-\\ntween Ferris,  1999 , and Truscott,  2007 ). A number of factors may explain \\nthe lack of de\\ufb01 nitive \\ufb01 ndings about the ef\\ufb01 cacy of CF, including the re-\\nsearch methods employed in studies on CF, and a host of contextual \\nand affective factors that relate to both teachers and learners (see Gold-\\nstein,  2005 ). \\n\\n A review of the literature on CF exempli\\ufb01 es these mixed \\ufb01 ndings and \\nbrie\\ufb02 y discusses the different methodological approaches that have \\nbeen  used  to  assess  the  impact  of  CF,  before  focusing  on  a  small \\nnumber of studies that have investigated an aspect of CF that has not \\nbeen  extensively  explored:  how  learners  process  the  feedback  they \\nreceive. Adopting a sociocultural theoretical perspective, it is argued \\nthat research that analyzes actual instances of learners engaging with \\nfeedback and revising their texts as well as research that looks more \\nclosely  at  how  learners\\u2019  beliefs  and  goals  impact  their  decisions  is \\nneeded  to  understand  how  and  why  learners  respond  to  different \\nforms of CF. \\n\\n Corrective feedback can be distinguished in terms of its directness \\n(for  a  comprehensive  description,  see  Gu\\xe9nette,   2007 ),  which  ranges \\nfrom direct (e.g., writing the correct form above the incorrect form) to \\nindirect  (e.g.,  using  editing  symbols  to  signal  an  error).  Research  on \\nwhich form of feedback is most effective has produced mixed results. \\nFor example, Lalande ( 1982 ) found that students who received indirect \\nfeedback  (editing  codes)  showed  greater  accuracy  on  subsequent \\nwriting than students who received direct feedback\\u2014but the differences \\n\\n\\x0cProcessing, Uptake, and Retention of CF\\n\\n305\\n\\nwere not statistically signi\\ufb01 cant. Chandler ( 2003 ), in contrast, found di-\\nrect feedback to be more effective than three different types of indirect \\nfeedback (with and without codes that explain the type of error). How-\\never, these effects were found only on immediate revisions; texts written \\nlater showed no statistically signi\\ufb01 cant differences in grammatical accu-\\nracy in relation to type of feedback. \\n\\n One factor that may explain these mixed results is whether the im-\\npact of CF on a revised text or on a subsequently written new text is \\nconsidered;  another  is  the  measure  used  to  assess  the  impact  of  the \\nfeedback.  For  example,  Chandler  ( 2003 )  used  mean  accuracy  scores \\n(mean number of errors per 100 words produced) on both revised and \\nnew texts but did not take into account the different types of errors or \\ntheir relative frequency. Bitchener ( 2008 ) used accuracy scores of new \\ntexts but only measured the accurate use of structures targeted by the \\nfeedback. Sachs and Polio ( 2007 ) considered revised texts and used a \\nscale of revisions of T-units, de\\ufb01 ned as an independent clause and any \\nsubordinate clause attached to it or embedded in it, noting what pro-\\nportion of these units were amended in the direction of the CF and dis-\\ntinguishing among partial, full, no revision, or not applicable. The use of \\ndifferent measures makes comparison across studies dif\\ufb01 cult. However, \\nit is not necessarily the case that using uniform measures will produce \\nconclusive research \\ufb01 ndings. \\n\\n The bulk of research on feedback has investigated the type of revi-\\nsions that students make (or do not make) in response to different types \\nof feedback (see Goldstein,  2004 , for a review) rather than how learners \\nactually engage with and process the feedback, and why they use (or \\nfail  to  use)  the  feedback  received.  Processing  of  feedback  is  perhaps \\nless  well  researched  and  understood  because  it  is  dif\\ufb01 cult  to  access \\nsuch learner-internal cognitive processes. Studies that have collected \\nfeedback  processing  data  through  think-aloud  protocols  (e.g.,  Qi  & \\nLapkin,   2001 ),  retrospective  interviews  (e.g.,  Hyland,   1998 ),  or  pair \\ndiscussions of the feedback received on a jointly produced text (e.g., \\nSwain & Lapkin,  2003 ) have suggested that two additional factors may \\naffect the impact of feedback\\u2014namely, depth of processing and learners\\u2019 \\nattitudes toward the feedback provided. \\n\\n In a small study ( n  = 2), Qi and Lapkin ( 2001 ) used think-aloud proto-\\ncols, during which learners were asked to verbalize their processing of \\nthe feedback received (reformulation), to investigate the effect of depth \\nof processing on uptake of feedback. The researchers analyzed the pro-\\ntocols  and  distinguished  two  types  of  noticing:  substantive  and  per-\\nfunctory. The main difference between these two types of noticing was \\nthat substantive noticing episodes were those in which the learners ar-\\nticulated reasons for the feedback received. This articulation was taken \\nas evidence that the learners understood why the CF was provided. Qi \\nand Lapkin found that substantive noticing led to greater improvements \\n\\n\\x0c306\\n\\nNeomy Storch and Gillian Wigglesworth\\n\\nin  the  revised  text  than  perfunctory  noticing.  Similarly,  Sachs  and \\nPolio\\u2019s ( 2007 ) larger study ( n  = 54), which also used think-aloud proto-\\ncols, found that when learners noticed and understood why a linguistic \\nitem was reformulated (as evident in the think-aloud protocols), they \\nwere more likely to revise the item on subsequent drafts. However, the \\nresearchers point out the highly inferential nature of coding for depth of \\nengagement. \\n\\n Other studies (e.g., Hyland,  1998 ,  2003 ; Swain,  2006 ; Swain & Lapkin, \\n 2003 ) have shown that learners\\u2019 goals, attitudes, and beliefs may also \\naffect the uptake of feedback. Hyland ( 1998 ,  2003 ) used retrospective \\ninterviews and case studies to investigate students\\u2019 use and reactions \\nto the feedback received. Hyland found that whether learners respond \\nto the feedback and the strategies they adopt may depend on the im-\\nportance  they  attribute  to  the  grammatical  accuracy  of  their  writing. \\nSwain and Swain and Lapkin showed, using pair work, that learners may \\nreject teacher feedback because it is perceived as violating their own \\nbeliefs  about  language  conventions  or  as  altering  their  intended \\nmeaning. \\n\\n The importance attributed to learners\\u2019 beliefs in explaining how and \\nwhy learners process feedback is in line with sociocultural theoretical \\nperspectives on learning. Sociocultural theorists view learners (partic-\\nularly  adult  learners)  as  intentional  agents  in  their  language  learning \\nactivity  who  assign  relevance  and  signi\\ufb01 cance  to  certain  events  and \\nwhose behavior is guided by their own goals (Lantolf & Pavlenko,  2001 ; \\nLantolf & Thorne,  2006 ). These beliefs and goals may affect what learners \\nnotice, whether they accept or reject the feedback provided, and how \\nmuch of the feedback they retain.   \\n\\n THE CURRENT STUDY  \\n\\n Aims \\n\\n The few studies that have investigated the nature of learners\\u2019 engage-\\nment with the feedback provided on their writing have considered only \\ndirect forms of feedback (e.g., Qi & Lapkin,  2001 ; Sachs & Polio,  2007 ; \\nSwain  &  Lapkin,   2003 ).  The  current  study  compared  the  processing, \\nuptake, and retention data of learners who received direct and indirect \\nforms  of  feedback.  Furthermore,  taking  a  sociocultural  approach  to \\ndata analysis (microgenesis), the present study investigated activity as \\nit  occurred  and  attempted  to  link  outcomes  (uptake  and  retention) \\nwith  processes  (nature  of  engagement)  and  the  learners\\u2019  underlying \\nbeliefs and goals. This study set out to explore (a) whether learners \\nprocess  direct  feedback  differently  from  indirect  feedback,  (b)  what \\nfactors affect learners\\u2019 uptake of feedback when revising their text, and \\n\\n\\x0cProcessing, Uptake, and Retention of CF\\n\\n307\\n\\n(c) what factors affect learners\\u2019 retention of feedback when writing a \\nnew text.   \\n\\n Design \\n\\n A  case  study  approach  was  adopted,  with  the  cases  selected  from  a \\nlarger research project that investigated the ef\\ufb01 cacy of different forms \\nof written CF using an experimental design. This larger project involved \\ntwo groups of English-as-a-second-language (ESL) learners, each group \\ncomposed of 12 pairs. One group received feedback in the form of refor-\\nmulation and the other received feedback in the form of editing sym-\\nbols.  Assignment  of  pairs  to  the  different  feedback  condition  groups \\nwas done randomly.   \\n\\n Participants \\n\\n All participants in the project were volunteers recruited from advertise-\\nments displayed on notice boards in a large Australian research univer-\\nsity.  The  advertisement  stated  that  the  aim  of  the  project  was  to \\ninvestigate the ef\\ufb01 cacy of different types of feedback on writing. Partic-\\nipants were invited to attend the project sessions in self-selected pairs. \\nAt  the  end  of  the  study,  each  participant  received  $100  for  their \\nparticipation. \\n\\n The participants (30 females, 18 males) were predominantly graduate \\nstudents ( n  = 40) pursuing a master\\u2019s degree. Their average age was 25. \\nOver half of the participants ( n  = 28) were enrolled in a commerce de-\\ngree  program  (e.g.,  master  of  applied  commerce,  master  of  applied \\n\\ufb01 nance). The majority came from Asia, from countries such as China \\n( n  = 24), Indonesia ( n  = 11), and Thailand ( n  = 4), which re\\ufb02 ects the typical \\nnative language background of international students at Australian uni-\\nversities.  Participants  were  of  advanced  L2  pro\\ufb01 ciency  and  provided \\ndocumentation that they had met the English language requirement for \\nuniversity entrance: an International English Language Testing System \\n(IELTS) score of 6.5, with 6.0 in writing, or a TOEFL Internet-Based Test \\nscore above 240, with a Test of Written English score of 4.5 or above. \\nThe majority had learned English as a foreign language in their home \\ncountries, on average for 8 years at secondary school and a university; \\nmost participants ( n  = 45) had been in Australia for less than 1 year at \\nthe time of data collection. Participant pairs generally came from the \\nsame course of study, where they had met each other. The period of \\nacquaintance with each other ranged from 1 to 8 months.   \\n\\n\\x0c308\\n\\nNeomy Storch and Gillian Wigglesworth\\n\\n Implementation \\n\\n Participants  attended  three  separate  sessions.  In  session  1,  all  pairs \\ncomposed a data commentary text based on a graphic prompt (see Ap-\\npendix A). They were given 30 min to complete the task and their pair \\ntalk was audio-recorded. In session 2 (day 5), the pairs were provided \\nwith feedback on their writing, either in the form of reformulations or \\nediting symbols. The pairs were given 15 min to discuss the feedback \\n(processing session), which was then removed by the researcher. The \\npairs were given the unmarked and original version of their text (written \\nin session 1) and had 30 min to rewrite it (rewriting session). Pair talk in \\nboth the feedback processing and rewriting sessions was recorded to \\nprovide data of how learners process feedback. In session 3 (day 28), \\nthe learners individually composed a data commentary text using the \\nsame prompt as in session 1. \\n\\n Direct feedback was provided in the form of reformulation and involved \\nrewriting the learners\\u2019 text, attending to grammatical and lexical errors but \\npreserving the original meaning as much as possible (Thornbury,  1997 ). \\nEditing, the indirect form of feedback, involved marking the text with codes \\nthat corresponded to certain types of errors and that were explained and \\nexempli\\ufb01 ed in a handout (see Appendix B). Both forms of feedback were \\nprovided by the same native speaker who was an experienced ESL teacher. \\nFeedback focused on errors in grammar (morphology and syntax), lexis \\n(word choice), and mechanics (spelling and punctuation). \\n\\n The following examples illustrate the two forms of feedback. In (1), \\nthe  reformulated  version  contains  \\ufb01 ve  reformulations:  insertion  of  a \\nphrase ( the graph ), a change in phrase order ( for all cities ), correction of \\nthe verb form ( occur ), adjunction of the second sentence to the \\ufb01 rst, \\nand deletion of a verb ( being ). The example in (2) contains two editing \\ncodes:  parentheses  around  the  entire  sentence,  which  signals  that  a \\nword  or  words  need  to  be  deleted,  and  the  code  (C)  placed  above  a \\nword, which indicates an error in word choice.\\n   \\n  \\n   (1)    \\n  \\n  \\n\\n    Reformulations \\n   a. Original \\n    Analysing seasonally, the highest rainfalls constantly occurs in summer for all \\ncities. And the lowest being in winter.  \\n   b. Reformulated version \\n    Analysing the graph seasonally for all cities, the highest rainfalls constantly \\noccur in summer and the lowest in winter.   \\n\\n  \\n  \\n\\n  (2)    \\n  \\n  \\n\\n    Editing \\n   a. Original \\n    Beijing also had the average rainfall in summer, which was 150 mm. How-\\never, in spring, autumn, and winter, they were roughly 20 mm.  \\n\\n\\x0cProcessing, Uptake, and Retention of CF\\n\\n309\\n\\n  \\n  \\n\\n  \\n  \\n\\n   b. Edited version \\n    (Beijing also had the average rainfall in summer, which was 150 mm).   \\n\\n    However, in   spring, autumn, and winter ,   they    were roughly 20 mm.       \\n\\n   C \\n\\n  \\n\\n    Data \\n\\n Three sources of data were used in this study: the feedback received \\n(reformulations and editing codes on the day 1 texts); the texts written \\nby the pairs on day 1, revised on day 5, and written individually on day \\n28; and the transcribed pair talk elicited on day 5 during the feedback \\nprocessing and the rewriting sessions.   \\n\\n Data Analysis \\n\\n For the \\ufb01 rst source of data, all editing codes and reformulations were \\ncounted. When counting the number of reformulations, each word or \\nphrase that was reformulated, deleted, or reordered was counted as a \\nsingle reformulation. Following researchers who work within a sociocul-\\ntural theoretical framework (Aljaafreh & Lantolf,  1994 ; Nassaji & Swain, \\n 2000 ), the data were analyzed microgenetically for evidence of uptake \\nand retention across the three sessions. To trace for evidence of uptake \\nof feedback, texts written on day 1 were compared with those rewritten \\non day 5 along with the feedback comments. Revisions were analyzed \\nfor whether they were correct or incorrect, whether they were made in \\nresponse to the feedback given or unsolicited, and whether the changes \\nwere made at the word, clause or phrase, or sentence level. Revisions \\nmade in response to feedback were counted to provide a measure of \\nuptake. The texts written on days 1, 5, and 28 were then compared to \\ntrace for retention of the feedback. \\n\\n The example in (3) illustrates the procedure used to analyze evidence \\nof uptake and retention of the feedback received. The data come from a \\npair (Bing and Lina) who received editing feedback and include relevant \\nexcerpts from the original and revised texts composed jointly as well as \\nrelevant excerpts from the students\\u2019 individually produced texts on day \\n28. In the investigation of evidence of uptake, revisions that were con-\\nsistent with either the reformulation or the intent of the editing symbol \\nwere coded as correct and indicated by a double check ( \\u02c9  \\u02c9 ). The sym-\\nbol ( \\u02c9 \\u02cd) indicated an incorrect change and (\\u02cd) no change. In (3), two \\nediting symbols were provided in the feedback (to indicate an error in \\n\\n\\x0c310\\n\\nNeomy Storch and Gillian Wigglesworth\\n\\nword choice and in spelling). The revised text contains two changes, \\nboth clearly in response to the feedback provided. The changes, deemed \\ncorrect in this instance, were considered as evidence of uptake. How-\\never, it soon became apparent that employing the same approach to \\nanalyze the new texts produced individually in session 3 (day 28) was \\nnot always possible because these new texts were often very different \\nfrom those produced and revised in sessions 1 and 2. As can be seen in \\n(3), the text produced by Lina on day 28 is very different from that pro-\\nduced collaboratively on day 1 and revised on day 5. There is no rele-\\nvant sentence to show evidence that the correction to the word  countries  \\nwas retained. Thus, the long-term impact of feedback (i.e., retention) \\nwas not quanti\\ufb01 ed. Instead, a process-product analysis (Nassaji & Swain, \\n 2000 ) was used, which allowed for a closer focus on the data of four \\ncase study pairs.\\n  \\n   \\n   (3)    \\n  \\n  \\n\\n    Evidence of uptake and retention of feedback \\n   a. Original version \\n    The graph shows the average rainfall (by season) for four countries namely \\nBucharest, Lagos, Beijing and Mexico city in the year 2000.  \\n   b. Feedback (editing) \\n\\n    The graph shows the average rainfall C (by season) for four    countries    namely  \\n\\n   C \\n\\n    Bucharest, Lagos, Beijing and Mexico    c   ity in the year 2000.  \\n   c. Revised version \\n\\n   X \\n\\n    \\u02c9  \\u02c9  \\n\\n    The  graph  shows  the  average  rainfall  (by  season)  for  four  cities  namely \\nBucharest,  \\n\\n    \\u02c9  \\u02c9  \\n\\n    Lagos, Beijing and Mexico City in the year 2000.  \\n   d. Bing\\u2019s new text \\n    The graph shows the average seasonal rainfall in the year 2000 at four different  \\n    \\u02c9  \\u02c9    \\n \\u02c9  \\u02c9  \\n    cities which are Bucharest, Lagos, Beijing and Mexico City.  \\n   e. Lina\\u2019s new text \\n    This report will explain the graph about the seasonal rainfall in Bucharest, \\nLagos,  \\n\\n   \\u02cd \\n\\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n\\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n\\n  \\n  \\n   \\n\\n    Beijing and Mexico city in 2000.    \\n\\n  \\n  The  transcribed  pair  talk  from  the  processing  and  the  rewriting \\nsessions were analyzed for language-related episodes (LREs; Swain & \\nLapkin,   1998 ).  LREs  were  de\\ufb01 ned  as  segments  in  the  pair  talk  during \\nwhich learners focused explicitly on language items. This included in-\\nstances in which learners read the reformulated text aloud, deliberated \\nover  the  reformulation,  discussed  how  to  revise  in  response  to  an \\nediting symbol, or deliberated over language items that, although not \\n\\n\\x0cProcessing, Uptake, and Retention of CF\\n\\n311\\n\\ntargeted by the feedback, the learners felt needed amendment. Thus, all \\nLREs were \\ufb01 rst identi\\ufb01 ed, regardless of whether they dealt with language \\nitems targeted by the feedback. LREs varied in length: Some were com-\\nposed of a short turn (e.g., a learner simply read aloud the reformula-\\ntion received) and others of multiple turns (e.g., learners deliberated \\nabout word choice). All LREs were then further analyzed for focus to \\ndistinguish between episodes that dealt with form (i.e., morphosyntax; \\nF-LRE),  lexis  (L-LRE),  and  mechanics  (i.e.,  spelling  and  punctuation; \\nM-LRE). It was also noted whether the LREs were resolved correctly ( \\u221a ), \\nincorrectly (X), or left unresolved (?). Correct resolution in this study \\nreferred to resolutions that accorded with the feedback or to alterna-\\ntives deemed as equally acceptable by the researchers. To link uptake \\nand retention to how the feedback was processed, a process-product \\nanalysis  (Nassaji  &  Swain,   2000 )  was  conducted.  The  LREs  that  dealt \\nwith  language  items  that  received  feedback  were  identi\\ufb01 ed  and  ana-\\nlyzed for the nature of engagement. \\n\\n Based on the work of several researchers (Qi & Lapkin,  2001 ; Storch, \\n 2008 ;  Tocalli-Beller  &  Swain,   2005 ),  a  distinction  was  made  between \\nLREs that showed extensive engagement and those that showed limited \\nor no engagement. LREs that showed evidence of extensive engagement \\nincluded episodes in which learners offered suggestions and counter-\\nsuggestions, explanations, or any comments that showed evidence of \\nmeta-awareness  of  the  feedback  received  (e.g.,   We  don\\u2019t  have  to  use  \\nbeing). LREs that showed evidence of limited engagement included epi-\\nsodes in which one member of the pair simply read the feedback and \\nthe other merely acknowledged or repeated it. \\n\\n To code for LREs, one of the researchers, who has extensive expe-\\nrience in coding pair talk data for LREs, developed coding guidelines \\nbased on the data (see Appendix C). Using these guidelines, a second \\nrater  was  trained  and  coded  four  transcripts  independently.  Reli-\\nability  scores  were  calculated  using  simple  percentage  agreement. \\nInterrater  reliability  scores  for  identifying  LREs  was  91%;  disagree-\\nments  seemed  to  be  due  mainly  to  oversights.  Interrater  reliability \\nwas the lowest (84%) when coding for the nature of engagement. Dis-\\ncussion  between  the  two  raters  led  to  some  modi\\ufb01 cations  of  the \\ncoding guidelines. \\n\\n  Figure 1  contains excerpts from the text (original and reformulated) \\nand pair talk data to illustrate the coding procedure for LREs. The data \\ncome from a pair (Gus and Jon) who received reformulations. The refor-\\nmulated  version  contains  \\ufb01 ve  reformulations  ( went ,   from ,   start ,   from , \\nand  to ). The pair talk excerpt was coded as containing \\ufb01 ve LREs related \\nto the feedback received: two F-LREs related to verb tenses and three \\nL-LREs that dealt with the choice of prepositions. All were resolved cor-\\nrectly. The \\ufb01 ve LREs were coded as showing limited engagement; during \\nthe processing session, Gus read and acknowledged the reformulations, \\n\\n\\x0c312\\n\\nNeomy Storch and Gillian Wigglesworth\\n\\nwith  no  involvement  from  Jon.  All  \\ufb01 ve  LREs  were  contained  within  a \\nsingle turn.     \\n\\n  Figure  2   provides  an  additional  example  that  illustrates  coding  for \\nLREs from Diana and Monica, who received editing feedback. The edit-\\ning symbol (underlining of the phrase) signals that there is something \\nwrong with the expression. The pair talk contained a segment coded as \\na  L-LRE  and  directly  related  to  the  feedback  provided.  The  LRE  was \\ncoded as correctly resolved because the alternative the learners settled \\non  ( winter  rainfall )  was  considered  acceptable  in  this  instance.  How-\\never, unlike the LREs given in  Figure 1 , this LRE shows evidence of ex-\\ntensive  engagement.  The  editing  symbol  initially  challenges  Diana\\u2019s \\npreviously held belief that this phrase is acceptable ( Why I think  rainfall \\nof winter  is correct ?), but she then realizes that it should be reduced to \\na  nominal  phrase,  and  Monica  agrees.  Diana  notes  that  this  is  some-\\nthing she has not previously learned ( This is the \\ufb01 rst time I know this ). \\nMonica  then  suggests  that  the  original  expression  is  not  necessarily \\nwrong but that the alternative is a simpler or more direct expression.     \\n\\n Finally, in analyzing the pair talk, any other salient features were also \\nnoted,  particularly  comments  that  re\\ufb02 ected  the  learners\\u2019  attitudes \\n\\nOriginal\\n\\nThe rainfall goes up during spring to summer and start to \\n\\ndecrease during autumn and winter.  \\n\\nReformulated \\n\\nThe rainfall went up from spring to summer and started to \\n\\ndecrease from autumn to winter.\\n\\nAnalysis of relevant pair talk data \\n\\nPair talk \\n\\nFocus\\n\\nFeedback \\n\\nResolution  Engagement \\n\\nGus: the rainfall went\\n\\nF-LRE (verb: went)\\n\\nup from spring to \\n\\nL-LRE (prep: from)\\n\\nsummer went okay \\n\\nand started to \\n\\nF-LRE (verb: started)\\n\\ndecrease from \\n\\nL-LRE (prep: from)\\n\\nautumn to winter \\n\\nL-LRE (prep: to)\\n\\nRelated \\n\\nRelated \\n\\nRelated \\n\\nRelated \\n\\nRelated \\n\\nokay.\\n\\nLimited\\n\\nLimited\\n\\nLimited\\n\\nLimited\\n\\nLimited\\n\\n \\n\\nNote:  The abbreviation \\u201cprep\\u201d corresponds to preposition.\\n\\n \\n\\n Figure 1.    \\n\\n    Analyzing and coding pair talk for LREs.    \\n\\n\\x0cProcessing, Uptake, and Retention of CF\\n\\n313\\n\\nOriginal\\n\\nHowever, the rainfall of winter in Lagos is the\\n\\nsecond highest among the four cities\\n\\nEdited versions\\n\\nHowever, the rainfall of winter in Lagos is the \\n\\nsecond highest among the four cities\\n\\nAnalysis of relevant pair talk data\\n\\nPair talk\\n\\nFocus\\n\\nFeedback\\n\\nResolution\\n\\nEngagement\\n\\nDiana: the rainfall of winter oh \\n\\nL-LRE\\n\\nRelated\\n\\nExtensive\\n\\nwinter rainfall right? winter rainfall. \\n\\nIt\\u2019s the rain of winter. Why I think \\n\\nrainfall of winter is correct? winter \\n\\nrainfall\\u2026 Ah huh, should be winter \\n\\nrainfall\\n\\nMonica: yeah, I think so\\n\\nDiana: oh, this is the first time I \\n\\nknow this\\n\\nMonica: not\\u2026not\\u2026no it\\u2019s more \\n\\nuh\\u2026most simple\\n\\nDiana: winter rainfall okay\\n\\n \\n\\n \\n\\n Figure 2.    \\n\\n    Analysis of pair talk for LREs.    \\n\\ntoward the feedback provided and their beliefs about conventions of \\nlanguage use. Thus, in the example in  Figure 2 , it was noted that the \\nfeedback posed a challenge to Diana\\u2019s previously held knowledge but \\nalso provided a learning occasion and that Monica seemed to think that \\nthe revised phrase was not more grammatical but more direct.    \\n\\n FINDINGS \\n\\n The  quantitative  \\ufb01 ndings  from  the  larger  research  project  (Storch  & \\nWigglesworth, 2006) will be presented \\ufb01 rst: a comparison of the amount \\nof feedback provided via editing and reformulations, the amount of up-\\ntake, and the number and nature of LREs. Case studies will then be used \\n\\n\\x0c314\\n\\nNeomy Storch and Gillian Wigglesworth\\n\\nto report on the qualitative analysis, which attempted to link evidence \\nof uptake (and retention) with the nature of the learners\\u2019 engagement \\nwith the feedback as well as their beliefs and goals. \\n\\n  Table 1  presents the amount of feedback provided via reformulations \\nand editing as well as the uptake of this feedback as evident in the re-\\nvised texts. The number of reformulations was almost double that of \\nthe editing symbols, but there was considerable variation in the number \\nof feedback comments received by each pair, as evident by the large \\nrange.  Table 1  also shows that there was more uptake following editing \\nthan following reformulations. However, it should be noted that analysis \\n(and quanti\\ufb01 cation) of uptake in revised texts following reformulations \\nproved quite dif\\ufb01 cult. Revisions in response to reformulations were of-\\nten made at the phrase and sentence level; thus, the revised texts con-\\ntained  new  and  completely  rewritten  sentences.  In  contrast,  texts \\nrevised  following  editing  feedback  contained  changes  mainly  at  the \\nword level, which were more easily traceable to the editing feedback \\nreceived.     \\n\\n  Table 2  summarizes the LREs for the entire dataset.  Table 2  shows \\nthat despite the larger number of reformulations in relation to the edit-\\ning symbols (as shown in  Table 1 ), reformulations elicited fewer LREs \\nthan editing feedback during the processing session (day 5, session 1). \\nIn the rewriting sessions, the number of LREs was similar, regardless of \\nfeedback type. Most of the LREs in both sessions and in response to \\nboth types of feedback focused on grammar and lexis rather than on \\nmechanics, and most were correctly resolved.     \\n\\n In both the processing and rewriting sessions, the percentage of LREs \\ndirectly related to the feedback was lower in the data of learners who \\nreceived  reformulations  compared  with  those  who  received  editing \\nfeedback. It should be noted that in the rewriting session, many of the \\nLREs directly related to the feedback dealt with the same language items \\nas  those  discussed  during  the  processing  session.  There  was  more \\n\\n Table 1.    \\ndataset          \\n\\n   Feedback \\n\\n Amount of feedback \\n    M  \\n   Range \\n Uptake \\n    M  \\n   Range \\n % of feedback \\n\\n    Feedback and uptake for the entire \\n\\n Reformulations \\n\\n Editing     \\n\\n 327 \\n 27.25 \\n 8\\u201353 \\n 182 \\n 15.17 \\n 3\\u201334 \\n 55.66% \\n\\n 171   \\n 14.25   \\n 2\\u201326   \\n 150   \\n 12.50   \\n 0\\u201326   \\n 87.72%   \\n\\n\\x0cProcessing, Uptake, and Retention of CF\\n\\n315\\n\\n Table 2.    \\n\\n    LREs during processing and rewriting sessions              \\n\\n   Summary of LREs \\n\\n Processing   Rewriting \\n\\n Processing   Rewriting     \\n\\n Reformulations \\n\\n Editing   \\n\\n N  \\n M  \\n\\n Occurrences   \\n  \\n  \\n   Range \\n Focus   \\n   F-LREs \\n   L-LREs \\n   M-LREs \\n Resolution   \\n   Correct \\n  \\n   Unresolved \\n   % correct of all LREs \\n Related to feedback   \\n  \\n   % of all LRES \\n Showing extensive engagement   \\n  \\n   % of LREs related to feedback \\n\\nIncorrect \\n\\n N  \\n\\n N  \\n\\n 128 \\n 10.67 \\n 1\\u201321 \\n\\n 207 \\n 18.82 \\n 1\\u201343 \\n\\n 189 \\n 15.75 \\n 4\\u201325 \\n\\n 227   \\n 20.64   \\n 7\\u201336   \\n\\n 58 (45%) \\n 65 (51%) \\n 5 (4%) \\n\\n 80 (39%) \\n 101 (49%) \\n 26 (13%) \\n\\n 79 (42%) \\n 82 (43%) \\n 28 (15%) \\n\\n 74 (33%)   \\n 123 (54%)   \\n 29 (13%)   \\n\\n 100 \\n 18 \\n 10 \\n 78% \\n\\n 73 \\n 22% \\n\\n 44 \\n 60% \\n\\n 161 \\n 36 \\n 10 \\n 78% \\n\\n 44 \\n 21% \\n\\n 20 \\n 45% \\n\\n 144 \\n 25 \\n 20 \\n 76% \\n\\n 125 \\n 66% \\n\\n 95 \\n 76% \\n\\n 187   \\n 33   \\n 7   \\n\\n 82%   \\n\\n 73   \\n 32%   \\n\\n 52   \\n 71%   \\n\\nextensive engagement with feedback in response to editing than in re-\\nsponse  to  reformulations,  in  both  the  processing  and  rewriting \\nsessions. \\n\\n The case studies illustrate how a number of factors, both linguistic \\nand  affective,  impacted  uptake  and  retention  of  feedback.  The  case \\nstudy  participants  were  fairly  representative  of  the  entire  participant \\ncohort in terms of language background, L2 pro\\ufb01 ciency, degree program, \\nand length of acquaintance. Pairs who received a large amount of feed-\\nback but who showed contrastive patterns of engagement with the feed-\\nback were selected. For two of the pairs, the feedback elicited a large \\nnumber of LREs, whereas for the two other pairs, it elicited few LREs.   \\n\\n FIRST CASE-STUDY PAIR  \\n\\n Background \\n\\n The \\ufb01 rst pair consisted of one male (Eko) and one female (Sherry) grad-\\nuate student, both Indonesian, with IELTS scores of 7.5 upon entry to \\nthe university. Both had studied English for several years at the high \\n\\n\\x0c316\\n\\nNeomy Storch and Gillian Wigglesworth\\n\\nschool and university level in Indonesia. They had known each other for \\n1 month. Eko was a graduate student in commerce and Sherry was a \\ngraduate student in cultural studies.   \\n\\n Analysis of Written Texts \\n\\n The pair received a large number of reformulations ( n  = 40) on the text \\nthey wrote in the \\ufb01 rst session. However, a closer analysis of their errors \\nand reformulations revealed that almost half of the errors (17/40) were \\nerrors in mechanics (spelling, capitalization). Word choice and expres-\\nsion errors made up the next largest category ( n  = 6). Errors in grammar \\nvaried and included errors in morphology (e.g., tense, agreement) and \\nsyntax (e.g., incomplete sentences, word order). \\n\\n The revised text (day 5) was very similar to the original text, with re-\\nvisions made mainly at the word level. Analysis for evidence of uptake \\nshowed that of the 40 reformulations, 30 were taken up. There were far \\nfewer errors in mechanics in the revised text ( n  = 6) and in the texts \\nproduced individually on day 28 (four errors in Eko\\u2019s text and none in \\nSherry\\u2019s text). There was also a decrease in word choice errors in the \\nrevised text on day 5 (only one error). However, the texts produced in-\\ndividually on day 28 showed a number of errors in word choice or ex-\\npression (three for Eko and \\ufb01 ve for Sherry), and some of these errors \\nwere the same as those found in the text produced jointly on day 1.   \\n\\n Analysis of Pair Talk \\n\\n Eko and Sherry\\u2019s pair talk showed that despite the large number of refor-\\nmulations ( n  = 40), only 14 LREs were generated by this pair in the pro-\\ncessing  session  and  22  in  the  rewriting  session.  Although  most  were \\nrelated to the feedback, these LREs focused mainly on lexical choices. \\nVery few LREs (only two in the processing and one in the rewriting ses-\\nsion)  dealt  with  errors  in  mechanics.  Furthermore,  whereas  L-LREs \\nshowed an extensive level of engagement, M-LREs showed a limited level \\nof engagement. Thus, it seemed that although the pair did not deliberate \\nover errors in mechanics, there was a high level of uptake and retention \\nof the reformulated feedback on these errors. However, a high level of \\nengagement with feedback on errors in lexis that led to uptake did not \\nalways result in retention. The examples given in (4)\\u2013(8), taken from the \\ndata of this pair, suggest possible explanations for these \\ufb01 ndings. \\n\\n The example in (4) contains relevant excerpts that discuss the choice \\nof  where  and  when , an error that occurred on two occasions in the orig-\\ninal text produced by this pair on day 1.\\n   \\n\\n  \\n\\n\\x0cProcessing, Uptake, and Retention of CF\\n\\n317\\n\\n   (4)    \\n  \\n  \\n\\n  \\n  \\n\\n  \\n  \\n\\n  \\n  \\n  \\n  \\n  \\n  \\n\\n  \\n  \\n\\n  \\n  \\n\\n    Excerpts from texts and pair talk dealing with  where  versus  when  \\n   a. Original version \\n    Constantly for every season, Lagos has the highest rainfall except for winter \\nwhere the highest rainfall being in Bucharest.  \\n   b. Reformulated version \\n    For every season, Lagos constantly has the highest rainfall except for in win-\\nter when the highest rainfall is in Bucharest.  \\n   c. Relevant LREs \\n   Eko:   Mmm.  Say  it\\u2019s   when   instead  of   where .  Which  makes  sense  because \\nwe\\u2019re comparing time.  \\n   [\\u2026] \\n   Sherry: except for winter when \\n   Eko: when , yes. Remember that,  when !  \\n   Sherry: when the highest rainfall \\n   d. Revised version \\n    For every season, lagos constantly has the highest rainfall, except for in win-\\nter when the highest rainfall is in Bucharest.  \\n   e. Eko\\u2019s new text \\n    the highest rainfall in all seasons occurs in Lagos, except in winter when the \\nhighest rainfall occurs in Bucharest.  \\n   f. Sherry\\u2019s new text \\n    Lagos constantly has the highest rainfall across all seasons except in winter \\nwhen the highest is Bucharest.    \\n\\n  \\n\\n   \\n  As illustrated in (4), during the feedback processing session on day 5, \\nthe learners gained an understanding of the difference in the use of the \\ntwo adverbs  where  and  when . The texts produced on days 5 and 28 by \\nboth learners showed evidence of uptake and retention of the  where - \\nwhen  distinction. These adverbs were used correctly in the revised text \\nand in the individually produced texts (Eko: all four instances; Sherry: \\nall three instances). \\n\\n The example in (4) and the outcome in terms of uptake and retention \\ncontrasts with how the learners dealt with the use of the word  \\ufb02 uctua-\\ntive , as in example (5). Here, there is limited engagement with the feed-\\nback provided. Sherry merely reads the reformulated version and Eko \\ndoes not respond, which may explain why no uptake or retention took \\nplace and the error persisted.\\n   \\n  \\n   (5)    \\n  \\n  \\n  \\n  \\n  \\n  \\n\\n    Excerpts from texts and pair talk dealing with  \\ufb02 uctuative  \\n   (a) Original version \\n    whereas the more \\ufb02 uctuative level of rainfall  \\n   (b) Reformulation \\n    whereas the more extreme levels of rainfall  \\n   (c) Relevant LRE \\n   Sherry: whereas the most extreme,  oh,  extreme\\u2026 most extreme level of \\nrainfall \\n   Eko: analysing the\\u2026 \\n\\n  \\n\\n\\x0c318\\n\\nNeomy Storch and Gillian Wigglesworth\\n\\n  \\n  \\n  \\n  \\n   \\n\\n   (d) Eko\\u2019s new text \\n    although more \\ufb02 uctuative that Bucharest  \\n   (e) Sherry\\u2019s new text \\n    Lagos, in the other hand, has the most \\ufb02 uctuative rainfall    \\n\\n  \\n  Example  (6)  shows  how  the  pair  dealt  with  a  stylistic  choice  of \\n\\nexpression.  Merely around  was reformulated as  around a mere .\\n   \\n  \\n   (6)    \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n\\n    Excerpts from texts and pair talk dealing with  merely around  \\n   (a) Original version \\n    it \\ufb02 uctuates merely around 50mm  \\n   (b) Reformulation \\n    it \\ufb02 uctuates around a mere 50mm  \\n   (c) Relevant LREs \\n   Sherry:  Don\\u2019t use  merely.  Oh,  around a mere,  oh,  around a mere \\n   Eko:  A more sophisticated way of getting it.  \\n   [\\u2026] \\n   Sherry:  where it \\ufb02 uctuates\\u2026 around\\u2026 around  \\n   Eko: a mere \\n   Sherry : Is that the how to put it?  around a mere \\ufb01 fty millimetres ?  \\n   Eko:  That\\u2019s how she put it.  \\n   Sherry :  I  don\\u2019t  know  how  to  put   merely   there.   Around  a  mere  \\ufb01 fty \\nmillimetres \\n   Eko:   I  think   merely  around  \\ufb01 fty  millimetres   would  be,  you  know,  it \\nequate.  \\n   Sherry:  Oh, OK  \\n   (d) Revised version \\n    it \\ufb02 uctuates around a mere 50mm  \\n   (e) Eko\\u2019s new text \\n    It \\ufb02 uctuates merely around 50 mm  \\n   (f) Sherry\\u2019s new text \\n    it only \\ufb02 uctuates a little around 50 mm    \\n\\n  \\n\\n  \\n\\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n   \\n  As can be seen in (6), in the processing session, there is extensive en-\\ngagement with this reformulation. Sherry notes the reformulated phrase, \\nand Eko comments that this is a more sophisticated way of expressing \\ntheir idea. However, during the rewriting session, although Sherry re-\\ncalls the reformulation after some hesitation and assistance from Eko, \\nshe expresses some doubts about her ability to use the expression. Eko \\nsuggests  that  both  expressions  are  equivalent  in  meaning.  Thus,  al-\\nthough the revised version shows evidence of uptake, it is not retained \\nat  day  28.  This  may  be  because  the  learners  do  not  feel  comfortable \\nusing this expression or because they believe that the two expressions \\nare equivalent in meaning. \\n\\n The strategy the learners adopted was to memorize the reformula-\\ntions, as shown in (7), and the goal driving their revisions was the de-\\nsire to improve the accuracy of their text, as can be seen in (8).\\n   \\n\\n  \\n\\n\\x0cProcessing, Uptake, and Retention of CF\\n\\n319\\n\\n    Excerpt from the pair talk in the processing session \\n   Sherry:  Maybe we ought to memorise, ah.  \\n   Eko:  This  \\n   Sherry:  So I memorise the \\ufb01 rst paragraph, you the second   \\n\\n    Excerpt from the pair talk in the rewriting session \\n   Sherry:  We\\u2019re not supposed to write anything totally different, right?  \\n   Eko:  I\\u2019m not sure. I think, I think\\u2026 we should, you know, make it better. You \\nknow, to improve it.    \\n\\n   (7)    \\n  \\n  \\n  \\n\\n  (8)    \\n  \\n  \\n\\n   \\n\\n  \\n\\n    Summary \\n\\n Eko and Sherry were very pro\\ufb01 cient (IELTS 7.5), and most of their errors \\nwere fairly super\\ufb01 cial (i.e., mechanical) and easily corrected. Therefore, \\nalthough the transcripts had only limited evidence of processing (i.e., \\nfew LREs), the learners clearly noticed the reformulations and were able \\nto address their errors in their subsequent writing. \\n\\n Other errors, particularly errors in word choice, required more overt \\nattention. Here there was evidence of extended engagement and under-\\nstanding  (e.g.,  the   when - where   distinction),  which  led  to  uptake  and \\nretention. However, where there was lack of engagement (e.g., as with \\nthe word  \\ufb02 uctuative ) or resistance to the reformulation, there was no \\nlong-term retention, despite evidence of uptake (e.g., as with the expres-\\nsion  around a mere ) at day 5.    \\n\\n SECOND CASE-STUDY PAIR  \\n\\n Background \\n\\n This pair was composed of two male Indonesian students, Gus and Jon. \\nBoth were graduate students in engineering and had an IELTS score of \\n6.5 upon entry to the university. Both had studied English for several \\nyears  at  the  high  school  and  university  level  in  Indonesia.  They  had \\nknown each other for 6 months.   \\n\\n Analysis of Written Texts \\n\\n As in the case of the \\ufb01 rst pair, the text produced by this pair showed a \\nlarge number of errors and, consequently, a large number of reformula-\\ntions ( n  = 43). Most of the errors were in verb use ( n  = 14), prepositions \\n( n  = 6), and sentence structure ( n  = 6). However, unlike the \\ufb01 rst pair, \\n\\n\\x0c320\\n\\nNeomy Storch and Gillian Wigglesworth\\n\\nanalysis of uptake revealed that of the 43 reformulations made to the \\noriginal version, there were only eight instances of uptake in the revised \\ntext. The revised text (day 5) and the subsequent individual texts were \\nvery different from the text produced on day 1. The revisions made on \\nday 5 were not always in response to the feedback provided and were at \\nthe sentence level, with deletions and additions of full sentences. Texts \\nproduced on day 28 bore little resemblance to the text produced jointly \\nand contained new types of errors. This meant that there was no way to \\ntrace for evidence of retention.   \\n\\n Analysis of Pair Talk \\n\\n Despite the large number of reformulations, there were few LREs ( n  = 13) \\nfound in the data of this pair during the processing session. Most dealt \\nwith verb tense choice and were thus related to the reformulations pro-\\nvided, but the level of engagement was limited. Most of the LREs con-\\nsisted of single turns during which one learner read the reformulated \\ntext  but  made  no  comment.  In  the  rewriting  session,  despite  a  large \\nnumber of LREs ( n  = 40), only four related to the feedback provided. \\n\\n The example in (9) illustrates the limited engagement with the refor-\\nmulations and the impact of this limited engagement on the revised and \\nnew texts. The sentence produced in the original version had a number \\nof errors and, consequently, a large number of reformulations ( n  = 6). In \\nthe pair talk, the learners focused only on the use of the phrase  followed \\nby  instead of  compared with  and the verb tense. Engagement with these \\ntwo  reformulations  was  limited.  Gus  simply  read  the  reformulations; \\nJon did not contribute at all. In the revised version, the sentence shows \\nsubstantial revision; however, these revisions were not consistent with \\nthe suggested reformulations. The corresponding sentences in the texts \\nproduced on day 28 are very different from that of the original version. \\nThe sentence produced by Gus has a number of agreement errors; Jon\\u2019s \\nsentence has errors in coherence.\\n   \\n  \\n   (9)    \\n  \\n  \\n\\n    Excerpts from texts and pair talk dealing with a number of reformulations \\n   (a) Original version \\n    In  detail,  Bucharest  and  Lagos  have  the  same  pattern,  that  the  rainfall  in \\nSpring is the second highest compared to autumn and winter, while for Beijing \\nand Mexico City\\u2026  \\n   (b) Reformulated version \\n    Speci\\ufb01 cally, Bucharest and Lagos had the same pattern: the rainfall in spring \\nwas the second highest followed by autumn and winter\\u2026  \\n   (c) Relevant LRE \\n   Gus:  okay,  followed\\u2026  oh  wait  was the second highest\\u2026however Beijing \\nand Mexico City, autumn is the second\\u2026  okay  was the second highest fol-\\nlowed by spring and winter  so it\\u2019s the same.  \\n\\n  \\n  \\n\\n  \\n  \\n\\n\\x0cProcessing, Uptake, and Retention of CF\\n\\n321\\n\\n  \\n  \\n\\n  \\n  \\n\\n  \\n  \\n\\n   (d) Revised version \\n    Bucharest and Lagos falls into the \\ufb01 rst category. And Beijing and Mexico City \\nare categorized in the second pattern.  \\n   (e) Gus\\u2019s new text \\n    Bucharest  and  Lagos  falls  into  the  \\ufb01 rst  category,  for  their  rainfall  patterns \\nreaches the peak in summer, then the rainfall patterns decline...  \\n   (f) Jon\\u2019s new text \\n    Bucharest  and  Lagos  had  the  same  order  for  the  second  and  third  highest \\nrainfall, which were spring and autumn. While in Beijing and Mexico City,\\u2026    \\n\\n  \\n\\n   \\n\\n  \\n  This pair\\u2019s lack of engagement with the feedback may be attributed to \\ntheir attitude to the form of feedback (reformulations) and the aspects \\nof language with which the feedback dealt. As shown in (10), Gus and \\nJon did not approve of this form of feedback.\\n   \\n   (10)    \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n\\n   Excerpts from the processing session \\n   Gus:  huh? I don\\u2019t think this kind of feedback is good, because\\u2026  \\n   Jon:  Yeah  \\n   Gus:  people will tend to memorise this  \\n   Jon:  yeah this still crap  \\n   [\\u2026] \\n   Jon:  yeah this not in good way to give a feedback  \\n   Gus:  yeah a feedback should not just give away the answer. Yeah that\\u2019s\\u2026\\nthat\\u2019s my opinion. Okay so, are we supposed to memorise this?  \\n   Jon:  yeah, you got paragraph one and two, I got paragraph three and four  \\n   Gus:  okay, okay now you\\u2026you memorise paragraph three then four    \\n\\n  \\n\\n  \\n\\n  \\n  \\n   \\n  Despite their disapproval, they elected to memorize the reformulated \\ntext, dividing the task between them. However, once the feedback was \\nremoved, they reconsidered their goals and felt that they should per-\\nhaps rewrite the text to improve it in any way that they saw \\ufb01 t.\\n   \\n   (11)    \\n  \\n  \\n  \\n  \\n  \\n  \\n   \\n\\n   Excerpts from pair talk during the rewriting session \\n   Jon:  is it necessary that we have to write it in this style or\\u2026?  \\n   Gus:  no, you change it in any way you want to  \\n   Jon:  okay  \\n   Gus:  any way that will make it better  \\n   [\\u2026] \\n   Jon:  We make our own improvements.    \\n\\n  \\n\\n    Summary \\n\\n This pair, like the \\ufb01 rst, had a large number of errors and received a large \\nnumber of reformulations ( n  = 43). However, unlike the \\ufb01 rst pair, this \\n\\n\\x0c322\\n\\nNeomy Storch and Gillian Wigglesworth\\n\\npair\\u2019s  revised  text  showed  little  evidence  of  uptake.  These  learners \\nshowed limited engagement with the feedback. There were no instances \\nof learning evident in the data, in contrast with the data of the \\ufb01 rst pair. \\nThis lack of engagement with the feedback could be attributed to the \\nlearners\\u2019 attitudes\\u2014their lack of approval of this type of feedback. Al-\\nthough the initial goal was to memorize the reformulated text, during \\nthe rewriting session, these learners\\u2019 goals changed. They decided to \\nrewrite the text in the way they felt improved it and thus made substan-\\ntial revisions to the text. Two pairs who received feedback in the form \\nof editing will now be considered.    \\n\\n THIRD CASE-STUDY PAIR  \\n\\n Background \\n\\n The  third  pair  was  composed  of  two  female  graduate  students  from \\nChina. Monica had an IELTS score of 6.5 and had studied English only at \\nthe university level (for 3 years prior to coming to Australia). Diana had \\nstudied English at the high school and university level and had a higher \\nIELTS score of 7.0. Both were pursuing a master\\u2019s degree in human re-\\nsource  management  (commerce)  and  had  known  each  other  for  7 \\nmonths.   \\n\\n Analysis of Written Texts \\n\\n The pair\\u2019s \\ufb01 rst version of the text elicited 17 editing symbols, mainly in \\nthe use of prepositions in phrases of time or location ( n  = 5), articles \\n( n  = 3), and word choice ( n  = 3). The revised text showed a high level of \\nuptake (14/17), with errors in use of prepositions almost disappearing \\n(one remaining error). There were also few errors in use of prepositions \\nin the learners\\u2019 texts produced on day 28 (only one such error in Diana\\u2019s \\ntext and two in Monica\\u2019s). In contrast, errors in use of articles and in \\nsome word choices persisted, both in the revised text and in the new \\ntexts.   \\n\\n Analysis of Pair Talk \\n\\n The feedback elicited 18 LREs in the processing session, of which 14 \\ndealt  directly  with  the  feedback.  In  the  rewriting  sessions,  of  the  19 \\nLREs, 11 dealt with aspects of language that received editing feedback. \\n\\n\\x0cProcessing, Uptake, and Retention of CF\\n\\n323\\n\\nThe majority of the LREs in the processing (14/18) and rewriting (12/19) \\nsessions dealt with lexical choices\\u2014namely, the choice of prepositions. \\nEngagement  with  this  feedback  was  extensive.  In  contrast,  few  LREs \\ndealt  with  feedback  on  articles  ( n   =  2),  and  engagement  was  also \\nlimited. \\n\\n The example in (12) shows the different levels of engagement with \\ndifferent types of errors and illustrates that engagement with the feed-\\nback on this preposition error led to an enhanced understanding about \\nwhen to use  in  rather than  of  in temporal expressions. This, in turn, led \\nto uptake and retention. Example (12) also shows that the feedback on \\nthe error in spelling was noticed in the processing stage but was not \\ndealt with extensively. As in the case of the \\ufb01 rst pair, this limited en-\\ngagement resulted in uptake and retention. However, in the case of arti-\\ncles,  no  attention  was  paid  to  the  feedback  provided  in  either  the \\nprocessing or rewriting session. This lack of attention may explain the \\npersistence in errors in articles.\\n   \\n   (12)    \\n\\n    Excerpts from texts and pair talk dealing with the use of prepositions and \\narticles \\n   (a) Original version \\n    It\\u2019s obvious that Lagas, Beijing and Mexico City have different rainfall of four \\nseasons  \\n   (b) Editing feedback \\n\\n  \\n\\n  \\n  \\n\\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n\\n  \\n  \\n\\n  \\n  \\n\\n    It\\u2019s obvious that    Lagas   , Beijing and Mexico City have different rainfall    of    four  \\n\\n   X   \\n\\nC \\n   \\u039b \\n\\n    seasons  \\n   (c) Relevant M-LRE \\n   Diana:  Okay  it\\u2019s obvious that Lagos\\u2026  I know this. Okay  \\n   (d) Relevant L-LREs: prepositions \\n   Diana:  in four seasons  \\n   Monica:  ah huh, yeah, yeah  \\n   Diana:  This is like\\u2026this is the \\ufb01 rst time I know okay the difference between  \\nin  and  of.  Okay  \\n   [\\u2026] \\n   Diana: it is obvious that Lagos, Beijing and Mexico\\u2026  L-A-G-O-S ah huh\\u2026  \\nLagos, Beijing and Mexico have different rainfall in four seasons\\u2026 \\n   (e) Revised version \\n    It is obvious that Lagos, Beijing and Mexico City have different rainfall in \\nfour seasons    \\n\\n   \\n\\n  \\n  Example (13) illustrates the learners\\u2019 engagement with feedback on \\nlexis.  This  example  shows  how  the  learners\\u2019  beliefs  about  the  use  of \\nlanguage\\u2014in this instance, the need to use linking phrases\\u2014based on \\nprevious language learning experience (IELTS training courses), resulted \\nin resistance to the feedback, which may help explain instances of no \\nuptake or no retention. \\n\\n\\x0c324\\n\\nNeomy Storch and Gillian Wigglesworth\\n\\n The phrase  as can be seen that  was underlined and put in parentheses \\nto indicate that there was an error and that some words in this phrase \\nshould be deleted. However, the learners mistook the symbols to mean \\nthat the entire linking phrase was unnecessary and proceeded to delete \\nthis  phrase  in  their  revision.  Similarly  to  the  \\ufb01 rst  pair  with   around  a \\nmere , these learners show uptake, but their resistance to this feedback \\nbased on their prior learning experience means that the linking phrase \\nis used on day 28.  1  \\n   \\n   (13)    \\n  \\n  \\n\\n   Excerpts from texts and pair talk dealing with the use of linking phrases \\n   (a) Original version \\n    As we can see that the rainfall of most seasons in Lagos is the highest among \\nthe four cities.  \\n   (b) Editing feedback \\n     (As  we  can  see  that)    the  rainfall  of  most  seasons  in  Lagos  is  the  highest \\namong the four cities.  \\n   (c) Relevant LRE \\n   Diana:  I just don\\u2019t understand why we don\\u2019t need the  as we can see that . I \\nthink this is\\u2026this is nice  \\n   Monica:  mm hmm  \\n   Diana:  I don\\u2019t know why  \\n   Monica:  should be deleted  \\n   Diana:  but I think  as we can see that \\u2026you see in the IELTS book, they said \\num  it can be seen,  blah, blah, blah so I think that this is nice, I don\\u2019t know \\nwhy  \\n   (d) Revised version \\n    The rainfall in Lagos in most seasons is the highest among the four cities . \\n   (e) Diana\\u2019s new text \\n    It can be seen that the rainfall in summer in Beijing\\u2026  \\n   (f) Monica\\u2019s new text \\n    As we can see, Bucharest\\u2026    \\n\\n  \\n\\n  \\n  \\n\\n  \\n  \\n\\n  \\n  \\n  \\n  \\n\\n  \\n  \\n  \\n  \\n  \\n  \\n   \\n\\n  \\n\\n    Summary \\n\\n Monica and Diana attended to most of the feedback received, particu-\\nlarly the feedback on their most frequent errors (use of prepositions in \\nlocative  and  temporal  expressions).  The  LREs  showed  that  these \\nlearners gained an understanding of this use of prepositions, which as-\\nsisted them in using these prepositions correctly in the revised version \\nand in subsequently produced new texts. Less attention was paid to the \\nediting feedback on the use of articles, which may explain\\u2014along with \\nthe fact that articles are a renowned area of dif\\ufb01 culty for L2 learners\\u2014\\nthe lack of retention of feedback on articles beyond the revised version. \\nThese learners seemed to show a higher level of uptake and retention \\n\\n\\x0cProcessing, Uptake, and Retention of CF\\n\\n325\\n\\nwhen the feedback was consistent with their beliefs about language use. \\nWhen the feedback contradicted those beliefs established in previous \\nlanguage learning courses, there was no retention.    \\n\\n FOURTH CASE-STUDY PAIR  \\n\\n Background \\n\\n Unlike the other three pairs, the learners in this pair were undergraduate \\nstudents in commerce. Bing was a male from Malaysia and Lina was a \\nfemale from Indonesia. Both had studied English in high school and both \\nhad high IELTS scores of 7.0. They had known each other for 6 weeks.   \\n\\n Analysis of Written Texts \\n\\n The  original  version  of  their  text  had  15  editing  symbols.  The  most \\ncommon errors were in the use of prepositions ( n  = 4) and verbs ( n  = 3). \\nThe revised text showed complete uptake (100%), with revisions made \\nmainly at the word level. All errors in prepositions were amended, but \\nnew errors in the use of verbs were introduced. On day 28, the texts \\nproduced  individually  had  no  errors  in  verb  use;  Bing  had  only  one \\nerror and Lina had three errors in the use of prepositions, which sug-\\ngests overall high levels of retention.   \\n\\n Analysis of Pair Talk \\n\\n In processing the feedback, the learners paid attention to the editing \\nfeedback  and  all  of  the  LREs  in  both  sessions  related  directly  to  the \\nfeedback received. Most of the F-LREs focused on the use of verbs and \\nmost of the L-LREs on the choice of prepositions. The level of engage-\\nment with the feedback was extensive, which can perhaps account for \\nthe high uptake. \\n\\n The example in (14) shows evidence of the high level of engagement \\nwith the feedback provided and evidence of the learners extending their \\nknowledge  to  new  contexts.  The  feedback  provided  suggested  that \\nthere were errors in that string of words. Although not speci\\ufb01 ed, some \\nof these errors were related to an inconsistent use of verb tense. While \\ndiscussing this sentence, the learners became aware of these inconsis-\\ntencies. Lina suggested that they use either the present or past tense \\nthroughout, and Bing agreed. In their revised text and their new texts, \\nthe present tense was used throughout.\\n   \\n\\n  \\n\\n\\x0cNeomy Storch and Gillian Wigglesworth\\n\\n   Excerpts from texts and pair talk dealing with the use of verbs \\n   (a) Original version \\n    Beijing also had the average rainfall in summer, which is 150mm  \\n   (b) Editing feedback \\n    (Beijing also had the average rainfall in summer, which is 150mm)  \\n   (c) Relevant LRE \\n   Lina:  Beijing had\\u2026  \\n   Bing:  had or\\u2026 had or has  \\n   Lina:  Don\\u2019t I say the other tenses we use\\u2026  \\n   Bing:  had  \\n   Lina:  Mmm  \\n   Bing:  which has  \\n   Lina:  You don\\u2019t use present, you know. But she has not correct our tense. Or \\ndid she?  \\n   Bing:  Good question.  \\n   Lina:  Haven\\u2019t I\\u2026 this one we use present, this is all present. If one use the \\npast then use past all.  \\n   Bing:  OK. Beijing has\\u2026  \\n   (d) Revised version \\n    Beijing has the highest average rainfall in summer of 150mm  \\n   (e) Bing\\u2019s new text \\n    Beijing has the lowest average rainfall of 5mm, followed by\\u2026  \\n   (f) Lina\\u2019s new text \\n    The second highest total average rainfall occurs in Beijing.    \\n\\n326\\n\\n   (14)    \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n\\n  \\n  \\n\\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n   \\n\\n  \\n  The excerpt from the pair talk during the rewriting session given in \\n(15) suggests that the learners\\u2019 goal was to focus on amending the er-\\nrors rather than rewriting the text. The pair talk also showed evidence \\nthat  the  learners  memorized  the  location  of  the  editing  symbols  and \\nrelied on this in their rewriting activity.\\n   \\n   (15)    \\n  \\n\\n   Excerpts from texts and pair talk \\n   Lina:  You see\\u2026 the difference. So we write the same the same\\u2026 just change \\nthe one that\\u2026  \\n   Lina:  This one just is not right.  \\n   Bing:  Mmm? What do you mean?  \\n   Lina:  Have to correct this and just write.  \\n   [\\u2026] \\n   Lina:  Mmm. and winter it has\\u2026 it. Remember there\\u2019s something missing.  \\n   Bing:  Yeah    \\n\\n  \\n\\n  \\n  \\n  \\n  \\n  \\n  \\n   \\n\\n  \\n\\n    Summary \\n\\n There was a high level of uptake and retention of the feedback provided \\nto  these  learners,  who  engaged  with  the  feedback  extensively.  Their \\n\\n\\x0cProcessing, Uptake, and Retention of CF\\n\\n327\\n\\ngoal was to amend the text at the word level in response to the feedback \\nprovided.    \\n\\n DISCUSSION \\n\\n This study sought to examine and compare how learners process direct \\nfeedback (reformulations) versus indirect feedback (editing symbols) \\non language errors and what impact, if any, the type of feedback and \\nprocessing has on uptake (immediate revision) and retention in the long \\nterm (23 days later), as evident in individually written texts. The \\ufb01 nd-\\nings for the whole cohort showed that editing feedback elicited more \\nLREs than reformulations and that these LREs tended to relate directly \\nto the feedback provided. The level of engagement also seemed more \\nextensive with editing feedback than in response to reformulations. As \\nsuggested by Ferris ( 2002 ), in response to editing, learners had to iden-\\ntify the nature of the error and attempt to supply the correct form, using \\ntheir own knowledge of grammar and word meanings to offer sugges-\\ntions and countersuggestions. In contrast, engagement with reformula-\\ntions tended to be limited to reading the reformulation, acknowledging \\nor merely expressing agreement, with fewer instances of extensive en-\\ngagement. However, it is important to reiterate that coding for level of \\nengagement  is  a  highly  inferential  process  (Sachs  &  Polio,   2007 )  and \\nthat  the  amount  of  verbalization  evident  in  the  LREs  may  not  neces-\\nsarily re\\ufb02 ect depth of cognitive processing. \\n\\n In line with the \\ufb01 ndings of other studies that elicited feedback pro-\\ncessing data (e.g., Qi & Lapkin,  2001 ; Sachs & Polio,  2007 ), this study \\nalso  found  that  extensive  engagement  with  the  feedback  led  to  high \\nlevels of uptake. This was evident in the \\ufb01 ndings for the whole cohort as \\nwell as in the case-study data. The third and fourth pairs, who received \\nediting  feedback,  engaged  with  the  feedback  extensively  and  showed \\nhigh levels of uptake. In the case of the third pair, for example, extensive \\nengagement  with  feedback  on  certain  prepositions  led  to  uptake  and \\ncorrect use of these prepositions. In contrast, limited or no engagement \\nwith feedback on articles resulted in no uptake and persistent inaccura-\\ncies in the use of articles. Similarly, in the case of the \\ufb01 rst pair, who re-\\nceived  reformulation \\nfeedback,  extensive  engagement  over  the \\n where - when  distinction led to uptake; limited engagement with the word \\nchoice   \\ufb02 uctuative   resulted  in  no  uptake.  However,  data  from  the  \\ufb01 rst \\npair showed that uptake also depends to some extent on the nature of \\nthe  errors.  For  more  super\\ufb01 cial  errors,  such  as  errors  in  mechanics, \\nperfunctory noticing, whether verbalized or not, may be suf\\ufb01 cient for \\nuptake to occur. \\n\\n Similarly, retention seemed to relate to the level of engagement with \\nthe  feedback  and  the  nature  of  the  errors.  Feedback  on  errors  in \\n\\n\\x0c328\\n\\nNeomy Storch and Gillian Wigglesworth\\n\\nmechanics was retained despite limited or no overt engagement. In the \\ncase of morphosyntactic and lexical errors, high levels of engagement \\nled to understanding and an ability to retain the feedback in the long \\nterm (e.g., the  where-when  distinction in the \\ufb01 rst pair or correct use of \\nverbs in the fourth pair). However, other affective factors also seemed \\nto in\\ufb02 uence retention. Speci\\ufb01 cally, learners\\u2019 beliefs, attitude toward the \\nform of feedback received, and their goals seemed to have an effect on \\nwhether the feedback was retained. Thus, in the case of the \\ufb01 rst pair, \\nthere  is  elaborate  engagement  with  the  reformulated  phrase   merely \\naround , which led to uptake but no retention. Lack of retention seemed \\nto be attributable to learners\\u2019 beliefs about language use. The pair did \\nnot adopt the reformulated phrase because they felt that the alternative \\nwas  not  necessarily  a  better  expression.  Similarly,  in  the  case  of  the \\nthird pair and the use of linking phrases (e.g.,  as we can see that ), there \\nwas extensive engagement and uptake but no retention. The learners \\nfelt that the feedback contradicted their beliefs, shaped by their pre-\\nvious  language  learning  experience  about  what  constitutes  a  good \\nwriting style. Studies by Swain ( 2006 ) and Swain and Lapkin ( 2003 ) also \\nfound evidence of resistance to feedback that resulted in no uptake. The \\ncase study data discussed here suggest that resistance is more likely to \\nlead to lack of retention. \\n\\n Another important affective factor that had an impact on both uptake \\nand retention was learners\\u2019 goals. The \\ufb01 rst and fourth pairs seemed to \\nbe driven by a goal to improve the accuracy of their text. This strategy \\nmay explain high uptake. In the case of the second pair, disapproval of \\nreformulations as a form of feedback coupled with the goal of improving \\ntheir  text  as  they  saw  \\ufb01 t  (see  also  Hyland,   1998 )  meant  that  these \\nlearners ignored the feedback received; hence, there was no uptake (or \\nretention). When the learners seemed to approve of the type of feed-\\nback received\\u2014and were driven by a goal to improve their text\\u2014they \\nsometimes adopted the strategy of memorizing the feedback (the \\ufb01 rst \\npair) or the location of the editing symbols (the fourth pair). \\n\\n In research on feedback, affective factors such as learners\\u2019 orienta-\\ntion (Lantolf & Thorne,  2006 ), which includes their attitudes toward the \\ntype  of  feedback  received  and  beliefs  about  language  conventions \\nshaped by previous language instruction as well as the goals and strat-\\negies adopted, are often ignored. The case study data showed that af-\\nfective factors in\\ufb02 uence not only the type of strategies learners adopt in \\ndealing with the feedback received (e.g., memorization) but also their \\nwillingness to accept the feedback and their likelihood of retaining it. \\n\\n However, these \\ufb01 ndings should be interpreted cautiously. The partic-\\nipants in the current study were advanced language learners, and this \\nmay have affected not only the type of errors they made in their writing \\nbut also their ability to notice and attend to the feedback received, as \\nwell as their attitude to the different types of feedback. Furthermore, \\n\\n\\x0cProcessing, Uptake, and Retention of CF\\n\\n329\\n\\nthe two types of written CF compared (reformulations and editing sym-\\nbols)  were  distinct  from  each  other  and,  thus,  inevitably  elicited  dif-\\nferent kinds of responses. Perhaps the greatest limitation is that data \\nwere collected in an experimental rather than a classroom setting and \\nthus, important contextual factors such as the relationship between the \\nlearners and the teacher who provided the feedback could not be inves-\\ntigated.  Recent  research  on  feedback  (e.g.,  Given  &  Schallert,   2008 ) \\nshows that this relationship may play a powerful role in determining \\nwhether learners take up the feedback provided. \\n\\n Nevertheless,  the  \\ufb01 ndings  suggest  that  whether  and  which  type  of \\nfeedback is effective depend on a complex and dynamic interaction of \\nlinguistic  and  affective  factors.  Future  research  on  feedback  needs \\nto  combine  an  examination  of  the  product  (revised  and  new  texts) \\nand processes in an integrated manner. To isolate and investigate the \\neffect of linguistic factors, studies in which feedback is given on speci\\ufb01 c \\nstructures are needed (e.g., Bitchener,  2008 ; Sheen,  2007 ) to establish \\nwhether some form of feedback (direct vs. indirect) is more effective for \\nparticular types of errors. However, an investigation of linguistic factors \\nalone is not enough. Researchers (e.g., Cumming, Busch, & Zhou, 2002; \\nHyland,   1998 ,   2003 ;  Sachs  &  Polio,   2007 )  have  called  for  classroom-\\nbased studies that more fully investigate affective factors (e.g., goals, \\norientation to task, preferences); however, such research is dif\\ufb01 cult to \\nconduct.  Collecting  feedback  processing  data  in  a  classroom  setting \\nmay be dif\\ufb01 cult; given the detailed analysis such an investigation neces-\\nsitates, studies on learners\\u2019 engagement with feedback have tended to \\nbe small-scale case studies (e.g., Given & Schallert,  2008 ; Hyland; Qi & \\nLapkin,  2001 ; Tardy,  2006 ). It is perhaps through case studies, such as \\nthe  study  reported  here,  that  insights  into  this  complex  issue  of  the \\nimpact of CF can be gained, along with an understanding of the inevita-\\nbility that experimental research on the impact of CF will continue to \\nyield mixed \\ufb01 ndings.     \\n\\n NOTE \\n\\n  1.     Even though the linking phrase was used correctly in the texts produced on day 28, \\nthis was coded for no retention because the learners did not adhere to their understanding \\nof the editing code (deletion of the phrase).    \\n\\n REFERENCES \\n\\n    Aljaafreh  ,   A.  , &   Lantolf  ,   J. P   . ( 1994 ).  Negative feedback as regulation and second language \\nlearning  in  the  zone  of  proximal  development .   Modern  Language  Journal ,   78 ,  \\n465 \\u2013 483 . \\n\\n    Bitchener  ,   J.    ( 2008 ).  Evidence in support of written corrective feedback .  Journal of Second \\n\\nLanguage Writing ,  17 ,  102 \\u2013 118 . \\n\\n    Bitchener  ,   J.  , &   Knoch  ,   U.    ( 2008 ).  The value of written corrective feedback for migrant and \\n\\ninternational students .  Language Teaching Research ,  12 ,  409 \\u2013 431 . \\n\\n\\x0c330\\n\\nNeomy Storch and Gillian Wigglesworth\\n\\n    Chandler  ,   J.    ( 2003 ).  The ef\\ufb01 cacy of various kinds of error feedback for improvement in the \\naccuracy and \\ufb02 uency of L2 student writing .  Journal of Second Language Writing ,  12 , \\n 267 \\u2013 296 . \\n\\n    Cummings  ,   A.  ,   Busch  ,   M.  , &   Zhou  ,   A.    ( 2002 ).  Investigating learners\\u2019 goals in the context of \\nadult second language writing . In    S.     Ransdell   &   M.     Barbier    (Eds.),  New directions for \\nresearch for L2 writing  (pp.  189 \\u2013 208 ).  Dordrecht :  Kluwer . \\n\\n    Ferris  ,   D. R   . ( 1999 ).  The case for grammar correction in L2 writing classes: A response to \\n\\nTruscott (1996) .  Journal of Second Language Writing ,  8 ,  1 \\u2013 10 . \\n\\n    Ferris  ,    D.  R   .  ( 2002 ).   Treatment  of  error  in  second  language  student  writing .   Ann  Arbor : \\n\\n University of Michigan Press . \\n\\n    Given  ,   L.  , &   Schallert  ,   D.    ( 2008 ).  Meeting in the margins: Effects of the teacher-student \\nrelationship  on  revision  processes  of  EFL  college  students  taking  a  composition \\ncourse .  Journal of Second Language Writing ,  17 ,  165 \\u2013 182 . \\n\\n    Goldstein  ,   L.    ( 2004 ).  Questions and answers about teacher written commentary and stu-\\ndent revisions: Teachers and students working together .  Journal of Second Language \\nWriting ,  13 ,  63 \\u2013 80 . \\n\\n    Goldstein  ,   L.    ( 2005 ).  Teacher written commentary in second language writing classrooms . \\n\\n Ann Arbor :  University of Michigan Press . \\n\\n    Gu\\xe9nette  ,   D.    ( 2007 ).  Is feedback pedagogically correct? Research design issues in studies \\n\\nof feedback on writing .  Journal of Second Language Writing ,  16 ,  40 \\u2013 53 . \\n\\n    Hyland  ,   F.    ( 1998 ).  The impact of teacher written feedback on individual writers .  Journal of \\n\\nSecond Language Writing ,  7 ,  255 \\u2013 286 . \\n\\n    Hyland  ,   F.    ( 2003 ).  Focusing on form: Student engagement with teacher feedback .  System , \\n\\n 31 ,  217 \\u2013 230 . \\n\\nTeaching ,  39 ,  83 \\u2013 101 . \\n\\n    Hyland  ,   F.  , &   Hyland  ,   K.    ( 2006 ).  Feedback on second language students\\u2019 writing .  Language \\n\\n    Hyland  ,   K.  , &   Hyland  ,   F.    ( 2006 ).  Context and issues in feedback on L2 writing: An introduc-\\ntion . In    K.     Hyland   &   F.     Hyland    (Eds.),  Feedback in second language writing  (pp.  1 \\u2013 20 ). \\n Oxford :  Oxford University Press . \\n\\n    Lalande  ,   J. F   . ( 1982 ).  Reducing composition errors: An experiment .  Modern Language Journal , \\n\\n 66 ,  140 \\u2013 149 . \\n\\n    Lantolf  ,   J. P.  , &   Pavlenko  ,   A.    ( 2001 ).  Second language activity theory: Understanding sec-\\nond language learners as people . In    M. P.     Breen    (Ed.),  Learner contributions to language \\nlearning: New directions in research  (pp.  141 \\u2013 158 ).  New York :  Pearson Education . \\n\\n    Lantolf  ,   J. P.  , &   Thorne  ,   S. L   . ( 2006 ).  Sociocultural theory and the genesis of second language \\n\\ndevelopment.   Oxford :  Oxford University Press . \\n\\n    Nassaji  ,   H.  , &   Swain  ,   M.    ( 2000 ).  A Vygotskian perspective on corrective feedback in L2: \\nThe  effect  of  random  versus  negotiated  help  on  the  learning  of  English  articles . \\n Language Awareness ,  9 ,  34 \\u2013 51 . \\n\\n    Qi  ,   D. S.  , &   Lapkin  ,   S.    ( 2001 ).  Exploring the role of noticing in a three-stage second language \\n\\nwriting task .  Journal of Second Language Writing ,  10 ,  277 \\u2013 303 . \\n\\n    Sachs  ,    R.  ,  &    Polio  ,    C.     ( 2007 ).   Learners\\u2019  uses  of  two  types  of  written  feedback  on  a  L2 \\n\\nwriting revision task .  Studies in Second Language Acquisition ,  29 ,  67 \\u2013 100 . \\n\\n    Sheen  ,   Y.    ( 2007 ).  The effect of focused written corrective feedback and language aptitude \\n\\non ESL learners\\u2019 acquisition of articles .  TESOL Quarterly ,  41 ,  255 \\u2013 283 . \\n\\n    Storch  ,   N.    ( 2008 ).  Metatalk in a pair work activity: Level of engagement and implications \\n\\nfor language development .  Language Awareness ,  17 ,  95 \\u2013 114 . \\n\\n    Storch  ,   N.  , &   Wigglesworth  ,   G.    ( 2006 , July).  Reformulate or edit? Investigating the impact \\nof different feedback practices. Paper presented at the 5th Paci\\ufb01 c Second Language \\nResearch Forum, University of Queensland, Australia . \\n\\n    Swain  ,   M.    ( 2006 ).  Languaging, agency and collaboration in advanced language pro\\ufb01 ciency . \\nIn     H.      Byrnes     (Ed.),   Advanced  language  learning:  The  contribution  of  Halliday  and \\nVygotsky  (pp.  95 \\u2013 108 ).  New York :  Continuum . \\n\\n    Swain  ,   M.  , &   Lapkin  ,   S.    ( 1998 ).  Interaction and second language learning: Two adolescent \\nFrench immersion students working together .  Modern Language Journal ,  82 ,  320 \\u2013 337 . \\n    Swain  ,    M.  ,  &    Lapkin  ,    S.     ( 2003 ).   Talking  it  through:  Two  French  immersion  learners\\u2019 \\nresponse to reformulation .  International Journal of Educational Research ,  37 ,  285 \\u2013 304 . \\n    Tardy  ,   C.    ( 2006 ).  Appropriation, ownership, and agency: Negotiating teacher feedback in \\nacademic  settings .  In     K.      Hyland    &    F.      Hyland     (Eds.),   Feedback  in  second  language \\nwriting  (pp.  60 \\u2013 78 ).  Oxford :  Oxford University Press . \\n\\n\\x0cProcessing, Uptake, and Retention of CF\\n\\n331\\n\\n    Thornbury  ,   S.    ( 1997 ).  Reformulation and reconstruction: Tasks that promote \\u201cnoticing.\\u201d  \\n\\n ELT Journal ,  51 ,  326 \\u2013 335 . \\n\\n    Tocalli-Beller  ,   A.  , &   Swain  ,   M.    ( 2005 ).  Reformulation: The cognitive con\\ufb02 ict and L2 learning \\n\\nit generates .  International Journal of Applied Linguistics ,  15 ,  5 \\u2013 28 . \\n\\n    Truscott  ,   J.    ( 2007 ).  The effect of error correction on learners\\u2019 ability to write accurately . \\n\\n Journal of Second Language Writing ,  16 ,  255 \\u2013 272 .  \\n\\n  APPENDIX A: GRAPH REPORT \\n\\n The graph below shows average rainfall (by season) for four cities. \\nWrite a report for a university lecturer describing the information shown \\nbelow. \\n\\n You should write at least 150 words.      \\n\\n\\x0c332\\n\\nNeomy Storch and Gillian Wigglesworth\\n\\n \\n\\n \\n\\nE\\nD\\nO\\nC\\nG\\nN\\nT\\nD\\nE\\n\\nI\\n\\nI\\n\\n \\n:\\n\\nI\\n\\n \\n\\nB\\nX\\nD\\nN\\nE\\nP\\nP\\n  A\\n\\n     \\ne\\nl\\np\\nm\\na\\nx\\n E\\n\\n   \\n\\n F\\n\\n    \\n)\\ng\\nn\\ni\\nt\\ns\\ne\\nr\\ne\\nt\\n\\nn\\ni\\n (\\n \\n .\\n\\nd\\ne\\nt\\ns\\ne\\nr\\ne\\nt\\nn\\n\\n \\n\\n i\\n \\ns\\na\\nw\\nm\\nu\\ne\\ns\\nu\\nm\\ne\\nh\\n T\\n\\n \\n\\n    \\n)\\ns\\nr\\na\\nC\\n (\\n \\n.\\nt\\nn\\ne\\nm\\nn\\no\\nr\\ni\\nv\\nn\\ne\\ne\\nh\\nt\\n \\nr\\no\\nf\\n \\n\\n \\n\\n \\n\\nd\\na\\nb\\ne\\nr\\na\\n  \\nr\\na\\n  C\\n\\n   \\n\\n F\\n\\n    \\n)\\nn\\no\\ni\\nt\\n\\na\\nm\\nr\\no\\nn\\ni\\n (\\n \\n .\\n\\nf\\n\\nm\\nr\\no\\nf\\nn\\n\\n i\\n \\ne\\nm\\no\\ns\\n \\nd\\ne\\ne\\nn\\ne\\n W\\n\\n \\n\\n   \\n\\n F\\n\\n    \\n)\\ne\\nr\\nu\\nt\\ni\\nn\\nr\\nu\\nf\\n (\\n \\n .\\ns\\ne\\nr\\nu\\nt\\ni\\nn\\nr\\nu\\n f\\n \\nr\\nu\\no\\ny\\ne\\nv\\no\\n\\n \\n\\nl\\n \\n I\\n\\n   \\n\\n F\\n\\n    \\n)\\ns\\ne\\nk\\ni\\nl\\n (\\n \\n.\\nt\\ni\\n  \\ne\\nk\\ni\\n l\\n \\n\\ne\\nh\\n S\\n\\n   \\n\\nF\\n  \\n\\n   \\n\\n F\\n\\n    \\n)\\ng\\nn\\ni\\nt\\nt\\nu\\nh\\ns\\n (\\n \\n?\\nr\\no\\no\\nd\\n \\ne\\nh\\nt\\n  \\nt\\nu\\nh\\ns\\n \\no\\n t\\n \\nd\\nn\\nm\\nu\\no\\ny\\nd\\nu\\no\\n W\\n\\nl\\n\\ni\\n\\n \\n\\n \\n\\n    \\n)\\nt\\n\\nh\\ng\\nu\\na\\nc\\n (\\n \\n.\\nf\\ne\\ni\\nh\\nt\\n \\ne\\nh\\nt\\n  \\nt\\nh\\ng\\nu\\na\\nc\\ne\\nr\\ne\\n w\\ne\\nc\\ni\\nl\\n\\n \\n\\n \\n\\n \\n\\no\\np\\ne\\nh\\n T\\n\\n   \\n\\n F\\n\\n \\n)\\nb\\nr\\ne\\nv\\n \\nr\\no\\nn\\nu\\no\\nn\\n\\n \\n\\n \\n,\\n.\\n\\n.\\n\\ng\\ne\\n(\\n \\n\\ne\\np\\ny\\nt\\n \\nd\\nr\\no\\n w\\n\\n \\n\\nm\\nr\\no\\nf\\n  \\ng\\nn\\n i\\n-\\n \\nr\\no\\ne\\nv\\ni\\nt\\ni\\nn\\n\\ufb01 \\nn\\n\\n \\n\\n i\\n\\n \\n\\ne\\nv\\ni\\ns\\ns\\na\\np\\n \\nr\\no\\n \\ne\\nv\\ni\\nt\\nc\\n a\\n\\n \\nt\\nn\\ne\\nm\\ne\\ne\\nr\\ng\\na\\n \\nt\\nc\\ne\\nj\\nb\\nu\\ns\\n-\\nb\\nr\\ne\\n v\\n\\nl\\n\\n \\nl\\na\\nr\\nu\\np\\n \\nr\\no\\n \\nr\\na\\nl\\nu\\ng\\nn\\ni\\n s\\n\\n \\n\\ne\\nl\\nb\\na\\nt\\nn\\nu\\no\\nc\\nn\\nu\\n \\nr\\no\\ne\\nl\\nb\\na\\nt\\nn\\nu\\no\\n c\\n\\n \\n\\n \\n\\nm\\nr\\no\\nf\\n \\nd\\nr\\no\\n w\\n\\n \\n\\nm\\ne\\nl\\nb\\no\\nr\\n P\\n\\n \\n\\nm\\nr\\no\\nf\\n \\ng\\nn\\no\\nr\\n W\\n\\n \\n\\n F\\n\\n \\n\\ne\\nd\\no\\n   C\\n\\n \\n\\n    \\n)\\nl\\nl\\ne\\nw\\nh\\ns\\ni\\nl\\ng\\nn\\nE\\n (\\n \\n.\\n\\nh\\ns\\ni\\nl\\ng\\nn\\nE\\n\\n \\nl\\nl\\ne\\nw\\n \\ns\\nk\\na\\ne\\np\\ns\\n \\ne\\n H\\n\\n    \\n)\\ns\\na\\nw\\n (\\n \\n.\\nt\\no\\nh\\n   \\n  \\u2227\\n \\ny\\na\\nd\\no\\n T\\n\\n    \\n)\\n.\\ne\\nm\\no\\nh\\n\\n \\nt\\n\\nn\\ne\\nw\\n\\n \\nI\\n (\\n \\n.\\n)\\ne\\nm\\no\\nh\\no\\nt\\n(\\n \\nt\\nn\\ne\\nw\\n\\n \\n\\n \\n I\\n\\n    \\n)\\na\\n (\\n \\n.\\n\\ne\\nm\\n\\ni\\nt\\n \\nd\\no\\no\\ng\\n  \\ne\\nh\\n t\\n \\nd\\na\\nh\\ne\\n W\\n\\n \\n\\n   \\n\\n T\\n\\n    \\n)\\nt\\n\\nn\\ne\\nw\\n (\\n \\n.\\n\\n \\n\\ny\\na\\nd\\nr\\ne\\nt\\ns\\ne\\ny\\ne\\nr\\ne\\nh\\nt\\n  \\ns\\ne\\no\\n g\\ne\\nh\\n S\\n\\n \\n\\n \\n)\\n\\nm\\ne\\nl\\nb\\no\\nr\\np\\ne\\nl\\nc\\ni\\nt\\nr\\na\\n\\n \\n\\n \\n,\\n\\ne\\ns\\nu\\n \\nr\\na\\ne\\nl\\nc\\nn\\nu\\n\\n \\n,\\n\\ny\\nr\\na\\ns\\ns\\ne\\nc\\ne\\nn\\nn\\nu\\n\\n \\n,\\ne\\nt\\na\\ni\\nr\\np\\no\\nr\\np\\np\\na\\nn\\n\\n i\\n\\n  \\n\\n \\n\\n \\n\\n \\n\\nl\\n\\nd\\ne\\nt\\ne\\nl\\ne\\nd\\ne\\nb\\nd\\nu\\no\\nh\\ns\\n \\ne\\ns\\na\\nr\\nh\\np\\n \\ns\\ni\\nh\\nt\\n \\nn\\n\\ni\\n \\ne\\nr\\ne\\nh\\nw\\ne\\nm\\no\\ns\\n \\n)\\ns\\n(\\nd\\nr\\no\\n W\\n\\n \\ng\\nn\\ni\\ns\\ns\\ni\\nm\\nd\\nr\\no\\n W\\n\\n \\n\\n \\nr\\ne\\nd\\nr\\no\\nd\\nr\\no\\n W\\n\\n \\n\\n \\nr\\no\\nr\\nr\\ne\\n\\n \\n\\ne\\ns\\nn\\ne\\n T\\n\\n   \\n\\n C\\n\\n \\n,\\n\\nn\\no\\ni\\nt\\na\\nc\\no\\n\\nl\\nl\\n\\n \\n\\no\\nc\\ng\\nn\\no\\nr\\nw\\n\\n \\n,\\n.\\n\\n.\\n\\ng\\ne\\n(\\n \\nd\\nr\\no\\nw\\n \\ne\\nh\\nt\\n \\ne\\ng\\nn\\na\\nh\\nC\\n \\n\\u2013\\n \\nm\\ne\\nl\\nb\\no\\nr\\np\\n \\ne\\nc\\ni\\no\\nh\\nc\\nd\\nr\\no\\n W\\n\\n \\n\\n    \\n)\\nn\\na\\nw\\ni\\na\\nT\\n\\u2026\\n\\nt\\n\\u2019\\n\\nn\\no\\nd\\n (\\n \\n .\\n\\nn\\na\\nw\\ni\\na\\n t\\n \\n\\nm\\no\\nr\\nf\\n \\ne\\nm\\no\\nc\\n  \\nt\\nn\\no\\n d\\n\\n \\n I\\n\\n   \\n\\n X\\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n  \\n \\n\\n X\\n\\n \\n\\nn\\no\\ni\\nt\\na\\ns\\ni\\nl\\na\\nt\\ni\\np\\na\\nc\\n \\nr\\no\\n \\ng\\nn\\n\\ni\\nl\\nl\\ne\\np\\ns\\n \\n,\\n\\nn\\no\\ni\\nt\\na\\nu\\nt\\nc\\nn\\nu\\n P\\n\\n   \\n .\\ne\\nm\\n\\ni\\nt\\n \\nd\\nn\\ne\\nl\\n \\ns\\ny\\na\\nw\\nl\\na\\nn\\na\\nc\\ng\\nn\\ni\\nv\\ni\\n\\n \\n\\n \\n\\n        \\n\\n   \\n ?\\n\\n  L\\n\\n \\n\\n \\n\\n \\n\\ne\\nr\\na\\nu\\no\\ny\\n \\nt\\na\\nh\\nw\\nd\\nn\\na\\nt\\ns\\nr\\ne\\nd\\nn\\nu\\n \\nt\\no\\nn\\nn\\na\\nc\\n \\nr\\ne\\nd\\na\\ne\\nr\\n \\ne\\nh\\nT\\n\\n \\n-\\n \\n\\nm\\ne\\nl\\nb\\no\\nr\\np\\n \\ng\\nn\\nn\\na\\ne\\n M\\n\\ni\\n\\n \\n.\\n\\ny\\na\\ns\\n \\no\\nt\\n \\ng\\nn\\ni\\ny\\nr\\n t\\n\\n  \\n\\n \\n)\\n\\n\\u2026\\n\\n (\\n\\n \\n\\n C\\n\\n \\n\\n T\\n\\n  \\n\\n  \\u2227\\n\\n      \\n\\n \\n ?\\n\\n \\n\\n X\\n\\n\\x0cProcessing, Uptake, and Retention of CF\\n\\n333\\n\\n         APPENDIX C: GUIDELINES FOR \\n\\nLRE ANALYSIS \\n\\n A  Language Related Episode (LRE ) is any segment in the data where there is an \\nexplicit focus on language. \\n\\n Note:\\n   \\n       \\u2022      This focus can be in response to the feedback the participants received but \\n\\n  \\n\\ncan also be unsolicited.  \\n\\n      \\u2022      LREs  can  vary  in  length.  They  can  be  short  (e.g.,  consisting  of  a  learner \\nsimply reading out aloud a reformulated word or phrase with no response \\nfrom  the  other  member  of  the  pair)  or  a  long  segment  (e.g.,  where  both \\nlearners discuss grammatical or lexical choices).  \\n\\n      \\u2022      LREs can be interrupted. For example, learners may deliberate over the use \\nof articles and decide to omit it. They may then return to this decision at a \\nlater stage in their pair talk and decide to reverse their decision, and insert \\nthe article. Since both segments deal with the same \\u2018error\\u2019 they are counted \\nas one episode.    \\n\\n  CODING LRES \\n\\n    \\n   \\n\\n  \\n   1.       Identify in the data segments where learners seem to be focusing explicitly \\n\\non language choice.  \\n\\n \\n\\n  2.       Distinguish  LREs  in  terms  of  focus:  form-focus  (F-LREs),  lexis-focus  (L-\\n\\n      \\n\\n   \\n\\n   \\n\\n   \\n  \\n\\n  \\n\\n    \\n\\n   \\n\\n   \\n\\nLREs), mechanics-focus (M-LREs). \\n  \\u2022        F-LRE : focus on morphology or syntax (e.g., verb tenses, word forms, use \\nof articles, prepositions, word order)  \\n\\n  \\u2022        L-LRE : deliberations on word meaning, searching for a word, suggesting \\n\\nalternative words/phrase  \\n\\n  \\u2022        M-LRE : deliberations on issues such as spelling or punctuations (or pro-\\n\\nnunciation)   \\n\\n  \\n  3.         Determine whether the LRE deals with language items that were targeted \\n\\nby the feedback given.  \\n\\n 4.         Determine whether the LRE is resolved correctly ( \\u221a ), incorrectly (X), or \\n\\nleft unresolved (?). \\n  \\u2022        Resolved correctly  ( \\u221a ): The resolution reached is in line with the intended \\nfeedback (or it could be an acceptable alternative in this instance).  \\n\\n  \\u2022        Resolved incorrectly  (X): The resolution reached is not in line with the \\n\\nintended feedback (or is an unacceptable alternative in this instance).  \\n\\n  \\u2022        Unresolved  (?): The learners seem unable to determine how to respond \\nto the feedback (in the case of editing) or seem reluctant to accept the \\nreformulation but cannot agree on an alternative.   \\n\\n\\x0c334\\n\\nNeomy Storch and Gillian Wigglesworth\\n\\n   \\n\\n 5.         LREs that deal with language items targeted by the feedback are further \\n\\nanalyzed for the nature of engagement. \\n\\n       \\u2022       LREs  which  show  extensive  engagement  (EE):   episodes  where  learners \\noffer  suggestions  and  counter  suggestions,  explanations,  or  any  com-\\nments  showing  evidence  of  meta-awareness  of  the  feedback  received \\n(e.g.,  We  don\\u2019t  have  to  use  being ). It also includes episodes where the \\ncorrection is repeated by learners a number of times.  \\n\\n   \\n\\n  \\u2022       LREs  which  show  limited  or  no  engagement  (LE):   episodes  where  one \\nmember of the pair just reads the feedback and the other simply acknowl-\\nedged or repeats it once, without making any other additional comments.   \\n\\n      \\n\\n\\x0c']\n"
     ]
    }
   ],
   "source": [
    "files = loadTextData()\n",
    "print files\n",
    "print len(files)\n",
    "text = []\n",
    "textAnalytics = []\n",
    "count = 0\n",
    "for each in files:\n",
    "    text.append(get_text(each))\n",
    "    analysis = runTextAnalysisSpacey(text[count])\n",
    "    textAnalytics.append(analysis)\n",
    "    print count\n",
    "    count = count + 1\n",
    "print text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(multi-source deep learning for human pose estimation\n",
      "\n",
      "wanli ouyang xiao chu xiaogang wang\n",
      "\n",
      "department of electronic engineering, the chinese university of hong kong\n",
      "\n",
      "wlouyang@ee.cuhk.edu.hk, xgwang@ee.cuhk.edu.hk\n",
      "\n",
      "abstract\n",
      "\n",
      "visual appearance score, appearance mixture type and\n",
      "deformation are three important information sources for\n",
      "human pose estimation., {'neg': 0.0, 'neu': 0.959, 'pos': 0.041, 'compound': 0.2023})\n",
      "(this paper proposes to build a\n",
      "multi-source deep model in order to extract non-linear\n",
      "representation from these different aspects of information\n",
      "sources. with the deep model, the global,, {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(high-order hu-\n",
      "man body articulation patterns in these information sources\n",
      "are extracted for pose estimation., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(the task for estimat-\n",
      "ing body locations and the task for human detection are\n",
      "jointly learned using a uniﬁed deep model., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(the proposed\n",
      "approach can be viewed as a post-processing of pose esti-\n",
      "mation results and can ﬂexibly integrate with existing meth-\n",
      "ods by taking their information sources as input. by extract-\n",
      ", {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(ing the non-linear representation from multiple information\n",
      "sources, the deep model outperforms state-of-the-art by up\n",
      "to 8.6 percent on three public benchmark datasets.\n",
      "\n",
      "1., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(new types of deep neural network learning for speech recognition \n",
      "\n",
      "and related applications: an overview  \n",
      "\n",
      " \n",
      "\n",
      "li deng1, geoffrey hinton2, and brian kingsbury3 \n",
      "\n",
      "1microsoft research, redmond, wa, usa  \n",
      "2university of toronto, ontario, canada  \n",
      "\n",
      "3ibm t. j. watson research center, yorktown heights, ny, usa \n",
      "\n",
      "abstract \n",
      "\n",
      " \n",
      "in  this  paper,  we  provide  an  overview  of  the  invited  and \n",
      "contributed  papers  presented  at  the  special  session  at  icassp-\n",
      "2013,  entitled  “, {'neg': 0.0, 'neu': 0.93, 'pos': 0.07, 'compound': 0.5859})\n",
      "(new  types  of  deep  neural  network  learning  for \n",
      "speech  recognition  and  related  applications,”  as  organized  by \n",
      "the  authors.  , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(we  also  describe  the  historical  context  in  which \n",
      "acoustic  models  based  on  deep  neural  networks  have  been \n",
      "developed. \n",
      "     , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(the  technical  overview  of  the  papers  presented  in  our  special \n",
      "session  is  organized  into  five  ways  of  improving  deep  learning \n",
      "methods:  (1)  better  optimization;  (2)  better  types  of  neural \n",
      "activation  function  and  better  network  architectures;  (3)  better \n",
      "ways  to  determine  the  myriad  hyper-parameters  of  deep  neural \n",
      "networks; (4) more appropriate ways to preprocess speech for deep \n",
      "neural networks; and (5) ways of leveraging multiple languages or \n",
      "dialects  that  are  more  easily  achieved  with  deep  neural  networks \n",
      "than with gaussian mixture models.  \n",
      " \n",
      "\n",
      "index terms— deep neural network, convolutional neural \n",
      "\n",
      "network, recurrent neural network, optimization, spectrogram \n",
      "features, multitask, multilingual, speech recognition, music \n",
      "processing \n",
      " \n",
      "\n",
      "1., {'neg': 0.0, 'neu': 0.788, 'pos': 0.212, 'compound': 0.9719})\n",
      "(ieee transactions on big data, tbd-2015-05-0037 \n",
      "\n",
      "1 \n",
      "\n",
      "methodologies for cross-domain data \n",
      "\n",
      "fusion: an overview \n",
      "\n",
      "yu zheng, senior member \n",
      "\n",
      "abstract— traditional data mining usually deals with data from a single domain. in the big data era, we face a diversity of datasets \n",
      "from  different  sources  in  different  domains.  , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(these  datasets  consist  of  multiple  modalities,  each  of  which  has  a  different \n",
      "representation,  distribution, scale,  and  density.  how  to  unlock  the power  of  knowledge  from multiple  disparate  (but  potentially \n",
      "connected) datasets is paramount in big data research, essentially distinguishing big data from traditional data mining tasks., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(this \n",
      "calls  for  advanced  techniques  that  can  fuse  the  knowledge  from  various  datasets  organically  in  a  machine  learning  and  data \n",
      "mining task., {'neg': 0.0, 'neu': 0.909, 'pos': 0.091, 'compound': 0.25})\n",
      "(this paper summarizes the data fusion methodologies, classifying them into three categories: stage-based, feature \n",
      "level-based, and semantic meaning-based data fusion methods., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(the last category of data fusion methods is further divided into \n",
      "four groups: multi-view learning-based, similarity-based, probabilistic dependency-based, and transfer learning-based methods. \n",
      ", {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(these methods focus on knowledge fusion rather than schema mapping and data merging, significantly distinguishing between \n",
      "cross-domain data fusion and traditional data fusion studied in the database community., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(this paper does not only introduce high-\n",
      "level principles of each category of methods, but also give examples in which these techniques are used to handle real big data \n",
      "problems.  in  addition,  , {'neg': 0.103, 'neu': 0.897, 'pos': 0.0, 'compound': -0.5499})\n",
      "(this  paper  positions  existing  works  in  a  framework,  exploring  the  relationship  and  difference  between \n",
      "different data fusion methods., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(this paper will help a wide range of communities find a solution for data fusion in big data projects. \n",
      "\n",
      ", {'neg': 0.0, 'neu': 0.75, 'pos': 0.25, 'compound': 0.6124})\n",
      "(index terms— big data, cross-domain data mining, data fusion, multi-modality data representation, deep neural networks, \n",
      "multi-view learning, matrix factorization, probabilistic graphical models, transfer learning, urban computing.    \n",
      "\n",
      "——————————   , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(   —————————— \n",
      "\n",
      "1  , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(http://www.diva-portal.org\n",
      "\n",
      "this is version of a paper published in pattern recognition letters.\n",
      "\n",
      ", {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(citation for the original published paper (version of record):\n",
      "\n",
      "längkvist, m., karlsson, l., loutfi, a. (2014)\n",
      "a review of unsupervised feature learning and deep learning for time-series modeling.\n",
      ", {'neg': 0.0, 'neu': 0.919, 'pos': 0.081, 'compound': 0.3182})\n",
      "(pattern recognition letters, 42(1): 11-24\n",
      "http://dx.doi.org/10.1016/j.patrec.2014.01.008\n",
      "\n",
      "access to the published version may require subscription.\n",
      "\n",
      "n.b., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(when citing this work, cite the original published paper.\n",
      "\n",
      ", {'neg': 0.0, 'neu': 0.777, 'pos': 0.223, 'compound': 0.3182})\n",
      "(permanent link to this version:\n",
      "http://urn.kb.se/resolve?urn=urn:nbn:se:oru:diva-34597\n",
      "\n",
      "\f",
      "a review of unsupervised feature learning and deep\n",
      "\n",
      "learning for time-series modeling\n",
      "\n",
      "martin längkvista,∗, lars karlssona, amy lout\u001c",
      "a\n",
      "\n",
      "aapplied autonomous sensor systems, school of science and technology, örebro\n",
      "\n",
      "university, se-701 82, örebro, sweden\n",
      "\n",
      "abstract\n",
      "\n",
      "this paper gives a review of the recent developments in deep learning and un-\n",
      "\n",
      "supervised feature learning for time-series problems. while these techniques\n",
      "\n",
      "have shown promise for modeling static data, such as computer vision, ap-\n",
      "\n",
      "plying them to time-series data is gaining increasing attention., {'neg': 0.031, 'neu': 0.888, 'pos': 0.081, 'compound': 0.5267})\n",
      "(this paper\n",
      "\n",
      "overviews the particular challenges present in time-series data and provides a\n",
      "\n",
      "review of the works that have either applied time-series data to unsupervised\n",
      "\n",
      "feature learning algorithms or alternatively have contributed to modi\u001c",
      "cations\n",
      "\n",
      "of feature learning algorithms to take into account the challenges present in\n",
      "\n",
      "time-series data.\n",
      "\n",
      ", {'neg': 0.0, 'neu': 0.945, 'pos': 0.055, 'compound': 0.1531})\n",
      "(keywords:\n",
      "\n",
      "time-series, unsupervised feature learning, deep learning\n",
      "\n",
      "1., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(deep domain confusion: maximizing for domain invariance\n",
      "\n",
      "eric tzeng, judy hoffman, ning zhang\n",
      "\n",
      "uc berkeley, eecs & icsi\n",
      "\n",
      "{etzeng,jhoffman,nzhang}@eecs.berkeley.edu\n",
      "\n",
      "kate saenko\n",
      "\n",
      "umass lowell, cs\n",
      "saenko@cs.uml.edu\n",
      "\n",
      "trevor darrell\n",
      "\n",
      "uc berkeley, eecs & icsi\n",
      "\n",
      "trevor@eecs.berkeley.edu\n",
      "\n",
      "4\n",
      "1\n",
      "0\n",
      "2\n",
      " \n",
      "c\n",
      "e\n",
      "d\n",
      "0\n",
      "1\n",
      "\n",
      " \n",
      "\n",
      " \n",
      " \n",
      "]\n",
      "\n",
      "v\n",
      "c\n",
      ".\n",
      ", {'neg': 0.068, 'neu': 0.932, 'pos': 0.0, 'compound': -0.296})\n",
      "(s\n",
      "c\n",
      "[\n",
      " \n",
      " \n",
      "\n",
      "1\n",
      "v\n",
      "4\n",
      "7\n",
      "4\n",
      "3\n",
      "\n",
      ".\n",
      "\n",
      ", {'neg': 0.0, 'neu': 0.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(2\n",
      "1\n",
      "4\n",
      "1\n",
      ":\n",
      "v\n",
      "i\n",
      "x\n",
      "r\n",
      "a\n",
      "\n",
      "abstract\n",
      "\n",
      "recent reports suggest that a generic supervised deep\n",
      "cnn model trained on a large-scale dataset reduces, but\n",
      "does not remove, dataset bias on a standard benchmark.\n",
      ", {'neg': 0.0, 'neu': 0.941, 'pos': 0.059, 'compound': 0.1139})\n",
      "(fine-tuning deep models in a new domain can require a\n",
      "signiﬁcant amount of data, which for many applications is\n",
      "simply not available., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(we propose a new cnn architecture\n",
      "which introduces an adaptation layer and an additional do-\n",
      "main confusion loss, to learn a representation that is both\n",
      "semantically meaningful and domain invariant., {'neg': 0.142, 'neu': 0.786, 'pos': 0.072, 'compound': -0.296})\n",
      "(we addi-\n",
      "tionally show that a domain confusion metric can be used\n",
      "for model selection to determine the dimension of an adap-\n",
      "tation layer and the best position for the layer in the cnn\n",
      "architecture., {'neg': 0.057, 'neu': 0.833, 'pos': 0.109, 'compound': 0.4588})\n",
      "(our proposed adaptation method offers em-\n",
      "pirical performance which exceeds previously published re-\n",
      "sults on a standard benchmark visual domain adaptation\n",
      "task.\n",
      "\n",
      "1., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(multilingual models for compositional distributed semantics\n",
      "\n",
      "karl moritz hermann and phil blunsom\n",
      "\n",
      "department of computer science\n",
      "\n",
      "{karl.moritz.hermann,phil.blunsom}@cs.ox.ac.uk\n",
      "\n",
      "university of oxford\n",
      "oxford, ox1 3qd, uk\n",
      "\n",
      "4\n",
      "1\n",
      "0\n",
      "2\n",
      "\n",
      " \n",
      "r\n",
      "p\n",
      "a\n",
      "7\n",
      "1\n",
      "\n",
      " \n",
      "\n",
      " \n",
      " \n",
      "]\n",
      "l\n",
      "c\n",
      ".\n",
      ", {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(s\n",
      "c\n",
      "[\n",
      " \n",
      " \n",
      "\n",
      "1\n",
      "v\n",
      "1\n",
      "4\n",
      "6\n",
      "4\n",
      "\n",
      ".\n",
      "\n",
      "4\n",
      "0\n",
      "4\n",
      "1\n",
      ":\n",
      "v\n",
      "i\n",
      "x\n",
      "r\n",
      "a\n",
      "\n",
      "abstract\n",
      "\n",
      "we present a novel technique for learn-\n",
      "ing semantic representations, which ex-\n",
      "tends the distributional hypothesis to mul-\n",
      "tilingual data and joint-space embeddings.\n",
      "our models leverage parallel data and\n",
      "learn to strongly align the embeddings of\n",
      "semantically equivalent sentences, while\n",
      "maintaining sufﬁcient distance between\n",
      "those of dissimilar sentences., {'neg': 0.0, 'neu': 0.866, 'pos': 0.134, 'compound': 0.5859})\n",
      "(the mod-\n",
      "els do not rely on word alignments or\n",
      "any syntactic information and are success-\n",
      "fully applied to a number of diverse lan-\n",
      "guages., {'neg': 0.0, 'neu': 0.807, 'pos': 0.193, 'compound': 0.6444})\n",
      "(we extend our approach to learn\n",
      "semantic representations at the document\n",
      "level, too., {'neg': 0.0, 'neu': 0.876, 'pos': 0.124, 'compound': 0.1779})\n",
      "(we evaluate these models on\n",
      "two cross-lingual document classiﬁcation\n",
      "tasks, outperforming the prior state of the\n",
      "art. through qualitative analysis and the\n",
      "study of pivoting effects, {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(we demonstrate\n",
      "that our representations are semantically\n",
      "plausible and can capture semantic rela-\n",
      "tionships across languages without paral-\n",
      "lel data.\n",
      "\n",
      ", {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(deep learning:\n",
      "methods and applications\n",
      "\n",
      "li deng\n",
      "microsoft research\n",
      "one microsoft way\n",
      "redmond, wa 98052; usa\n",
      "deng@microsoft.com\n",
      "dong yu\n",
      "microsoft research\n",
      "one microsoft way\n",
      "redmond, wa 98052; usa\n",
      "dong.yu@microsoft.com\n",
      "\n",
      "boston — delft\n",
      "\n",
      "full text available at: http://dx.doi.org/10.1561/2000000039\f",
      "foundations and trends r(cid:1) in signal processing\n",
      "\n",
      "published, sold and distributed by:\n",
      "now publishers inc.\n",
      ", {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(po box 1024\n",
      "hanover, ma 02339\n",
      "united states\n",
      "tel. +1-781-985-4510\n",
      "www.nowpublishers.com\n",
      "sales@nowpublishers.com\n",
      "outside north america:\n",
      "now publishers inc.\n",
      ", {'neg': 0.161, 'neu': 0.714, 'pos': 0.125, 'compound': -0.2023})\n",
      "(po box 179\n",
      "2600 ad delft\n",
      "the netherlands\n",
      "tel., {'neg': 0.31, 'neu': 0.69, 'pos': 0.0, 'compound': -0.5574})\n",
      "(+31-6-51115274\n",
      "the preferred citation for this publication is\n",
      "l. deng and d. yu. deep learning: methods and applications., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(foundations and\n",
      "trends r(cid:1) in signal processing, vol. 7, nos., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(3–4, pp. 197–387, 2013.\n",
      ", {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(this foundations and trends r(cid:1) issue was typeset in latex using a class ﬁle designed\n",
      "by neal parikh. printed on acid-free paper.\n",
      ", {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(isbn: 978-1-60198-815-7\n",
      "c(cid:1) 2014 l. deng and d. yu\n",
      "\n",
      "all rights reserved., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(no part of this publication may be reproduced, stored in a retrieval\n",
      "system, or transmitted in any form or by any means, mechanical, photocopying, recording\n",
      "or otherwise, without prior written permission of the publishers.\n",
      "photocopying. in the usa: this journal is registered at the copyright clearance cen-\n",
      "ter, inc., {'neg': 0.045, 'neu': 0.955, 'pos': 0.0, 'compound': -0.296})\n",
      "(, 222 rosewood drive, danvers, ma 01923., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(authorization to photocopy items for\n",
      "internal or personal use, or the internal or personal use of speciﬁc clients, is granted by\n",
      "now publishers inc for users registered with the copyright clearance center (ccc)., {'neg': 0.0, 'neu': 0.941, 'pos': 0.059, 'compound': 0.25})\n",
      "(the\n",
      "‘services’ for users can be found on the internet at: www.copyright.com\n",
      "for those organizations that have been granted a photocopy license, a separate system\n",
      "of payment has been arranged., {'neg': 0.0, 'neu': 0.931, 'pos': 0.069, 'compound': 0.25})\n",
      "(authorization does not extend to other kinds of copy-\n",
      "ing, such as that for general distribution, for advertising or promotional purposes, for\n",
      "creating new collective works, or for resale. in the rest of the world: permission to pho-\n",
      "tocopy must be obtained from the copyright owner., {'neg': 0.032, 'neu': 0.922, 'pos': 0.046, 'compound': 0.1734})\n",
      "(please apply to now publishers inc.,\n",
      "po box 1024, hanover, ma 02339, usa; tel., {'neg': 0.201, 'neu': 0.67, 'pos': 0.128, 'compound': -0.3182})\n",
      "(+1 781 871 0245; www.nowpublishers.com;\n",
      "sales@nowpublishers.com\n",
      "now publishers inc., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(has an exclusive license to publish this material worldwide., {'neg': 0.0, 'neu': 0.842, 'pos': 0.158, 'compound': 0.128})\n",
      "(permission\n",
      "to use this content must be obtained from the copyright license holder., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(please apply to\n",
      "now publishers, po box 179, 2600 ad delft, the netherlands, www.nowpublishers.com;\n",
      "e-mail: sales@nowpublishers.com\n",
      "\n",
      "full text available at: http://dx.doi.org/10.1561/2000000039\f",
      "foundations and trends r(cid:1) in signal processing\n",
      "\n",
      "volume 7, issues 3–4, 2013\n",
      "\n",
      "editorial board\n",
      "\n",
      "editor-in-chief\n",
      "\n",
      "yonina eldar\n",
      "technion - israel institute of technology\n",
      "israel\n",
      "editors\n",
      "\n",
      "robert m. gray\n",
      "founding editor-in-chief\n",
      "stanford university\n",
      "pao-chi chang\n",
      "ncu, taiwan\n",
      "pamela cosman\n",
      "uc san diego\n",
      "michelle eﬀros\n",
      "caltech\n",
      "yariv ephraim\n",
      "gmu\n",
      "alfonso farina\n",
      "selex es\n",
      "sadaoki furui\n",
      "tokyo tech\n",
      "georgios giannakis\n",
      "university of minnesota\n",
      "vivek goyal\n",
      "boston university\n",
      "sinan gunturk\n",
      "courant institute\n",
      "christine guillemot\n",
      "inria\n",
      "robert w. heath, jr.\n",
      ", {'neg': 0.036, 'neu': 0.94, 'pos': 0.023, 'compound': -0.3182})\n",
      "(ut austin\n",
      "\n",
      "sheila hemami\n",
      "cornell university\n",
      "lina karam\n",
      "arizona state u\n",
      "nick kingsbury\n",
      "university of cambridge\n",
      "alex kot\n",
      "ntu, singapore\n",
      "jelena kovacevic\n",
      "cmu\n",
      "geert leus\n",
      "tu delft\n",
      "jia li\n",
      "penn state\n",
      "henrique malvar\n",
      "microsoft research\n",
      "b.s., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(manjunath\n",
      "uc santa barbara\n",
      "urbashi mitra\n",
      "usc\n",
      "björn ottersten\n",
      "kth stockholm\n",
      "thrasos pappas\n",
      "northwestern university\n",
      "\n",
      "vincent poor\n",
      "princeton university\n",
      "anna scaglione\n",
      "uc davis\n",
      "mihaela van der shaar\n",
      "ucla\n",
      "nicholas d. sidiropoulos\n",
      "tu crete\n",
      "michael unser\n",
      "epfl\n",
      "p. p. vaidyanathan\n",
      "caltech\n",
      "ami wiesel\n",
      "hebrew u\n",
      "min wu\n",
      "university of maryland\n",
      "josiane zerubia\n",
      "inria\n",
      "\n",
      "full text available at: http://dx.doi.org/10.1561/2000000039\f",
      "editorial scope\n",
      "\n",
      "topics\n",
      "foundations and trends r(cid:1) in signal processing publishes survey and\n",
      "tutorial articles in the following topics:\n",
      "\n",
      "processing\n",
      "\n",
      "processing\n",
      "\n",
      "• adaptive signal processing\n",
      "• audio signal processing\n",
      "• biological and biomedical signal\n",
      "• complexity in signal processing\n",
      "• digital signal processing\n",
      "• distributed and network signal\n",
      "• image and video processing\n",
      "• linear and nonlinear ﬁltering\n",
      "• multidimensional signal\n",
      "• multimodal signal processing\n",
      "• multirate signal processing\n",
      "• multiresolution signal processing\n",
      "• nonlinear signal processing\n",
      "• randomized algorithms in signal\n",
      "• sensor and multiple source signal\n",
      "\n",
      "processing\n",
      "\n",
      "processing\n",
      "\n",
      "processing, source separation\n",
      "\n",
      "• signal decompositions, subband\n",
      "and transform methods, sparse\n",
      "representations\n",
      "\n",
      "• signal processing for\n",
      "\n",
      "communications\n",
      "\n",
      "• signal processing for security and\n",
      "forensic analysis, biometric signal\n",
      "processing\n",
      "\n",
      "• signal quantization, sampling,\n",
      "analog-to-digital conversion,\n",
      "coding and compression\n",
      "• signal reconstruction,\n",
      "\n",
      "digital-to-analog conversion,\n",
      "enhancement, decoding and\n",
      "inverse problems\n",
      "\n",
      "• speech/audio/image/video\n",
      "\n",
      "• speech and spoken language\n",
      "\n",
      "compression\n",
      "\n",
      "processing\n",
      "\n",
      "• statistical/machine learning\n",
      "• statistical signal processing\n",
      "\n",
      "information for librarians\n",
      "foundations and trends r(cid:1) in signal processing, 2013, volume 7, 4 issues., {'neg': 0.025, 'neu': 0.964, 'pos': 0.011, 'compound': -0.5267})\n",
      "(issn\n",
      "paper version 1932-8346., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(issn online version 1932-8354. also available as a\n",
      "combined paper and online subscription, {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(.\n",
      "\n",
      "full text available at: http://dx.doi.org/10.1561/2000000039\f",
      "foundations and trends r(cid:1) in signal processing\n",
      "vol. 7, nos., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(3–4 (2013) 197–387\n",
      "c(cid:1) 2014 l. deng and d., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(yu\n",
      "doi: 10.1561/2000000039\n",
      "\n",
      "deep learning: methods and applications\n",
      "\n",
      "li deng\n",
      "\n",
      "microsoft research\n",
      "one microsoft way\n",
      "\n",
      "redmond, wa 98052; usa\n",
      "\n",
      "deng@microsoft.com\n",
      "\n",
      "dong yu\n",
      "\n",
      "microsoft research\n",
      "one microsoft way\n",
      "\n",
      "redmond, wa 98052; usa\n",
      "dong.yu@microsoft.com\n",
      "\n",
      "full text available at: http://dx.doi.org/10.1561/2000000039\f",
      "contents\n",
      "\n",
      "endorsement\n",
      "\n",
      "1, {'neg': 0.0, 'neu': 0.943, 'pos': 0.057, 'compound': 0.3182})\n",
      "(hindawi publishing corporation\n",
      "advances in artiﬁcial neural systems\n",
      "volume 2012, article id 107046, 9 pages\n",
      "doi:10.1155/2012/107046\n",
      "\n",
      "research article\n",
      "sleep stage classiﬁcation using unsupervised feature learning\n",
      "\n",
      "martin l¨angkvist, lars karlsson, and amy loutﬁ\n",
      "\n",
      "center for applied autonomous sensor systems, ¨orebro university, 701 82 ¨orebro, sweden\n",
      "\n",
      "correspondence should be addressed to amy loutﬁ, amy.loutﬁ@oru.se\n",
      "\n",
      "received 17 february 2012; revised 5 may 2012; accepted 6 may 2012\n",
      "\n",
      "academic editor: juan manuel gorriz saez\n",
      "\n",
      "copyright © 2012 martin l¨angkvist et al., {'neg': 0.0, 'neu': 0.972, 'pos': 0.028, 'compound': 0.2732})\n",
      "(this is an open access article distributed under the creative commons attribution\n",
      "license, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly\n",
      "cited.\n",
      "\n",
      ", {'neg': 0.0, 'neu': 0.843, 'pos': 0.157, 'compound': 0.6369})\n",
      "(most attempts at training computers for the diﬃcult and time-consuming task of sleep stage classiﬁcation involve a feature\n",
      "extraction step., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(due to the complexity of multimodal sleep data, the size of the feature space can grow to the extent that it is\n",
      "also necessary to include a feature selection step. in this paper, we propose the use of an unsupervised feature learning architecture\n",
      "called deep belief nets (dbns) and show how to apply it to sleep data in order to eliminate the use of handmade features. using\n",
      "a postprocessing step of hidden markov model (hmm) to accurately capture sleep stage switching, we compare our results to a\n",
      "feature-based approach., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(a study of anomaly detection with the application to home environment data collection is also presented.\n",
      ", {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(the results using raw data with a deep architecture, such as the dbn, were comparable to a feature-based approach when validated\n",
      "on clinical datasets.\n",
      "\n",
      "1., {'neg': 0.0, 'neu': 0.921, 'pos': 0.079, 'compound': 0.2263})\n",
      "(zero-shot learning through cross-modal transfer\n",
      "\n",
      "richard socher, milind ganjoo, christopher d. manning, andrew y. ng\n",
      "computer science department, stanford university, stanford, ca 94305, usa\n",
      "\n",
      "richard@socher.org, {mganjoo, manning}@stanford.edu, ang@cs.stanford.edu\n",
      "\n",
      "abstract\n",
      "\n",
      "this work introduces a model that can recognize objects in images even if no\n",
      "training data is available for the object class., {'neg': 0.043, 'neu': 0.957, 'pos': 0.0, 'compound': -0.296})\n",
      "(the only necessary knowledge about\n",
      "unseen visual categories comes from unsupervised text corpora. unlike previous\n",
      "zero-shot learning models, which can only differentiate between unseen classes,\n",
      ", {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(our model can operate on a mixture of seen and unseen classes, simultaneously\n",
      "obtaining state of the art performance on classes with thousands of training im-\n",
      "ages and reasonable performance on unseen classes., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(this is achieved by seeing\n",
      "the distributions of words in texts as a semantic space for understanding what ob-\n",
      "jects look like., {'neg': 0.0, 'neu': 0.889, 'pos': 0.111, 'compound': 0.3612})\n",
      "(our deep learning model does not require any manually deﬁned\n",
      "semantic or visual features for either words or images., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(images are mapped to be\n",
      "close to semantic word vectors corresponding to their classes, and the resulting\n",
      "image embeddings can be used to distinguish whether an image is of a seen or un-\n",
      "seen class., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(we then use novelty detection methods to differentiate unseen classes\n",
      "from seen classes., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(we demonstrate two novelty detection strategies; the ﬁrst gives\n",
      "high accuracy on unseen classes, while the second is conservative in its prediction\n",
      "of novelty and keeps the seen classes’ accuracy high.\n",
      "\n",
      ", {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(4\n",
      "1\n",
      "0\n",
      "2\n",
      "\n",
      " \n",
      "\n",
      "v\n",
      "o\n",
      "n\n",
      "0\n",
      "1\n",
      "\n",
      " \n",
      "\n",
      " \n",
      " \n",
      "]\n",
      "\n",
      ", {'neg': 0.0, 'neu': 0.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(g\n",
      "l\n",
      ".\n",
      ", {'neg': 0.0, 'neu': 0.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(s\n",
      "c\n",
      "[\n",
      " \n",
      " \n",
      "\n",
      "1\n",
      "v\n",
      "9\n",
      "3\n",
      "5\n",
      "2\n",
      "\n",
      ".\n",
      "\n",
      "1\n",
      "1\n",
      "4\n",
      "1\n",
      ":\n",
      "v\n",
      "i\n",
      "x\n",
      "r\n",
      "a\n",
      "\n",
      "unifying visual-semantic embeddings with\n",
      "\n",
      "multimodal neural language models\n",
      "\n",
      "ryan kiros, ruslan salakhutdinov, richard s. zemel\n",
      "\n",
      "university of toronto\n",
      "\n",
      "canadian institute for advanced research\n",
      "\n",
      "{rkiros, rsalakhu, zemel}@cs.toronto.edu\n",
      "\n",
      ", {'neg': 0.0, 'neu': 0.926, 'pos': 0.074, 'compound': 0.25})\n",
      "(abstract\n",
      "\n",
      "inspired by recent advances in multimodal learning and machine translation, we\n",
      "introduce an encoder-decoder pipeline that learns (a): a multimodal joint embed-\n",
      "ding space with images and text and (b): a novel language model for decoding\n",
      "distributed representations from our space., {'neg': 0.0, 'neu': 0.874, 'pos': 0.126, 'compound': 0.6705})\n",
      "(our pipeline effectively uniﬁes joint\n",
      "image-text embedding models with multimodal neural language models., {'neg': 0.0, 'neu': 0.805, 'pos': 0.195, 'compound': 0.4404})\n",
      "(we in-\n",
      "troduce the structure-content neural language model that disentangles the structure\n",
      "of a sentence to its content, conditioned on representations produced by the en-\n",
      "coder., {'neg': 0.0, 'neu': 0.949, 'pos': 0.051, 'compound': 0.0772})\n",
      "(the encoder allows one to rank images and sentences while the decoder\n",
      "can generate novel descriptions from scratch. using lstm to encode sentences,\n",
      "we match the state-of-the-art performance on flickr8k and flickr30k without\n",
      "using object detections., {'neg': 0.0, 'neu': 0.875, 'pos': 0.125, 'compound': 0.4019})\n",
      "(we also set new best results when using the 19-layer ox-\n",
      "ford, {'neg': 0.0, 'neu': 0.724, 'pos': 0.276, 'compound': 0.6369})\n",
      "(convolutional network., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(furthermore we show that with linear encoders, the\n",
      "learned embedding space captures multimodal regularities in terms of vector space\n",
      "arithmetic e.g. *image of a blue car* - \"blue\" + \"red\" is near images of red cars.\n",
      ", {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(sample captions generated for 800 images are made available for comparison.\n",
      "\n",
      "1\n",
      "\n",
      ", {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(recent advances in deep learning for speech research at microsoft  \n",
      "\n",
      " \n",
      "\n",
      "li deng, jinyu li, jui-ting huang, kaisheng yao, dong yu, frank seide, michael l. seltzer, geoff zweig, \n",
      "\n",
      "xiaodong he, jason williams, yifan gong, and alex acero \n",
      "\n",
      "microsoft corporation, one microsoft way, redmond, wa 98052, usa \n",
      "\n",
      " \n",
      "\n",
      "abstract \n",
      "\n",
      " \n",
      "deep  learning  is  becoming  a  mainstream  technology  for  speech \n",
      "recognition  at  industrial  scale.  in  this  paper,  we  provide  an \n",
      "overview of the  work by microsoft speech researchers since 2009 \n",
      "in this area, focusing on more recent advances which shed light to \n",
      "the  basic  capabilities  and  limitations  of  the  current  deep  learning \n",
      "technology.  , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(we  organize  this  overview  along  the  feature-domain \n",
      "and  model-domain  dimensions  according  to  the  conventional \n",
      "approach  to  analyzing  speech  systems.  , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(selected  experimental \n",
      "results, including speech recognition and related applications such \n",
      "as  spoken  dialogue  and  language  modeling,  are  presented  to \n",
      "demonstrate  and  analyze  the  strengths  and  weaknesses  of  the \n",
      "techniques described in the paper., {'neg': 0.069, 'neu': 0.856, 'pos': 0.075, 'compound': 0.0516})\n",
      "(potential improvement of these \n",
      "techniques and future research directions are discussed.  \n",
      " , {'neg': 0.0, 'neu': 0.769, 'pos': 0.231, 'compound': 0.4588})\n",
      "(index  terms—  deep  learning,  neural  network,  multilingual, \n",
      "speech recognition, spectral features, convolution, dialogue    \n",
      " \n",
      "\n",
      "1., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(modeling spatial-temporal clues in a hybrid deep\n",
      "\n",
      "learning framework for video classiﬁcation\n",
      "\n",
      "†\n",
      ", {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(zuxuan wu, xi wang, yu-gang jiang\n",
      "\n",
      ", hao ye, xiangyang xue\n",
      "\n",
      "school of computer science, shanghai key lab of intelligent information processing,\n",
      "\n",
      "fudan university, shanghai, china\n",
      "\n",
      "{zxwu, xwang10, ygj, haoye10, xyxue}@fudan.edu.cn\n",
      "\n",
      "5\n",
      "1\n",
      "0\n",
      "2\n",
      "\n",
      " \n",
      "r\n",
      "p\n",
      "a\n",
      "7\n",
      "\n",
      " \n",
      "\n",
      " \n",
      " \n",
      "]\n",
      "\n",
      "v\n",
      "c\n",
      ".\n",
      "s\n",
      "c\n",
      "[\n",
      " \n",
      " \n",
      "\n",
      "1\n",
      "v\n",
      "1\n",
      "6\n",
      "5\n",
      "1\n",
      "0\n",
      "\n",
      ".\n",
      "\n",
      "4\n",
      "0\n",
      "5\n",
      "1\n",
      ":\n",
      "v\n",
      "i\n",
      "x\n",
      "r\n",
      "a\n",
      "\n",
      "abstract\n",
      "classifying videos according to content semantics is an im-\n",
      "portant problem with a wide range of applications. in this\n",
      "paper, we propose a hybrid deep learning framework for\n",
      "video classiﬁcation, which is able to model static spatial\n",
      "information, short-term motion, as well as long-term tem-\n",
      "poral clues in the videos., {'neg': 0.032, 'neu': 0.907, 'pos': 0.061, 'compound': 0.34})\n",
      "(speciﬁcally, the spatial and the\n",
      "short-term motion features are extracted separately by two\n",
      "convolutional neural networks (cnn)., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(these two types of\n",
      "cnn-based features are then combined in a regularized fea-\n",
      "ture fusion network for classiﬁcation, which is able to learn\n",
      "and utilize feature relationships for improved performance.\n",
      ", {'neg': 0.0, 'neu': 0.9, 'pos': 0.1, 'compound': 0.4767})\n",
      "(in addition, long short term memory (lstm) networks\n",
      "are applied on top of the two features to further model\n",
      "longer-term temporal clues., {'neg': 0.0, 'neu': 0.921, 'pos': 0.079, 'compound': 0.2023})\n",
      "(the main contribution of this\n",
      "work is the hybrid learning framework that can model sev-\n",
      "eral important aspects of the video data., {'neg': 0.0, 'neu': 0.921, 'pos': 0.079, 'compound': 0.2023})\n",
      "(we also show\n",
      "that (1) combining the spatial and the short-term motion\n",
      "features in the regularized fusion network is better than di-\n",
      "rect classiﬁcation and fusion using the cnn with a softmax\n",
      "layer, and (2) the sequence-based lstm is highly comple-\n",
      "mentary to the traditional classiﬁcation strategy without\n",
      "considering the temporal frame orders., {'neg': 0.0, 'neu': 0.946, 'pos': 0.054, 'compound': 0.4404})\n",
      "(extensive experi-\n",
      "ments are conducted on two popular and challenging bench-\n",
      "marks, the ucf-101 human actions and the columbia con-\n",
      "sumer videos (ccv). on both benchmarks, our framework\n",
      "achieves to-date the best reported performance: 91.3% on\n",
      "the ucf-101 and 83.5% on the ccv.\n",
      "\n",
      ", {'neg': 0.0, 'neu': 0.823, 'pos': 0.177, 'compound': 0.8225})\n",
      "(categories and subject descriptors\n",
      "h.3.1 [information storage and retrieval]: content\n",
      "analysis and indexing—indexing methods; i.5.2 [pattern\n",
      "recognition]: design methodology—classiﬁer design and\n",
      "evaluation\n",
      "\n",
      "keywords\n",
      "video classiﬁcation, deep learning, cnn, lstm, fusion.\n",
      "\n",
      "1.\n",
      "\n",
      ", {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(deep learning for robust feature generation\n",
      "\n",
      "in audiovisual emotion recognition\n",
      "yelin kim, honglak lee, and emily mower provost ∗\n",
      "\n",
      "electrical engineering and computer science, ann arbor, michigan, usa\n",
      "\n",
      "{yelinkim, honglak, emilykmp}@umich.edu\n",
      "\n",
      "university of michigan\n",
      "\n",
      "abstract\n",
      "\n",
      "automatic emotion recognition systems predict high-level affective\n",
      "content from low-level human-centered signal cues., {'neg': 0.0, 'neu': 0.951, 'pos': 0.049, 'compound': 0.34})\n",
      "(these systems\n",
      "have seen great improvements in classiﬁcation accuracy, due in part\n",
      "to advances in feature selection methods. however, many of these\n",
      "feature selection methods capture only linear relationships between\n",
      "features or alternatively require the use of labeled data. in this paper\n",
      ", {'neg': 0.0, 'neu': 0.862, 'pos': 0.138, 'compound': 0.7506})\n",
      "(we focus on deep learning techniques, which can overcome these\n",
      "limitations by explicitly capturing complex non-linear feature inter-\n",
      "actions in multimodal data. we propose and evaluate a suite of deep\n",
      "belief network models, and, {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(demonstrate that these models show im-\n",
      "provement in emotion classiﬁcation performance over baselines that\n",
      "do not employ deep learning., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(this suggests that the learned high-\n",
      "order non-linear relationships are effective for emotion recognition.\n",
      ", {'neg': 0.0, 'neu': 0.807, 'pos': 0.193, 'compound': 0.4767})\n",
      "(index terms— emotion classiﬁcation, deep learning, multi-\n",
      "modal features, unsupervised feature learning, deep belief networks\n",
      "\n",
      "1., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(improving neural networks with dropout\n",
      "\n",
      "by\n",
      "\n",
      "nitish srivastava\n",
      "\n",
      "a thesis submitted in conformity with the requirements\n",
      "\n",
      "for the degree of master of science\n",
      "\n",
      "graduate department of computer science\n",
      "\n",
      "university of toronto\n",
      "\n",
      "c(cid:13) copyright 2013 by nitish srivastava\n",
      "\n",
      "\f",
      "abstract\n",
      "\n",
      "improving neural networks with dropout\n",
      "\n",
      "nitish srivastava\n",
      "\n",
      "master of science\n",
      "\n",
      "graduate department of computer science\n",
      "\n",
      "university of toronto\n",
      "\n",
      "2013\n",
      "\n",
      "deep neural nets with a huge number of parameters are very powerful machine learning systems., {'neg': 0.0, 'neu': 0.841, 'pos': 0.159, 'compound': 0.8832})\n",
      "(how-\n",
      "\n",
      "ever, overﬁtting is a serious problem in such networks., {'neg': 0.364, 'neu': 0.636, 'pos': 0.0, 'compound': -0.4588})\n",
      "(large networks are also slow to use, making it\n",
      "\n",
      "diﬃcult to deal with overﬁtting by combining many diﬀerent large neural nets at test time., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(dropout\n",
      "\n",
      "is a technique for addressing this problem., {'neg': 0.342, 'neu': 0.658, 'pos': 0.0, 'compound': -0.481})\n",
      "(the key idea is to randomly drop units (along with their\n",
      "\n",
      "connections) from a neural network during training., {'neg': 0.116, 'neu': 0.884, 'pos': 0.0, 'compound': -0.2732})\n",
      "(this prevents the units from co-adapting too much.\n",
      "\n",
      ", {'neg': 0.0, 'neu': 0.843, 'pos': 0.157, 'compound': 0.0772})\n",
      "(dropping units creates thinned networks during training., {'neg': 0.0, 'neu': 0.741, 'pos': 0.259, 'compound': 0.2732})\n",
      "(the number of possible thinned networks is\n",
      "\n",
      "exponential in the number of units in the network. at test time, {'neg': 0.0, 'neu': 0.867, 'pos': 0.133, 'compound': 0.1531})\n",
      "(all possible thinned networks are com-\n",
      "\n",
      "bined using an approximate model averaging procedure., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(dropout training followed by this approximate\n",
      "\n",
      "model combination signiﬁcantly reduces overﬁtting and gives major improvements over other regulariza-\n",
      "\n",
      "tion methods. in this work, we describe models that improve the performance of neural networks using\n",
      "\n",
      "dropout, often obtaining state-of-the-art results on benchmark datasets.\n",
      "\n",
      ", {'neg': 0.0, 'neu': 0.885, 'pos': 0.115, 'compound': 0.6369})\n",
      "(ii\n",
      "\n",
      "\f",
      "contents\n",
      "\n",
      "1, {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(deep fragment embeddings for bidirectional image\n",
      "\n",
      "sentence mapping\n",
      "\n",
      "andrej karpathy\n",
      "\n",
      "armand joulin\n",
      "\n",
      "department of computer science, stanford university, stanford, ca 94305, usa\n",
      "\n",
      "{karpathy,ajoulin,feifeili}@cs.stanford.edu\n",
      "\n",
      "li fei-fei\n",
      "\n",
      "abstract\n",
      "\n",
      "we introduce a model for bidirectional retrieval of images and sentences through\n",
      "a deep, multi-modal embedding of visual and natural language data. unlike pre-\n",
      "vious models that directly map images or sentences into a common embedding\n",
      "space, our model works on a ﬁner level and embeds fragments of images (ob-\n",
      "jects) and fragments of sentences (typed dependency tree relations) into a com-\n",
      "mon space., {'neg': 0.0, 'neu': 0.915, 'pos': 0.085, 'compound': 0.5267})\n",
      "(we then introduce a structured max-margin objective that allows our\n",
      "model to explicitly associate these fragments across modalities., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(extensive exper-\n",
      "imental evaluation shows that reasoning on both the global level of images and\n",
      "sentences and the ﬁner level of their respective fragments improves performance\n",
      "on image-sentence retrieval tasks., {'neg': 0.0, 'neu': 0.799, 'pos': 0.201, 'compound': 0.7003})\n",
      "(additionally, our model provides interpretable\n",
      "predictions for the image-sentence retrieval task since the inferred inter-modal\n",
      "alignment of fragments is explicit.\n",
      "\n",
      ", {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(effective multi(cid:173)modal retrieval based on stacked\n",
      "\n",
      "auto(cid:173)encoders\n",
      "\n",
      "wei wang†, beng chin ooi†, xiaoyan yang‡, dongxiang zhang†, yueting zhuang§\n",
      "\n",
      "†school of computing, national university of singapore, singapore\n",
      "\n",
      "‡advanced digital sciences center, illinois at singapore pte, singapore\n",
      "\n",
      "§college of computer science, zhejiang university, china\n",
      "\n",
      "†{wangwei, ooibc, zhangdo}@comp.nus.edu.sg, ‡xiaoyan.yang@adsc.com.sg, §yzhuang@zju.edu.cn\n",
      "\n",
      "abstract\n",
      "\n",
      "multi-modal retrieval is emerging as a new search paradigm that\n",
      "enables seamless information retrieval from various types of me-\n",
      "dia. for example, users can simply snap a movie poster to search\n",
      "relevant reviews and trailers. to solve the problem, a set of map-\n",
      "ping functions, {'neg': 0.029, 'neu': 0.92, 'pos': 0.052, 'compound': 0.296})\n",
      "(are learned to project high-dimensional features ex-\n",
      "tracted from data of different media types into a common low-\n",
      "dimensional space so that metric distance measures can be applied.\n",
      "in this paper, we propose an effective mapping mechanism based\n",
      "on deep learning (i.e., stacked auto-encoders) for multi-modal re-\n",
      "trieval., {'neg': 0.042, 'neu': 0.896, 'pos': 0.062, 'compound': 0.25})\n",
      "(mapping functions are learned by optimizing a new ob-\n",
      "jective function, which captures both intra-modal and inter-modal\n",
      "semantic relationships of data from heterogeneous sources effec-\n",
      "tively., {'neg': 0.0, 'neu': 0.889, 'pos': 0.111, 'compound': 0.4588})\n",
      "(compared with previous works which require a substan-\n",
      "tial amount of prior knowledge such as similarity matrices of intra-\n",
      "modal data and ranking examples, our method requires little prior\n",
      "knowledge. given a large training dataset, we split it into mini-\n",
      "batches and continually adjust the mapping functions for each batch\n",
      "of input., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(hence, our method is memory efﬁcient with respect to the\n",
      "data volume., {'neg': 0.0, 'neu': 0.78, 'pos': 0.22, 'compound': 0.4767})\n",
      "(experiments on three real datasets illustrate that our\n",
      "proposed method achieves signiﬁcant improvement in search accu-\n",
      "racy over the state-of-the-art methods.\n",
      "\n",
      "1.\n",
      "\n",
      ", {'neg': 0.0, 'neu': 0.875, 'pos': 0.125, 'compound': 0.4588})\n",
      "(deep learning for detecting robotic grasps\n",
      "\n",
      "ian lenz,† honglak lee,∗ and ashutosh saxena†\n",
      "\n",
      "† department of computer science, cornell university.\n",
      "\n",
      ", {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(∗ department of eecs, university of michigan, ann arbor.\n",
      "\n",
      ", {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(email: ianlenz@cs.cornell.edu, honglak@eecs.umich.edu, asaxena@cs.cornell.edu\n",
      "\n",
      "4\n",
      "1\n",
      "0\n",
      "2\n",
      "\n",
      " \n",
      "\n",
      "g\n",
      "u\n",
      "a\n",
      "1\n",
      "2\n",
      "\n",
      " \n",
      "\n",
      " \n",
      " \n",
      "]\n",
      "\n",
      "g\n",
      "l\n",
      ".\n",
      ", {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(s\n",
      "c\n",
      "[\n",
      " \n",
      " \n",
      "\n",
      "6\n",
      "v\n",
      "2\n",
      "9\n",
      "5\n",
      "3\n",
      "\n",
      ".\n",
      "\n",
      "1\n",
      "0\n",
      "3\n",
      "1\n",
      ":\n",
      "v\n",
      "i\n",
      "x\n",
      "r\n",
      "a\n",
      "\n",
      "abstract, {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(—we consider the problem of detecting robotic grasps\n",
      "in an rgb-d view of a scene containing objects. in this work, {'neg': 0.13, 'neu': 0.87, 'pos': 0.0, 'compound': -0.4019})\n",
      "(,\n",
      "we apply a deep learning approach to solve this problem, which\n",
      "avoids time-consuming hand-design of features., {'neg': 0.259, 'neu': 0.644, 'pos': 0.097, 'compound': -0.4633})\n",
      "(this presents two\n",
      "main challenges., {'neg': 0.0, 'neu': 0.755, 'pos': 0.245, 'compound': 0.0772})\n",
      "(first, we need to evaluate a huge number of\n",
      "candidate grasps. in order to make detection fast and robust, {'neg': 0.0, 'neu': 0.714, 'pos': 0.286, 'compound': 0.6124})\n",
      "(,\n",
      "we present a two-step cascaded system with two deep networks,\n",
      "where the top detections from the ﬁrst are re-evaluated by\n",
      "the second., {'neg': 0.0, 'neu': 0.917, 'pos': 0.083, 'compound': 0.2023})\n",
      "(the ﬁrst network has fewer features, is faster to\n",
      "run, and can effectively prune out unlikely candidate grasps.\n",
      ", {'neg': 0.0, 'neu': 0.854, 'pos': 0.146, 'compound': 0.4404})\n",
      "(the second, with more features,\n",
      "is slower but has to run\n",
      "only on the top few detections., {'neg': 0.0, 'neu': 0.879, 'pos': 0.121, 'compound': 0.296})\n",
      "(second, we need to handle\n",
      "multimodal inputs effectively, for which we present a method\n",
      "that applies structured regularization on the weights based on\n",
      "multimodal group regularization. we show that our method\n",
      "improves performance on an rgbd robotic grasping dataset,\n",
      "and can be used to successfully execute grasps on two different\n",
      "robotic platforms., {'neg': 0.0, 'neu': 0.844, 'pos': 0.156, 'compound': 0.836})\n",
      "(1\n",
      "\n",
      "keywords: robotic grasping, deep learning, rgb-d multi-\n",
      "modal data, baxter, pr2, 3d feature learning.\n",
      "\n",
      ", {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(i., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(simultaneous deep transfer across domains and tasks\n",
      "\n",
      "eric tzeng∗, judy hoffman∗, trevor darrell\n",
      "\n",
      "uc berkeley, eecs & icsi\n",
      "\n",
      "kate saenko\n",
      "\n",
      "umass lowell, cs\n",
      "\n",
      "{etzeng,jhoffman,trevor}@eecs.berkeley.edu\n",
      "\n",
      "saenko@cs.uml.edu\n",
      "\n",
      "abstract\n",
      "\n",
      "recent reports suggest that a generic supervised deep\n",
      "cnn model trained on a large-scale dataset reduces, but\n",
      "does not remove, dataset bias., {'neg': 0.0, 'neu': 0.968, 'pos': 0.032, 'compound': 0.1139})\n",
      "(fine-tuning deep models in\n",
      "a new domain can require a signiﬁcant amount of labeled\n",
      "data, which for many applications is simply not available.\n",
      ", {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(we propose a new cnn architecture to exploit unlabeled and\n",
      "sparsely labeled target domain data., {'neg': 0.097, 'neu': 0.903, 'pos': 0.0, 'compound': -0.1027})\n",
      "(our approach simulta-\n",
      "neously optimizes for domain invariance to facilitate domain\n",
      "transfer and uses a soft label distribution matching loss to\n",
      "transfer information between tasks., {'neg': 0.085, 'neu': 0.812, 'pos': 0.103, 'compound': 0.128})\n",
      "(our proposed adapta-\n",
      "tion method offers empirical performance which exceeds\n",
      "previously published results on two standard benchmark vi-\n",
      "sual domain adaptation tasks, evaluated across supervised\n",
      "and semi-supervised adaptation settings.\n",
      "\n",
      "1., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(cross-modal retrieval with correspondence autoencoder\n",
      "\n",
      "fangxiang feng\n",
      "\n",
      "telecommunications\n",
      "\n",
      "beijing, china\n",
      "\n",
      "xiaojie wang\n",
      "\n",
      "telecommunications\n",
      "\n",
      "beijing, china\n",
      "\n",
      "ruifan li\n",
      "\n",
      "telecommunications\n",
      "\n",
      "beijing, china\n",
      "\n",
      "rﬂi@bupt.edu.cn\n",
      "\n",
      "beijing university of posts and\n",
      "\n",
      "beijing university of posts and\n",
      "\n",
      "beijing university of posts and\n",
      "\n",
      "f.fangxiang@gmail.com\n",
      "\n",
      "xjwang@bupt.edu.cn\n",
      "\n",
      "abstract\n",
      "the problem of cross-modal retrieval, e.g., using a text query\n",
      "to search for images and vice-versa, is considered in this pa-\n",
      "per. a novel model involving correspondence autoencoder\n",
      "(corr-ae), {'neg': 0.039, 'neu': 0.928, 'pos': 0.033, 'compound': -0.1027})\n",
      "(is proposed here for solving this problem., {'neg': 0.297, 'neu': 0.475, 'pos': 0.228, 'compound': -0.184})\n",
      "(the\n",
      "model is constructed by correlating hidden representation-\n",
      "s of two uni-modal autoencoders., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(a novel optimal objec-\n",
      "tive, which minimizes a linear combination of representation\n",
      "learning errors for each modality and correlation learning er-\n",
      "ror between hidden representations of two modalities, is used\n",
      "to train the model as a whole., {'neg': 0.063, 'neu': 0.812, 'pos': 0.126, 'compound': 0.34})\n",
      "(minimization of correlation\n",
      "learning error forces the model to learn hidden representa-\n",
      "tions with only common information in diﬀerent modalities,\n",
      "while minimization of representation learning error makes\n",
      "hidden representations are good enough to reconstruct in-\n",
      "put of each modality., {'neg': 0.122, 'neu': 0.813, 'pos': 0.065, 'compound': -0.3612})\n",
      "(a parameter α is used to balance\n",
      "the representation learning error and the correlation learn-\n",
      "ing error., {'neg': 0.278, 'neu': 0.722, 'pos': 0.0, 'compound': -0.6597})\n",
      "(based on two diﬀerent multi-modal autoencoders,\n",
      "corr-ae is extended to other two correspondence models,\n",
      "here we called corr-cross-ae and corr-full-ae., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(the pro-\n",
      "posed models are evaluated on three publicly available data\n",
      "sets from real scenes., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(we demonstrate that the three cor-\n",
      "respondence autoencoders perform signiﬁcantly better than\n",
      "three canonical correlation analysis based models and two\n",
      "popular multi-modal deep models on cross-modal retrieval\n",
      "tasks.\n",
      "\n",
      ", {'neg': 0.0, 'neu': 0.82, 'pos': 0.18, 'compound': 0.6908})\n",
      "(categories and subject descriptors\n",
      "h.3.3 [information search and retrieval]: retrieval\n",
      "models; i.2.6 [artiﬁcial intelligence]: learning\n",
      "\n",
      "general terms\n",
      "algorithms; design\n",
      "\n",
      "keywords\n",
      "cross-modal; retrieval; image and text; deep learning; au-\n",
      "toencoder\n",
      "\n",
      "permission to make digital or hard copies of all or part of this work for personal or\n",
      "classroom use is granted without fee provided that copies are not made or distributed\n",
      "for proﬁt or commercial advantage and that copies bear this notice and the full cita-\n",
      "tion on theﬁrst page., {'neg': 0.017, 'neu': 0.934, 'pos': 0.049, 'compound': 0.3818})\n",
      "(copyrights for components of this work owned by others than\n",
      "acm must be honored., {'neg': 0.0, 'neu': 0.774, 'pos': 0.226, 'compound': 0.5859})\n",
      "(abstracting with credit is permitted. to copy, {'neg': 0.0, 'neu': 0.698, 'pos': 0.302, 'compound': 0.3818})\n",
      "(otherwise, or re-\n",
      "publish, to post on servers or to redistribute to lists, requires prior speciﬁc permission\n",
      "and/or a fee., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(request permissions from permissions@acm.org.\n",
      ", {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(mm’14, november 03–07, 2014, orlando, fl, usa, {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(.\n",
      "copyright 2014 acm 978-1-4503-3063-3/14/11 ...$15.00.\n",
      ", {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(http://dx.doi.org/10.1145/2647868.2654902.\n",
      "\n",
      "1.\n",
      "\n",
      ", {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(multimodal neural language models\n",
      "\n",
      "ryan kiros\n",
      "ruslan salakhutdinov\n",
      "richard zemel\n",
      "department of computer science, university of toronto\n",
      "canadian institute for advanced research\n",
      "\n",
      "rkiros@cs.toronto.edu\n",
      "rsalakhu@cs.toronto.edu\n",
      "zemel@cs.toronto.edu\n",
      "\n",
      "abstract\n",
      "\n",
      "we introduce two multimodal neural language\n",
      "models: models of natural language that can\n",
      "be conditioned on other modalities. an image-\n",
      "text multimodal neural language model can be\n",
      "used to retrieve images given complex sentence\n",
      "queries, retrieve phrase descriptions given image\n",
      "queries, as well as generate text conditioned on\n",
      "images., {'neg': 0.0, 'neu': 0.9, 'pos': 0.1, 'compound': 0.7096})\n",
      "(we show that in the case of image-text\n",
      "modelling we can jointly learn word representa-\n",
      "tions and image features by training our models\n",
      "together with a convolutional network., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(unlike\n",
      "many of the existing methods, our approach can\n",
      "generate sentence descriptions for images with-\n",
      "out the use of templates, structured prediction,\n",
      "and/or syntactic trees., {'neg': 0.0, 'neu': 0.949, 'pos': 0.051, 'compound': 0.0772})\n",
      "(while we focus on image-\n",
      "text modelling, our algorithms can be easily ap-\n",
      "plied to other modalities such as audio.\n",
      "\n",
      "1., {'neg': 0.0, 'neu': 0.893, 'pos': 0.107, 'compound': 0.34})\n",
      "(2013 ieee international conference on computer vision\n",
      "\n",
      "pedestrian parsing via deep decompositional network\n",
      "\n",
      "ping luo1,3\n",
      "\n",
      "xiaogang wang2\n",
      "\n",
      "1department of information engineering, the chinese university of hong kong\n",
      "2department of electronic engineering, the chinese university of hong kong\n",
      "3shenzhen institutes of advanced technology, chinese academy of sciences\n",
      "\n",
      "xiaoou tang1,3∗\n",
      "\n",
      ", {'neg': 0.0, 'neu': 0.92, 'pos': 0.08, 'compound': 0.4588})\n",
      "(pluo.lhi@gmail.com\n",
      "\n",
      ", {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(xgwang@ee.cuhk.edu.hk\n",
      "\n",
      ", {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(xtang@ie.cuhk.edu.hk\n",
      "\n",
      "abstract\n",
      "\n",
      "we propose a new deep decompositional network\n",
      "(ddn) for parsing pedestrian images into semantic regions,\n",
      "such as hair, head, body, arms, and legs, where the pedestri-\n",
      "ans can be heavily occluded., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(unlike existing methods based\n",
      "on template matching or bayesian inference, our approach\n",
      "directly maps low-level visual features to the label maps of\n",
      "body parts with ddn, which is able to accurately estimate\n",
      "complex pose variations with good robustness to occlusions\n",
      "and background clutters., {'neg': 0.0, 'neu': 0.935, 'pos': 0.065, 'compound': 0.4404})\n",
      "(ddn jointly estimates occluded\n",
      "regions and segments body parts by stacking three types\n",
      "of hidden layers: occlusion estimation layers, completion\n",
      "layers, and decomposition layers., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(the occlusion estimation\n",
      "layers estimate a binary mask, indicating which part of a\n",
      "pedestrian is invisible., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(the completion layers synthesize\n",
      "low-level features of the invisible part from the original\n",
      "features and the occlusion mask., {'neg': 0.0, 'neu': 0.881, 'pos': 0.119, 'compound': 0.3182})\n",
      "(the decomposition layers\n",
      "directly transform the synthesized visual features to label\n",
      "maps., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(we devise a new strategy to pre-train these hidden\n",
      "layers, and then ﬁne-tune the entire network using the\n",
      "stochastic gradient descent., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(experimental results show that\n",
      "our approach achieves better segmentation accuracy than\n",
      "the state-of-the-art methods on pedestrian images with or\n",
      "without occlusions., {'neg': 0.0, 'neu': 0.873, 'pos': 0.127, 'compound': 0.4404})\n",
      "(another important contribution of this\n",
      "paper is that it provides a large scale benchmark human\n",
      "parsing dataset1 that includes 3, 673 annotated samples\n",
      "collected from 171 surveillance videos., {'neg': 0.0, 'neu': 0.935, 'pos': 0.065, 'compound': 0.2023})\n",
      "(it is 20 times larger\n",
      "than existing public datasets.\n",
      "\n",
      "1., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(hierarchical face parsing via deep learning\n",
      "\n",
      "ping luo1,3\n",
      "\n",
      "xiaogang wang2,3\n",
      "\n",
      "xiaoou tang1,3\n",
      "\n",
      "1department of information engineering, the chinese university of hong kong\n",
      "2department of electronic engineering, the chinese university of hong kong\n",
      "3shenzhen institutes of advanced technology, chinese academy of sciences\n",
      "\n",
      "pluo.lhi@gmail.com\n",
      "\n",
      "xgwang@ee.cuhk.edu.hk\n",
      "\n",
      "xtang@ie.cuhk.edu.hk\n",
      "\n",
      "abstract\n",
      "\n",
      "this paper investigates how to parse (segment) facial\n",
      "components from face images which may be partially oc-\n",
      "cluded., {'neg': 0.0, 'neu': 0.969, 'pos': 0.031, 'compound': 0.25})\n",
      "(we propose a novel face parser, which recasts\n",
      "segmentation of face components as a cross-modality data\n",
      "transformation problem, i.e., transforming an image patch\n",
      "to a label map., {'neg': 0.1, 'neu': 0.815, 'pos': 0.085, 'compound': -0.1027})\n",
      "(speciﬁcally, a face is represented hierarchi-\n",
      "cally by parts, components, and pixel-wise labels. with this\n",
      "representation,, {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(our approach ﬁrst detects faces at both the\n",
      "part- and component-levels, and then computes the pixel-\n",
      "wise label maps (fig.1)., {'neg': 0.0, 'neu': 0.86, 'pos': 0.14, 'compound': 0.4767})\n",
      "(our part-based and component-\n",
      "based detectors are generatively trained with the deep belief\n",
      "network (dbn), and are discriminatively tuned by logistic\n",
      "regression., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(the segmentators transform the detected face\n",
      "components to label maps, which are obtained by learning\n",
      "a highly nonlinear mapping with the deep autoencoder., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(the\n",
      "proposed hierarchical face parsing is not only robust to par-\n",
      "tial occlusions but also provide richer information for face\n",
      "analysis and face synthesis compared with face keypoint de-\n",
      "tection and face alignment., {'neg': 0.041, 'neu': 0.835, 'pos': 0.124, 'compound': 0.6227})\n",
      "(the effectiveness of our algo-\n",
      "rithm is shown through several tasks on 2, 239 images se-\n",
      "lected from three datasets (e.g., lfw [12], bioid [13] and\n",
      "cufsf [29]).\n",
      "\n",
      "1., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(multimodal compact bilinear pooling\n",
      "\n",
      "for visual question answering and visual grounding\n",
      "\n",
      "akira fukui*1,2\n",
      "anna rohrbach*1,3\n",
      "\n",
      "dong huk park*1\n",
      "trevor darrell1 marcus rohrbach1\n",
      "\n",
      "daylen yang*1\n",
      "\n",
      "1uc berkeley eecs, ca, united states\n",
      "\n",
      "2sony corp., {'neg': 0.0, 'neu': 0.917, 'pos': 0.083, 'compound': 0.4215})\n",
      "(, tokyo, japan\n",
      "\n",
      "3max planck institute for informatics, saarbr¨ucken, germany\n",
      "\n",
      "6\n",
      "1\n",
      "0\n",
      "2\n",
      "\n",
      " \n",
      "\n",
      "p\n",
      "e\n",
      "s\n",
      "4\n",
      "2\n",
      "\n",
      " \n",
      "\n",
      " \n",
      " \n",
      "]\n",
      "\n",
      "v\n",
      "c\n",
      ".\n",
      ", {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(s\n",
      "c\n",
      "[\n",
      " \n",
      " \n",
      "\n",
      "3\n",
      "v\n",
      "7\n",
      "4\n",
      "8\n",
      "1\n",
      "0\n",
      "\n",
      ".\n",
      "\n",
      ", {'neg': 0.0, 'neu': 0.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(6\n",
      "0\n",
      "6\n",
      "1\n",
      ":\n",
      "v\n",
      "i\n",
      "x\n",
      "r\n",
      "a\n",
      "\n",
      "abstract\n",
      "\n",
      "modeling textual or visual information with\n",
      "vector representations trained from large lan-\n",
      "guage or visual datasets has been successfully\n",
      "explored in recent years., {'neg': 0.0, 'neu': 0.878, 'pos': 0.122, 'compound': 0.4939})\n",
      "(however, tasks such\n",
      "as visual question answering require combin-\n",
      "ing these vector representations with each other.\n",
      "approaches, {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(to multimodal pooling include\n",
      "element-wise product or sum, as well as con-\n",
      "catenation of the visual and textual represen-\n",
      "tations., {'neg': 0.0, 'neu': 0.9, 'pos': 0.1, 'compound': 0.2732})\n",
      "(we hypothesize that these methods\n",
      "are not as expressive as an outer product of\n",
      "the visual and textual vectors., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(as the outer\n",
      "product is typically infeasible due to its high\n",
      "dimensionality, we instead propose utilizing\n",
      "multimodal compact bilinear pooling (mcb)\n",
      "to efﬁciently and expressively combine multi-\n",
      "modal features., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(we extensively evaluate mcb\n",
      "on the visual question answering and ground-\n",
      "ing tasks., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(we consistently show the beneﬁt of\n",
      "mcb over ablations without mcb. for visual\n",
      "question answering, we present an architec-\n",
      "ture which uses mcb twice, once for predict-\n",
      "ing attention over spatial features and again\n",
      "to combine the attended representation with\n",
      "the question representation., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(this model out-\n",
      "performs the state-of-the-art on the visual7w\n",
      "dataset and the vqa challenge.\n",
      "\n",
      ", {'neg': 0.0, 'neu': 0.909, 'pos': 0.091, 'compound': 0.0772})\n",
      "(attribute2image: conditional image generation\n",
      "\n",
      "from visual attributes\n",
      "\n",
      "xinchen yan1, jimei yang2, kihyuk sohn3 and honglak lee1\n",
      "\n",
      "1computer science and engineering,\n",
      "\n",
      "university of michigan\n",
      "\n",
      "2adobe research, 3nec labs\n",
      "\n",
      "xcyan@umich.edu, jimyang@adobe.com, ksohn@nec-labs.com, honglak@umich.edu\n",
      "\n",
      "6\n",
      "1\n",
      "0\n",
      "2\n",
      "\n",
      " \n",
      "t\n",
      "c\n",
      "o\n",
      "8\n",
      "\n",
      " \n",
      "\n",
      " \n",
      " \n",
      "]\n",
      "\n",
      "g\n",
      "l\n",
      ".\n",
      ", {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(s\n",
      "c\n",
      "[\n",
      " \n",
      " \n",
      "\n",
      "2\n",
      "v\n",
      "0\n",
      "7\n",
      "5\n",
      "0\n",
      "0\n",
      "\n",
      ".\n",
      "\n",
      "2\n",
      "1\n",
      "5\n",
      "1\n",
      ":\n",
      "v\n",
      "i\n",
      "x\n",
      "r\n",
      "a\n",
      "\n",
      "abstract., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(this paper investigates a novel problem of generating images\n",
      "from visual attributes., {'neg': 0.193, 'neu': 0.643, 'pos': 0.164, 'compound': -0.1027})\n",
      "(we model the image as a composite of foreground\n",
      "and background and develop a layered generative model with disentan-\n",
      "gled latent variables that can be learned end-to-end using a variational\n",
      "auto-encoder., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(we experiment with natural images of faces and birds and\n",
      "demonstrate that the proposed models are capable of generating realistic\n",
      "and diverse samples with disentangled latent representations., {'neg': 0.0, 'neu': 0.831, 'pos': 0.169, 'compound': 0.6249})\n",
      "(we use a\n",
      "general energy minimization algorithm for posterior inference of latent\n",
      "variables given novel images., {'neg': 0.0, 'neu': 0.747, 'pos': 0.253, 'compound': 0.5267})\n",
      "(therefore, the learned generative models\n",
      "show excellent quantitative and visual results in the tasks of attribute-\n",
      "conditioned image reconstruction and completion.\n",
      "\n",
      "1, {'neg': 0.0, 'neu': 0.844, 'pos': 0.156, 'compound': 0.5719})\n",
      "(multimodal learning with deep boltzmann machines\n",
      "\n",
      "nitish srivastava\n",
      "\n",
      "ruslan salakhutdinov\n",
      "\n",
      "department of computer science\n",
      "\n",
      "department of statistics and computer science\n",
      "\n",
      "university of toronto\n",
      "\n",
      "nitish@cs.toronto.edu\n",
      "\n",
      "university of toronto\n",
      "\n",
      "rsalakhu@cs.toronto.edu\n",
      "\n",
      "abstract\n",
      "\n",
      "a deep boltzmann machine is described for learning a generative model of data\n",
      "that consists of multiple and diverse input modalities., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(the model can be used\n",
      "to extract a uniﬁed representation that fuses modalities together., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(we ﬁnd that\n",
      "this representation is useful for classiﬁcation and information retrieval tasks., {'neg': 0.0, 'neu': 0.805, 'pos': 0.195, 'compound': 0.4404})\n",
      "(the\n",
      "model works by learning a probability density over the space of multimodal inputs.\n",
      ", {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(it uses states of latent variables as representations of the input., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(the model can\n",
      "extract this representation even when some modalities are absent by sampling\n",
      "from the conditional distribution over them and ﬁlling them in., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(our experimental\n",
      "results on bi-modal data consisting of images and text show that the multimodal\n",
      "dbm can learn a good generative model of the joint space of image and text\n",
      "inputs that is useful for information retrieval from both unimodal and multimodal\n",
      "queries., {'neg': 0.0, 'neu': 0.873, 'pos': 0.127, 'compound': 0.7003})\n",
      "(we further demonstrate that this model signiﬁcantly outperforms svms\n",
      "and lda on discriminative tasks., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(finally, we compare our model to other deep\n",
      "learning methods, including autoencoders and deep belief networks, and show that\n",
      "it achieves noticeable gains.\n",
      "\n",
      "1\n",
      "\n",
      ", {'neg': 0.0, 'neu': 0.902, 'pos': 0.098, 'compound': 0.34})\n",
      "(deep canonical correlation analysis\n",
      "\n",
      "galen andrew\n",
      "university of washington\n",
      "\n",
      "raman arora\n",
      "toyota technological institute at chicago\n",
      "\n",
      "jeﬀ bilmes\n",
      "university of washington\n",
      "\n",
      "karen livescu\n",
      "toyota technological institute at chicago\n",
      "\n",
      "galen@cs.washington.edu\n",
      "\n",
      "arora@ttic.edu\n",
      "\n",
      "bilmes@ee.washington.edu\n",
      "\n",
      "klivescu@ttic.edu\n",
      "\n",
      "abstract\n",
      "\n",
      "1., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(colorful image colorization\n",
      "\n",
      "richard zhang, phillip isola, alexei a. efros\n",
      "\n",
      "{rich.zhang,isola,efros}@eecs.berkeley.edu\n",
      "\n",
      "university of california, berkeley\n",
      "\n",
      "abstract. given a grayscale photograph as input, this paper attacks\n",
      "the problem of hallucinating a plausible color version of the photograph.\n",
      ", {'neg': 0.149, 'neu': 0.851, 'pos': 0.0, 'compound': -0.6808})\n",
      "(this problem is clearly underconstrained, so previous approaches have\n",
      "either relied on signiﬁcant user interaction or resulted in desaturated col-\n",
      "orizations., {'neg': 0.111, 'neu': 0.779, 'pos': 0.111, 'compound': 0.0})\n",
      "(we propose a fully automatic approach that produces vibrant\n",
      "and realistic colorizations., {'neg': 0.0, 'neu': 0.746, 'pos': 0.254, 'compound': 0.5267})\n",
      "(we embrace the underlying uncertainty of the\n",
      "problem by posing it as a classiﬁcation task and use class-rebalancing at\n",
      "training time to increase the diversity of colors in the result., {'neg': 0.147, 'neu': 0.72, 'pos': 0.133, 'compound': -0.128})\n",
      "(the sys-\n",
      "tem is implemented as a feed-forward pass in a cnn at test time and is\n",
      "trained on over a million color images., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(we evaluate our algorithm using a\n",
      "“colorization turing test,” asking human participants to choose between\n",
      "a generated and ground truth color image., {'neg': 0.0, 'neu': 0.892, 'pos': 0.108, 'compound': 0.3182})\n",
      "(our method successfully fools\n",
      "humans on 32% of the trials, signiﬁcantly higher than previous methods.\n",
      ", {'neg': 0.165, 'neu': 0.67, 'pos': 0.165, 'compound': 0.0})\n",
      "(moreover, we show that colorization can be a powerful pretext task for\n",
      "self-supervised feature learning, acting as a cross-channel encoder. this\n",
      "approach results in state-of-the-art performance on several feature learn-\n",
      "ing benchmarks.\n",
      "\n",
      ", {'neg': 0.0, 'neu': 0.912, 'pos': 0.088, 'compound': 0.4215})\n",
      "(keywords: colorization, vision for graphics, cnns, self-supervised\n",
      "learning\n",
      "\n",
      "6\n",
      "1\n",
      "0\n",
      "2\n",
      "\n",
      " \n",
      "t\n",
      "c\n",
      "o\n",
      "5\n",
      "\n",
      " \n",
      "\n",
      " \n",
      " \n",
      "]\n",
      "\n",
      "v\n",
      "c\n",
      ".\n",
      ", {'neg': 0.0, 'neu': 0.778, 'pos': 0.222, 'compound': 0.25})\n",
      "(s\n",
      "c\n",
      "[\n",
      " \n",
      " \n",
      "\n",
      "5\n",
      "v\n",
      "1\n",
      "1\n",
      "5\n",
      "8\n",
      "0\n",
      "\n",
      ".\n",
      "\n",
      "3\n",
      "0\n",
      "6\n",
      "1\n",
      ":\n",
      "v\n",
      "i\n",
      "x\n",
      "r\n",
      "a\n",
      "\n",
      "1, {'neg': 0.0, 'neu': 0.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(ieee transactions on audio, speech, and language processing, vol., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(21, no., {'neg': 0.688, 'neu': 0.313, 'pos': 0.0, 'compound': -0.296})\n",
      "(5, may 2013\n",
      "\n",
      "1\n",
      "\n",
      "machine learning paradigms for speech recognition:\n",
      "\n",
      "an overview\n",
      "\n",
      "li deng, fellow, ieee, and xiao li, member, ieee\n",
      "\n",
      "abstract, {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(—automatic speech recognition (asr) has histori-\n",
      "cally been a driving force behind many machine learning (ml)\n",
      "including the ubiquitously used hidden markov\n",
      "techniques,\n",
      "model, discriminative learning, structured sequence learning,\n",
      "bayesian learning, and adaptive learning, {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(. moreover, ml can and\n",
      "occasionally does use asr as a large-scale, realistic application\n",
      "to rigorously test the effectiveness of a given technique, and to\n",
      "inspire new problems arising from the inherently sequential and\n",
      "dynamic nature of speech. on the other hand, even though asr\n",
      "is available commercially for some applications, it is largely an\n",
      "unsolved problem—for almost all applications, the performance\n",
      "of asr is not on par with human performance. new insight from\n",
      "modern ml methodology shows great promise to advance the\n",
      "state-of-the-art in asr technology., {'neg': 0.042, 'neu': 0.794, 'pos': 0.164, 'compound': 0.9153})\n",
      "(this overview article provides\n",
      "readers with an overview of modern ml techniques as utilized in\n",
      "the current and as relevant to future asr research and systems.\n",
      ", {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(the intent is to foster further cross-pollination between the ml\n",
      "and asr communities than has occurred in the past., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(the article\n",
      "is organized according to the major ml paradigms that are either\n",
      "popular already or have potential for making signiﬁcant contribu-\n",
      "tions to asr technology., {'neg': 0.0, 'neu': 0.899, 'pos': 0.101, 'compound': 0.4215})\n",
      "(the paradigms presented and elaborated\n",
      "in this overview include: generative and discriminative learning;\n",
      "supervised, unsupervised, semi-supervised, and active learning;\n",
      "adaptive and multi-task learning; and bayesian learning., {'neg': 0.0, 'neu': 0.903, 'pos': 0.097, 'compound': 0.4019})\n",
      "(these\n",
      "learning paradigms are motivated and discussed in the context of\n",
      "asr technology and applications., {'neg': 0.0, 'neu': 0.824, 'pos': 0.176, 'compound': 0.4588})\n",
      "(we ﬁnally present and analyze\n",
      "recent developments of deep learning and learning with sparse\n",
      "representations, focusing on their direct relevance to advancing\n",
      "asr technology.\n",
      "\n",
      ", {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(index terms—machine learning,\n",
      "\n",
      "su-\n",
      "pervised, unsupervised, discriminative, generative, dynamics,\n",
      "adaptive, bayesian, deep learning.\n",
      "\n",
      ", {'neg': 0.0, 'neu': 0.851, 'pos': 0.149, 'compound': 0.2732})\n",
      "(speech recognition,\n",
      "\n",
      "i., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(generative adversarial text to image synthesis\n",
      "\n",
      "scott reed, zeynep akata, xinchen yan, lajanugen logeswaran\n",
      "bernt schiele, honglak lee\n",
      "1 university of michigan, ann arbor, mi, usa (umich.edu)\n",
      "2 max planck institute for informatics, saarbr¨ucken, germany (mpi-inf.mpg.de)\n",
      "\n",
      "reedscot1, akata2, xcyan1, llajan1\n",
      "schiele2,honglak1\n",
      "\n",
      "abstract\n",
      "\n",
      "automatic synthesis of realistic images from text\n",
      "would be interesting and useful, but current ai\n",
      "systems are still far from this goal., {'neg': 0.027, 'neu': 0.914, 'pos': 0.059, 'compound': 0.2617})\n",
      "(however, in\n",
      "recent years generic and powerful recurrent neu-\n",
      "ral network architectures have been developed\n",
      "to learn discriminative text feature representa-\n",
      "tions., {'neg': 0.0, 'neu': 0.882, 'pos': 0.118, 'compound': 0.4215})\n",
      "(meanwhile, deep convolutional generative\n",
      "adversarial networks (gans) have begun to gen-\n",
      "erate highly compelling images of speciﬁc cat-\n",
      "egories, such as faces, album covers, and room\n",
      "interiors. in this work, {'neg': 0.076, 'neu': 0.856, 'pos': 0.067, 'compound': -0.079})\n",
      "(, we develop a novel deep\n",
      "architecture and gan formulation to effectively\n",
      "bridge these advances in text and image model-\n",
      "ing, translating visual concepts from characters\n",
      "to pixels., {'neg': 0.0, 'neu': 0.822, 'pos': 0.178, 'compound': 0.6369})\n",
      "(we demonstrate the capability of our\n",
      "model to generate plausible images of birds and\n",
      "ﬂowers from detailed text descriptions.\n",
      "\n",
      "1., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(207\n",
      "\n",
      "transactions of the association for computational linguistics, 2 (2014) 207–218., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(action editor: alexander clark.\n",
      "\n",
      "submitted 10/2013; revised 3/2014; published 4/2014., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(c(cid:13)2014 association for computational linguistics.\n",
      "\n",
      ", {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(groundedcompositionalsemanticsforfindinganddescribingimageswithsentencesrichardsocher,andrejkarpathy,quocv.le*,christopherd.manning,andrewy.ngstanforduniversity,computersciencedepartment,*googleinc.richard@socher.org,karpathy@cs.stanford.edu,qvl@google.com,manning@stanford.edu,ang@cs.stanford.eduabstractpreviousworkonrecursiveneuralnetworks(rnns)showsthatthesemodelscanproducecompositionalfeaturevectorsforaccuratelyrepresentingandclassifyingsentencesorim-ages.however,thesentencevectorsofprevi-ousmodelscannotaccuratelyrepresentvisu-allygroundedmeaning.weintroducethedt-rnnmodelwhichusesdependencytreestoembedsentencesintoavectorspaceinordertoretrieveimagesthataredescribedbythosesentences.unlikepreviousrnn-basedmod-elswhichuseconstituencytrees,dt-rnnsnaturallyfocusontheactionandagentsinasentence.theyarebetterabletoabstractfromthedetailsofwordorderandsyntacticexpression.dt-rnnsoutperformotherre-cursiveandrecurrentneuralnetworks,kernel-izedccaandabag-of-wordsbaselineonthetasksofﬁndinganimagethatﬁtsasentencedescriptionandviceversa.theyalsogivemoresimilarrepresentationstosentencesthatdescribethesameimage.1, {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(on deep multi-view representation learning\n",
      "\n",
      "weiran wang\n",
      "toyota technological institute at chicago\n",
      "\n",
      "raman arora\n",
      "johns hopkins university\n",
      "\n",
      "karen livescu\n",
      "toyota technological institute at chicago\n",
      "\n",
      "jeff bilmes\n",
      "university of washington, seattle\n",
      "\n",
      "abstract\n",
      "\n",
      "we consider learning representations (features)\n",
      "in the setting in which we have access to mul-\n",
      "tiple unlabeled views of the data for representa-\n",
      "tion learning while only one view is available at\n",
      "test time., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(previous work on this problem has pro-\n",
      "posed several techniques based on deep neural\n",
      "networks, typically involving either autoencoder-\n",
      "like networks with a reconstruction objective or\n",
      "paired feedforward networks with a correlation-\n",
      "based objective., {'neg': 0.088, 'neu': 0.842, 'pos': 0.07, 'compound': -0.1593})\n",
      "(we analyze several techniques\n",
      "based on prior work, as well as new variants, and\n",
      "compare them experimentally on visual, speech,\n",
      "and language domains. to our knowledge this\n",
      "is the ﬁrst head-to-head comparison of a vari-\n",
      "ety of such techniques on multiple tasks., {'neg': 0.0, 'neu': 0.95, 'pos': 0.05, 'compound': 0.2732})\n",
      "(we\n",
      "ﬁnd an advantage for correlation-based represen-\n",
      "tation learning, while the best results on most\n",
      "tasks are obtained with our new variant, deep\n",
      "canonically correlated autoencoders (dccae).\n",
      "\n",
      ", {'neg': 0.0, 'neu': 0.801, 'pos': 0.199, 'compound': 0.7351})\n",
      "(test\n",
      "\n",
      "time.\n",
      "\n",
      "1., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(deep learning:\n",
      "methods and applications\n",
      "\n",
      "li deng\n",
      "microsoft research\n",
      "one microsoft way\n",
      "redmond, wa 98052; usa\n",
      "deng@microsoft.com\n",
      "dong yu\n",
      "microsoft research\n",
      "one microsoft way\n",
      "redmond, wa 98052; usa\n",
      "dong.yu@microsoft.com\n",
      "\n",
      "boston — delft\n",
      "\n",
      "full text available at: http://dx.doi.org/10.1561/2000000039\f",
      "foundations and trends r(cid:1) in signal processing\n",
      "\n",
      "published, sold and distributed by:\n",
      "now publishers inc.\n",
      ", {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(po box 1024\n",
      "hanover, ma 02339\n",
      "united states\n",
      "tel. +1-781-985-4510\n",
      "www.nowpublishers.com\n",
      "sales@nowpublishers.com\n",
      "outside north america:\n",
      "now publishers inc.\n",
      ", {'neg': 0.161, 'neu': 0.714, 'pos': 0.125, 'compound': -0.2023})\n",
      "(po box 179\n",
      "2600 ad delft\n",
      "the netherlands\n",
      "tel., {'neg': 0.31, 'neu': 0.69, 'pos': 0.0, 'compound': -0.5574})\n",
      "(+31-6-51115274\n",
      "the preferred citation for this publication is\n",
      "l. deng and d. yu. deep learning: methods and applications., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(foundations and\n",
      "trends r(cid:1) in signal processing, vol. 7, nos., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(3–4, pp. 197–387, 2013.\n",
      ", {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(this foundations and trends r(cid:1) issue was typeset in latex using a class ﬁle designed\n",
      "by neal parikh. printed on acid-free paper.\n",
      ", {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(isbn: 978-1-60198-815-7\n",
      "c(cid:1) 2014 l. deng and d. yu\n",
      "\n",
      "all rights reserved., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(no part of this publication may be reproduced, stored in a retrieval\n",
      "system, or transmitted in any form or by any means, mechanical, photocopying, recording\n",
      "or otherwise, without prior written permission of the publishers.\n",
      "photocopying. in the usa: this journal is registered at the copyright clearance cen-\n",
      "ter, inc., {'neg': 0.045, 'neu': 0.955, 'pos': 0.0, 'compound': -0.296})\n",
      "(, 222 rosewood drive, danvers, ma 01923., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(authorization to photocopy items for\n",
      "internal or personal use, or the internal or personal use of speciﬁc clients, is granted by\n",
      "now publishers inc for users registered with the copyright clearance center (ccc)., {'neg': 0.0, 'neu': 0.941, 'pos': 0.059, 'compound': 0.25})\n",
      "(the\n",
      "‘services’ for users can be found on the internet at: www.copyright.com\n",
      "for those organizations that have been granted a photocopy license, a separate system\n",
      "of payment has been arranged., {'neg': 0.0, 'neu': 0.931, 'pos': 0.069, 'compound': 0.25})\n",
      "(authorization does not extend to other kinds of copy-\n",
      "ing, such as that for general distribution, for advertising or promotional purposes, for\n",
      "creating new collective works, or for resale. in the rest of the world: permission to pho-\n",
      "tocopy must be obtained from the copyright owner., {'neg': 0.032, 'neu': 0.922, 'pos': 0.046, 'compound': 0.1734})\n",
      "(please apply to now publishers inc.,\n",
      "po box 1024, hanover, ma 02339, usa; tel., {'neg': 0.201, 'neu': 0.67, 'pos': 0.128, 'compound': -0.3182})\n",
      "(+1 781 871 0245; www.nowpublishers.com;\n",
      "sales@nowpublishers.com\n",
      "now publishers inc., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(has an exclusive license to publish this material worldwide., {'neg': 0.0, 'neu': 0.842, 'pos': 0.158, 'compound': 0.128})\n",
      "(permission\n",
      "to use this content must be obtained from the copyright license holder., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(please apply to\n",
      "now publishers, po box 179, 2600 ad delft, the netherlands, www.nowpublishers.com;\n",
      "e-mail: sales@nowpublishers.com\n",
      "\n",
      "full text available at: http://dx.doi.org/10.1561/2000000039\f",
      "foundations and trends r(cid:1) in signal processing\n",
      "\n",
      "volume 7, issues 3–4, 2013\n",
      "\n",
      "editorial board\n",
      "\n",
      "editor-in-chief\n",
      "\n",
      "yonina eldar\n",
      "technion - israel institute of technology\n",
      "israel\n",
      "editors\n",
      "\n",
      "robert m. gray\n",
      "founding editor-in-chief\n",
      "stanford university\n",
      "pao-chi chang\n",
      "ncu, taiwan\n",
      "pamela cosman\n",
      "uc san diego\n",
      "michelle eﬀros\n",
      "caltech\n",
      "yariv ephraim\n",
      "gmu\n",
      "alfonso farina\n",
      "selex es\n",
      "sadaoki furui\n",
      "tokyo tech\n",
      "georgios giannakis\n",
      "university of minnesota\n",
      "vivek goyal\n",
      "boston university\n",
      "sinan gunturk\n",
      "courant institute\n",
      "christine guillemot\n",
      "inria\n",
      "robert w. heath, jr.\n",
      ", {'neg': 0.036, 'neu': 0.94, 'pos': 0.023, 'compound': -0.3182})\n",
      "(ut austin\n",
      "\n",
      "sheila hemami\n",
      "cornell university\n",
      "lina karam\n",
      "arizona state u\n",
      "nick kingsbury\n",
      "university of cambridge\n",
      "alex kot\n",
      "ntu, singapore\n",
      "jelena kovacevic\n",
      "cmu\n",
      "geert leus\n",
      "tu delft\n",
      "jia li\n",
      "penn state\n",
      "henrique malvar\n",
      "microsoft research\n",
      "b.s., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(manjunath\n",
      "uc santa barbara\n",
      "urbashi mitra\n",
      "usc\n",
      "björn ottersten\n",
      "kth stockholm\n",
      "thrasos pappas\n",
      "northwestern university\n",
      "\n",
      "vincent poor\n",
      "princeton university\n",
      "anna scaglione\n",
      "uc davis\n",
      "mihaela van der shaar\n",
      "ucla\n",
      "nicholas d. sidiropoulos\n",
      "tu crete\n",
      "michael unser\n",
      "epfl\n",
      "p. p. vaidyanathan\n",
      "caltech\n",
      "ami wiesel\n",
      "hebrew u\n",
      "min wu\n",
      "university of maryland\n",
      "josiane zerubia\n",
      "inria\n",
      "\n",
      "full text available at: http://dx.doi.org/10.1561/2000000039\f",
      "editorial scope\n",
      "\n",
      "topics\n",
      "foundations and trends r(cid:1) in signal processing publishes survey and\n",
      "tutorial articles in the following topics:\n",
      "\n",
      "processing\n",
      "\n",
      "processing\n",
      "\n",
      "• adaptive signal processing\n",
      "• audio signal processing\n",
      "• biological and biomedical signal\n",
      "• complexity in signal processing\n",
      "• digital signal processing\n",
      "• distributed and network signal\n",
      "• image and video processing\n",
      "• linear and nonlinear ﬁltering\n",
      "• multidimensional signal\n",
      "• multimodal signal processing\n",
      "• multirate signal processing\n",
      "• multiresolution signal processing\n",
      "• nonlinear signal processing\n",
      "• randomized algorithms in signal\n",
      "• sensor and multiple source signal\n",
      "\n",
      "processing\n",
      "\n",
      "processing\n",
      "\n",
      "processing, source separation\n",
      "\n",
      "• signal decompositions, subband\n",
      "and transform methods, sparse\n",
      "representations\n",
      "\n",
      "• signal processing for\n",
      "\n",
      "communications\n",
      "\n",
      "• signal processing for security and\n",
      "forensic analysis, biometric signal\n",
      "processing\n",
      "\n",
      "• signal quantization, sampling,\n",
      "analog-to-digital conversion,\n",
      "coding and compression\n",
      "• signal reconstruction,\n",
      "\n",
      "digital-to-analog conversion,\n",
      "enhancement, decoding and\n",
      "inverse problems\n",
      "\n",
      "• speech/audio/image/video\n",
      "\n",
      "• speech and spoken language\n",
      "\n",
      "compression\n",
      "\n",
      "processing\n",
      "\n",
      "• statistical/machine learning\n",
      "• statistical signal processing\n",
      "\n",
      "information for librarians\n",
      "foundations and trends r(cid:1) in signal processing, 2013, volume 7, 4 issues., {'neg': 0.025, 'neu': 0.964, 'pos': 0.011, 'compound': -0.5267})\n",
      "(issn\n",
      "paper version 1932-8346., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(issn online version 1932-8354. also available as a\n",
      "combined paper and online subscription, {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(.\n",
      "\n",
      "full text available at: http://dx.doi.org/10.1561/2000000039\f",
      "foundations and trends r(cid:1) in signal processing\n",
      "vol. 7, nos., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(3–4 (2013) 197–387\n",
      "c(cid:1) 2014 l. deng and d., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(yu\n",
      "doi: 10.1561/2000000039\n",
      "\n",
      "deep learning: methods and applications\n",
      "\n",
      "li deng\n",
      "\n",
      "microsoft research\n",
      "one microsoft way\n",
      "\n",
      "redmond, wa 98052; usa\n",
      "\n",
      "deng@microsoft.com\n",
      "\n",
      "dong yu\n",
      "\n",
      "microsoft research\n",
      "one microsoft way\n",
      "\n",
      "redmond, wa 98052; usa\n",
      "dong.yu@microsoft.com\n",
      "\n",
      "full text available at: http://dx.doi.org/10.1561/2000000039\f",
      "contents\n",
      "\n",
      "endorsement\n",
      "\n",
      "1, {'neg': 0.0, 'neu': 0.943, 'pos': 0.057, 'compound': 0.3182})\n",
      "(multimodal deep learning\n",
      "\n",
      "jiquan ngiam1\n",
      "aditya khosla1\n",
      "mingyu kim1\n",
      "juhan nam1\n",
      "honglak lee2\n",
      "andrew y. ng1\n",
      "1 computer science department, stanford university, stanford, ca 94305, usa\n",
      "2 computer science and engineering division, university of michigan, ann arbor, mi 48109, usa\n",
      "\n",
      "jngiam@cs.stanford.edu\n",
      "aditya86@cs.stanford.edu\n",
      "minkyu89@cs.stanford.edu\n",
      "juhan@ccrma.stanford.edu\n",
      "honglak@eecs.umich.edu\n",
      "ang@cs.stanford.edu\n",
      "\n",
      "abstract\n",
      "\n",
      "deep networks have been successfully applied\n",
      "to unsupervised feature learning for single\n",
      "modalities (e.g., text, images or audio).\n",
      ", {'neg': 0.0, 'neu': 0.951, 'pos': 0.049, 'compound': 0.4939})\n",
      "(in\n",
      "this work, we propose a novel application of\n",
      "deep networks to learn features over multiple\n",
      "modalities., {'neg': 0.0, 'neu': 0.867, 'pos': 0.133, 'compound': 0.3182})\n",
      "(we present a series of tasks for\n",
      "multimodal learning and show how to train\n",
      "deep networks that learn features to address\n",
      "these tasks.\n",
      ", {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(in particular, we demonstrate\n",
      "cross modality feature learning, where better\n",
      "features for one modality (e.g., video) can be\n",
      "learned if multiple modalities (e.g., audio and\n",
      "video) are present at feature learning time.\n",
      ", {'neg': 0.0, 'neu': 0.914, 'pos': 0.086, 'compound': 0.4404})\n",
      "(furthermore, we show how to learn a shared\n",
      "representation between modalities and evalu-\n",
      "ate it on a unique task, where the classiﬁer is\n",
      "trained with audio-only data but tested with\n",
      "video-only data and vice-versa., {'neg': 0.0, 'neu': 0.948, 'pos': 0.052, 'compound': 0.1779})\n",
      "(our mod-\n",
      "els are validated on the cuave and avlet-\n",
      "ters datasets on audio-visual speech classiﬁ-\n",
      "cation, demonstrating best published visual\n",
      "speech classiﬁcation on avletters and eﬀec-\n",
      "tive shared representation learning.\n",
      "\n",
      "1., {'neg': 0.0, 'neu': 0.773, 'pos': 0.227, 'compound': 0.8176})\n",
      "(ideawall: improving creative collaboration\n",
      "\n",
      "through combinatorial visual stimuli\n",
      "\n",
      "yang shi∗\n",
      "\n",
      "central south university\n",
      "shiyang1230@gmail.com\n",
      "\n",
      "john chen\n",
      "\n",
      "university of california, davis\n",
      "\n",
      "jhochen@ucdavis.edu\n",
      "\n",
      "yang wang\n",
      "\n",
      "university of california, davis\n",
      "\n",
      "ywang@ucdavis.edu\n",
      "\n",
      "xiaoyao xu∗\n",
      "\n",
      "zhejiang university\n",
      "\n",
      "hsu.xiaoyao@gmail.com\n",
      "\n",
      "ye qi∗\n",
      "\n",
      "zhejiang university\n",
      "\n",
      "ye.charlotte.qi@gmail.com\n",
      "\n",
      "kwan-liu ma\n",
      "\n",
      "university of california, davis\n",
      "\n",
      "ma@cs.ucdavis.edu\n",
      "\n",
      "abstract\n",
      "with the recent advances in computer-supported cooperative\n",
      "work systems and increasing popularization of speech-based\n",
      "interfaces, groupware attempting to emulate a knowledgeable\n",
      "participant in a collaborative environment is bound to become\n",
      "a reality in the near future. in this paper, we present ideawall,\n",
      "a real-time system that continuously extracts essential informa-\n",
      "tion from a verbal discussion and augments that information\n",
      "with web-search materials., {'neg': 0.0, 'neu': 0.926, 'pos': 0.074, 'compound': 0.7906})\n",
      "(ideawall provides combinatorial\n",
      "visual stimuli to the participants to facilitate their creative\n",
      "process., {'neg': 0.0, 'neu': 0.805, 'pos': 0.195, 'compound': 0.4404})\n",
      "(we develop three cognitive strategies, from which a\n",
      "prototype application with three display modes was designed,\n",
      "implemented, and evaluated., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(the results of the user study\n",
      "with twelve groups show that ideawall effectively presents\n",
      "visual cues to facilitate verbal creative collaboration for idea\n",
      "generation and sets the stage for future research on intelligent\n",
      "systems that assist collaborative work.\n",
      "\n",
      ", {'neg': 0.0, 'neu': 0.799, 'pos': 0.201, 'compound': 0.8316})\n",
      "(acm classiﬁcation keywords\n",
      "h.5.3., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(information interfaces and presentation (e.g. hci):\n",
      "group and organization interfaces\n",
      "\n",
      "author keywords\n",
      "verbal collaboration; brainstorming; visual cues;\n",
      "groupware.\n",
      "\n",
      ", {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(6\n",
      "1\n",
      "0\n",
      "2\n",
      "\n",
      " \n",
      "t\n",
      "c\n",
      "o\n",
      "2\n",
      "1\n",
      "\n",
      " \n",
      "\n",
      " \n",
      " \n",
      "]\n",
      "l\n",
      "c\n",
      ".\n",
      ", {'neg': 0.0, 'neu': 0.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(s\n",
      "c\n",
      "[\n",
      " \n",
      " \n",
      "\n",
      "1\n",
      "v\n",
      "5\n",
      "8\n",
      "5\n",
      "3\n",
      "0\n",
      "\n",
      ", {'neg': 0.0, 'neu': 0.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(.\n",
      "\n",
      "0\n",
      "1\n",
      "6\n",
      "1\n",
      ":\n",
      "v\n",
      "i\n",
      "x\n",
      "r\n",
      "a\n",
      "\n",
      "a paradigm for situated and goal-driven language learning\n",
      "\n",
      "jon gauthier1,2\n",
      "jon@gauthiers.net\n",
      "\n",
      "igor mordatch1,3\n",
      "\n",
      "mordatch@openai.com\n",
      "\n",
      "1openai\n",
      "\n",
      "2stanford nlp group 3uc berkeley\n",
      "\n",
      "a distinguishing property of human intelligence is the ability to ﬂexibly use language in\n",
      "order to communicate complex ideas with other humans in a variety of contexts., {'neg': 0.0, 'neu': 0.886, 'pos': 0.114, 'compound': 0.6597})\n",
      "(research\n",
      "in natural language dialogue should focus on designing communicative agents which can\n",
      "integrate themselves into these contexts and productively collaborate with humans.\n",
      "\n",
      "in this abstract, {'neg': 0.0, 'neu': 0.909, 'pos': 0.091, 'compound': 0.3612})\n",
      "(, we propose a general situated language learning paradigm which is\n",
      "designed to bring about robust language agents able to cooperate productively with humans.\n",
      ", {'neg': 0.0, 'neu': 0.897, 'pos': 0.103, 'compound': 0.34})\n",
      "(this dialogue paradigm is built on a utilitarian deﬁnition of language understanding.\n",
      ", {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(language is one of multiple tools which an agent may use to accomplish goals in its\n",
      "environment., {'neg': 0.0, 'neu': 0.851, 'pos': 0.149, 'compound': 0.4215})\n",
      "(we say an agent “understands” language only when it is able to use language\n",
      "productively to accomplish these goals. under this deﬁnition, {'neg': 0.0, 'neu': 0.882, 'pos': 0.118, 'compound': 0.4215})\n",
      "(, an agent’s communication\n",
      "success reduces to its success on tasks within its environment.\n",
      "\n",
      ", {'neg': 0.0, 'neu': 0.598, 'pos': 0.402, 'compound': 0.8126})\n",
      "(this setup contrasts with many conventional natural language tasks, which maximize\n",
      "linguistic objectives derived from static datasets., {'neg': 0.0, 'neu': 0.865, 'pos': 0.135, 'compound': 0.3612})\n",
      "(such applications often make the mistake\n",
      "of reifying language as an end in itself., {'neg': 0.156, 'neu': 0.844, 'pos': 0.0, 'compound': -0.34})\n",
      "(the tasks prioritize an isolated measure of linguistic\n",
      "intelligence (often one of linguistic competence, in the sense of chomsky (1965)), rather\n",
      "than measuring a model’s eﬀectiveness in real-world scenarios.1 our utilitarian deﬁnition\n",
      "is motivated by recent successes in reinforcement learning methods. in a reinforcement\n",
      "learning setting, agents maximize success metrics on real-world tasks, without requiring\n",
      "direct supervision of linguistic behavior.\n",
      "\n",
      "the environment\n",
      "\n",
      "we propose an end-to-end learning environment with multiple language-enabled agents,\n",
      "each with the capacity to deﬁne their own internal goals and plans to reach those goals.\n",
      ", {'neg': 0.024, 'neu': 0.826, 'pos': 0.15, 'compound': 0.9042})\n",
      "(each agent may also have diﬀerent capacities to observe or act in this environment., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(their\n",
      "goals are grounded non-linguistic objectives: for example, to reach a desired location,\n",
      "manipulate objects in the environment, or transmit a piece of information.2 (we address\n",
      "the issue of non-linguistic grounding at length in the next section.), {'neg': 0.0, 'neu': 0.912, 'pos': 0.088, 'compound': 0.296})\n",
      "(some ﬁxed-language\n",
      "agents in the environment speak an existing conventional language (e.g. english), and\n",
      "other learning agents are tasked with jointly learning this language while also solving other\n",
      "goals in the environment., {'neg': 0.0, 'neu': 0.928, 'pos': 0.072, 'compound': 0.34})\n",
      "(the agents are assigned diﬃcult (possibly distinct) tasks which\n",
      "\n",
      "1our motivation here is similar to that of dagan et al., {'neg': 0.0, 'neu': 0.888, 'pos': 0.112, 'compound': 0.34})\n",
      "((2006), who recognized the negative eﬀects of a community\n",
      "fragmented across diﬀerent isolated application-speciﬁc tasks, and suggested the uniﬁed task of recognizing textual\n",
      "entailment (rte) as a solution.\n",
      "\n",
      ", {'neg': 0.192, 'neu': 0.735, 'pos': 0.073, 'compound': -0.5719})\n",
      "(2a related line of work in evolutionary linguistics constructs a similar language learning scenario entirely without\n",
      "ﬁxed-language agents (smith et al.,, {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(2003; steels, 2012; kirby et al., 2014)., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(all of the agents in these environments\n",
      "construct a novel language simultaneously to accomplish some shared task., {'neg': 0.0, 'neu': 0.634, 'pos': 0.366, 'compound': 0.7579})\n",
      "(this is an interesting separate line\n",
      "of research, but ultimately a separate task from the understanding and acquisition problems discussed in this\n",
      "abstract.\n",
      "\n",
      "\f",
      ", {'neg': 0.14, 'neu': 0.787, 'pos': 0.073, 'compound': -0.4019})\n",
      "(require them to cooperate by communicating via their language channel.3\n",
      "\n",
      "in the simplest instance of the paradigm above, a single ﬁxed-language parent agent\n",
      "interacts with a learning child agent., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(the parent cooperates with the child, but only\n",
      "when prompted through language. as a result, the child acquires the language of the\n",
      "parent in order to accomplish its task, {'neg': 0.0, 'neu': 0.879, 'pos': 0.121, 'compound': 0.5719})\n",
      "(., {'neg': 0.0, 'neu': 0.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(this speciﬁc scenario is designed to train single\n",
      "communicative agents which can accomplish tasks in their environments with the help of\n",
      "human or simulated counterparts.\n",
      "\n",
      ", {'neg': 0.0, 'neu': 0.807, 'pos': 0.193, 'compound': 0.6705})\n",
      "(it is possible to include ﬁxed-language agents in these environments without requiring\n",
      "human involvement., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(we have developed several instantiations of this paradigm in which\n",
      "the ﬁxed-language agents speak some simpliﬁed hard-coded english or an artiﬁcial language\n",
      "(e.g. a programming language or query language). near the limit of artiﬁcial complexity,\n",
      "agents might take advantage of the internet to access world knowledge and synthesize\n",
      "responses. as our algorithms for learning in these environments improve, however, we\n",
      "expect that involving human agents in the environment would be an extremely eﬀective\n",
      "way to train language-enabled artiﬁcial agents.\n",
      "\n",
      "environment grounding\n",
      "\n",
      "our language-learning agents crucially need to be grounded in a world which is not only\n",
      "linguistic., {'neg': 0.0, 'neu': 0.95, 'pos': 0.05, 'compound': 0.5994})\n",
      "(this grounding may be physical — for example, agents which are embodied\n",
      "in the real world — or virtual., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(this grounding is what allows us to evaluate the agents\n",
      "in some way that does not prioritize language as the only objective., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(grounding imposes\n",
      "additional responsibilities on the agent, such as competently sensing and acting in the\n",
      "world, but this should be seen as an opportunity rather than a problem., {'neg': 0.146, 'neu': 0.74, 'pos': 0.114, 'compound': -0.0129})\n",
      "(as we discuss in\n",
      "the following paragraphs, a grounded agent may use its experience in its environment to\n",
      "build better models of language, and likewise use its language to better reason about its\n",
      "environment.\n",
      "\n",
      ", {'neg': 0.0, 'neu': 0.842, 'pos': 0.158, 'compound': 0.7003})\n",
      "(this grounded environment is designed to bring about agents with comprehensive predic-\n",
      "tive models of the world, which combine linguistic knowledge with more general intelligent\n",
      "behavior. by design, we do not separate the activity of language model construction\n",
      "from many other intelligent predictive activities — whether physical (predicting physical\n",
      "behavior of objects), psychological (modeling the beliefs and intentions of other agents), or\n",
      "social (understanding group membership and group-level action) (lake et al., 2016; clark,\n",
      "2013). while early instantiations of this environment will limit the complexity necessary\n",
      "for the agents to model, {'neg': 0.0, 'neu': 0.911, 'pos': 0.089, 'compound': 0.8204})\n",
      "(, we expect all of these diﬀerent factors to eventually be relevant in\n",
      "a comprehensive learning environment.\n",
      "\n",
      ", {'neg': 0.0, 'neu': 0.875, 'pos': 0.125, 'compound': 0.25})\n",
      "(mikolov et al., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "((2015) make a similar argument for grounding, but arrive at an environment\n",
      "in which perception and action is mediated only through a linguistic channel., {'neg': 0.077, 'neu': 0.923, 'pos': 0.0, 'compound': -0.1901})\n",
      "(while this\n",
      "textual interface certainly has the potential to simplify the problem, we believe there are\n",
      "several issues with this setup.\n",
      "\n",
      ", {'neg': 0.112, 'neu': 0.788, 'pos': 0.1, 'compound': -0.0772})\n",
      "(textual interfaces impose a lossy representation of an agent’s actual environment (brooks,\n",
      "\n",
      "3 much of the recent novel work in dialogue-based learning (fleischman and roy, 2005; vogel et al., 2014;\n",
      "wang et al., 2016; weston, 2016) and multi-agent communication (lazaridou et al., 2016; andreas and klein,\n",
      "2016; foerster et al, {'neg': 0.085, 'neu': 0.87, 'pos': 0.044, 'compound': -0.2732})\n",
      "(., {'neg': 0.0, 'neu': 0.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(, 2016; sukhbaatar et al.,, {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(2016) can be ﬁt into the paradigm described so far., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(this paper is\n",
      "concerned with designing a general paradigm for language-learning which contains these experiments, and picking\n",
      "out properties of the learning environment which are important for future research.\n",
      "\n",
      "\f",
      ", {'neg': 0.0, 'neu': 0.938, 'pos': 0.063, 'compound': 0.2023})\n",
      "(1991), and necessarily bias the focus of an agent’s observations., {'neg': 0.135, 'neu': 0.865, 'pos': 0.0, 'compound': -0.1027})\n",
      "(consider the following\n",
      "text-only interaction in the style of mikolov et al., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "((2015) between a learning agent a and\n",
      "omniscient agent b, situated in some simulated physical world:\n",
      "\n",
      "b: you are carrying a box of eggs and need to set them down.\n",
      ", {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(a: is there a table nearby?\n",
      ", {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(b: there is a table to your left.\n",
      ", {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(a: i put the box on the table.\n",
      ", {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(b: the box slides off of the table and the eggs break open.\n",
      ", {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "([the table is missing a leg and tilted when the box was laid down.]\n",
      "\n",
      ", {'neg': 0.155, 'neu': 0.845, 'pos': 0.0, 'compound': -0.296})\n",
      "(the agent could have avoided this disaster if it had queried to ﬁnd out that the table was\n",
      "missing a leg before placing weight on it., {'neg': 0.301, 'neu': 0.699, 'pos': 0.0, 'compound': -0.8582})\n",
      "(but we cannot expect an agent to exhaustively\n",
      "query its environment via text in general., {'neg': 0.128, 'neu': 0.872, 'pos': 0.0, 'compound': -0.2617})\n",
      "((should it also ask whether the table is made of\n",
      "solid material, or whether the table is on ﬁre?) by contrast, a lossless visual observation\n",
      "does not remove possibly relevant information about the properties of tables, and it is up\n",
      "to the agent to learn what to attend to in such an environment and how to interpret these\n",
      "visual percepts.\n",
      "\n",
      "for environments involving pre-programmed ﬁxed-language agents, generating textual\n",
      "environment descriptions, {'neg': 0.0, 'neu': 0.977, 'pos': 0.023, 'compound': 0.1531})\n",
      "(also places a signiﬁcant implementational burden., {'neg': 0.42, 'neu': 0.58, 'pos': 0.0, 'compound': -0.4404})\n",
      "(it is relatively\n",
      "easy to generate physical motion and visual renderings of three-legged tables or toppling\n",
      "events, but much more diﬃcult to describe these concepts in natural language.\n",
      "\n",
      ", {'neg': 0.0, 'neu': 0.833, 'pos': 0.167, 'compound': 0.6369})\n",
      "(of course, even if we claim that text data is not enough, we must settle for some level of\n",
      "abstraction., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(a similar argument to the one above could be made for including sound-wave\n",
      "speech data in the agent’s observations rather than only providing text and visual data.\n",
      "while this argument is likely, {'neg': 0.147, 'neu': 0.853, 'pos': 0.0, 'compound': -0.6124})\n",
      "(also correct, we have to settle for a level of representation\n",
      "which is computationally tractable., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(given that we want to make near-term progress on\n",
      "language comprehension and production, it seems reasonable to work with messages in text\n",
      "representation for the time being., {'neg': 0.0, 'neu': 0.859, 'pos': 0.141, 'compound': 0.4767})\n",
      "(it is likewise tractable to work in a virtual environment\n",
      "with simulated physics and visual inputs.\n",
      "\n",
      ", {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(conclusion\n",
      "\n",
      "in this abstract, we outlined a paradigm for grounded and goal-driven language learning\n",
      "in artiﬁcial agents., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(the paradigm is centered around a utilitarian deﬁnition of language\n",
      "understanding, which equates language understanding with the ability to cooperate with\n",
      "other language users in real-world environments., {'neg': 0.0, 'neu': 0.916, 'pos': 0.084, 'compound': 0.3182})\n",
      "(this position demotes language from\n",
      "its position as a separate task to be solved to one of several communicative tools agents\n",
      "might use to accomplish their real-world goals.\n",
      "\n",
      ", {'neg': 0.0, 'neu': 0.836, 'pos': 0.164, 'compound': 0.5994})\n",
      "(while this paradigm does already capture a small amount of recent work in dialogue, on\n",
      "the whole it has not received the focus it deserves in the research communities of natural\n",
      "language processing and machine learning., {'neg': 0.0, 'neu': 0.932, 'pos': 0.068, 'compound': 0.3612})\n",
      "(we hope this paper brings focus to the task of\n",
      "situated language learning as a way forward for research in natural language dialogue.\n",
      "\n",
      ", {'neg': 0.0, 'neu': 0.787, 'pos': 0.213, 'compound': 0.6597})\n",
      "(acknowledgments\n",
      "\n",
      "the authors wish to thank their colleagues at openai and stanford for their useful\n",
      "comments and critiques.\n",
      "\n",
      "\f",
      "references\n",
      "\n",
      "jacob andreas and dan klein. reasoning about pragmatics with neural listeners and\n",
      "speakers., {'neg': 0.0, 'neu': 0.782, 'pos': 0.218, 'compound': 0.7964})\n",
      "(arxiv:1604.00562, {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "([cs], april 2016., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(url http://arxiv.org/abs/1604.00562.\n",
      ", {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(arxiv: 1604.00562.\n",
      "\n",
      ", {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(rodney brooks., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(intelligence without representation., {'neg': 0.0, 'neu': 0.392, 'pos': 0.608, 'compound': 0.4767})\n",
      "(artiﬁcial intelligence, 47:139–159,\n",
      "\n",
      "1991.\n",
      "\n",
      ", {'neg': 0.0, 'neu': 0.492, 'pos': 0.508, 'compound': 0.4767})\n",
      "(noam chomsky., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(aspects of the theory of syntax., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(m.i.t., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(press, 1965.\n",
      ", {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(andy clark., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(whatever next? predictive brains, situated agents, and the future of cognitive\n",
      "science., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(behavioral and brain sciences, 36(03):181–204, 5 2013.\n",
      ", {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(issn 1469-1825.\n",
      ", {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(doi: 10.1017/s0140525x12000477., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(url http://journals.cambridge.org/article_\n",
      "s0140525x12000477.\n",
      "\n",
      ", {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(ido dagan, oren glickman, and bernardo magnini. the pascal recognising textual\n",
      "entailment challenge. in proceedings of the first international conference on machine\n",
      "learning challenges: evaluating predictive uncertainty visual object classiﬁcation, and\n",
      "recognizing textual entailment, mlcw’05, pages 177–190, berlin, heidelberg, 2006.\n",
      ", {'neg': 0.057, 'neu': 0.881, 'pos': 0.062, 'compound': -0.2023})\n",
      "(springer-verlag., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(isbn 3-540-33427-0, 978-3-540-33427-9., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(doi: 10.1007/11736790 9., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(url\n",
      "http://dx.doi.org/10.1007/11736790_9.\n",
      "\n",
      ", {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(michael fleischman and deb roy., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(proceedings of the ninth conference on computational\n",
      "natural language learning (conll-2005), chapter intentional context in situated\n",
      "natural language learning, pages 104–111., {'neg': 0.0, 'neu': 0.792, 'pos': 0.208, 'compound': 0.6124})\n",
      "(association for computational linguistics,\n",
      "2005., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(url http://aclweb.org/anthology/w05-0614.\n",
      "\n",
      ", {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(jakob n. foerster, yannis m. assael, nando de freitas, and shimon whiteson., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(learn-\n",
      "ing to communicate to solve riddles with deep distributed recurrent q-networks.\n",
      ", {'neg': 0.0, 'neu': 0.859, 'pos': 0.141, 'compound': 0.2023})\n",
      "(arxiv:1602.02672 [cs], february 2016., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(url http://arxiv.org/abs/1602.02672.\n",
      ", {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(arxiv: 1602.02672.\n",
      "\n",
      ", {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(simon kirby, tom griﬃths, and kenny smith., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(iterated learning and the evolution of\n",
      "language., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(current opinion in neurobiology, 28:108 – 114, 2014., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(issn 0959-4388., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(doi:\n",
      "http://dx.doi.org/10.1016/j.conb.2014.07.014., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(url http://www.sciencedirect.com/\n",
      "science/article/pii/s0959438814001421., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(si: communication and language.\n",
      "\n",
      "brenden m. lake, tomer d. ullman, joshua b. tenenbaum, and samuel j. gershman.\n",
      ", {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(building machines that learn and think like people., {'neg': 0.0, 'neu': 0.737, 'pos': 0.263, 'compound': 0.3612})\n",
      "(corr, abs/1604.00289, 2016., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(url\n",
      "http://arxiv.org/abs/1604.00289.\n",
      "\n",
      ", {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(angeliki lazaridou, nghia the pham, and marco baroni. towards multi-agent\n",
      "communication-based language learning., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(arxiv:1605.07133, {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "([cs], may 2016., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(url\n",
      "http://arxiv.org/abs/1605.07133., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(arxiv: 1605.07133.\n",
      "\n",
      ", {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(tomas mikolov, armand joulin, and marco baroni., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(a roadmap towards machine\n",
      "intelligence., {'neg': 0.0, 'neu': 0.492, 'pos': 0.508, 'compound': 0.4767})\n",
      "(arxiv:1511.08130, {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "([cs], november 2015., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(url http://arxiv.org/abs/\n",
      "1511.08130., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(arxiv: 1511.08130.\n",
      "\n",
      ", {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(kenneth smith, simon kirby, and henry brighton. iterated learning: a framework for\n",
      "the emergence of language., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(artiﬁcial life, 9(4):371–386, 10 2003., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(issn 1064-5462., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(doi:\n",
      "10.1162/106454603322694825., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(doi: 10.1162/106454603322694825.\n",
      "\n",
      "\f",
      ", {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(luc steels., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(grounding language through evolutionary language games.\n",
      "\n",
      "in language\n",
      "grounding in robots, pages 1–22., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(springer, 2012., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(url http://link.springer.com/\n",
      "chapter/10.1007/978-1-4614-3064-3_1.\n",
      "\n",
      ", {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(sainbayar sukhbaatar, arthur szlam, and rob fergus. learning multiagent communication\n",
      "with backpropagation., {'neg': 0.247, 'neu': 0.753, 'pos': 0.0, 'compound': -0.5574})\n",
      "(arxiv:1605.07736, {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "([cs], may 2016., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(url http://arxiv.org/\n",
      "abs/1605.07736., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(arxiv: 1605.07736.\n",
      "\n",
      ", {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(adam vogel, andr´es g´omez emilsson, michael c. frank, dan jurafsky, and christopher\n",
      "potts., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(learning to reason pragmatically with cognitive limitations. in proceedings of the\n",
      "36th annual meeting of the cognitive science society, pages 3055–3060, wheat ridge,\n",
      "co,, {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(july 2014., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(cognitive science society.\n",
      "\n",
      ", {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(sida i. wang, percy liang, and christopher d. manning., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(learning language games\n",
      "through interaction. in proceedings of the 54th annual meeting of the association for\n",
      "computational linguistics (volume 1: long papers),, {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(pages 2368–2378, berlin, germany,\n",
      "august 2016., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(association for computational linguistics., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(url http://www.aclweb.\n",
      "org/anthology/p16-1224.\n",
      "\n",
      ", {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(jason weston., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(dialog-based language learning., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(arxiv preprint arxiv:1604.06045, 2016.\n",
      "\n",
      "\f",
      ", {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "( studies in second language acquisition ,  2010 ,  32 , 303– 334 . \n",
      ", {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(doi:10.1017/s0272263109990532\n",
      "\n",
      "                             learners’ processing, uptake, \n",
      "and retention of corrective \n",
      "\n",
      "feedback on writing \n",
      "\n",
      " case studies \n",
      "\n",
      "       neomy     storch          and     gillian     wigglesworth       \n",
      "   university of melbourne  \n",
      "\n",
      "         the  literature  on  corrective  feedback  (cf)  that  second  language \n",
      "writers receive in response to their grammatical and lexical errors is \n",
      "plagued by controversies and conﬂ icting ﬁ ndings about the merits of \n",
      "feedback., {'neg': 0.042, 'neu': 0.911, 'pos': 0.047, 'compound': 0.0772})\n",
      "(although more recent studies suggest that cf is valuable \n",
      "(e.g., bitchener,  2008 ; sheen,  2007 ), it is still not clear whether direct \n",
      "or indirect feedback is the most effective, or why., {'neg': 0.06, 'neu': 0.763, 'pos': 0.177, 'compound': 0.6496})\n",
      "(this study explored \n",
      "the efﬁ cacy of two different forms of cf., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(the investigation focused on \n",
      "the nature of the learners’ engagement with the feedback received to \n",
      "gain a better understanding of why some feedback is taken up and \n",
      "retained  and  some  is  not.  , {'neg': 0.0, 'neu': 0.667, 'pos': 0.333, 'compound': 0.9001})\n",
      "(the  study  was  composed  of  three  ses-\n",
      "sions. in session 1, {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(, learners worked in pairs to compose a text based \n",
      "on a graphic prompt., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(feedback was provided either in the form of \n",
      "reformulations  (direct  feedback)  or  editing  symbols  (indirect  feed-\n",
      "back). in session 2 (day 5), the learners reviewed the feedback they \n",
      "received and rewrote their text., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(all pair talk was audio-recorded. in \n",
      "session 3 (day 28), each of the learners composed a text individually \n",
      "using the same prompt as in session 1., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(the texts produced by the \n",
      "pairs  after  feedback  were  analyzed  for  evidence  of  uptake  of  the \n",
      "feedback given and texts produced individually in session 3 for evi-\n",
      "dence of retention., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(the learners’ transcribed pair talk proved a very \n",
      "rich source of data that showed not only how learners processed the \n",
      "\n",
      "  this project was supported by an australian research council grant dpo450422.  \n",
      "\n",
      "   , {'neg': 0.0, 'neu': 0.756, 'pos': 0.244, 'compound': 0.8268})\n",
      "(address correspondence to: neomy storch, school of languages & linguistics, the \n",
      "\n",
      "university of melbourne, victoria, 3010, australia; e-mail:  neomys@unimelb.edu.au . \n",
      "\n",
      ", {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(© cambridge university press, 2010 0272-2631/10 $15.00\n",
      "\n",
      "303\n",
      "\n",
      "\f",
      "304\n",
      "\n",
      "neomy storch and gillian wigglesworth\n",
      "\n",
      "feedback received but also their attitudes toward the feedback and \n",
      "their beliefs about language conventions and use., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(closer analysis of \n",
      "four case study pairs suggests that uptake and retention may be af-\n",
      "fected by a host of linguistic and affective factors, including the type \n",
      "of  errors  the  learners  make  in  their  writing  and,  more  importantly, \n",
      "learners’  attitudes,  beliefs,  and  goals.  , {'neg': 0.053, 'neu': 0.889, 'pos': 0.058, 'compound': 0.0498})\n",
      "(the  ﬁ ndings  suggest  that, \n",
      "although often ignored in research on cf, these affective factors play \n",
      "an important role in uptake and retention of feedback.      \n",
      "\n",
      " in second language (l2) writing classes, {'neg': 0.067, 'neu': 0.812, 'pos': 0.122, 'compound': 0.2263})\n",
      "(, teachers generally give correc-\n",
      "tive feedback (cf) on their learners’ writing, particularly on errors in \n",
      "grammar and lexis., {'neg': 0.136, 'neu': 0.864, 'pos': 0.0, 'compound': -0.3976})\n",
      "(the underlying assumption for giving feedback is \n",
      "that  it  will  help  learners  to  notice  their  errors  and,  subsequently,  to \n",
      "produce  the  correct  forms.  , {'neg': 0.087, 'neu': 0.727, 'pos': 0.185, 'compound': 0.4019})\n",
      "(however,  although  some  recent  studies \n",
      "(e.g., bitchener,  2008 ; bitchener & knoch,  2008 ; sheen,  2007 ) show that \n",
      "targeted cf can be effective, extensive reviews of the available empir-\n",
      "ical research (e.g., f. hyland & k. hyland, 2006; k. hyland & f. hyland, \n",
      "2006; goldstein,  2004 ,  2005 ) conclude that the ﬁ ndings about the efﬁ -\n",
      "cacy  of  cf  are  mixed  and  thus  inconclusive  (see  also  the  debate  be-\n",
      "tween ferris,  1999 , and truscott,  2007 )., {'neg': 0.0, 'neu': 0.957, 'pos': 0.043, 'compound': 0.4767})\n",
      "(a number of factors may explain \n",
      "the lack of deﬁ nitive ﬁ ndings about the efﬁ cacy of cf, including the re-\n",
      "search methods employed in studies on cf, and a host of contextual \n",
      "and affective factors that relate to both teachers and learners (see gold-\n",
      "stein,  2005 ). \n",
      "\n",
      " a review of the literature on cf exempliﬁ es these mixed ﬁ, {'neg': 0.039, 'neu': 0.94, 'pos': 0.022, 'compound': -0.25})\n",
      "(ndings and \n",
      "brieﬂ y discusses the different methodological approaches that have \n",
      "been  used  to  assess  the  impact  of  cf,  before  focusing  on  a  small \n",
      "number of studies that have investigated an aspect of cf that has not \n",
      "been  extensively  explored:  how  learners  process  the  feedback  they \n",
      "receive. adopting a sociocultural theoretical perspective,, {'neg': 0.0, 'neu': 0.974, 'pos': 0.026, 'compound': 0.0772})\n",
      "(it is argued \n",
      "that research that analyzes actual instances of learners engaging with \n",
      "feedback and revising their texts as well as research that looks more \n",
      "closely  at  how  learners’  beliefs  and  goals  impact  their  decisions  is \n",
      "needed  to  understand  how  and  why  learners  respond  to  different \n",
      "forms of cf. \n",
      "\n",
      " , {'neg': 0.047, 'neu': 0.868, 'pos': 0.085, 'compound': 0.25})\n",
      "(corrective feedback can be distinguished in terms of its directness \n",
      "(for  a  comprehensive  description,  see  guénette,   2007 ),  which  ranges \n",
      "from direct (e.g., writing the correct form above the incorrect form) to \n",
      "indirect  (e.g.,  using  editing  symbols  to  signal  an  error).  , {'neg': 0.0, 'neu': 0.951, 'pos': 0.049, 'compound': 0.25})\n",
      "(research  on \n",
      "which form of feedback is most effective has produced mixed results. \n",
      ", {'neg': 0.0, 'neu': 0.78, 'pos': 0.22, 'compound': 0.5256})\n",
      "(for example,, {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(lalande ( 1982 ) found that students who received indirect \n",
      "feedback  (editing  codes)  showed  greater  accuracy  on  subsequent \n",
      "writing than students who received direct feedback—but the differences \n",
      "\n",
      "\f",
      "processing, uptake, and retention of cf\n",
      "\n",
      "305\n",
      "\n",
      "were not statistically signiﬁ cant., {'neg': 0.0, 'neu': 0.935, 'pos': 0.065, 'compound': 0.3612})\n",
      "(chandler ( 2003 ), in contrast, found di-\n",
      "rect feedback to be more effective than three different types of indirect \n",
      "feedback (with and without codes that explain the type of error)., {'neg': 0.0, 'neu': 0.895, 'pos': 0.105, 'compound': 0.5256})\n",
      "(how-\n",
      ", {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(ever, these effects were found only on immediate revisions; texts written \n",
      "later showed no statistically signiﬁ cant differences in grammatical accu-\n",
      "racy in relation to type of feedback. \n",
      "\n",
      " , {'neg': 0.075, 'neu': 0.925, 'pos': 0.0, 'compound': -0.296})\n",
      "(one factor that may explain these mixed results is whether the im-\n",
      "pact of cf on a revised text or on a subsequently written new text is \n",
      "considered;  another  is  the  measure  used  to  assess  the  impact  of  the \n",
      "feedback.  , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(for  example,  chandler  ( 2003 )  used  mean  accuracy  scores \n",
      "(mean number of errors per 100 words produced) on both revised and \n",
      "new texts but did not take into account the different types of errors or \n",
      "their relative frequency., {'neg': 0.12, 'neu': 0.851, 'pos': 0.029, 'compound': -0.5647})\n",
      "(bitchener ( 2008 ) used accuracy scores of new \n",
      "texts but only measured the accurate use of structures targeted by the \n",
      "feedback., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(sachs and polio ( 2007 ) considered revised texts and used a \n",
      "scale of revisions of t-units, deﬁ ned as an independent clause and any \n",
      "subordinate clause attached to it or embedded in it, noting what pro-\n",
      "portion of these units were amended in the direction of the cf and dis-\n",
      "tinguishing among partial, full, no revision, or not applicable., {'neg': 0.038, 'neu': 0.962, 'pos': 0.0, 'compound': -0.296})\n",
      "(the use of \n",
      "different measures makes comparison across studies difﬁ cult., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(however, \n",
      "it is not necessarily the case that using uniform measures will produce \n",
      "conclusive research ﬁ ndings. \n",
      "\n",
      " , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(the bulk of research on feedback has investigated the type of revi-\n",
      "sions that students make (or do not make) in response to different types \n",
      "of feedback (see goldstein,  2004 , for a review) rather than how learners \n",
      "actually engage with and process the feedback, and why they use (or \n",
      "fail  to  use)  the  feedback  received.  , {'neg': 0.06, 'neu': 0.898, 'pos': 0.041, 'compound': -0.2732})\n",
      "(processing  of  feedback  is  perhaps \n",
      "less  well  researched  and  understood  because  it  is  difﬁ cult  to  access \n",
      "such learner-internal cognitive processes., {'neg': 0.0, 'neu': 0.917, 'pos': 0.083, 'compound': 0.204})\n",
      "(studies that have collected \n",
      "feedback  processing  data  through  think-aloud  protocols  (e.g.,  qi  & \n",
      "lapkin,   2001 ),  retrospective  interviews  (e.g.,  hyland,   1998 ),  or  pair \n",
      "discussions of the feedback received on a jointly produced text (e.g., \n",
      "swain & lapkin,  2003 ) have suggested that two additional factors may \n",
      "affect the impact of feedback—namely, depth of processing and learners’ \n",
      "attitudes toward the feedback provided. \n",
      "\n",
      " in a small study ( n  = 2), qi and lapkin ( 2001 ) used think-aloud proto-\n",
      "cols, during which learners were asked to verbalize their processing of \n",
      "the feedback received (reformulation), to investigate the effect of depth \n",
      "of processing on uptake of feedback., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(the researchers analyzed the pro-\n",
      "tocols  and  distinguished  two  types  of  noticing:  substantive  and  per-\n",
      "functory., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(the main difference between these two types of noticing was \n",
      "that substantive noticing episodes were those in which the learners ar-\n",
      "ticulated reasons for the feedback received., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(this articulation was taken \n",
      "as evidence that the learners understood why the cf was provided. qi \n",
      "and lapkin found that substantive noticing led to greater improvements \n",
      "\n",
      "\f",
      "306\n",
      "\n",
      "neomy storch and gillian wigglesworth\n",
      "\n",
      "in  the  revised  text  than  perfunctory  noticing.  , {'neg': 0.0, 'neu': 0.885, 'pos': 0.115, 'compound': 0.5859})\n",
      "(similarly,  sachs  and \n",
      "polio’s ( 2007 ) larger study ( n  = 54), which also used think-aloud proto-\n",
      "cols, found that when learners noticed and understood why a linguistic \n",
      "item was reformulated (as evident in the think-aloud protocols), they \n",
      "were more likely to revise the item on subsequent drafts., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(however, the \n",
      "researchers point out the highly inferential nature of coding for depth of \n",
      "engagement. \n",
      "\n",
      " other studies (e.g., hyland,  1998 ,  2003 ; swain,  2006 ; swain & lapkin, \n",
      " 2003 ) have shown that learners’ goals, attitudes, and beliefs may also \n",
      "affect the uptake of feedback., {'neg': 0.0, 'neu': 0.93, 'pos': 0.07, 'compound': 0.4588})\n",
      "(hyland ( 1998 ,  2003 ) used retrospective \n",
      "interviews and case studies to investigate students’ use and reactions \n",
      "to the feedback received., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(hyland found that whether learners respond \n",
      "to the feedback and the strategies they adopt may depend on the im-\n",
      "portance  they  attribute  to  the  grammatical  accuracy  of  their  writing. \n",
      ", {'neg': 0.0, 'neu': 0.943, 'pos': 0.057, 'compound': 0.1779})\n",
      "(swain and swain and lapkin showed, using pair work, that learners may \n",
      "reject teacher feedback because it is perceived as violating their own \n",
      "beliefs  about  language  conventions  or  as  altering  their  intended \n",
      "meaning. \n",
      "\n",
      " the importance attributed to learners’ beliefs in explaining how and \n",
      "why learners process feedback is in line with sociocultural theoretical \n",
      "perspectives on learning., {'neg': 0.1, 'neu': 0.859, 'pos': 0.041, 'compound': -0.5719})\n",
      "(sociocultural theorists view learners (partic-\n",
      "ularly  adult  learners)  as  intentional  agents  in  their  language  learning \n",
      "activity  who  assign  relevance  and  signiﬁ cance  to  certain  events  and \n",
      "whose behavior is guided by their own goals (lantolf & pavlenko,  2001 ; \n",
      "lantolf & thorne,  2006 )., {'neg': 0.0, 'neu': 0.95, 'pos': 0.05, 'compound': 0.2732})\n",
      "(these beliefs and goals may affect what learners \n",
      "notice, whether they accept or reject the feedback provided, and how \n",
      "much of the feedback they retain.   \n",
      "\n",
      " , {'neg': 0.095, 'neu': 0.813, 'pos': 0.092, 'compound': -0.0258})\n",
      "(the current study  \n",
      "\n",
      " aims \n",
      "\n",
      " the few studies that have investigated the nature of learners’ engage-\n",
      "ment with the feedback provided on their writing have considered only \n",
      "direct forms of feedback (e.g., qi & lapkin,  2001 ; sachs & polio,  2007 ; \n",
      "swain  &  lapkin,   2003 ).  the  current  study  compared  the  processing, \n",
      "uptake, and retention data of learners who received direct and indirect \n",
      "forms  of  feedback.  furthermore,  taking  a  sociocultural  approach  to \n",
      "data analysis (microgenesis), the present study investigated activity as \n",
      "it  occurred  and  attempted  to  link  outcomes  (uptake  and  retention) \n",
      "with  processes  (nature  of  engagement)  and  the  learners’  underlying \n",
      "beliefs and goals., {'neg': 0.0, 'neu': 0.976, 'pos': 0.024, 'compound': 0.34})\n",
      "(this study set out to explore (a) whether learners \n",
      "process  direct  feedback  differently  from  indirect  feedback,  (b)  what \n",
      "factors affect learners’ uptake of feedback when revising their text, and \n",
      "\n",
      "\f",
      "processing, uptake, and retention of cf\n",
      "\n",
      "307\n",
      "\n",
      "(c) what factors affect learners’ retention of feedback when writing a \n",
      "new text.   \n",
      "\n",
      " , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(design \n",
      "\n",
      " a  case  study  approach  was  adopted,  with  the  cases  selected  from  a \n",
      "larger research project that investigated the efﬁ cacy of different forms \n",
      "of written cf using an experimental design., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(this larger project involved \n",
      "two groups of english-as-a-second-language (esl) learners, each group \n",
      "composed of 12 pairs. one group received feedback in the form of refor-\n",
      "mulation and the other received feedback in the form of editing sym-\n",
      "bols.  , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(assignment  of  pairs  to  the  different  feedback  condition  groups \n",
      "was done randomly.   \n",
      "\n",
      " , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(participants \n",
      "\n",
      " all participants in the project were volunteers recruited from advertise-\n",
      "ments displayed on notice boards in a large australian research univer-\n",
      "sity.  , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(the  advertisement  stated  that  the  aim  of  the  project  was  to \n",
      "investigate the efﬁ cacy of different types of feedback on writing., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(partic-\n",
      "ipants were invited to attend the project sessions in self-selected pairs. \n",
      "at  the  end  of  the  study,  each  participant  received  $100  for  their \n",
      "participation. \n",
      "\n",
      " , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(the participants (30 females, 18 males) were predominantly graduate \n",
      "students ( n  = 40) pursuing a master’s degree., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(their average age was 25. \n",
      "over half of the participants ( n  = 28), {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(were enrolled in a commerce de-\n",
      "gree  program  (e.g.,  master  of  applied  commerce,  master  of  applied \n",
      "ﬁ nance)., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(the majority came from asia, from countries such as china \n",
      "( n  = 24), indonesia ( n  = 11), and thailand ( n  = 4), which reﬂ ects the typical \n",
      "native language background of international students at australian uni-\n",
      "versities.  , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(participants  were  of  advanced  l2  proﬁ ciency  and  provided \n",
      "documentation that they had met the english language requirement for \n",
      "university entrance: an international english language testing system \n",
      "(ielts) score of 6.5, with 6.0 in writing, or a toefl internet-based test \n",
      "score above 240, with a test of written english score of 4.5 or above. \n",
      ", {'neg': 0.0, 'neu': 0.962, 'pos': 0.038, 'compound': 0.25})\n",
      "(the majority had learned english as a foreign language in their home \n",
      "countries, on average for 8 years at secondary school and a university; \n",
      "most participants ( n  = 45) had been in australia for less than 1 year at \n",
      "the time of data collection., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(participant pairs generally came from the \n",
      "same course of study, where they had met each other., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(the period of \n",
      "acquaintance with each other ranged from 1 to 8 months.   \n",
      "\n",
      "\f",
      ", {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(308\n",
      "\n",
      "neomy storch and gillian wigglesworth\n",
      "\n",
      " implementation \n",
      "\n",
      " participants  attended  three  separate  sessions.  in  session  1,  , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(all  pairs \n",
      "composed a data commentary text based on a graphic prompt (see ap-\n",
      "pendix a)., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(they were given 30 min to complete the task and their pair \n",
      "talk was audio-recorded. in session 2 (, {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(day 5), the pairs were provided \n",
      "with feedback on their writing, either in the form of reformulations or \n",
      "editing symbols., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(the pairs were given 15 min to discuss the feedback \n",
      "(processing session), which was then removed by the researcher., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(the \n",
      "pairs were given the unmarked and original version of their text (written \n",
      "in session 1) and had 30 min to rewrite it (rewriting session)., {'neg': 0.0, 'neu': 0.913, 'pos': 0.087, 'compound': 0.3182})\n",
      "(pair talk in \n",
      "both the feedback processing and rewriting sessions was recorded to \n",
      "provide data of how learners process feedback. in session 3 (day 28), \n",
      "the learners individually composed a data commentary text using the \n",
      "same prompt as in session 1. \n",
      "\n",
      " , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(direct feedback was provided in the form of reformulation and involved \n",
      "rewriting the learners’ text, attending to grammatical and lexical errors but \n",
      "preserving the original meaning as much as possible (thornbury,  1997 ). \n",
      ", {'neg': 0.048, 'neu': 0.87, 'pos': 0.083, 'compound': 0.3071})\n",
      "(editing, the indirect form of feedback, involved marking the text with codes \n",
      "that corresponded to certain types of errors and that were explained and \n",
      "exempliﬁ ed in a handout (see appendix b). both forms of feedback were \n",
      "provided by the same native speaker who was an experienced esl teacher. \n",
      ", {'neg': 0.048, 'neu': 0.911, 'pos': 0.042, 'compound': -0.0772})\n",
      "(feedback focused on errors in grammar (morphology and syntax), lexis \n",
      "(word choice), and mechanics (spelling and punctuation). \n",
      "\n",
      " , {'neg': 0.12, 'neu': 0.75, 'pos': 0.13, 'compound': 0.0516})\n",
      "(the following examples illustrate the two forms of feedback. in (1), {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(, \n",
      "the  reformulated  version  contains  ﬁ ve  reformulations:  insertion  of  a \n",
      "phrase ( the graph ), a change in phrase order ( for all cities ), correction of \n",
      "the verb form ( occur ), adjunction of the second sentence to the ﬁ rst, \n",
      "and deletion of a verb ( being )., {'neg': 0.0, 'neu': 0.97, 'pos': 0.03, 'compound': 0.0772})\n",
      "(the example in (2) contains two editing \n",
      "codes:  parentheses  around  the  entire  sentence,  which  signals  that  a \n",
      "word  or  words  need  to  be  deleted,  and  the  code  (c)  placed  above  a \n",
      "word, which indicates an error in word choice.\n",
      "   \n",
      "  \n",
      "   , {'neg': 0.069, 'neu': 0.897, 'pos': 0.033, 'compound': -0.34})\n",
      "((1)    \n",
      "  \n",
      "  \n",
      "\n",
      "    reformulations \n",
      "   a. original \n",
      "    analysing seasonally, the highest rainfalls constantly occurs in summer for all \n",
      "cities. and the lowest being in winter.  \n",
      "   , {'neg': 0.104, 'neu': 0.803, 'pos': 0.092, 'compound': -0.0772})\n",
      "(b. reformulated version \n",
      "    analysing the graph seasonally for all cities, the highest rainfalls constantly \n",
      "occur in summer and the lowest in winter.   \n",
      "\n",
      "  \n",
      "  \n",
      "\n",
      "  , {'neg': 0.11, 'neu': 0.89, 'pos': 0.0, 'compound': -0.3818})\n",
      "((2)    \n",
      "  \n",
      "  \n",
      "\n",
      "    editing \n",
      "   a. original \n",
      "    beijing also had the average rainfall in summer, which was 150 mm., {'neg': 0.0, 'neu': 0.867, 'pos': 0.133, 'compound': 0.3182})\n",
      "(how-\n",
      "ever,, {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(in spring, autumn, and winter, they were roughly 20 mm.  \n",
      "\n",
      "\f",
      ", {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(processing, uptake, and retention of cf\n",
      "\n",
      "309\n",
      "\n",
      "  \n",
      "  \n",
      "\n",
      "  \n",
      "  \n",
      "\n",
      "   b. edited version \n",
      "    (beijing also had the average rainfall in summer, which was 150 mm).   \n",
      "\n",
      "    , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(however, in   spring, autumn, and winter ,   they    were roughly 20 mm.       \n",
      "\n",
      "   , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(c \n",
      "\n",
      "  \n",
      "\n",
      "    data \n",
      "\n",
      " three sources of data were used in this study: the feedback received \n",
      "(reformulations and editing codes on the day 1 texts); the texts written \n",
      "by the pairs on day 1, revised on day 5, and written individually on day \n",
      "28; and the transcribed pair talk elicited on day 5 during the feedback \n",
      "processing and the rewriting sessions.   \n",
      "\n",
      " , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(data analysis \n",
      "\n",
      " for the ﬁ rst source of data, all editing codes and reformulations were \n",
      "counted. when counting the number of reformulations, each word or \n",
      "phrase that was reformulated, deleted, or reordered was counted as a \n",
      "single reformulation., {'neg': 0.0, 'neu': 0.965, 'pos': 0.035, 'compound': 0.0772})\n",
      "(following researchers who work within a sociocul-\n",
      "tural theoretical framework (aljaafreh & lantolf,  1994 ; nassaji & swain, \n",
      " 2000 ), the data were analyzed microgenetically for evidence of uptake \n",
      "and retention across the three sessions. to trace for evidence of uptake \n",
      "of feedback,, {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(texts written on day 1 were compared with those rewritten \n",
      "on day 5 along with the feedback comments., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(revisions were analyzed \n",
      "for whether they were correct or incorrect, whether they were made in \n",
      "response to the feedback given or unsolicited, and whether the changes \n",
      "were made at the word, clause or phrase, or sentence level., {'neg': 0.0, 'neu': 0.965, 'pos': 0.035, 'compound': 0.0772})\n",
      "(revisions \n",
      "made in response to feedback were counted to provide a measure of \n",
      "uptake., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(the texts written on days 1, 5, and 28 were then compared to \n",
      "trace for retention of the feedback. \n",
      "\n",
      " , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(the example in (3) illustrates the procedure used to analyze evidence \n",
      "of uptake and retention of the feedback received., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(the data come from a \n",
      "pair (bing and lina) who received editing feedback and include relevant \n",
      "excerpts from the original and revised texts composed jointly as well as \n",
      "relevant excerpts from the students’ individually produced texts on day \n",
      "28. in the investigation of evidence of uptake, revisions that were con-\n",
      "sistent with either the reformulation or the intent of the editing symbol \n",
      "were coded as correct and indicated by a double check ( ˉ  ˉ )., {'neg': 0.0, 'neu': 0.942, 'pos': 0.058, 'compound': 0.5267})\n",
      "(the sym-\n",
      "bol ( ˉ ˍ) indicated an incorrect change and (ˍ) no change. in (3), two \n",
      "editing symbols were provided in the feedback (to indicate an error in \n",
      "\n",
      "\f",
      "310\n",
      "\n",
      "neomy storch and gillian wigglesworth\n",
      "\n",
      "word choice and in spelling). the revised text contains two changes, \n",
      "both clearly in response to the feedback provided., {'neg': 0.085, 'neu': 0.868, 'pos': 0.047, 'compound': -0.296})\n",
      "(the changes, deemed \n",
      "correct in this instance, were considered as evidence of uptake., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(how-\n",
      "ever, it soon became apparent that employing the same approach to \n",
      "analyze the new texts produced individually in session 3 (day 28) was \n",
      "not always possible because these new texts were often very different \n",
      "from those produced and revised in sessions 1 and 2., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(as can be seen in \n",
      "(3), the text produced by lina on day 28 is very different from that pro-\n",
      "duced collaboratively on day 1 and revised on day 5., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(there is no rele-\n",
      "vant sentence to show evidence that the correction to the word  countries  \n",
      "was retained., {'neg': 0.112, 'neu': 0.765, 'pos': 0.122, 'compound': -0.2023})\n",
      "(thus, the long-term impact of feedback (i.e., retention) \n",
      "was not quantiﬁ ed. instead, a process-product analysis (nassaji & swain, \n",
      " 2000 ) was used, which allowed for a closer focus on the data of four \n",
      "case study pairs.\n",
      "  \n",
      "   \n",
      "   , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "((3)    \n",
      "  \n",
      "  \n",
      "\n",
      "    evidence of uptake and retention of feedback \n",
      "   a. original version \n",
      "    the graph shows the average rainfall (by season) for four countries namely \n",
      "bucharest, lagos, beijing and mexico city in the year 2000.  \n",
      "   , {'neg': 0.0, 'neu': 0.933, 'pos': 0.067, 'compound': 0.3182})\n",
      "(b. feedback (editing) \n",
      "\n",
      "    the graph shows the average rainfall c (by season) for four    countries    namely  \n",
      "\n",
      "   c \n",
      "\n",
      "    bucharest, lagos, beijing and mexico    c   ity in the year 2000.  \n",
      "   , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(c. revised version \n",
      "\n",
      "   x \n",
      "\n",
      "    ˉ  ˉ  \n",
      "\n",
      "    the  graph  shows  the  average  rainfall  (by  season)  for  four  cities  namely \n",
      "bucharest,  \n",
      "\n",
      "    ˉ  ˉ  \n",
      "\n",
      "    lagos, beijing and mexico city in the year 2000.  \n",
      "   , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(d. bing’s new text \n",
      "    the graph shows the average seasonal rainfall in the year 2000 at four different  \n",
      "    ˉ  ˉ    \n",
      " ˉ  ˉ  \n",
      "    cities which are bucharest, lagos, beijing and mexico city.  \n",
      "   , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(e. lina’s new text \n",
      "    this report will explain the graph about the seasonal rainfall in bucharest, \n",
      "lagos,  \n",
      "\n",
      "   ˍ \n",
      "\n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "\n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "\n",
      "  \n",
      "  \n",
      "   \n",
      "\n",
      "    beijing and mexico city in 2000.    \n",
      "\n",
      "  \n",
      "  , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(the  transcribed  pair  talk  from  the  processing  and  the  rewriting \n",
      "sessions were analyzed for language-related episodes (lres; swain & \n",
      "lapkin,   1998 ).  , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(lres  were  deﬁ ned  as  segments  in  the  pair  talk  during \n",
      "which learners focused explicitly on language items., {'neg': 0.0, 'neu': 0.867, 'pos': 0.133, 'compound': 0.3818})\n",
      "(this included in-\n",
      "stances in which learners read the reformulated text aloud, deliberated \n",
      "over  the  reformulation,  discussed  how  to  revise  in  response  to  an \n",
      "editing symbol, or deliberated over language items that, although not \n",
      "\n",
      "\f",
      "processing, uptake, and retention of cf\n",
      "\n",
      "311\n",
      "\n",
      "targeted by the feedback, the learners felt needed amendment., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(thus, all \n",
      "lres were ﬁ rst identiﬁ ed, regardless of whether they dealt with language \n",
      "items targeted by the feedback., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(lres varied in length: some were com-\n",
      "posed of a short turn (e.g., a learner simply read aloud the reformula-\n",
      "tion received) and others of multiple turns (e.g., learners deliberated \n",
      "about word choice)., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(all lres were then further analyzed for focus to \n",
      "distinguish between episodes that dealt with form (i.e., morphosyntax; \n",
      "f-lre),  lexis  (l-lre),  and  mechanics  (i.e.,  spelling  and  punctuation; \n",
      "m-lre)., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(it was also noted whether the lres were resolved correctly ( √ ), \n",
      "incorrectly (x), or left unresolved (?)., {'neg': 0.0, 'neu': 0.909, 'pos': 0.091, 'compound': 0.1779})\n",
      "(correct resolution in this study \n",
      "referred to resolutions that accorded with the feedback or to alterna-\n",
      "tives deemed as equally acceptable by the researchers. to link uptake \n",
      "and retention to how the feedback was processed, a process-product \n",
      "analysis  (nassaji  &  swain,   2000 )  was  conducted.  , {'neg': 0.0, 'neu': 0.947, 'pos': 0.053, 'compound': 0.3182})\n",
      "(the  lres  that  dealt \n",
      "with  language  items  that  received  feedback  were  identiﬁ ed  and  ana-\n",
      "lyzed for the nature of engagement. \n",
      "\n",
      " , {'neg': 0.0, 'neu': 0.87, 'pos': 0.13, 'compound': 0.4588})\n",
      "(based on the work of several researchers (qi & lapkin,  2001 ; storch, \n",
      " 2008 ;  tocalli-beller  &  swain,   2005 ),  a  distinction  was  made  between \n",
      "lres that showed extensive engagement and those that showed limited \n",
      "or no engagement., {'neg': 0.105, 'neu': 0.742, 'pos': 0.153, 'compound': 0.4404})\n",
      "(lres that showed evidence of extensive engagement \n",
      "included episodes in which learners offered suggestions and counter-\n",
      "suggestions, explanations, or any comments that showed evidence of \n",
      "meta-awareness  of  the  feedback  received  (e.g.,   we  don’t  have  to  use  \n",
      "being)., {'neg': 0.0, 'neu': 0.923, 'pos': 0.077, 'compound': 0.4588})\n",
      "(lres that showed evidence of limited engagement included epi-\n",
      "sodes in which one member of the pair simply read the feedback and \n",
      "the other merely acknowledged or repeated it. \n",
      "\n",
      " to code for lres, one of the researchers, who has extensive expe-\n",
      "rience in coding pair talk data for lres, developed coding guidelines \n",
      "based on the data (see appendix c)., {'neg': 0.031, 'neu': 0.921, 'pos': 0.048, 'compound': 0.2732})\n",
      "(using these guidelines, a second \n",
      "rater  was  trained  and  coded  four  transcripts  independently.  , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(reli-\n",
      "ability  scores  were  calculated  using  simple  percentage  agreement. \n",
      ", {'neg': 0.0, 'neu': 0.56, 'pos': 0.44, 'compound': 0.6705})\n",
      "(interrater  reliability  scores  for  identifying  lres  was  91%;  disagree-\n",
      "ments  seemed  to  be  due  mainly  to  oversights.  , {'neg': 0.14, 'neu': 0.86, 'pos': 0.0, 'compound': -0.3818})\n",
      "(interrater  reliability \n",
      "was the lowest (84%) when coding for the nature of engagement., {'neg': 0.157, 'neu': 0.663, 'pos': 0.181, 'compound': 0.1027})\n",
      "(dis-\n",
      "cussion  between  the  two  raters  led  to  some  modiﬁ cations  of  the \n",
      "coding guidelines. \n",
      "\n",
      "  , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(figure 1  contains excerpts from the text (original and reformulated) \n",
      "and pair talk data to illustrate the coding procedure for lres., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(the data \n",
      "come from a pair (gus and jon) who received reformulations., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(the refor-\n",
      "mulated  version  contains  ﬁ ve  reformulations  ( went ,   from ,   start ,   from , \n",
      "and  to )., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(the pair talk excerpt was coded as containing ﬁ ve lres related \n",
      "to the feedback received: two f-lres related to verb tenses and three \n",
      "l-lres that dealt with the choice of prepositions., {'neg': 0.058, 'neu': 0.942, 'pos': 0.0, 'compound': -0.2263})\n",
      "(all were resolved cor-\n",
      "rectly., {'neg': 0.0, 'neu': 0.702, 'pos': 0.298, 'compound': 0.1779})\n",
      "(the ﬁ ve lres were coded as showing limited engagement; during \n",
      "the processing session, gus read and acknowledged the reformulations, \n",
      "\n",
      "\f",
      "312\n",
      "\n",
      "neomy storch and gillian wigglesworth\n",
      "\n",
      "with  no  involvement  from  jon.  , {'neg': 0.117, 'neu': 0.798, 'pos': 0.085, 'compound': -0.0258})\n",
      "(all  ﬁ ve  lres  were  contained  within  a \n",
      "single turn.     \n",
      "\n",
      "  , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(figure  2   provides  an  additional  example  that  illustrates  coding  for \n",
      "lres from diana and monica, who received editing feedback. the edit-\n",
      "ing symbol (underlining of the phrase), {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(signals that there is something \n",
      "wrong with the expression., {'neg': 0.279, 'neu': 0.721, 'pos': 0.0, 'compound': -0.4767})\n",
      "(the pair talk contained a segment coded as \n",
      "a  l-lre  and  directly  related  to  the  feedback  provided.  , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(the  lre  was \n",
      "coded as correctly resolved because the alternative the learners settled \n",
      "on  ( winter  rainfall )  was  considered  acceptable  in  this  instance.  , {'neg': 0.0, 'neu': 0.833, 'pos': 0.167, 'compound': 0.4588})\n",
      "(how-\n",
      "ever, unlike the lres given in  figure 1 , this lre shows evidence of ex-\n",
      "tensive  engagement.  , {'neg': 0.0, 'neu': 0.833, 'pos': 0.167, 'compound': 0.4588})\n",
      "(the  editing  symbol  initially  challenges  diana’s \n",
      "previously held belief that this phrase is acceptable ( why i think  rainfall \n",
      "of winter  is correct ?), but she then realizes that it should be reduced to \n",
      "a  nominal  phrase,  and  monica  agrees.  , {'neg': 0.0, 'neu': 0.872, 'pos': 0.128, 'compound': 0.4588})\n",
      "(diana  notes  that  this  is  some-\n",
      "thing she has not previously learned ( this is the ﬁ rst time i know this ). \n",
      ", {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(monica  then  suggests  that  the  original  expression  is  not  necessarily \n",
      "wrong but that the alternative is a simpler or more direct expression.     \n",
      "\n",
      " finally,, {'neg': 0.0, 'neu': 0.854, 'pos': 0.146, 'compound': 0.3457})\n",
      "(in analyzing the pair talk, any other salient features were also \n",
      "noted,  particularly  comments  that  reﬂ ected  the  learners’  attitudes \n",
      "\n",
      "original\n",
      "\n",
      "the rainfall goes up during spring to summer and start to \n",
      "\n",
      "decrease during autumn and winter.  \n",
      "\n",
      ", {'neg': 0.0, 'neu': 0.888, 'pos': 0.112, 'compound': 0.5267})\n",
      "(reformulated \n",
      "\n",
      "the rainfall went up from spring to summer and started to \n",
      "\n",
      "decrease from autumn to winter.\n",
      "\n",
      ", {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(analysis of relevant pair talk data \n",
      "\n",
      "pair talk \n",
      "\n",
      "focus\n",
      "\n",
      "feedback \n",
      "\n",
      "resolution  engagement \n",
      "\n",
      "gus: the rainfall went\n",
      "\n",
      "f-lre (verb: went)\n",
      "\n",
      "up from spring to \n",
      "\n",
      "l-lre (prep: from)\n",
      "\n",
      "summer went okay \n",
      "\n",
      "and started to \n",
      "\n",
      "f-lre (verb: started)\n",
      "\n",
      "decrease from \n",
      "\n",
      "l-lre (prep: from)\n",
      "\n",
      "autumn to winter \n",
      "\n",
      "l-lre (prep: to)\n",
      "\n",
      "related \n",
      "\n",
      "related \n",
      "\n",
      "related \n",
      "\n",
      "related \n",
      "\n",
      "related \n",
      "\n",
      "okay.\n",
      "\n",
      "limited\n",
      "\n",
      "limited\n",
      "\n",
      "limited\n",
      "\n",
      "limited\n",
      "\n",
      "limited\n",
      "\n",
      " \n",
      "\n",
      "note:  the abbreviation “prep, {'neg': 0.137, 'neu': 0.765, 'pos': 0.098, 'compound': -0.1779})\n",
      "(” corresponds to preposition.\n",
      "\n",
      " \n",
      "\n",
      " , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(figure 1.    \n",
      "\n",
      "    analyzing and coding pair talk for lres.    \n",
      "\n",
      "\f",
      ", {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(processing, uptake, and retention of cf\n",
      "\n",
      "313\n",
      "\n",
      "original\n",
      "\n",
      "however, the rainfall of winter in lagos is the\n",
      "\n",
      "second highest among the four cities\n",
      "\n",
      "edited versions\n",
      "\n",
      "however, the rainfall of winter in lagos is the \n",
      "\n",
      "second highest among the four cities\n",
      "\n",
      "analysis of relevant pair talk data\n",
      "\n",
      "pair talk\n",
      "\n",
      "focus\n",
      "\n",
      "feedback\n",
      "\n",
      "resolution\n",
      "\n",
      "engagement\n",
      "\n",
      "diana: the rainfall of winter oh \n",
      "\n",
      "l-lre\n",
      "\n",
      "related\n",
      "\n",
      "extensive\n",
      "\n",
      "winter rainfall right?, {'neg': 0.0, 'neu': 0.921, 'pos': 0.079, 'compound': 0.6486})\n",
      "(winter rainfall. \n",
      "\n",
      ", {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(it’s the rain of winter., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(why i think \n",
      "\n",
      "rainfall of winter is correct?, {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(winter \n",
      "\n",
      "rainfall… ah huh, should be winter \n",
      "\n",
      "rainfall\n",
      "\n",
      "monica: yeah,, {'neg': 0.0, 'neu': 0.804, 'pos': 0.196, 'compound': 0.296})\n",
      "(i think so\n",
      "\n",
      "diana: oh, this is the first time i \n",
      "\n",
      "know this\n",
      "\n",
      "monica: not…not…no it’s more \n",
      "\n",
      "uh…most simple\n",
      "\n",
      "diana: winter rainfall okay\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " figure 2.    \n",
      "\n",
      "    , {'neg': 0.0, 'neu': 0.921, 'pos': 0.079, 'compound': 0.2263})\n",
      "(analysis of pair talk for lres.    \n",
      "\n",
      "toward the feedback provided and their beliefs about conventions of \n",
      "language use., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(thus, in the example in  figure 2 , it was noted that the \n",
      "feedback posed a challenge to diana’s previously held knowledge but \n",
      "also provided a learning occasion and that monica seemed to think that \n",
      "the revised phrase was not more grammatical but more direct.    \n",
      "\n",
      " , {'neg': 0.0, 'neu': 0.972, 'pos': 0.028, 'compound': 0.0387})\n",
      "(findings \n",
      "\n",
      " the  quantitative  ﬁ ndings  from  the  larger  research  project  (storch  & \n",
      "wigglesworth, 2006) will be presented ﬁ rst: a comparison of the amount \n",
      "of feedback provided via editing and reformulations, the amount of up-\n",
      "take, and the number and nature of lres., {'neg': 0.0, 'neu': 0.969, 'pos': 0.031, 'compound': 0.0772})\n",
      "(case studies will then be used \n",
      "\n",
      "\f",
      "314\n",
      "\n",
      "neomy storch and gillian wigglesworth\n",
      "\n",
      "to report on the qualitative analysis, which attempted to link evidence \n",
      "of uptake (and retention) with the nature of the learners’ engagement \n",
      "with the feedback as well as their beliefs and goals. \n",
      "\n",
      "  , {'neg': 0.0, 'neu': 0.892, 'pos': 0.108, 'compound': 0.6249})\n",
      "(table 1  presents the amount of feedback provided via reformulations \n",
      "and editing as well as the uptake of this feedback as evident in the re-\n",
      "vised texts., {'neg': 0.0, 'neu': 0.923, 'pos': 0.077, 'compound': 0.2732})\n",
      "(the number of reformulations was almost double that of \n",
      "the editing symbols, but there was considerable variation in the number \n",
      "of feedback comments received by each pair, as evident by the large \n",
      "range.  , {'neg': 0.0, 'neu': 0.923, 'pos': 0.077, 'compound': 0.1531})\n",
      "(table 1  also shows that there was more uptake following editing \n",
      "than following reformulations., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(however, it should be noted that analysis \n",
      "(and quantiﬁ cation) of uptake in revised texts following reformulations \n",
      "proved quite difﬁ cult., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(revisions in response to reformulations were of-\n",
      "ten made at the phrase and sentence level; thus, the revised texts con-\n",
      "tained  new  and  completely  rewritten  sentences.  in  contrast,  texts \n",
      "revised  following  editing  feedback  contained  changes  mainly  at  the \n",
      "word level, which were more easily traceable to the editing feedback \n",
      "received.     \n",
      "\n",
      "  , {'neg': 0.0, 'neu': 0.896, 'pos': 0.104, 'compound': 0.5379})\n",
      "(table 2  summarizes the lres for the entire dataset.  , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(table 2  shows \n",
      "that despite the larger number of reformulations in relation to the edit-\n",
      "ing symbols (as shown in  table 1 ), reformulations elicited fewer lres \n",
      "than editing feedback during the processing session (day 5, session 1). \n",
      "in the rewriting sessions,, {'neg': 0.03, 'neu': 0.97, 'pos': 0.0, 'compound': -0.0572})\n",
      "(the number of lres was similar, regardless of \n",
      "feedback type., {'neg': 0.0, 'neu': 0.874, 'pos': 0.126, 'compound': 0.0772})\n",
      "(most of the lres in both sessions and in response to \n",
      "both types of feedback focused on grammar and lexis rather than on \n",
      "mechanics, and most were correctly resolved.     \n",
      "\n",
      " in both the processing and rewriting sessions, {'neg': 0.0, 'neu': 0.882, 'pos': 0.118, 'compound': 0.552})\n",
      "(, the percentage of lres \n",
      "directly related to the feedback was lower in the data of learners who \n",
      "received  reformulations  compared  with  those  who  received  editing \n",
      "feedback., {'neg': 0.081, 'neu': 0.919, 'pos': 0.0, 'compound': -0.296})\n",
      "(it should be noted that in the rewriting session, many of the \n",
      "lres directly related to the feedback dealt with the same language items \n",
      "as  those  discussed  during  the  processing  session.  , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(there  was  more \n",
      "\n",
      " table 1.    \n",
      ", {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(dataset          \n",
      "\n",
      "   feedback \n",
      "\n",
      " amount of feedback \n",
      "    m  \n",
      "   range \n",
      " uptake \n",
      "    m  \n",
      "   range \n",
      " % of feedback \n",
      "\n",
      "    feedback and uptake for the entire \n",
      "\n",
      " reformulations \n",
      "\n",
      " editing     \n",
      "\n",
      " 327 \n",
      " 27.25 \n",
      " 8–53 \n",
      " 182 \n",
      " 15.17 \n",
      " 3–34 \n",
      " 55.66% \n",
      "\n",
      " 171   \n",
      " 14.25   \n",
      " 2–26   \n",
      " 150   \n",
      " 12.50   \n",
      " 0–26   \n",
      " 87.72%   \n",
      "\n",
      "\f",
      "processing, uptake, and retention of cf\n",
      "\n",
      "315\n",
      "\n",
      " table 2.    \n",
      "\n",
      "    , {'neg': 0.089, 'neu': 0.911, 'pos': 0.0, 'compound': -0.5994})\n",
      "(lres during processing and rewriting sessions              \n",
      "\n",
      "   summary of lres \n",
      "\n",
      " processing   rewriting \n",
      "\n",
      " processing   rewriting     \n",
      "\n",
      " reformulations \n",
      "\n",
      " editing   \n",
      "\n",
      " n  \n",
      " m  \n",
      "\n",
      " occurrences   \n",
      "  \n",
      "  \n",
      "   range \n",
      " focus   \n",
      "   f-lres \n",
      "   l-lres \n",
      "   m-lres \n",
      " resolution   \n",
      "   correct \n",
      "  \n",
      "   unresolved \n",
      "   % correct of all lres \n",
      " related to feedback   \n",
      "  \n",
      "   % of all lres \n",
      " showing extensive engagement   \n",
      "  \n",
      "   % of lres related to feedback \n",
      "\n",
      "incorrect \n",
      "\n",
      " n  \n",
      "\n",
      " n  \n",
      "\n",
      " 128 \n",
      " 10.67 \n",
      " 1–21 \n",
      "\n",
      " 207 \n",
      " 18.82 \n",
      " 1–43 \n",
      "\n",
      " 189 \n",
      " 15.75 \n",
      " 4–25 \n",
      "\n",
      " 227   \n",
      " 20.64   \n",
      " 7–36   \n",
      "\n",
      " 58 (45%) \n",
      " 65 (51%) \n",
      " 5 (4%) \n",
      "\n",
      " 80 (39%) \n",
      " 101 (49%) \n",
      " 26 (13%) \n",
      "\n",
      " 79 (42%) \n",
      " 82 (43%) \n",
      " 28 (15%) \n",
      "\n",
      " 74 (33%)   \n",
      " 123 (54%)   \n",
      " 29 (13%)   \n",
      "\n",
      " 100 \n",
      " 18 \n",
      " 10 \n",
      " 78% \n",
      "\n",
      " 73 \n",
      " 22% \n",
      "\n",
      " 44 \n",
      " 60% \n",
      "\n",
      " 161 \n",
      " 36 \n",
      " 10 \n",
      " 78% \n",
      "\n",
      " 44 \n",
      " 21% \n",
      "\n",
      " 20 \n",
      " 45% \n",
      "\n",
      " 144 \n",
      " 25 \n",
      " 20 \n",
      " 76% \n",
      "\n",
      " 125 \n",
      " 66% \n",
      "\n",
      " 95 \n",
      " 76% \n",
      "\n",
      " 187   \n",
      " 33   \n",
      " 7   \n",
      "\n",
      " 82%   \n",
      "\n",
      " 73   \n",
      " 32%   \n",
      "\n",
      " 52   \n",
      " 71%   \n",
      "\n",
      "extensive engagement with feedback in response to editing than in re-\n",
      "sponse  to  reformulations,  in  both  the  processing  and  rewriting \n",
      "sessions. \n",
      "\n",
      " , {'neg': 0.03, 'neu': 0.926, 'pos': 0.044, 'compound': 0.2263})\n",
      "(the case studies illustrate how a number of factors, both linguistic \n",
      "and  affective,  impacted  uptake  and  retention  of  feedback.  , {'neg': 0.0, 'neu': 0.929, 'pos': 0.071, 'compound': 0.0772})\n",
      "(the  case \n",
      "study  participants  were  fairly  representative  of  the  entire  participant \n",
      "cohort in terms of language background, l2 proﬁ ciency, degree program, \n",
      "and length of acquaintance. pairs who received a large amount of feed-\n",
      "back but who showed contrastive patterns of engagement with the feed-\n",
      "back were selected. for two of the pairs,, {'neg': 0.0, 'neu': 0.927, 'pos': 0.073, 'compound': 0.6124})\n",
      "(the feedback elicited a large \n",
      "number of lres, whereas for the two other pairs, it elicited few lres.   \n",
      "\n",
      " first case-study pair  \n",
      "\n",
      " background \n",
      "\n",
      " the ﬁ rst pair consisted of one male (eko) and one female (sherry) grad-\n",
      "uate student, both indonesian, with ielts scores of 7.5 upon entry to \n",
      "the university., {'neg': 0.0, 'neu': 0.974, 'pos': 0.026, 'compound': 0.0772})\n",
      "(both had studied english for several years at the high \n",
      "\n",
      "\f",
      "316\n",
      "\n",
      "neomy storch and gillian wigglesworth\n",
      "\n",
      "school and university level in indonesia., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(they had known each other for \n",
      "1 month., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(eko was a graduate student in commerce and sherry was a \n",
      "graduate student in cultural studies.   \n",
      "\n",
      " , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(analysis of written texts \n",
      "\n",
      " the pair received a large number of reformulations ( n  = 40) on the text \n",
      "they wrote in the ﬁ rst session., {'neg': 0.0, 'neu': 0.942, 'pos': 0.058, 'compound': 0.0772})\n",
      "(however, a closer analysis of their errors \n",
      "and reformulations revealed that almost half of the errors (17/40) were \n",
      "errors in mechanics (spelling, capitalization)., {'neg': 0.275, 'neu': 0.725, 'pos': 0.0, 'compound': -0.7351})\n",
      "(word choice and expres-\n",
      "sion errors made up the next largest category ( n  = 6)., {'neg': 0.167, 'neu': 0.833, 'pos': 0.0, 'compound': -0.34})\n",
      "(errors in grammar \n",
      "varied and included errors in morphology (e.g., tense, agreement) and \n",
      "syntax (e.g., incomplete sentences, word order). \n",
      "\n",
      " the revised text (day 5) was very similar to the original text, with re-\n",
      "visions made mainly at the word level., {'neg': 0.155, 'neu': 0.73, 'pos': 0.116, 'compound': -0.4215})\n",
      "(analysis for evidence of uptake \n",
      "showed that of the 40 reformulations, 30 were taken up., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(there were far \n",
      "fewer errors in mechanics in the revised text ( n  = 6) and in the texts \n",
      "produced individually on day 28 (four errors in eko’s text and none in \n",
      "sherry’s text)., {'neg': 0.142, 'neu': 0.858, 'pos': 0.0, 'compound': -0.5859})\n",
      "(there was also a decrease in word choice errors in the \n",
      "revised text on day 5 (only one error)., {'neg': 0.13, 'neu': 0.87, 'pos': 0.0, 'compound': -0.34})\n",
      "(however, the texts produced in-\n",
      "dividually on day 28 showed a number of errors in word choice or ex-\n",
      "pression (three for eko and ﬁ ve for sherry), and some of these errors \n",
      "were the same as those found in the text produced jointly on day 1.   \n",
      "\n",
      " , {'neg': 0.098, 'neu': 0.876, 'pos': 0.026, 'compound': -0.5423})\n",
      "(analysis of pair talk \n",
      "\n",
      " eko and sherry’s pair talk showed that despite the large number of refor-\n",
      "mulations ( n  = 40), only 14 lres were generated by this pair in the pro-\n",
      "cessing  session  and  22  in  the  rewriting  session.  , {'neg': 0.032, 'neu': 0.968, 'pos': 0.0, 'compound': -0.0572})\n",
      "(although  most  were \n",
      "related to the feedback, these lres focused mainly on lexical choices. \n",
      ", {'neg': 0.0, 'neu': 0.833, 'pos': 0.167, 'compound': 0.3818})\n",
      "(very few lres (only two in the processing and one in the rewriting ses-\n",
      "sion)  dealt  with  errors  in  mechanics.  furthermore,  whereas  l-lres \n",
      "showed an extensive level of engagement, m-lres showed a limited level \n",
      "of engagement., {'neg': 0.104, 'neu': 0.751, 'pos': 0.145, 'compound': 0.4019})\n",
      "(thus, it seemed that although the pair did not deliberate \n",
      "over errors in mechanics, there was a high level of uptake and retention \n",
      "of the reformulated feedback on these errors., {'neg': 0.0, 'neu': 0.869, 'pos': 0.131, 'compound': 0.4717})\n",
      "(however, a high level of \n",
      "engagement with feedback on errors in lexis that led to uptake did not \n",
      "always result in retention., {'neg': 0.098, 'neu': 0.779, 'pos': 0.123, 'compound': 0.1531})\n",
      "(the examples given in (4)–(8), taken from the \n",
      "data of this pair, suggest possible explanations for these ﬁ ndings. \n",
      "\n",
      " , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(the example in (4) contains relevant excerpts that discuss the choice \n",
      "of  where  and  when , an error that occurred on two occasions in the orig-\n",
      "inal text produced by this pair on day 1.\n",
      "   \n",
      "\n",
      "  \n",
      "\n",
      "\f",
      ", {'neg': 0.076, 'neu': 0.924, 'pos': 0.0, 'compound': -0.4019})\n",
      "(processing, uptake, and retention of cf\n",
      "\n",
      "317\n",
      "\n",
      "   (4)    \n",
      "  \n",
      "  \n",
      "\n",
      "  \n",
      "  \n",
      "\n",
      "  \n",
      "  \n",
      "\n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "\n",
      "  \n",
      "  \n",
      "\n",
      "  \n",
      "  \n",
      "\n",
      "    excerpts from texts and pair talk dealing with  where  versus  when  \n",
      "   a. original version \n",
      "    constantly for every season, lagos has the highest rainfall except for winter \n",
      "where the highest rainfall being in bucharest.  \n",
      "   b. reformulated version \n",
      "    for every season, lagos constantly has the highest rainfall except for in win-\n",
      "ter when the highest rainfall is in bucharest.  \n",
      "   , {'neg': 0.0, 'neu': 0.912, 'pos': 0.088, 'compound': 0.7269})\n",
      "(c. relevant lres \n",
      "   eko:   mmm.  , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(say  it’s   when   instead  of   where .  which  makes  sense  because \n",
      "we’re comparing time.  \n",
      "   , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "([…] \n",
      "   sherry: except for winter when \n",
      "   eko: when , yes., {'neg': 0.0, 'neu': 0.748, 'pos': 0.252, 'compound': 0.4019})\n",
      "(remember that,  when !  \n",
      "   sherry: when the highest rainfall \n",
      "   d. revised version \n",
      "    for every season, lagos constantly has the highest rainfall, except for in win-\n",
      "ter when the highest rainfall is in bucharest.  \n",
      "   , {'neg': 0.0, 'neu': 0.883, 'pos': 0.117, 'compound': 0.6239})\n",
      "(e. eko’s new text \n",
      "    the highest rainfall in all seasons occurs in lagos, except in winter when the \n",
      "highest rainfall occurs in bucharest.  \n",
      "   , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(f. sherry’s new text \n",
      "    lagos constantly has the highest rainfall across all seasons except in winter \n",
      "when the highest is bucharest.    \n",
      "\n",
      "  \n",
      "\n",
      "   \n",
      "  as illustrated in (4), during the feedback processing session on day 5, \n",
      ", {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(the learners gained an understanding of the difference in the use of the \n",
      "two adverbs  where  and  when ., {'neg': 0.0, 'neu': 0.867, 'pos': 0.133, 'compound': 0.3818})\n",
      "(the texts produced on days 5 and 28 by \n",
      "both learners showed evidence of uptake and retention of the  where - \n",
      "when  distinction. these adverbs were used correctly in the revised text \n",
      "and in the individually produced texts (eko: all four instances; sherry: \n",
      "all three instances). \n",
      "\n",
      " , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(the example in (4) and the outcome in terms of uptake and retention \n",
      "contrasts with how the learners dealt with the use of the word  ﬂ uctua-\n",
      "tive , as in example (5). here, {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(, there is limited engagement with the feed-\n",
      "back provided., {'neg': 0.16, 'neu': 0.588, 'pos': 0.252, 'compound': 0.2732})\n",
      "(sherry merely reads the reformulated version and eko \n",
      "does not respond, which may explain why no uptake or retention took \n",
      "place and the error persisted.\n",
      "   \n",
      "  \n",
      "   , {'neg': 0.176, 'neu': 0.824, 'pos': 0.0, 'compound': -0.5994})\n",
      "((5)    \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "\n",
      "    excerpts from texts and pair talk dealing with  ﬂ uctuative  \n",
      "   (a) original version \n",
      "    whereas the more ﬂ uctuative level of rainfall  \n",
      "   (b) reformulation \n",
      "    whereas the more extreme levels of rainfall  \n",
      "   (c) relevant lre \n",
      "   sherry: whereas the most extreme,  oh,  extreme… most extreme level of \n",
      "rainfall \n",
      "   eko: analysing the… \n",
      "\n",
      "  \n",
      "\n",
      "\f",
      "318\n",
      "\n",
      "neomy storch and gillian wigglesworth\n",
      "\n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "   \n",
      "\n",
      "   (d) eko’s new text \n",
      "    although more ﬂ uctuative that bucharest  \n",
      "   (e) sherry’s new text \n",
      "    lagos, in the other hand, has the most ﬂ uctuative rainfall    \n",
      "\n",
      "  \n",
      "  example  (6)  shows  how  the  pair  dealt  with  a  stylistic  choice  of \n",
      "\n",
      "expression.  merely around  was reformulated as  around a mere .\n",
      "   \n",
      "  \n",
      "   , {'neg': 0.0, 'neu': 0.946, 'pos': 0.054, 'compound': 0.6705})\n",
      "((6)    \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "\n",
      "    excerpts from texts and pair talk dealing with  merely around  \n",
      "   (a) original version \n",
      "    it ﬂ uctuates merely around 50mm  \n",
      "   (b) reformulation \n",
      "    it ﬂ uctuates around a mere 50mm  \n",
      "   (c) relevant lres \n",
      "   sherry:  don’t use  merely.  , {'neg': 0.0, 'neu': 0.937, 'pos': 0.063, 'compound': 0.3182})\n",
      "(oh,  around a mere,  oh,  around a mere \n",
      "   eko:  a more sophisticated way of getting it.  \n",
      "   , {'neg': 0.0, 'neu': 0.755, 'pos': 0.245, 'compound': 0.5984})\n",
      "([…] \n",
      "   sherry:  where it ﬂ uctuates… around… around  \n",
      "   eko: a mere \n",
      "   sherry : is that the how to put it?  around a mere ﬁ fty millimetres ?  \n",
      "   , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(eko:  that’s how she put it.  \n",
      "   , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(sherry :  i  don’t  know  how  to  put   merely   there.   around  a  mere  , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(ﬁ fty \n",
      "millimetres \n",
      "   eko:   i  think   merely  around  ﬁ fty  millimetres   would  be,  you  know,  it \n",
      "equate.  \n",
      "   , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(sherry:  oh, ok  \n",
      "   (d) revised version \n",
      "    it ﬂ uctuates around a mere 50mm  \n",
      "   (e) eko’s new text \n",
      "    it ﬂ uctuates merely around 50 mm  \n",
      "   (f) sherry’s new text \n",
      "    it only ﬂ uctuates a little around 50 mm    \n",
      "\n",
      "  \n",
      "\n",
      "  \n",
      "\n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "   \n",
      "  as can be seen in (6), in the processing session, there is extensive en-\n",
      "gagement with this reformulation., {'neg': 0.0, 'neu': 0.959, 'pos': 0.041, 'compound': 0.296})\n",
      "(sherry notes the reformulated phrase, \n",
      "and eko comments that this is a more sophisticated way of expressing \n",
      "their idea., {'neg': 0.0, 'neu': 0.814, 'pos': 0.186, 'compound': 0.5984})\n",
      "(however, during the rewriting session, although sherry re-\n",
      "calls the reformulation after some hesitation and assistance from eko, \n",
      "she expresses some doubts about her ability to use the expression., {'neg': 0.132, 'neu': 0.798, 'pos': 0.071, 'compound': -0.25})\n",
      "(eko \n",
      "suggests  that  both  expressions  are  equivalent  in  meaning.  , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(thus,  al-\n",
      "though the revised version shows evidence of uptake, it is not retained \n",
      "at  day  28.  , {'neg': 0.063, 'neu': 0.937, 'pos': 0.0, 'compound': -0.0191})\n",
      "(this  may  be  because  the  learners  do  not  feel  comfortable \n",
      "using this expression or because they believe that the two expressions \n",
      "are equivalent in meaning. \n",
      "\n",
      " , {'neg': 0.101, 'neu': 0.899, 'pos': 0.0, 'compound': -0.4023})\n",
      "(the strategy the learners adopted was to memorize the reformula-\n",
      "tions, as shown in (7), and the goal driving their revisions was the de-\n",
      "sire to improve the accuracy of their text, as can be seen in (8).\n",
      "   \n",
      "\n",
      "  \n",
      "\n",
      "\f",
      ", {'neg': 0.0, 'neu': 0.927, 'pos': 0.073, 'compound': 0.4404})\n",
      "(processing, uptake, and retention of cf\n",
      "\n",
      "319\n",
      "\n",
      "    excerpt from the pair talk in the processing session \n",
      "   sherry:  maybe we ought to memorise, ah.  \n",
      "   , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(eko:  this  \n",
      "   sherry:  , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(so i memorise the ﬁ rst paragraph, you the second   \n",
      "\n",
      "    excerpt from the pair talk in the rewriting session \n",
      "   sherry:  we’re not supposed to write anything totally different, right?  \n",
      "   , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(eko:  i’m not sure., {'neg': 0.395, 'neu': 0.605, 'pos': 0.0, 'compound': -0.2411})\n",
      "(i think, i think… we should, you know, make it better., {'neg': 0.0, 'neu': 0.734, 'pos': 0.266, 'compound': 0.4404})\n",
      "(you \n",
      "know, to improve it.    \n",
      "\n",
      "   , {'neg': 0.0, 'neu': 0.58, 'pos': 0.42, 'compound': 0.4404})\n",
      "((7)    \n",
      "  \n",
      "  \n",
      "  \n",
      "\n",
      "  (8)    \n",
      "  \n",
      "  \n",
      "\n",
      "   \n",
      "\n",
      "  \n",
      "\n",
      "    summary \n",
      "\n",
      " eko and sherry were very proﬁ cient (ielts 7.5), and most of their errors \n",
      "were fairly superﬁ cial (i.e., mechanical) and easily corrected. therefore, \n",
      "although the transcripts had only limited evidence of processing (i.e., \n",
      "few lres), the learners clearly noticed the reformulations and were able \n",
      "to address their errors in their subsequent writing. \n",
      "\n",
      " , {'neg': 0.114, 'neu': 0.805, 'pos': 0.081, 'compound': -0.2795})\n",
      "(other errors, particularly errors in word choice, required more overt \n",
      "attention., {'neg': 0.348, 'neu': 0.652, 'pos': 0.0, 'compound': -0.5859})\n",
      "(here there was evidence of extended engagement and under-\n",
      "standing  (e.g.,  the   when - where   distinction),  which  led  to  uptake  and \n",
      "retention. however, where there was lack of engagement (e.g., as with \n",
      "the word  ﬂ uctuative ) or resistance to the reformulation, there was no \n",
      "long-term retention, despite evidence of uptake (e.g., as with the expres-\n",
      "sion  around a mere ) at day 5.    \n",
      "\n",
      " , {'neg': 0.068, 'neu': 0.842, 'pos': 0.09, 'compound': 0.3612})\n",
      "(second case-study pair  \n",
      "\n",
      " background \n",
      "\n",
      " this pair was composed of two male indonesian students, gus and jon. \n",
      ", {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(both were graduate students in engineering and had an ielts score of \n",
      "6.5 upon entry to the university., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(both had studied english for several \n",
      "years  at  the  high  school  and  university  level  in  indonesia.  , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(they  had \n",
      "known each other for 6 months.   \n",
      "\n",
      " , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(analysis of written texts \n",
      "\n",
      " as in the case of the ﬁ rst pair, the text produced by this pair showed a \n",
      "large number of errors and, consequently, a large number of reformula-\n",
      "tions ( n  = 43)., {'neg': 0.071, 'neu': 0.853, 'pos': 0.076, 'compound': -0.2023})\n",
      "(most of the errors were in verb use ( n  = 14), prepositions \n",
      "( n  = 6), and sentence structure ( n  = 6)., {'neg': 0.157, 'neu': 0.766, 'pos': 0.077, 'compound': -0.3321})\n",
      "(however, unlike the ﬁ rst pair, \n",
      "\n",
      "\f",
      "320\n",
      "\n",
      "neomy storch and gillian wigglesworth\n",
      "\n",
      "analysis of uptake revealed that of the 43 reformulations made to the \n",
      "original version, there were only eight instances of uptake in the revised \n",
      "text. the revised text (day 5) and the subsequent individual texts were \n",
      "very different from the text produced on day 1., {'neg': 0.0, 'neu': 0.961, 'pos': 0.039, 'compound': 0.3182})\n",
      "(the revisions made on \n",
      "day 5 were not always in response to the feedback provided and were at \n",
      "the sentence level, with deletions and additions of full sentences., {'neg': 0.0, 'neu': 0.909, 'pos': 0.091, 'compound': 0.128})\n",
      "(texts \n",
      "produced on day 28 bore little resemblance to the text produced jointly \n",
      "and contained new types of errors., {'neg': 0.206, 'neu': 0.794, 'pos': 0.0, 'compound': -0.5267})\n",
      "(this meant that there was no way to \n",
      "trace for evidence of retention.   \n",
      "\n",
      " , {'neg': 0.155, 'neu': 0.845, 'pos': 0.0, 'compound': -0.296})\n",
      "(analysis of pair talk \n",
      "\n",
      " despite the large number of reformulations, there were few lres ( n  = 13) \n",
      "found in the data of this pair during the processing session., {'neg': 0.047, 'neu': 0.953, 'pos': 0.0, 'compound': -0.0572})\n",
      "(most dealt \n",
      "with verb tense choice and were thus related to the reformulations pro-\n",
      "vided, but the level of engagement was limited., {'neg': 0.15, 'neu': 0.702, 'pos': 0.148, 'compound': 0.2382})\n",
      "(most of the lres con-\n",
      "sisted of single turns during which one learner read the reformulated \n",
      "text  but  made  no  comment.  in  the  rewriting  session,  despite  a  large \n",
      "number of lres ( n  = 40), {'neg': 0.125, 'neu': 0.875, 'pos': 0.0, 'compound': -0.4824})\n",
      "(, only four related to the feedback provided. \n",
      "\n",
      " , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(the example in (9) illustrates the limited engagement with the refor-\n",
      "mulations and the impact of this limited engagement on the revised and \n",
      "new texts. the sentence produced in the original version had a number \n",
      "of errors and, consequently, a large number of reformulations ( n  = 6). in \n",
      "the pair, {'neg': 0.112, 'neu': 0.668, 'pos': 0.22, 'compound': 0.6124})\n",
      "(talk, the learners focused only on the use of the phrase  followed \n",
      "by  instead of  compared with  and the verb tense., {'neg': 0.1, 'neu': 0.792, 'pos': 0.108, 'compound': 0.0516})\n",
      "(engagement with these \n",
      "two  reformulations  was  limited.  , {'neg': 0.192, 'neu': 0.505, 'pos': 0.303, 'compound': 0.2732})\n",
      "(gus  simply  read  the  reformulations; \n",
      "jon did not contribute at all., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(in the revised version, the sentence shows \n",
      "substantial revision; however, these revisions were not consistent with \n",
      "the suggested reformulations., {'neg': 0.0, 'neu': 0.846, 'pos': 0.154, 'compound': 0.2732})\n",
      "(the corresponding sentences in the texts \n",
      "produced on day 28 are very different from that of the original version. \n",
      ", {'neg': 0.0, 'neu': 0.829, 'pos': 0.171, 'compound': 0.3612})\n",
      "(the sentence produced by gus has a number of agreement errors; jon’s \n",
      "sentence has errors in coherence.\n",
      "   \n",
      "  \n",
      "   , {'neg': 0.219, 'neu': 0.457, 'pos': 0.324, 'compound': 0.0772})\n",
      "((9)    \n",
      "  \n",
      "  \n",
      "\n",
      "    excerpts from texts and pair talk dealing with a number of reformulations \n",
      "   (a) original version \n",
      "    in  detail,  bucharest  and  lagos  have  the  same  pattern,  that  the  rainfall  in \n",
      "spring is the second highest compared to autumn and winter, while for beijing \n",
      "and mexico city…  \n",
      "   , {'neg': 0.0, 'neu': 0.921, 'pos': 0.079, 'compound': 0.3818})\n",
      "((b) reformulated version \n",
      "    speciﬁ cally, bucharest and lagos had the same pattern: the rainfall in spring \n",
      "was the second highest followed by autumn and winter…  \n",
      "   , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "((c) relevant lre \n",
      "   gus:  okay,  followed…  , {'neg': 0.0, 'neu': 0.725, 'pos': 0.275, 'compound': 0.2263})\n",
      "(oh  wait  was the second highest…however beijing \n",
      "and mexico city, autumn is the second…  okay  was the second highest fol-\n",
      "lowed by spring and winter  so it’s the same.  \n",
      "\n",
      "  \n",
      "  \n",
      "\n",
      "  \n",
      "  \n",
      "\n",
      "\f",
      ", {'neg': 0.059, 'neu': 0.879, 'pos': 0.062, 'compound': 0.0258})\n",
      "(processing, uptake, and retention of cf\n",
      "\n",
      "321\n",
      "\n",
      "  \n",
      "  \n",
      "\n",
      "  \n",
      "  \n",
      "\n",
      "  \n",
      "  \n",
      "\n",
      "   (d) revised version \n",
      "    bucharest and lagos falls into the ﬁ rst category. and, {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(beijing and mexico city \n",
      "are categorized in the second pattern.  \n",
      "   , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "((e) gus’s new text \n",
      "    bucharest  and  lagos  falls  into  the  ﬁ rst  category,  for  their  rainfall  patterns \n",
      "reaches the peak in summer, then the rainfall patterns decline...  \n",
      "   (f) jon’s new text \n",
      "    bucharest  and  lagos  had  the  same  order  for  the  second  and  third  highest \n",
      "rainfall, which were spring and autumn. while in beijing and mexico city,…    \n",
      "\n",
      "  \n",
      "\n",
      "   \n",
      "\n",
      "  \n",
      "  , {'neg': 0.0, 'neu': 0.979, 'pos': 0.021, 'compound': 0.0516})\n",
      "(this pair’s lack of engagement with the feedback may be attributed to \n",
      "their attitude to the form of feedback (reformulations) and the aspects \n",
      "of language with which the feedback dealt. as shown in (10), gus and \n",
      "jon did not approve of this form of feedback.\n",
      "   \n",
      "   , {'neg': 0.048, 'neu': 0.89, 'pos': 0.062, 'compound': 0.1779})\n",
      "((10)    \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "\n",
      "   excerpts from the processing session \n",
      "   gus:  huh?, {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(i don’t think this kind of feedback is good, because…  \n",
      "   jon:  yeah  \n",
      "   gus:  people will tend to memorise this  \n",
      "   jon:  yeah this still crap  \n",
      "   […] \n",
      "   jon:  yeah this not in good way to give a feedback  \n",
      "   gus:  yeah a feedback should not just give away the answer., {'neg': 0.048, 'neu': 0.683, 'pos': 0.269, 'compound': 0.875})\n",
      "(yeah that’s…\n",
      ", {'neg': 0.0, 'neu': 0.313, 'pos': 0.688, 'compound': 0.296})\n",
      "(that’s my opinion., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(okay, {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.2263})\n",
      "(so, are we supposed to memorise this?  \n",
      "   , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(jon:  yeah, you got paragraph one and two, i got paragraph three and four  \n",
      "   gus:  okay, okay now you…you memorise paragraph three then four    \n",
      "\n",
      "  \n",
      "\n",
      "  \n",
      "\n",
      "  \n",
      "  \n",
      "   \n",
      "  despite their disapproval, they elected to memorize the reformulated \n",
      "text, dividing the task between them., {'neg': 0.0, 'neu': 0.854, 'pos': 0.146, 'compound': 0.6124})\n",
      "(however, once the feedback was \n",
      "removed, they reconsidered their goals and felt that they should per-\n",
      "haps rewrite the text to improve it in any way that they saw ﬁ t.\n",
      "   \n",
      "   (11)    \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "   \n",
      "\n",
      "   excerpts from pair talk during the rewriting session \n",
      "   jon:  is it necessary that we have to write it in this style or…?  \n",
      "   , {'neg': 0.0, 'neu': 0.948, 'pos': 0.052, 'compound': 0.4404})\n",
      "(gus:  no, you change it in any way you want to  \n",
      "   jon:  okay  \n",
      "   gus:  any way that will make it better  \n",
      "   […] \n",
      "   jon:  we make our own improvements.    \n",
      "\n",
      "  \n",
      "\n",
      "    , {'neg': 0.065, 'neu': 0.685, 'pos': 0.25, 'compound': 0.6369})\n",
      "(summary \n",
      "\n",
      " this pair, like the ﬁ rst, had a large number of errors and received a large \n",
      "number of reformulations ( n  = 43). however, unlike the ﬁ rst pair, this \n",
      "\n",
      "\f",
      "322\n",
      "\n",
      "neomy storch and gillian wigglesworth\n",
      "\n",
      "pair’s  revised  text  showed  little  evidence  of  uptake.  , {'neg': 0.055, 'neu': 0.828, 'pos': 0.117, 'compound': 0.1779})\n",
      "(these  learners \n",
      "showed limited engagement with the feedback., {'neg': 0.174, 'neu': 0.55, 'pos': 0.275, 'compound': 0.2732})\n",
      "(there were no instances \n",
      "of learning evident in the data, in contrast with the data of the ﬁ rst pair. \n",
      ", {'neg': 0.104, 'neu': 0.896, 'pos': 0.0, 'compound': -0.296})\n",
      "(this lack of engagement with the feedback could be attributed to the \n",
      "learners’ attitudes—their lack of approval of this type of feedback., {'neg': 0.16, 'neu': 0.627, 'pos': 0.213, 'compound': 0.3612})\n",
      "(al-\n",
      "though the initial goal was to memorize the reformulated text, during \n",
      "the rewriting session, these learners’ goals changed., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(they decided to \n",
      "rewrite the text in the way they felt improved it and thus made substan-\n",
      "tial revisions to the text., {'neg': 0.0, 'neu': 0.871, 'pos': 0.129, 'compound': 0.4767})\n",
      "(two pairs who received feedback in the form \n",
      "of editing will now be considered.    \n",
      "\n",
      " , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(third case-study pair  \n",
      "\n",
      " background \n",
      "\n",
      " the  third  pair  was  composed  of  two  female  graduate  students  from \n",
      "china., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(monica had an ielts score of 6.5 and had studied english only at \n",
      "the university level (for 3 years prior to coming to australia)., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(diana had \n",
      "studied english at the high school and university level and had a higher \n",
      "ielts score of 7.0., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(both were pursuing a master’s degree in human re-\n",
      "source  management  (commerce)  and  had  known  each  other  for  7 \n",
      "months.   \n",
      "\n",
      " analysis of written texts \n",
      "\n",
      " the pair’s ﬁ rst version of the text elicited 17 editing symbols, mainly in \n",
      "the use of prepositions in phrases of time or location ( n  = 5), articles \n",
      "( n  = 3), and word choice ( n  = 3). the revised text showed a high level of \n",
      "uptake (14/17), with errors in use of prepositions almost disappearing \n",
      "(one remaining error)., {'neg': 0.032, 'neu': 0.968, 'pos': 0.0, 'compound': -0.34})\n",
      "(there were also few errors in use of prepositions \n",
      "in the learners’ texts produced on day 28 (only one such error in diana’s \n",
      "text and two in monica’s)., {'neg': 0.164, 'neu': 0.836, 'pos': 0.0, 'compound': -0.6249})\n",
      "(in contrast, errors in use of articles and in \n",
      "some word choices persisted, both in the revised text and in the new \n",
      "texts.   \n",
      "\n",
      " , {'neg': 0.098, 'neu': 0.902, 'pos': 0.0, 'compound': -0.34})\n",
      "(analysis of pair talk \n",
      "\n",
      " the feedback elicited 18 lres in the processing session, of which 14 \n",
      "dealt  directly  with  the  feedback.  in  the  rewriting  sessions,  of  the  19 \n",
      "lres, 11 dealt with aspects of language that received editing feedback. \n",
      "\n",
      "\f",
      ", {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(processing, uptake, and retention of cf\n",
      "\n",
      "323\n",
      "\n",
      "the majority of the lres in the processing (14/18) and rewriting (12/19) \n",
      "sessions dealt with lexical choices—, {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(namely, the choice of prepositions. \n",
      "engagement  with  this  feedback  was  extensive.  in  contrast,  few  lres \n",
      "dealt  with  feedback  on  articles  ( n   =  2),  and  engagement  was  also \n",
      "limited. \n",
      "\n",
      " , {'neg': 0.061, 'neu': 0.744, 'pos': 0.194, 'compound': 0.6249})\n",
      "(the example in (12) shows the different levels of engagement with \n",
      "different types of errors and illustrates that engagement with the feed-\n",
      "back on this preposition error led to an enhanced understanding about \n",
      "when to use  in  rather than  of  in temporal expressions., {'neg': 0.102, 'neu': 0.778, 'pos': 0.12, 'compound': 0.2263})\n",
      "(this, in turn, led \n",
      "to uptake and retention., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(example (12) also shows that the feedback on \n",
      "the error in spelling was noticed in the processing stage but was not \n",
      "dealt with extensively. as in the case of the ﬁ rst pair,, {'neg': 0.055, 'neu': 0.945, 'pos': 0.0, 'compound': -0.2144})\n",
      "(this limited en-\n",
      "gagement resulted in uptake and retention., {'neg': 0.192, 'neu': 0.808, 'pos': 0.0, 'compound': -0.2263})\n",
      "(however, in the case of arti-\n",
      "cles,  no  attention  was  paid  to  the  feedback  provided  in  either  the \n",
      "processing or rewriting session., {'neg': 0.095, 'neu': 0.905, 'pos': 0.0, 'compound': -0.296})\n",
      "(this lack of attention may explain the \n",
      "persistence in errors in articles.\n",
      "   \n",
      "   , {'neg': 0.32, 'neu': 0.68, 'pos': 0.0, 'compound': -0.5719})\n",
      "((12)    \n",
      "\n",
      "    excerpts from texts and pair talk dealing with the use of prepositions and \n",
      "articles \n",
      "   (a) original version \n",
      "    it’s obvious that lagas, beijing and mexico city have different rainfall of four \n",
      "seasons  \n",
      "   (b) editing feedback \n",
      "\n",
      "  \n",
      "\n",
      "  \n",
      "  \n",
      "\n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "\n",
      "  \n",
      "  \n",
      "\n",
      "  \n",
      "  \n",
      "\n",
      "    it’s obvious that    lagas   , beijing and mexico city have different rainfall    of    four  \n",
      "\n",
      "   x   \n",
      "\n",
      "c \n",
      "   λ \n",
      "\n",
      "    seasons  \n",
      "   (c) relevant m-lre \n",
      "   diana:  okay  it’s obvious that lagos…  i know, {'neg': 0.0, 'neu': 0.932, 'pos': 0.068, 'compound': 0.4939})\n",
      "(this., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(okay  \n",
      "   (d) relevant l-lres: prepositions \n",
      "   diana:  in four seasons  \n",
      "   monica:  ah huh, yeah,, {'neg': 0.0, 'neu': 0.728, 'pos': 0.272, 'compound': 0.4767})\n",
      "(yeah  \n",
      "   diana:  this is like…this is the ﬁ rst time i know okay the difference between  \n",
      "in  and  of.  , {'neg': 0.0, 'neu': 0.796, 'pos': 0.204, 'compound': 0.4767})\n",
      "(okay  \n",
      "   […] \n",
      "   diana: it is obvious that lagos, beijing and mexico…  , {'neg': 0.0, 'neu': 0.84, 'pos': 0.16, 'compound': 0.2263})\n",
      "(l-a-g-o-s ah huh…  \n",
      ", {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(lagos, beijing and mexico have different rainfall in four seasons… \n",
      "   , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "((e) revised version \n",
      "    it is obvious that lagos, beijing and mexico city have different rainfall in \n",
      "four seasons    \n",
      "\n",
      "   \n",
      "\n",
      "  \n",
      "  example (13) illustrates the learners’ engagement with feedback on \n",
      "lexis.  , {'neg': 0.0, 'neu': 0.9, 'pos': 0.1, 'compound': 0.4588})\n",
      "(this  example  shows  how  the  learners’  beliefs  about  the  use  of \n",
      "language—in this instance, the need to use linking phrases—based on \n",
      "previous language learning experience (ielts training courses), resulted \n",
      "in resistance to the feedback, which may help explain instances of no \n",
      "uptake or no retention. \n",
      "\n",
      "\f",
      "324\n",
      "\n",
      "neomy storch and gillian wigglesworth\n",
      "\n",
      " the phrase  as can be seen that  was underlined and put in parentheses \n",
      "to indicate that there was an error and that some words in this phrase \n",
      "should be deleted., {'neg': 0.082, 'neu': 0.887, 'pos': 0.031, 'compound': -0.5267})\n",
      "(however, the learners mistook the symbols to mean \n",
      "that the entire linking phrase was unnecessary and proceeded to delete \n",
      "this  phrase  in  their  revision.  similarly  to  the  ﬁ rst  , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(pair  with   around  a \n",
      "mere , these learners show uptake, but their resistance to this feedback \n",
      "based on their prior learning experience means that the linking phrase \n",
      "is used on day 28.  , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(1  \n",
      "   \n",
      "   (13)    \n",
      "  \n",
      "  \n",
      "\n",
      "   excerpts from texts and pair talk dealing with the use of linking phrases \n",
      "   (a) original version \n",
      "    as we can see that the rainfall of most seasons in lagos is the highest among \n",
      "the four cities.  \n",
      "   , {'neg': 0.0, 'neu': 0.938, 'pos': 0.062, 'compound': 0.3182})\n",
      "((b) editing feedback \n",
      "     (as  we  can  see  that)    the  rainfall  of  most  seasons  in  lagos  is  the  highest \n",
      "among the four cities.  \n",
      "   , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "((c) relevant lre \n",
      "   diana:  i just don’t understand why we don’t need the  as we can see that ., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(i \n",
      "think this is…this is nice  \n",
      "   monica:  mm hmm  \n",
      "   diana:  i don’t know why  \n",
      "   monica:  should be deleted  \n",
      "   diana:  but i think  as we can see that …you see in the ielts book, they said \n",
      "um  it can be seen,  blah, blah, blah so i think that this is nice, i don’t know \n",
      "why  \n",
      "   (d) revised version \n",
      "    the rainfall in lagos in most seasons is the highest among the four cities . \n",
      "   , {'neg': 0.067, 'neu': 0.854, 'pos': 0.078, 'compound': 0.4215})\n",
      "((e) diana’s new text \n",
      "    it can be seen that the rainfall in summer in beijing…  \n",
      "   , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "((f) monica’s new text \n",
      "    as we can see, bucharest…    \n",
      "\n",
      "  \n",
      "\n",
      "  \n",
      "  \n",
      "\n",
      "  \n",
      "  \n",
      "\n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "\n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "   \n",
      "\n",
      "  \n",
      "\n",
      "    summary \n",
      "\n",
      " monica and diana attended to most of the feedback received, particu-\n",
      "larly the feedback on their most frequent errors (use of prepositions in \n",
      "locative  and  temporal  expressions).  , {'neg': 0.069, 'neu': 0.931, 'pos': 0.0, 'compound': -0.3976})\n",
      "(the  lres  showed  that  these \n",
      "learners gained an understanding of this use of prepositions, which as-\n",
      "sisted them in using these prepositions correctly in the revised version \n",
      "and in subsequently produced new texts., {'neg': 0.0, 'neu': 0.925, 'pos': 0.075, 'compound': 0.3818})\n",
      "(less attention was paid to the \n",
      "editing feedback on the use of articles, which may explain—along with \n",
      "the fact that articles are a renowned area of difﬁ culty for l2 learners—\n",
      "the lack of retention of feedback on articles beyond the revised version. \n",
      ", {'neg': 0.053, 'neu': 0.947, 'pos': 0.0, 'compound': -0.3182})\n",
      "(these learners seemed to show a higher level of uptake and retention \n",
      "\n",
      "\f",
      "processing, uptake, and retention of cf\n",
      "\n",
      "325\n",
      "\n",
      "when the feedback was consistent with their beliefs about language use. \n",
      ", {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(when the feedback contradicted those beliefs established in previous \n",
      "language learning courses, there was no retention.    \n",
      "\n",
      " , {'neg': 0.243, 'neu': 0.757, 'pos': 0.0, 'compound': -0.5423})\n",
      "(fourth case-study pair  \n",
      "\n",
      " background \n",
      "\n",
      " unlike the other three pairs, the learners in this pair were undergraduate \n",
      "students in commerce., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(bing was a male from malaysia and lina was a \n",
      "female from indonesia., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(both had studied english in high school and both \n",
      "had high ielts scores of 7.0., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(they had known each other for 6 weeks.   \n",
      "\n",
      " , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(analysis of written texts \n",
      "\n",
      " the  original  version  of  their  text  had  15  editing  symbols.  , {'neg': 0.0, 'neu': 0.85, 'pos': 0.15, 'compound': 0.3182})\n",
      "(the  most \n",
      "common errors were in the use of prepositions ( n  = 4) and verbs ( n  = 3). \n",
      "the revised text showed complete uptake (100%), with revisions made \n",
      "mainly at the word level., {'neg': 0.087, 'neu': 0.913, 'pos': 0.0, 'compound': -0.3976})\n",
      "(all errors in prepositions were amended, but \n",
      "new errors in the use of verbs were introduced. on day 28, the texts \n",
      "produced  individually  had  no  errors  in  verb  use;  bing  had  only  one \n",
      "error and lina had three errors in the use of prepositions, which sug-\n",
      "gests overall high levels of retention.   \n",
      "\n",
      " , {'neg': 0.274, 'neu': 0.726, 'pos': 0.0, 'compound': -0.9464})\n",
      "(analysis of pair talk \n",
      "\n",
      " in processing the feedback, the learners paid attention to the editing \n",
      "feedback  and  all  of  the  lres  in  both  sessions  related  directly  to  the \n",
      "feedback received., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(most of the f-lres focused on the use of verbs and \n",
      "most of the l-lres on the choice of prepositions., {'neg': 0.0, 'neu': 0.88, 'pos': 0.12, 'compound': 0.3818})\n",
      "(the level of engage-\n",
      "ment with the feedback was extensive, which can perhaps account for \n",
      "the high uptake. \n",
      "\n",
      " , {'neg': 0.0, 'neu': 0.876, 'pos': 0.124, 'compound': 0.34})\n",
      "(the example in (14) shows evidence of the high level of engagement \n",
      "with the feedback provided and evidence of the learners extending their \n",
      "knowledge  to  new  contexts.  , {'neg': 0.0, 'neu': 0.897, 'pos': 0.103, 'compound': 0.4588})\n",
      "(the  feedback  provided  suggested  that \n",
      "there were errors in that string of words., {'neg': 0.167, 'neu': 0.833, 'pos': 0.0, 'compound': -0.34})\n",
      "(although not speciﬁ ed, some \n",
      "of these errors were related to an inconsistent use of verb tense. while \n",
      "discussing this sentence, the learners became aware of these inconsis-\n",
      "tencies., {'neg': 0.149, 'neu': 0.808, 'pos': 0.043, 'compound': -0.5307})\n",
      "(lina suggested that they use either the present or past tense \n",
      "throughout, and bing agreed. in their revised text and their new texts, \n",
      ", {'neg': 0.094, 'neu': 0.824, 'pos': 0.082, 'compound': -0.0772})\n",
      "(the present tense was used throughout.\n",
      "   \n",
      "\n",
      "  \n",
      "\n",
      "\f",
      ", {'neg': 0.324, 'neu': 0.676, 'pos': 0.0, 'compound': -0.34})\n",
      "(neomy storch and gillian wigglesworth\n",
      "\n",
      "   excerpts from texts and pair talk dealing with the use of verbs \n",
      "   (a) original version \n",
      "    beijing also had the average rainfall in summer, which is 150mm  \n",
      "   (b) editing feedback \n",
      "    (beijing also had the average rainfall in summer, which is 150mm)  \n",
      "   (c) relevant lre \n",
      "   lina:  beijing had…  \n",
      "   , {'neg': 0.0, 'neu': 0.956, 'pos': 0.044, 'compound': 0.3182})\n",
      "(bing:  had or… had or has  \n",
      "   lina:  don’t i say the other tenses we use…  \n",
      "   , {'neg': 0.128, 'neu': 0.872, 'pos': 0.0, 'compound': -0.2263})\n",
      "(bing:  had  \n",
      "   lina:  mmm  \n",
      "   bing:  which has  \n",
      "   lina:  you don’t use present, you know., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(but she has not correct our tense. or \n",
      "did she?  \n",
      "   , {'neg': 0.0, 'neu': 0.779, 'pos': 0.221, 'compound': 0.3724})\n",
      "(bing:  good question.  \n",
      "   , {'neg': 0.0, 'neu': 0.408, 'pos': 0.592, 'compound': 0.4404})\n",
      "(lina:  haven’t i… this one we use present, this is all present., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(if one use the \n",
      "past then use past all.  \n",
      "   , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(bing:  ok. beijing has…  \n",
      "   (d) revised version \n",
      "    beijing has the highest average rainfall in summer of 150mm  \n",
      "   (e) bing’s new text \n",
      "    beijing has the lowest average rainfall of 5mm, followed by…  \n",
      "   (f) lina’s new text \n",
      "    the second highest total average rainfall occurs in beijing.    \n",
      "\n",
      ", {'neg': 0.056, 'neu': 0.897, 'pos': 0.047, 'compound': -0.1027})\n",
      "(326\n",
      "\n",
      "   (14)    \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "\n",
      "  \n",
      "  \n",
      "\n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "   \n",
      "\n",
      "  \n",
      "  the excerpt from the pair talk during the rewriting session given in \n",
      "(15) suggests that the learners’ goal was to focus on amending the er-\n",
      "rors rather than rewriting the text., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(the pair talk also showed evidence \n",
      "that  the  learners  memorized  the  location  of  the  editing  symbols  and \n",
      "relied on this in their rewriting activity.\n",
      "   \n",
      "   , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "((15)    \n",
      "  \n",
      "\n",
      "   excerpts from texts and pair talk \n",
      "   lina:  you see… the difference., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(so we write the same the same… just change \n",
      "the one that…  \n",
      "   , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(lina:  this one just is not right.  \n",
      "   , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(bing:  mmm?, {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(what do you mean?  \n",
      "   , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(lina:  have to correct this and just write.  \n",
      "   , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "([…] \n",
      "   lina:  mmm. and winter it has… it., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(remember there’s something missing.  \n",
      "   , {'neg': 0.423, 'neu': 0.577, 'pos': 0.0, 'compound': -0.296})\n",
      "(bing:  yeah    \n",
      "\n",
      "  \n",
      "\n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "   \n",
      "\n",
      "  \n",
      "\n",
      "    summary \n",
      "\n",
      " there was a high level of uptake and retention of the feedback provided \n",
      "to  these  learners,  who  engaged  with  the  feedback  extensively.  , {'neg': 0.0, 'neu': 0.818, 'pos': 0.182, 'compound': 0.5994})\n",
      "(their \n",
      "\n",
      "\f",
      "processing, uptake, and retention of cf\n",
      "\n",
      "327\n",
      "\n",
      "goal was to amend the text at the word level in response to the feedback \n",
      "provided.    \n",
      "\n",
      " , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(discussion \n",
      "\n",
      " this study sought to examine and compare how learners process direct \n",
      "feedback (reformulations) versus indirect feedback (editing symbols) \n",
      "on language errors and what impact, if any, the type of feedback and \n",
      "processing has on uptake (immediate revision) and retention in the long \n",
      "term (23 days later), as evident in individually written texts., {'neg': 0.044, 'neu': 0.956, 'pos': 0.0, 'compound': -0.34})\n",
      "(the ﬁ nd-\n",
      "ings for the whole cohort showed that editing feedback elicited more \n",
      "lres than reformulations and that these lres tended to relate directly \n",
      "to the feedback provided., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(the level of engagement also seemed more \n",
      "extensive with editing feedback than in response to reformulations., {'neg': 0.0, 'neu': 0.833, 'pos': 0.167, 'compound': 0.4588})\n",
      "(as \n",
      "suggested by ferris ( 2002 ), in response to editing, learners had to iden-\n",
      "tify the nature of the error and attempt to supply the correct form, using \n",
      "their own knowledge of grammar and word meanings to offer sugges-\n",
      "tions and countersuggestions., {'neg': 0.062, 'neu': 0.938, 'pos': 0.0, 'compound': -0.4019})\n",
      "(in contrast, engagement with reformula-\n",
      "tions tended to be limited to reading the reformulation, acknowledging \n",
      "or merely expressing agreement, with fewer instances of extensive en-\n",
      "gagement., {'neg': 0.061, 'neu': 0.74, 'pos': 0.199, 'compound': 0.6486})\n",
      "(however, it is important to reiterate that coding for level of \n",
      "engagement  is  a  highly  inferential  process  (sachs  &  polio,   2007 )  and \n",
      "that  the  amount  of  verbalization  evident  in  the  lres  may  not  neces-\n",
      "sarily reﬂ ect depth of cognitive processing. \n",
      "\n",
      " in line with the ﬁ ndings of other studies that elicited feedback pro-\n",
      "cessing data (e.g., qi & lapkin,  2001 ; sachs & polio,  2007 ), {'neg': 0.0, 'neu': 0.925, 'pos': 0.075, 'compound': 0.5859})\n",
      "(, this study \n",
      "also  found  that  extensive  engagement  with  the  feedback  led  to  high \n",
      "levels of uptake., {'neg': 0.0, 'neu': 0.833, 'pos': 0.167, 'compound': 0.4588})\n",
      "(this was evident in the ﬁ ndings for the whole cohort as \n",
      "well as in the case-study data., {'neg': 0.0, 'neu': 0.89, 'pos': 0.11, 'compound': 0.2732})\n",
      "(the third and fourth pairs, who received \n",
      "editing  feedback,  engaged  with  the  feedback  extensively  and  showed \n",
      "high levels of uptake. in the case, {'neg': 0.0, 'neu': 0.891, 'pos': 0.109, 'compound': 0.4019})\n",
      "(of the third pair, for example, extensive \n",
      "engagement  with  feedback  on  certain  prepositions  led  to  uptake  and \n",
      "correct use of these prepositions., {'neg': 0.0, 'neu': 0.797, 'pos': 0.203, 'compound': 0.6249})\n",
      "(in contrast, limited or no engagement \n",
      "with feedback on articles resulted in no uptake and persistent inaccura-\n",
      "cies in the use of articles., {'neg': 0.223, 'neu': 0.671, 'pos': 0.106, 'compound': -0.3182})\n",
      "(similarly, in the case of the ﬁ rst pair, who re-\n",
      "ceived  reformulation \n",
      "feedback,  extensive  engagement  over  the \n",
      " where - when  distinction led to uptake; limited engagement with the word \n",
      "choice   ﬂ uctuative   resulted  in  no  uptake.  , {'neg': 0.097, 'neu': 0.76, 'pos': 0.143, 'compound': 0.4404})\n",
      "(however,  data  from  the  ﬁ rst \n",
      "pair showed that uptake also depends to some extent on the nature of \n",
      "the  errors.  for  more  superﬁ cial  errors,  such  as  errors  in  mechanics, \n",
      "perfunctory noticing, whether verbalized or not, may be sufﬁ cient for \n",
      "uptake to occur. \n",
      "\n",
      " , {'neg': 0.146, 'neu': 0.854, 'pos': 0.0, 'compound': -0.7351})\n",
      "(similarly, retention seemed to relate to the level of engagement with \n",
      "the  feedback  and  the  nature  of  the  errors.  , {'neg': 0.107, 'neu': 0.759, 'pos': 0.134, 'compound': 0.1531})\n",
      "(feedback  on  errors  in \n",
      "\n",
      "\f",
      "328\n",
      "\n",
      "neomy storch and gillian wigglesworth\n",
      "\n",
      "mechanics was retained despite limited or no overt engagement. in the \n",
      "case of morphosyntactic and lexical errors,, {'neg': 0.143, 'neu': 0.628, 'pos': 0.229, 'compound': 0.2153})\n",
      "(high levels of engagement \n",
      "led to understanding and an ability to retain the feedback in the long \n",
      "term (e.g., the  where-when  distinction in the ﬁ rst pair or correct use of \n",
      "verbs in the fourth pair)., {'neg': 0.0, 'neu': 0.865, 'pos': 0.135, 'compound': 0.6486})\n",
      "(however, other affective factors also seemed \n",
      "to inﬂ uence retention., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(speciﬁ cally, learners’ beliefs, attitude toward the \n",
      "form of feedback received, and their goals seemed to have an effect on \n",
      "whether the feedback was retained., {'neg': 0.0, 'neu': 0.956, 'pos': 0.044, 'compound': 0.0258})\n",
      "(thus, in the case of the ﬁ rst pair, \n",
      "there  is  elaborate  engagement  with  the  reformulated  phrase   merely \n",
      "around , which led to uptake but no retention., {'neg': 0.097, 'neu': 0.833, 'pos': 0.069, 'compound': -0.2023})\n",
      "(lack of retention seemed \n",
      "to be attributable to learners’ beliefs about language use., {'neg': 0.161, 'neu': 0.839, 'pos': 0.0, 'compound': -0.3182})\n",
      "(the pair did \n",
      "not adopt the reformulated phrase because they felt that the alternative \n",
      "was  not  necessarily  a  better  expression.  , {'neg': 0.188, 'neu': 0.812, 'pos': 0.0, 'compound': -0.4449})\n",
      "(similarly,  in  the  case  of  the \n",
      "third pair and the use of linking phrases (e.g.,  as we can see that ), there \n",
      "was extensive engagement and uptake but no retention., {'neg': 0.085, 'neu': 0.854, 'pos': 0.061, 'compound': -0.2023})\n",
      "(the learners \n",
      "felt that the feedback contradicted their beliefs, shaped by their pre-\n",
      "vious  language  learning  experience  about  what  constitutes  a  good \n",
      "writing style., {'neg': 0.088, 'neu': 0.802, 'pos': 0.111, 'compound': 0.1531})\n",
      "(studies by swain ( 2006 ) and swain and lapkin ( 2003 ) also \n",
      "found evidence of resistance to feedback that resulted in no uptake., {'neg': 0.099, 'neu': 0.901, 'pos': 0.0, 'compound': -0.296})\n",
      "(the \n",
      "case study data discussed here suggest that resistance is more likely to \n",
      "lead to lack of retention. \n",
      "\n",
      " , {'neg': 0.119, 'neu': 0.881, 'pos': 0.0, 'compound': -0.3182})\n",
      "(another important affective factor that had an impact on both uptake \n",
      "and retention was learners’ goals., {'neg': 0.0, 'neu': 0.893, 'pos': 0.107, 'compound': 0.2023})\n",
      "(the ﬁ rst and fourth pairs seemed to \n",
      "be driven by a goal to improve the accuracy of their text., {'neg': 0.0, 'neu': 0.861, 'pos': 0.139, 'compound': 0.4404})\n",
      "(this strategy \n",
      "may explain high uptake., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(in the case of the second pair, disapproval of \n",
      "reformulations as a form of feedback coupled with the goal of improving \n",
      "their  text  as  they  saw  ﬁ t  (see  also  hyland,   1998 )  meant  that  these \n",
      "learners ignored the feedback received; hence, there was no uptake (or \n",
      "retention)., {'neg': 0.091, 'neu': 0.852, 'pos': 0.057, 'compound': -0.1779})\n",
      "(when the learners seemed to approve of the type of feed-\n",
      "back received—and were driven by a goal to improve their text—they \n",
      "sometimes adopted the strategy of memorizing the feedback (the ﬁ rst \n",
      "pair) or the location of the editing symbols (the fourth pair). \n",
      "\n",
      " in research on feedback, affective factors such as learners’ orienta-\n",
      "tion (lantolf & thorne,  2006 ), which includes their attitudes toward the \n",
      "type  of  feedback  received  and  beliefs  about  language  conventions \n",
      "shaped by previous language instruction as well as the goals and strat-\n",
      "egies adopted, are often ignored., {'neg': 0.024, 'neu': 0.923, 'pos': 0.053, 'compound': 0.4019})\n",
      "(the case study data showed that af-\n",
      "fective factors inﬂ uence not only the type of strategies learners adopt in \n",
      "dealing with the feedback received (e.g., memorization) but also their \n",
      "willingness to accept the feedback and their likelihood of retaining it. \n",
      "\n",
      " , {'neg': 0.0, 'neu': 0.837, 'pos': 0.163, 'compound': 0.7506})\n",
      "(however, these ﬁ ndings should be interpreted cautiously., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(the partic-\n",
      "ipants in the current study were advanced language learners, and this \n",
      "may have affected not only the type of errors they made in their writing \n",
      "but also their ability to notice and attend to the feedback received, as \n",
      "well as their attitude to the different types of feedback. furthermore, \n",
      "\n",
      "\f",
      "processing, uptake, and retention of cf\n",
      "\n",
      "329\n",
      "\n",
      "the two types of written cf compared (reformulations and editing sym-\n",
      "bols)  were  distinct  from  each  other  and,  thus,  inevitably  elicited  dif-\n",
      "ferent kinds of responses., {'neg': 0.034, 'neu': 0.887, 'pos': 0.08, 'compound': 0.6249})\n",
      "(perhaps the greatest limitation is that data \n",
      "were collected in an experimental rather than a classroom setting and \n",
      "thus, important contextual factors such as the relationship between the \n",
      "learners and the teacher who provided the feedback could not be inves-\n",
      "tigated.  , {'neg': 0.049, 'neu': 0.819, 'pos': 0.133, 'compound': 0.5859})\n",
      "(recent  research  on  feedback  (e.g.,  given  &  schallert,   2008 ) \n",
      "shows that this relationship may play a powerful role in determining \n",
      "whether learners take up the feedback provided. \n",
      "\n",
      " nevertheless,  the  ﬁ ndings  suggest  that  whether  and  which  type  of \n",
      "feedback is effective depend on a complex and dynamic interaction of \n",
      "linguistic  and  affective  factors.  , {'neg': 0.0, 'neu': 0.808, 'pos': 0.192, 'compound': 0.872})\n",
      "(future  research  on  feedback  needs \n",
      "to  combine  an  examination  of  the  product  (revised  and  new  texts) \n",
      "and processes in an integrated manner. to isolate and investigate the \n",
      "effect of linguistic factors, studies in which feedback is given on speciﬁ c \n",
      "structures are needed (e.g., bitchener,  2008 ; sheen,  2007 ) to establish \n",
      "whether some form of feedback (direct vs. indirect) is more effective for \n",
      "particular types of errors. however, an investigation of linguistic factors \n",
      "alone is not enough., {'neg': 0.077, 'neu': 0.881, 'pos': 0.042, 'compound': -0.204})\n",
      "(researchers (e.g., cumming, busch, & zhou, 2002; \n",
      "hyland,   1998 ,   2003 ;  sachs  &  polio,   2007 )  have  called  for  classroom-\n",
      "based studies that more fully investigate affective factors (e.g., goals, \n",
      "orientation to task, preferences); however, such research is difﬁ cult to \n",
      "conduct.  , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(collecting  feedback  processing  data  in  a  classroom  setting \n",
      "may be difﬁ cult; given the detailed analysis such an investigation neces-\n",
      "sitates, studies on learners’ engagement with feedback have tended to \n",
      "be small-scale case studies (e.g., given & schallert,  2008 ; hyland; qi & \n",
      "lapkin,  2001 ; tardy,  2006 ). it is perhaps through case studies, such as \n",
      "the  study  reported  here,  that  insights  into  this  complex  issue  of  the \n",
      "impact of cf can be gained, along with an understanding of the inevita-\n",
      "bility that experimental research on the impact of cf will continue to \n",
      "yield mixed ﬁ ndings.     \n",
      "\n",
      " , {'neg': 0.0, 'neu': 0.942, 'pos': 0.058, 'compound': 0.6808})\n",
      "(note \n",
      "\n",
      "  1.     , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(even though the linking phrase was used correctly in the texts produced on day 28, \n",
      "this was coded for no retention because the learners did not adhere to their understanding \n",
      "of the editing code (deletion of the phrase).    \n",
      "\n",
      " , {'neg': 0.056, 'neu': 0.944, 'pos': 0.0, 'compound': -0.296})\n",
      "(references \n",
      "\n",
      "    aljaafreh  ,   a.  , &   lantolf  ,   j. p   ., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(( 1994 ).  , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(negative feedback as regulation and second language \n",
      "learning  in  the  zone  of  proximal  development .   , {'neg': 0.222, 'neu': 0.778, 'pos': 0.0, 'compound': -0.5719})\n",
      "(modern  language  journal ,   78 ,  \n",
      "465 – 483 . \n",
      "\n",
      "    , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(bitchener  ,   j.    ( 2008 ).  , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(evidence in support of written corrective feedback .  , {'neg': 0.0, 'neu': 0.69, 'pos': 0.31, 'compound': 0.4019})\n",
      "(journal of second \n",
      "\n",
      "language writing ,  17 ,  102 – 118 . \n",
      "\n",
      "    , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(bitchener  ,   j.  , &   knoch  ,   u.    ( 2008 ).  the value of written corrective feedback for migrant and \n",
      "\n",
      "international students .  , {'neg': 0.0, 'neu': 0.87, 'pos': 0.13, 'compound': 0.34})\n",
      "(language teaching research ,  12 ,  409 – 431 . \n",
      "\n",
      "\f",
      "330\n",
      "\n",
      "neomy storch and gillian wigglesworth\n",
      "\n",
      "    chandler  ,   j.    ( 2003 ).  , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(the efﬁ cacy of various kinds of error feedback for improvement in the \n",
      "accuracy and ﬂ uency of l2 student writing .  , {'neg': 0.109, 'neu': 0.769, 'pos': 0.121, 'compound': 0.0772})\n",
      "(journal of second language writing ,  12 , \n",
      " 267 – 296 . \n",
      "\n",
      "    , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(cummings  ,   a.  ,   busch  ,   m.  , &   zhou  ,   a.    ( 2002 ).  investigating learners’ goals in the context of \n",
      "adult second language writing . in    s.     , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(ransdell   &   m.     barbier    (eds.),  new directions for \n",
      "research for l2 writing  (pp.  , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(189 – 208 ).  , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(dordrecht :  kluwer . \n",
      "\n",
      "    , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(ferris  ,   d. r   ., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(( 1999 ).  , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(the case for grammar correction in l2 writing classes: a response to \n",
      "\n",
      "truscott (1996) .  , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(journal of second language writing ,  8 ,  1 – 10 . \n",
      "\n",
      "    , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(ferris  ,    d.  r   .  , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(( 2002 ).   treatment  of  error  in  second  language  student  writing .   , {'neg': 0.231, 'neu': 0.769, 'pos': 0.0, 'compound': -0.4019})\n",
      "(ann  arbor : \n",
      "\n",
      " university of michigan press . \n",
      "\n",
      "    given  ,   l.  , &   schallert  ,   d.    ( 2008 ).  , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(meeting in the margins: effects of the teacher-student \n",
      "relationship  on  revision  processes  of  efl  college  students  taking  a  composition \n",
      "course .  , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(journal of second language writing ,  17 ,  165 – 182 . \n",
      "\n",
      "    , {'neg': 0.328, 'neu': 0.672, 'pos': 0.0, 'compound': -0.5994})\n",
      "(goldstein  ,   l.    ( 2004 ).  , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(questions and answers about teacher written commentary and stu-\n",
      "dent revisions: teachers and students working together .  journal of second language \n",
      "writing ,  13 ,  63 – 80 . \n",
      "\n",
      "    , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(goldstein  ,   l.    ( 2005 ).  , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(teacher written commentary in second language writing classrooms . \n",
      "\n",
      " ann arbor :  university of michigan press . \n",
      "\n",
      "    , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(guénette  ,   d.    ( 2007 ).  is feedback pedagogically correct?, {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(research design issues in studies \n",
      "\n",
      "of feedback on writing .  , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(journal of second language writing ,  16 ,  40 – 53 . \n",
      "\n",
      "    , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(hyland  ,   f.    ( 1998 ).  , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(the impact of teacher written feedback on individual writers .  , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(journal of \n",
      "\n",
      "second language writing ,  7 ,  255 – 286 . \n",
      "\n",
      "    , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(hyland  ,   f.    ( 2003 ).  , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(focusing on form: student engagement with teacher feedback .  , {'neg': 0.0, 'neu': 0.7, 'pos': 0.3, 'compound': 0.4588})\n",
      "(system , \n",
      "\n",
      " 31 ,  217 – 230 . \n",
      "\n",
      ", {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(teaching ,  39 ,  83 – 101 . \n",
      "\n",
      "    , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(hyland  ,   f.  , &   hyland  ,   k.    ( 2006 ).  , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(feedback on second language students’ writing .  , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(language \n",
      "\n",
      "    hyland  ,   k.  , &   hyland  ,   f.    ( 2006 ).  , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(context and issues in feedback on l2 writing: an introduc-\n",
      "tion . in    k.     , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(hyland   &   f.     hyland    (eds.),  feedback in second language writing  (pp.  1 – 20 ). \n",
      " , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(oxford :  oxford university press . \n",
      "\n",
      "    , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(lalande  ,   j. f   ., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(( 1982 ).  reducing composition errors:, {'neg': 0.375, 'neu': 0.625, 'pos': 0.0, 'compound': -0.34})\n",
      "(an experiment .  , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(modern language journal , \n",
      "\n",
      " 66 ,  140 – 149 . \n",
      "\n",
      "    , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(lantolf  ,   j. p.  , &   pavlenko  ,   a.    ( 2001 ).  , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(second language activity theory: understanding sec-\n",
      "ond language learners as people . in    m., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(p.     breen    (ed.),  learner contributions to language \n",
      "learning: new directions in research  (pp.  141 – 158 ).  , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(new york :  pearson education . \n",
      "\n",
      "    , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(lantolf  ,   j. p.  , &   thorne  ,   s. l   ., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(( 2006 ).  , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(sociocultural theory and the genesis of second language \n",
      "\n",
      "development.   , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(oxford :  oxford university press . \n",
      "\n",
      "    , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(nassaji  ,   h.  , &   swain  ,   m.    ( 2000 ).  a vygotskian perspective on corrective feedback in l2: \n",
      "the  effect  of  random  versus  negotiated  help  on  the  learning  of  english  articles . \n",
      " , {'neg': 0.0, 'neu': 0.903, 'pos': 0.097, 'compound': 0.4019})\n",
      "(language awareness ,  9 ,  34 – 51 . \n",
      "\n",
      "    , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(qi  ,   d. s.  , &   lapkin  ,   s.    ( 2001 ).  exploring the role of noticing in a three-stage second language \n",
      "\n",
      "writing task .  , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(journal of second language writing ,  10 ,  277 – 303 . \n",
      "\n",
      "    , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(sachs  ,    r.  ,  &    polio  ,    c.     ( 2007 ).   , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(learners’  uses  of  two  types  of  written  feedback  on  a  l2 \n",
      "\n",
      "writing revision task .  , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(studies in second language acquisition ,  29 ,  67 – 100 . \n",
      "\n",
      "    sheen  ,   y.    ( 2007 ).  , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(the effect of focused written corrective feedback and language aptitude \n",
      "\n",
      "on esl learners’ acquisition of articles .  , {'neg': 0.0, 'neu': 0.852, 'pos': 0.148, 'compound': 0.3818})\n",
      "(tesol quarterly ,  41 ,  255 – 283 . \n",
      "\n",
      "    , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(storch  ,   n.    ( 2008 ).  , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(metatalk in a pair work activity: level of engagement and implications \n",
      "\n",
      "for language development .  , {'neg': 0.0, 'neu': 0.8, 'pos': 0.2, 'compound': 0.4588})\n",
      "(language awareness ,  17 ,  95 – 114 . \n",
      "\n",
      "    , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(storch  ,   n.  , &   wigglesworth  ,   g.    ( 2006 , july).  , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(reformulate or edit? investigating the impact \n",
      "of different feedback practices., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(paper presented at the 5th paciﬁ c second language \n",
      "research forum, university of queensland, australia . \n",
      "\n",
      "    , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(swain  ,   m.    ( 2006 ).  , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(languaging, agency and collaboration in advanced language proﬁ ciency . \n",
      "in     h.      byrnes     (ed.),   , {'neg': 0.0, 'neu': 0.857, 'pos': 0.143, 'compound': 0.25})\n",
      "(advanced  language  learning:  the  contribution  of  halliday  and \n",
      "vygotsky  (pp.  95 – 108 ).  , {'neg': 0.0, 'neu': 0.867, 'pos': 0.133, 'compound': 0.25})\n",
      "(new york :  continuum . \n",
      "\n",
      "    , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(swain  ,   m.  , &   lapkin  ,   s.    ( 1998 ).  , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(interaction and second language learning: two adolescent \n",
      "french immersion students working together .  , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(modern language journal ,  82 ,  320 – 337 . \n",
      "    , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(swain  ,    m.  ,  &    lapkin  ,    s.     ( 2003 ).   , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(talking  it  through:  two  french  immersion  learners’ \n",
      "response to reformulation .  international journal of educational research ,  37 ,  285 – 304 . \n",
      "    , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(tardy  ,   c.    ( 2006 ).  , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(appropriation, ownership, and agency: negotiating teacher feedback in \n",
      "academic  settings .  in     k.      , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(hyland    &    f.      hyland     (eds.),   feedback  in  second  language \n",
      "writing  (pp.  60 – 78 ).  , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(oxford :  oxford university press . \n",
      "\n",
      "\f",
      ", {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(processing, uptake, and retention of cf\n",
      "\n",
      "331\n",
      "\n",
      "    thornbury  ,   s.    ( 1997 ).  reformulation and reconstruction: tasks that promote “noticing., {'neg': 0.0, 'neu': 0.867, 'pos': 0.133, 'compound': 0.3818})\n",
      "(”  \n",
      "\n",
      " elt journal ,  51 ,  326 – 335 . \n",
      "\n",
      "    , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(tocalli-beller  ,   a.  , &   swain  ,   m.    ( 2005 ).  , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(reformulation: the cognitive conﬂ ict and l2 learning \n",
      "\n",
      "it generates .  , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(international journal of applied linguistics ,  15 ,  5 – 28 . \n",
      "\n",
      "    , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(truscott  ,   j.    ( 2007 ).  the effect of error correction on learners’ ability to write accurately . \n",
      "\n",
      " , {'neg': 0.15, 'neu': 0.722, 'pos': 0.128, 'compound': -0.1027})\n",
      "(journal of second language writing ,  16 ,  255 – 272 .  \n",
      "\n",
      "  , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(appendix a: graph report \n",
      "\n",
      " the graph below shows average rainfall (by season) for four cities. \n",
      ", {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(write a report for a university lecturer describing the information shown \n",
      "below. \n",
      "\n",
      " , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(you should write at least 150 words.      \n",
      "\n",
      "\f",
      ", {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(332\n",
      "\n",
      "neomy storch and gillian wigglesworth\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "e\n",
      "d\n",
      "o\n",
      "c\n",
      "g\n",
      "n\n",
      "t\n",
      "d\n",
      "e\n",
      "\n",
      "i\n",
      "\n",
      "i\n",
      "\n",
      " \n",
      ":\n",
      "\n",
      "i\n",
      "\n",
      " \n",
      "\n",
      "b\n",
      "x\n",
      "d\n",
      "n\n",
      "e\n",
      "p\n",
      "p\n",
      "  a\n",
      "\n",
      "     \n",
      "e\n",
      "l\n",
      "p\n",
      "m\n",
      "a\n",
      "x\n",
      " e\n",
      "\n",
      "   \n",
      "\n",
      " f\n",
      "\n",
      "    \n",
      ")\n",
      "g\n",
      "n\n",
      "i\n",
      "t\n",
      "s\n",
      "e\n",
      "r\n",
      "e\n",
      "t\n",
      "\n",
      "n\n",
      "i\n",
      " (\n",
      " \n",
      " .\n",
      "\n",
      ", {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(d\n",
      "e\n",
      "t\n",
      "s\n",
      "e\n",
      "r\n",
      "e\n",
      "t\n",
      "n\n",
      "\n",
      " \n",
      "\n",
      " i\n",
      " \n",
      "s\n",
      "a\n",
      "w\n",
      "m\n",
      "u\n",
      "e\n",
      "s\n",
      "u\n",
      "m\n",
      "e\n",
      "h\n",
      " t\n",
      "\n",
      " \n",
      "\n",
      "    \n",
      ")\n",
      "s\n",
      "r\n",
      "a\n",
      "c\n",
      " (\n",
      " \n",
      ".\n",
      ", {'neg': 0.0, 'neu': 0.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(t\n",
      "n\n",
      "e\n",
      "m\n",
      "n\n",
      "o\n",
      "r\n",
      "i\n",
      "v\n",
      "n\n",
      "e\n",
      "e\n",
      "h\n",
      "t\n",
      " \n",
      "r\n",
      "o\n",
      "f\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "d\n",
      "a\n",
      "b\n",
      "e\n",
      "r\n",
      "a\n",
      "  \n",
      "r\n",
      "a\n",
      "  c\n",
      "\n",
      "   \n",
      "\n",
      " f\n",
      "\n",
      "    \n",
      ")\n",
      "n\n",
      "o\n",
      "i\n",
      "t\n",
      "\n",
      "a\n",
      "m\n",
      "r\n",
      "o\n",
      "n\n",
      "i\n",
      " (\n",
      " \n",
      " , {'neg': 0.0, 'neu': 0.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(.\n",
      "\n",
      "f\n",
      "\n",
      "m\n",
      "r\n",
      "o\n",
      "f\n",
      "n\n",
      "\n",
      " i\n",
      " \n",
      "e\n",
      "m\n",
      "o\n",
      "s\n",
      " \n",
      "d\n",
      "e\n",
      "e\n",
      "n\n",
      "e\n",
      " w\n",
      "\n",
      " \n",
      "\n",
      "   \n",
      "\n",
      " f\n",
      "\n",
      "    \n",
      ")\n",
      "e\n",
      "r\n",
      "u\n",
      "t\n",
      "i\n",
      "n\n",
      "r\n",
      "u\n",
      "f\n",
      " (\n",
      " \n",
      " .\n",
      ", {'neg': 0.0, 'neu': 0.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(s\n",
      "e\n",
      "r\n",
      "u\n",
      "t\n",
      "i\n",
      "n\n",
      "r\n",
      "u\n",
      " f\n",
      " \n",
      "r\n",
      "u\n",
      "o\n",
      "y\n",
      "e\n",
      "v\n",
      "o\n",
      "\n",
      " \n",
      "\n",
      "l\n",
      " \n",
      " i\n",
      "\n",
      "   \n",
      "\n",
      " f\n",
      "\n",
      "    \n",
      ")\n",
      "s\n",
      "e\n",
      "k\n",
      "i\n",
      "l\n",
      " (\n",
      " \n",
      ".\n",
      ", {'neg': 0.0, 'neu': 0.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(t\n",
      "i\n",
      "  \n",
      "e\n",
      "k\n",
      "i\n",
      " l\n",
      " \n",
      "\n",
      "e\n",
      "h\n",
      " s\n",
      "\n",
      "   \n",
      "\n",
      "f\n",
      "  \n",
      "\n",
      "   \n",
      "\n",
      " f\n",
      "\n",
      "    \n",
      ")\n",
      "g\n",
      "n\n",
      "i\n",
      "t\n",
      "t\n",
      "u\n",
      "h\n",
      "s\n",
      " (\n",
      " \n",
      "?\n",
      ", {'neg': 0.0, 'neu': 0.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(r\n",
      "o\n",
      "o\n",
      "d\n",
      " \n",
      "e\n",
      "h\n",
      "t\n",
      "  \n",
      "t\n",
      "u\n",
      "h\n",
      "s\n",
      " \n",
      "o\n",
      " t\n",
      " \n",
      "d\n",
      "n\n",
      "m\n",
      "u\n",
      "o\n",
      "y\n",
      "d\n",
      "u\n",
      "o\n",
      " w\n",
      "\n",
      ", {'neg': 0.0, 'neu': 0.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(l\n",
      "\n",
      "i\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "    \n",
      ")\n",
      "t\n",
      "\n",
      "h\n",
      "g\n",
      "u\n",
      "a\n",
      "c\n",
      " (\n",
      " \n",
      ".\n",
      ", {'neg': 0.0, 'neu': 0.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(f\n",
      "e\n",
      "i\n",
      "h\n",
      "t\n",
      " \n",
      "e\n",
      "h\n",
      "t\n",
      "  \n",
      "t\n",
      "h\n",
      "g\n",
      "u\n",
      "a\n",
      "c\n",
      "e\n",
      "r\n",
      "e\n",
      " w\n",
      "e\n",
      "c\n",
      "i\n",
      "l\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "o\n",
      "p\n",
      "e\n",
      "h\n",
      " t\n",
      "\n",
      "   \n",
      "\n",
      " f\n",
      "\n",
      " \n",
      ")\n",
      "b\n",
      "r\n",
      "e\n",
      "v\n",
      " \n",
      "r\n",
      "o\n",
      "n\n",
      "u\n",
      "o\n",
      "n\n",
      "\n",
      " \n",
      "\n",
      " \n",
      ",\n",
      ".\n",
      "\n",
      ".\n",
      "\n",
      ", {'neg': 0.0, 'neu': 0.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(g\n",
      "e\n",
      "(\n",
      " \n",
      "\n",
      "e\n",
      "p\n",
      "y\n",
      "t\n",
      " \n",
      "d\n",
      "r\n",
      "o\n",
      " w\n",
      "\n",
      " \n",
      "\n",
      "m\n",
      "r\n",
      "o\n",
      "f\n",
      "  \n",
      "g\n",
      "n\n",
      " i\n",
      "-\n",
      " \n",
      "r\n",
      "o\n",
      "e\n",
      "v\n",
      "i\n",
      "t\n",
      "i\n",
      "n\n",
      "ﬁ \n",
      "n\n",
      "\n",
      " \n",
      "\n",
      " i\n",
      "\n",
      " \n",
      "\n",
      "e\n",
      "v\n",
      "i\n",
      "s\n",
      "s\n",
      "a\n",
      "p\n",
      " \n",
      "r\n",
      "o\n",
      " \n",
      "e\n",
      "v\n",
      "i\n",
      "t\n",
      "c\n",
      " a\n",
      "\n",
      " \n",
      "t\n",
      "n\n",
      "e\n",
      ", {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(m\n",
      "e\n",
      "e\n",
      "r\n",
      "g\n",
      "a\n",
      " \n",
      "t\n",
      "c\n",
      "e\n",
      "j\n",
      "b\n",
      "u\n",
      "s\n",
      "-\n",
      "b\n",
      "r\n",
      "e\n",
      " v\n",
      "\n",
      "l\n",
      "\n",
      " \n",
      "l\n",
      "a\n",
      "r\n",
      "u\n",
      "p\n",
      " \n",
      "r\n",
      "o\n",
      " \n",
      "r\n",
      "a\n",
      "l\n",
      "u\n",
      "g\n",
      "n\n",
      "i\n",
      " s\n",
      "\n",
      " \n",
      "\n",
      "e\n",
      "l\n",
      "b\n",
      "a\n",
      "t\n",
      "n\n",
      "u\n",
      "o\n",
      "c\n",
      "n\n",
      "u\n",
      " \n",
      "r\n",
      "o\n",
      "e\n",
      "l\n",
      "b\n",
      "a\n",
      "t\n",
      "n\n",
      "u\n",
      "o\n",
      " c\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "m\n",
      "r\n",
      "o\n",
      "f\n",
      " \n",
      "d\n",
      "r\n",
      "o\n",
      " w\n",
      "\n",
      " \n",
      "\n",
      "m\n",
      "e\n",
      "l\n",
      "b\n",
      "o\n",
      "r\n",
      " p\n",
      "\n",
      " \n",
      "\n",
      "m\n",
      "r\n",
      "o\n",
      "f\n",
      " \n",
      ", {'neg': 0.0, 'neu': 0.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(g\n",
      "n\n",
      "o\n",
      "r\n",
      " w\n",
      "\n",
      " \n",
      "\n",
      " f\n",
      "\n",
      " \n",
      "\n",
      "e\n",
      "d\n",
      "o\n",
      "   c\n",
      "\n",
      " \n",
      "\n",
      "    \n",
      ")\n",
      "l\n",
      "l\n",
      "e\n",
      "w\n",
      "h\n",
      "s\n",
      "i\n",
      "l\n",
      "g\n",
      "n\n",
      "e\n",
      " (\n",
      " \n",
      ".\n",
      "\n",
      "h\n",
      "s\n",
      "i\n",
      "l\n",
      "g\n",
      ", {'neg': 0.0, 'neu': 0.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(n\n",
      "e\n",
      "\n",
      " \n",
      "l\n",
      "l\n",
      "e\n",
      "w\n",
      " \n",
      "s\n",
      "k\n",
      "a\n",
      "e\n",
      "p\n",
      "s\n",
      " \n",
      "e\n",
      " h\n",
      "\n",
      "    \n",
      ")\n",
      "s\n",
      "a\n",
      "w\n",
      " (\n",
      " \n",
      ".\n",
      ", {'neg': 0.0, 'neu': 0.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(t\n",
      "o\n",
      "h\n",
      "   \n",
      "  ∧\n",
      " \n",
      "y\n",
      "a\n",
      "d\n",
      "o\n",
      " t\n",
      "\n",
      "    \n",
      ")\n",
      ".\n",
      ", {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(e\n",
      "m\n",
      "o\n",
      "h\n",
      "\n",
      " \n",
      "t\n",
      "\n",
      "n\n",
      "e\n",
      "w\n",
      "\n",
      " \n",
      "i\n",
      " (\n",
      " \n",
      ".\n",
      ")\n",
      "e\n",
      "m\n",
      "o\n",
      "h\n",
      "o\n",
      "t\n",
      "(\n",
      " \n",
      "t\n",
      "n\n",
      "e\n",
      "w\n",
      "\n",
      " \n",
      "\n",
      " \n",
      " i\n",
      "\n",
      "    \n",
      ")\n",
      "a\n",
      " (\n",
      " \n",
      ".\n",
      "\n",
      ", {'neg': 0.0, 'neu': 0.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(e\n",
      "m\n",
      "\n",
      "i\n",
      "t\n",
      " \n",
      "d\n",
      "o\n",
      "o\n",
      "g\n",
      "  \n",
      "e\n",
      "h\n",
      " t\n",
      " \n",
      "d\n",
      "a\n",
      "h\n",
      "e\n",
      " w\n",
      "\n",
      " \n",
      "\n",
      "   \n",
      "\n",
      " t\n",
      "\n",
      "    \n",
      ")\n",
      "t\n",
      "\n",
      "n\n",
      "e\n",
      "w\n",
      " (\n",
      " \n",
      ".\n",
      "\n",
      " \n",
      "\n",
      ", {'neg': 0.0, 'neu': 0.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(y\n",
      "a\n",
      "d\n",
      "r\n",
      "e\n",
      "t\n",
      "s\n",
      "e\n",
      "y\n",
      "e\n",
      "r\n",
      "e\n",
      "h\n",
      "t\n",
      "  \n",
      "s\n",
      "e\n",
      "o\n",
      " g\n",
      "e\n",
      "h\n",
      " s\n",
      "\n",
      " \n",
      "\n",
      " \n",
      ")\n",
      "\n",
      "m\n",
      "e\n",
      "l\n",
      "b\n",
      "o\n",
      "r\n",
      "p\n",
      "e\n",
      "l\n",
      "c\n",
      "i\n",
      "t\n",
      "r\n",
      "a\n",
      "\n",
      " \n",
      "\n",
      " \n",
      ",\n",
      "\n",
      "e\n",
      "s\n",
      "u\n",
      " \n",
      "r\n",
      "a\n",
      "e\n",
      "l\n",
      "c\n",
      "n\n",
      "u\n",
      "\n",
      " \n",
      ",\n",
      "\n",
      "y\n",
      "r\n",
      "a\n",
      "s\n",
      "s\n",
      "e\n",
      "c\n",
      "e\n",
      "n\n",
      "n\n",
      "u\n",
      "\n",
      " \n",
      ",\n",
      "e\n",
      "t\n",
      ", {'neg': 0.0, 'neu': 0.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(a\n",
      "i\n",
      "r\n",
      "p\n",
      "o\n",
      "r\n",
      "p\n",
      "p\n",
      "a\n",
      "n\n",
      "\n",
      " i\n",
      "\n",
      "  \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "l\n",
      "\n",
      "d\n",
      "e\n",
      "t\n",
      "e\n",
      "l\n",
      "e\n",
      "d\n",
      "e\n",
      "b\n",
      "d\n",
      "u\n",
      "o\n",
      "h\n",
      "s\n",
      " \n",
      "e\n",
      "s\n",
      "a\n",
      "r\n",
      "h\n",
      "p\n",
      " \n",
      "s\n",
      "i\n",
      "h\n",
      "t\n",
      " \n",
      ", {'neg': 0.0, 'neu': 0.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(n\n",
      "\n",
      "i\n",
      " \n",
      "e\n",
      "r\n",
      "e\n",
      "h\n",
      "w\n",
      "e\n",
      "m\n",
      "o\n",
      "s\n",
      " \n",
      ")\n",
      "s\n",
      "(\n",
      "d\n",
      "r\n",
      "o\n",
      " w\n",
      "\n",
      " \n",
      "g\n",
      "n\n",
      "i\n",
      "s\n",
      "s\n",
      "i\n",
      "m\n",
      "d\n",
      "r\n",
      "o\n",
      " w\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "r\n",
      "e\n",
      "d\n",
      "r\n",
      "o\n",
      "d\n",
      "r\n",
      "o\n",
      " w\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "r\n",
      "o\n",
      "r\n",
      "r\n",
      "e\n",
      "\n",
      " \n",
      "\n",
      "e\n",
      "s\n",
      "n\n",
      "e\n",
      " t\n",
      "\n",
      "   \n",
      "\n",
      " c\n",
      "\n",
      " \n",
      ",\n",
      "\n",
      "n\n",
      "o\n",
      "i\n",
      "t\n",
      "a\n",
      "c\n",
      "o\n",
      "\n",
      "l\n",
      "l\n",
      "\n",
      " \n",
      "\n",
      "o\n",
      "c\n",
      "g\n",
      "n\n",
      "o\n",
      "r\n",
      "w\n",
      "\n",
      " \n",
      ",\n",
      ".\n",
      "\n",
      ".\n",
      "\n",
      ", {'neg': 0.0, 'neu': 0.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(g\n",
      "e\n",
      "(\n",
      " \n",
      "d\n",
      "r\n",
      "o\n",
      "w\n",
      " \n",
      "e\n",
      "h\n",
      "t\n",
      " \n",
      "e\n",
      "g\n",
      "n\n",
      "a\n",
      "h\n",
      "c\n",
      " \n",
      "–\n",
      " \n",
      ", {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(m\n",
      "e\n",
      "l\n",
      "b\n",
      "o\n",
      "r\n",
      "p\n",
      " \n",
      "e\n",
      "c\n",
      "i\n",
      "o\n",
      "h\n",
      "c\n",
      "d\n",
      "r\n",
      "o\n",
      " w\n",
      "\n",
      " \n",
      "\n",
      "    \n",
      ")\n",
      ", {'neg': 0.0, 'neu': 0.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(n\n",
      "a\n",
      "w\n",
      "i\n",
      "a\n",
      "t\n",
      "…\n",
      "\n",
      ", {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(t\n",
      "’\n",
      "\n",
      ", {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(n\n",
      "o\n",
      "d\n",
      " (\n",
      " \n",
      " .\n",
      "\n",
      ", {'neg': 0.0, 'neu': 0.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(n\n",
      "a\n",
      "w\n",
      "i\n",
      "a\n",
      " t\n",
      " \n",
      "\n",
      "m\n",
      "o\n",
      "r\n",
      "f\n",
      " \n",
      "e\n",
      "m\n",
      "o\n",
      "c\n",
      "  \n",
      "t\n",
      "n\n",
      "o\n",
      " d\n",
      "\n",
      " \n",
      " i\n",
      "\n",
      "   \n",
      "\n",
      " x\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "  \n",
      " \n",
      "\n",
      " x\n",
      "\n",
      " \n",
      "\n",
      "n\n",
      "o\n",
      "i\n",
      ", {'neg': 0.0, 'neu': 0.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(t\n",
      "a\n",
      "s\n",
      "i\n",
      "l\n",
      "a\n",
      "t\n",
      "i\n",
      "p\n",
      "a\n",
      "c\n",
      " \n",
      "r\n",
      "o\n",
      " \n",
      "g\n",
      "n\n",
      "\n",
      "i\n",
      "l\n",
      "l\n",
      "e\n",
      "p\n",
      "s\n",
      " \n",
      ",\n",
      "\n",
      "n\n",
      "o\n",
      "i\n",
      "t\n",
      "a\n",
      "u\n",
      "t\n",
      "c\n",
      "n\n",
      "u\n",
      " p\n",
      "\n",
      "   \n",
      " .\n",
      "e\n",
      "m\n",
      "\n",
      "i\n",
      "t\n",
      " \n",
      "d\n",
      "n\n",
      ", {'neg': 0.0, 'neu': 0.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(e\n",
      "l\n",
      " \n",
      "s\n",
      "y\n",
      "a\n",
      "w\n",
      "l\n",
      "a\n",
      "n\n",
      "a\n",
      "c\n",
      "g\n",
      "n\n",
      "i\n",
      "v\n",
      "i\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "        \n",
      "\n",
      "   \n",
      " ?\n",
      "\n",
      "  , {'neg': 0.0, 'neu': 0.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(l\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "e\n",
      "r\n",
      "a\n",
      "u\n",
      "o\n",
      "y\n",
      " \n",
      "t\n",
      "a\n",
      "h\n",
      "w\n",
      "d\n",
      "n\n",
      "a\n",
      "t\n",
      "s\n",
      "r\n",
      "e\n",
      "d\n",
      "n\n",
      "u\n",
      " \n",
      "t\n",
      "o\n",
      "n\n",
      "n\n",
      "a\n",
      "c\n",
      " \n",
      "r\n",
      "e\n",
      "d\n",
      "a\n",
      "e\n",
      "r\n",
      " \n",
      "e\n",
      "h\n",
      "t\n",
      "\n",
      " \n",
      "-\n",
      " \n",
      "\n",
      "m\n",
      "e\n",
      "l\n",
      "b\n",
      "o\n",
      "r\n",
      "p\n",
      " \n",
      "g\n",
      "n\n",
      "n\n",
      "a\n",
      "e\n",
      " m\n",
      "\n",
      "i\n",
      "\n",
      " \n",
      ".\n",
      "\n",
      ", {'neg': 0.0, 'neu': 0.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(y\n",
      "a\n",
      "s\n",
      " \n",
      "o\n",
      "t\n",
      " \n",
      "g\n",
      ", {'neg': 0.0, 'neu': 0.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(n\n",
      "i\n",
      "y\n",
      "r\n",
      " t\n",
      "\n",
      "  \n",
      "\n",
      " \n",
      ")\n",
      "\n",
      "…\n",
      "\n",
      " (\n",
      "\n",
      " \n",
      "\n",
      " c\n",
      "\n",
      " \n",
      "\n",
      " t\n",
      "\n",
      "  \n",
      "\n",
      "  ∧\n",
      "\n",
      "      \n",
      "\n",
      " \n",
      " ?\n",
      "\n",
      " \n",
      "\n",
      " x\n",
      "\n",
      "\f",
      "processing, uptake, and retention of cf\n",
      "\n",
      "333\n",
      "\n",
      "         appendix c: guidelines for \n",
      "\n",
      "lre analysis \n",
      "\n",
      " a  language related episode (lre ) is any segment in the data where there is an \n",
      "explicit focus on language. \n",
      "\n",
      " note:\n",
      "   \n",
      "       •      , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(this focus can be in response to the feedback the participants received but \n",
      "\n",
      "  \n",
      "\n",
      "can also be unsolicited.  \n",
      "\n",
      "      •      , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(lres  can  vary  in  length.  , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(they  can  be  short  (e.g.,  consisting  of  a  learner \n",
      "simply reading out aloud a reformulated word or phrase with no response \n",
      "from  the  other  member  of  the  pair)  or  a  long  segment  (e.g.,  where  both \n",
      "learners discuss grammatical or lexical choices).  \n",
      "\n",
      "      •      , {'neg': 0.055, 'neu': 0.945, 'pos': 0.0, 'compound': -0.296})\n",
      "(lres can be interrupted., {'neg': 0.423, 'neu': 0.577, 'pos': 0.0, 'compound': -0.296})\n",
      "(for example, learners may deliberate over the use \n",
      "of articles and decide to omit it., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(they may then return to this decision at a \n",
      "later stage in their pair talk and decide to reverse their decision, and insert \n",
      "the article., {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(since both segments deal with the same ‘, {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(error’ they are counted \n",
      "as one episode.    \n",
      "\n",
      "  coding lres \n",
      "\n",
      "    \n",
      "   \n",
      "\n",
      "  \n",
      "   1.       , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(identify in the data segments where learners seem to be focusing explicitly \n",
      "\n",
      "on language choice.  \n",
      "\n",
      " \n",
      "\n",
      "  2.       , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(distinguish  lres  in  terms  of  focus:  form-focus  (f-lres),  lexis-focus  (l-\n",
      "\n",
      "      \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "  \n",
      "\n",
      "  \n",
      "\n",
      "    \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "lres), mechanics-focus (m-lres). \n",
      "  •        f-lre : focus on morphology or syntax (e.g., verb tenses, word forms, use \n",
      "of articles, prepositions, word order)  \n",
      "\n",
      "  •        , {'neg': 0.058, 'neu': 0.942, 'pos': 0.0, 'compound': -0.2263})\n",
      "(l-lre : deliberations on word meaning, searching for a word, suggesting \n",
      "\n",
      "alternative words/phrase  \n",
      "\n",
      "  •        , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(m-lre : deliberations on issues such as spelling or punctuations (or pro-\n",
      "\n",
      "nunciation)   \n",
      "\n",
      "  \n",
      "  3.         , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(determine whether the lre deals with language items that were targeted \n",
      "\n",
      "by the feedback given.  \n",
      "\n",
      " , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(4.         , {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0})\n",
      "(determine whether the lre is resolved correctly ( √ ), incorrectly (x), or \n",
      "\n",
      "left unresolved (?). \n",
      "  •        , {'neg': 0.0, 'neu': 0.898, 'pos': 0.102, 'compound': 0.1779})\n",
      "(resolved correctly  ( √ ): the resolution reached is in line with the intended \n",
      "feedback (or it could be an acceptable alternative in this instance).  \n",
      "\n",
      "  •        , {'neg': 0.096, 'neu': 0.719, 'pos': 0.185, 'compound': 0.1531})\n",
      "(resolved incorrectly  (x): the resolution reached is not in line with the \n",
      "\n",
      "intended feedback (or is an unacceptable alternative in this instance).  \n",
      "\n",
      "  •        , {'neg': 0.115, 'neu': 0.766, 'pos': 0.119, 'compound': -0.2263})\n",
      "(unresolved  (?): the learners seem unable to determine how to respond \n",
      "to the feedback (in the case of editing) or seem reluctant to accept the \n",
      "reformulation but cannot agree on an alternative.   \n",
      "\n",
      "\f",
      "334\n",
      "\n",
      "neomy storch and gillian wigglesworth\n",
      "\n",
      "   \n",
      "\n",
      " 5.         , {'neg': 0.099, 'neu': 0.858, 'pos': 0.043, 'compound': -0.3324})\n",
      "(lres that deal with language items targeted by the feedback are further \n",
      "\n",
      "analyzed for the nature of engagement. \n",
      "\n",
      "       •       , {'neg': 0.0, 'neu': 0.857, 'pos': 0.143, 'compound': 0.4588})\n",
      "(lres  which  show  extensive  engagement  (ee):   episodes  where  learners \n",
      "offer  suggestions  and  counter  suggestions,  explanations,  or  any  com-\n",
      "ments  showing  evidence  of  meta-awareness  of  the  feedback  received \n",
      "(e.g.,  we  don’t  have  to  use  being )., {'neg': 0.0, 'neu': 0.919, 'pos': 0.081, 'compound': 0.4588})\n",
      "(it also includes episodes where the \n",
      "correction is repeated by learners a number of times.  \n",
      "\n",
      "   \n",
      "\n",
      "  , {'neg': 0.0, 'neu': 0.909, 'pos': 0.091, 'compound': 0.0772})\n",
      "(•       lres  which  show  limited  or  no  engagement  (le):   episodes  where  one \n",
      "member of the pair just reads the feedback and the other simply acknowl-\n",
      "edged or repeats it once, without making any other additional comments.   \n",
      "\n",
      "      \n",
      "\n",
      "\f",
      ", {'neg': 0.102, 'neu': 0.823, 'pos': 0.075, 'compound': -0.0258})\n"
     ]
    }
   ],
   "source": [
    "for doc in textAnalytics:\n",
    "    if doc['abstract'] != False:\n",
    "        for sent in doc['abstract']['sent_analysis']:\n",
    "            if sent[1] > 0.2:\n",
    "                print sent\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.00000012  0.03390312  0.05097634  0.03979823  0.0481088   0.03399898\n",
      "  0.04389757  0.03610792  0.05845335  0.04130838  0.03907927  0.06473102\n",
      "  0.07154746  0.05932158  0.0378841   0.08124913  0.04131949  0.04432919\n",
      "  0.05244407  0.04781858  0.00496908  0.05387572  0.0481088   0.05585184\n",
      "  0.03395354  0.01902193  0.02988975]\n"
     ]
    }
   ],
   "source": [
    "tfidf = calculateTFidf(textAnalytics,'lemmatizedVocab')\n",
    "testQuery = textAnalytics[0]['lemmatizedVocab']\n",
    "#testQuery =['deep','learning','machine','speech']\n",
    "#Create dictionary and transform to tf_idf vector\n",
    "testQuery_doc_bow = tfidf['dict'].doc2bow(testQuery)\n",
    "#print testQuery_doc_bow\n",
    "#print testQuery_doc_bow\n",
    "# test_tf_idf = calculateTFidf(testQuery,False)\n",
    "testQuery_tf_idf = tfidf['tf_idf'][testQuery_doc_bow]\n",
    "vector = tfidf['sims'][testQuery_tf_idf]\n",
    "print vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## WORD TO VECT FOR DOCUMENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('doc_0', 0.8861734867095947),\n",
       " ('doc_19', 0.8507816195487976),\n",
       " ('doc_3', 0.8429484963417053),\n",
       " ('doc_25', 0.8331794738769531),\n",
       " ('doc_21', 0.8325570821762085),\n",
       " ('doc_8', 0.8274330496788025),\n",
       " ('doc_20', 0.8213350772857666),\n",
       " ('doc_17', 0.8203819394111633),\n",
       " ('doc_12', 0.8165305256843567),\n",
       " ('doc_23', 0.8138983249664307)]"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vecDoc = word2VectDocumentTraining(textAnalytics,'tokens')\n",
    "testVector = word2vecDoc.infer_vector(textAnalytics[0]['tokens'])\n",
    "word2vecDoc.docvecs.most_similar([testVector])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Get The topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ldaFunction = get_lda(textAnalytics,'tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, u'0.010*\"domain\" + 0.009*\"cnn\" + 0.007*\"image\" + 0.007*\"e\"'), (1, u'0.000*\"learning\" + 0.000*\"deep\" + 0.000*\"speech\" + 0.000*\"processing\"'), (2, u'0.011*\"features\" + 0.011*\"audio\" + 0.010*\"data\" + 0.009*\"deep\"'), (3, u'0.015*\"model\" + 0.013*\"image\" + 0.011*\"images\" + 0.009*\"neural\"'), (4, u'0.016*\"text\" + 0.010*\"image\" + 0.009*\"images\" + 0.008*\"language\"'), (5, u'0.009*\"view\" + 0.009*\"cca\" + 0.008*\"correlation\" + 0.007*\"learning\"'), (6, u'0.016*\"learning\" + 0.012*\"speech\" + 0.011*\"\\u201c\" + 0.011*\"\\u201d\"'), (7, u'0.021*\"speech\" + 0.021*\"learning\" + 0.020*\"deep\" + 0.016*\"processing\"'), (8, u'0.012*\"feedback\" + 0.006*\"e\" + 0.006*\"n\" + 0.005*\"t\"')]\n"
     ]
    }
   ],
   "source": [
    "topics = get_topics(ldaFunction,10,4)\n",
    "print topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(17809 unique tokens: [u'i|W', u'Andreas', u'LATEX', u'nunnery', u'tweenthelastlayerofthenetworkandthesoftmaxlayer']...)\n",
      "LdaModel(num_terms=16675, num_topics=9, decay=0.5, chunksize=2000)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 16675 is out of bounds for axis 1 with size 16675",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-262-a1077dbec4df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlda_display\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLDAvis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'20top_100.gensim'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlda_display\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-f4dc902bcb98>\u001b[0m in \u001b[0;36mLDAvis\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0mlda\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mlda_display\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlda\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msort_topics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlda_display\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/agonzamart/anaconda/lib/python2.7/site-packages/pyLDAvis/gensim.pyc\u001b[0m in \u001b[0;36mprepare\u001b[0;34m(topic_model, corpus, dictionary, doc_topic_dist, **kwargs)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0mSee\u001b[0m \u001b[0;34m`\u001b[0m\u001b[0mpyLDAvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m`\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \"\"\"\n\u001b[0;32m--> 111\u001b[0;31m     \u001b[0mopts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_extract_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_topic_dist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mvis_prepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mopts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/agonzamart/anaconda/lib/python2.7/site-packages/pyLDAvis/gensim.pyc\u001b[0m in \u001b[0;36m_extract_data\u001b[0;34m(topic_model, corpus, dictionary, doc_topic_dists)\u001b[0m\n\u001b[1;32m     40\u001b[0m           \u001b[0mgamma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtopic_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m           \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtopic_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m       \u001b[0mdoc_topic_dists\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/agonzamart/anaconda/lib/python2.7/site-packages/gensim/models/ldamodel.pyc\u001b[0m in \u001b[0;36minference\u001b[0;34m(self, chunk, collect_sstats)\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0mElogthetad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mElogtheta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m             \u001b[0mexpElogthetad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexpElogtheta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 436\u001b[0;31m             \u001b[0mexpElogbetad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpElogbeta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m             \u001b[0;31m# The optimal phi_{dwk} is proportional to expElogthetad_k * expElogbetad_w.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 16675 is out of bounds for axis 1 with size 16675"
     ]
    }
   ],
   "source": [
    "lda_display = LDAvis('20top_100.gensim')\n",
    "pyLDAvis.display(lda_display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(14280 unique tokens: [u'orthogon', u'fig-', u'statistical/machin', u'l.a.n.', u'yellow']...)\n",
      "LdaModel(num_terms=14280, num_topics=9, decay=0.5, chunksize=2000)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el169948234518562783493763\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el169948234518562783493763_data = {\"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [1, 2, 3, 4, 5, 6, 7, 8, 9], \"token.table\": {\"Topic\": [5, 7, 1, 3, 5, 7, 8, 1, 3, 5, 7, 8, 1, 3, 5, 7, 8, 1, 3, 7, 1, 3, 5, 7, 8, 1, 3, 5, 7, 8, 8, 1, 1, 7, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 7, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 7, 1, 3, 5, 7, 8, 1, 1, 1, 1, 1, 3, 5, 7, 1, 3, 5, 7, 8, 1, 3, 5, 7, 8, 1, 3, 5, 7, 8, 1, 3, 5, 7, 8, 3, 8, 1, 3, 8, 8, 3, 1, 3, 7, 8, 3, 3, 5, 7, 8, 3, 8, 1, 3, 5, 7, 8, 8, 1, 3, 1, 1, 3, 7, 8, 5, 1, 1, 3, 5, 7, 8, 3, 8, 1, 3, 5, 7, 8, 3, 7, 3, 5, 7, 1, 1, 3, 8, 1, 1, 1, 7, 8, 1, 3, 7, 7, 5, 1, 3, 5, 7, 8, 3, 7, 3, 7, 1, 8, 1, 1, 3, 7, 8, 8, 1, 3, 3, 8, 7, 3, 7, 8, 3, 8, 1, 1, 1, 1, 3, 5, 7, 8, 1, 3, 5, 7, 8, 1, 3, 5, 7, 8, 3, 5, 1, 3, 3, 5, 1, 3, 7, 8, 7, 3, 7, 8, 3, 5, 7, 8, 1, 3, 5, 7, 8, 1, 3, 7, 8, 1, 3, 7, 8, 3, 5, 1, 3, 7, 8, 1, 1, 3, 7, 8, 1, 3, 7, 1, 3, 8, 1, 1, 3, 5, 7, 3, 5, 8, 1, 5, 3, 5, 7, 8, 3, 8, 3, 7, 8, 5, 8, 1, 3, 5, 7, 8, 1, 3, 7, 8, 3, 5, 8, 5, 7, 1, 3, 7, 8, 3, 7, 8, 1, 3, 5, 7, 8, 1, 3, 7, 1, 3, 5, 7, 8, 1, 1, 3, 7, 8, 8, 3, 3, 7, 8, 1, 3, 5, 7, 8, 3, 1, 1, 3, 5, 7, 8, 1, 1, 1, 5, 8, 8, 8, 5, 1, 3, 5, 7, 3, 8, 1, 3, 7, 8, 1, 1, 1, 3, 7, 8, 1, 5, 3, 8, 1, 3, 7, 8, 1, 3, 7, 8, 3, 1, 3, 5, 7, 8, 1, 1, 3, 5, 7, 8, 8, 1, 3, 5, 7, 8, 1, 1, 3, 8, 8, 1, 1, 1, 1, 3, 5, 7, 8, 1, 1, 3, 7, 8, 5, 8, 1, 3, 5, 8, 5, 1, 3, 7, 8, 1, 3, 1, 3, 5, 7, 8, 3, 5, 8, 1, 3, 5, 7, 8, 3, 1, 7, 5, 3, 7, 8, 1, 3, 5, 7, 8, 3, 7, 8, 1, 5, 1, 3, 5, 7, 8, 1, 1, 1, 8, 3, 1, 3, 7, 8, 8, 1, 3, 7, 8, 1, 7, 8, 3, 7, 1, 3, 5, 7, 8, 1, 1, 5, 5, 1, 3, 7, 8, 1, 3, 7, 8, 1, 3, 7, 8, 1, 1, 7, 1, 3, 1, 3, 5, 7, 1, 3, 7, 8, 7, 7, 1, 3, 7, 8, 1, 3, 7, 1, 3, 5, 7, 1, 1, 3, 5, 7, 8, 8, 3, 7, 8, 5, 8, 7, 1, 3, 7, 8, 1, 1, 1, 3, 3, 5, 7, 8, 1, 3, 7, 8, 1, 1, 3, 5, 7, 8, 1, 3, 5, 7, 8, 8, 1, 1, 3, 5, 7, 8, 5, 1, 3, 7, 1, 5, 7, 8, 1, 3, 7, 8, 8, 5, 1, 3, 7, 8, 1, 3, 5, 7, 8, 5, 1, 5, 8, 5, 8, 8, 1, 3, 1, 1, 3, 7, 8, 5, 8, 1, 3, 5, 7, 8, 5, 8, 1, 3, 5, 7, 8, 1, 1, 5, 1, 1, 3, 5, 7, 8, 1, 1, 3, 5, 8, 1, 3, 7, 8, 3, 7, 8, 1, 1, 3, 5, 7, 8, 1, 1, 3, 7, 8, 1, 3, 8, 1, 8, 1, 1, 1, 3, 5, 1, 5, 1, 3, 5, 7, 8, 1, 1, 1, 5, 1, 3, 5, 7, 8, 3, 5, 7, 3, 5, 1, 3, 5, 7, 8, 1, 3, 5, 7, 8, 1, 3, 5, 7, 8, 1, 3, 5, 7, 8, 1, 3, 8, 1, 3, 5, 7, 8, 3, 1, 1, 3, 5, 7, 8, 1, 3, 7, 8, 7, 1, 7, 7, 1, 7, 1, 3, 5, 7, 8, 3, 7, 5, 1, 3, 5, 7, 8, 7, 1, 3, 5, 7, 8, 7, 8, 3, 1, 3, 5, 7, 8, 1, 3, 7, 8, 3, 5, 5, 3, 1, 1, 3, 5, 7, 8, 1, 3, 5, 7, 8, 3, 5, 8, 1, 7, 1, 3, 7, 8, 1, 7, 7, 8, 1, 1, 7, 3, 3, 1, 7, 7, 7, 7, 8, 8, 7, 8, 3, 3, 1, 5, 1, 3, 5, 7, 3], \"Freq\": [0.048437895435807413, 0.92032001328034085, 0.062620695465359574, 0.21632603888033308, 0.082545462204337616, 0.50381195897130204, 0.13378057667599544, 0.02973574640978667, 0.41772613620871546, 0.065785247194254062, 0.19450436863935799, 0.29206205720297318, 0.029422772419211429, 0.40911664506713036, 0.064850192270914986, 0.19395011206949575, 0.3026342305976033, 0.10502068961430164, 0.58165305017151669, 0.31506206884290489, 0.016000869665858043, 0.28508668123305042, 0.03476799137564409, 0.13934316668335361, 0.52482852504014388, 0.062236010484680526, 0.21813051815667869, 0.031787864454993382, 0.13671217567329527, 0.55111144313733729, 0.99445777559615645, 0.73699701492837988, 0.73699701485464042, 0.98580055420738244, 0.73699701481125701, 0.73699701483145552, 0.73699701488298597, 0.73699701483644953, 0.73699701491810188, 0.98695882029134507, 0.98695882015714553, 0.98695882025463721, 0.73699701486681535, 0.73699701490444314, 0.73699701495476933, 0.73699701493747294, 0.73699701491378244, 0.73699701488871061, 0.6874263247254625, 0.19640752135013215, 0.098203760675066074, 0.73699701477608748, 0.73699701493009417, 0.73699701489985514, 0.77299028715003559, 0.77299028717493323, 0.73699701480606716, 0.73699701489484248, 0.7369970149370314, 0.73699701491030356, 0.73699701483301583, 0.73699701489963121, 0.73699701483257352, 0.73699701485051072, 0.73699701487389446, 0.73699701483492575, 0.73699701492539571, 0.73699701494177916, 0.73699701487199742, 0.73699701487909763, 0.73699701493739844, 0.73699701493487946, 0.73699701486339386, 0.73699701498271275, 0.73699701494071301, 0.99033651042725068, 0.012438649055961419, 0.27957344544827573, 0.068116411496931581, 0.11668637447735236, 0.523015576972092, 0.73699701488808511, 0.73699701487919522, 0.73699701478071011, 0.73699701495280845, 0.072287155646636569, 0.5216397448014044, 0.054703793462319558, 0.35166724368634006, 0.024890985793191867, 0.047054192321376406, 0.0017048620406295799, 0.19776399671303127, 0.72865803616508251, 0.024890985757121009, 0.04705419225318766, 0.0017048620381589731, 0.19776399642644088, 0.72865803510914506, 0.020610276416880235, 0.27411667634450715, 0.022671304058568261, 0.11953960321790537, 0.56059951853914247, 0.0054426610383236859, 0.11066744111258162, 0.0054426610383236859, 0.11248166145868951, 0.76741520640363969, 0.95893577111312434, 0.99893322289242348, 0.0031580610433847483, 0.069477342954464463, 0.92531188571173117, 1.0013676200316275, 1.0022403021723256, 0.73699701497600167, 0.57120528520873659, 0.39579578817613242, 0.026986076466554484, 1.000169065014338, 0.20882560309031006, 0.0097128187483865158, 0.3569460890032044, 0.42493582024191001, 1.0008795771161312, 0.99680827907244829, 0.22583071358072199, 0.15397548653231044, 0.12318038922584836, 0.4208663298549819, 0.071855227048411546, 1.0008784917743547, 0.77299028716952778, 0.98041717696239794, 0.73699701485999691, 0.73699701496961056, 0.99940402705689313, 0.98580055409673328, 0.99857876441221005, 0.96728258572912584, 0.90981501918319341, 0.054548785938084959, 0.30745679346920612, 0.034712863778781342, 0.28431488428335189, 0.32068074157540855, 0.97904306909323979, 0.019580861381864795, 0.01363401726273687, 0.068170086313684347, 0.020451025894105305, 0.81804103576421228, 0.081804103576421219, 0.99079584030862189, 0.97082299914252412, 0.054290193978294848, 0.84149800666357011, 0.1085803879565897, 0.73699701489977998, 0.57885931677028513, 0.055129458740027157, 0.3583414818101765, 0.7369970148851992, 0.89866398085795218, 0.032024894613322204, 0.92872194378634387, 0.032024894613322204, 0.027731979026643101, 0.87355733933925772, 0.097061926593250858, 0.99749766580248, 0.95564563286254245, 0.011569480787250832, 0.1002688334895072, 0.0077129871915005543, 0.65946040487329738, 0.21982013495776581, 0.98322048482992697, 0.97358445707615526, 0.0077770523392613946, 0.9876856470861971, 0.73060300822058966, 0.18265075205514741, 0.73699701495283987, 0.031461628336586361, 0.09753104784341772, 0.015730814168293181, 0.85575629075514903, 0.98087204352571744, 0.71863266851065621, 0.2395442228368854, 0.015768999193158991, 0.97767794997585755, 0.98580050987371093, 0.74340504968775301, 0.18290124238349481, 0.076700520999530078, 0.99937198348472112, 0.99921216230698895, 0.73699701492143821, 0.73699701487899105, 0.73699701487705616, 0.0021892312769897079, 0.067866169586680944, 0.013135387661938247, 0.0569200132017324, 0.8581786605799655, 0.026643586568345048, 0.43739887949699791, 0.034414632650779022, 0.1820645082170245, 0.31972303882014058, 0.13210037145897652, 0.387778509766673, 0.0085226046102565498, 0.43039153281795572, 0.042613023051282749, 0.056251764608793164, 0.95627999834948374, 0.84702273883852186, 0.99765782358533572, 0.99937198775628588, 0.9558010193675549, 0.012730544987156346, 0.21005399228807972, 0.11855320019289348, 0.65880570308534092, 0.9645306137144094, 0.0026321203853125609, 0.0026321203853125609, 0.99230938526283541, 0.13341815931722478, 0.066709079658612391, 0.76715441607404244, 0.033354539829306196, 0.013342587849779226, 0.90062467986009775, 0.0066712939248896128, 0.02001388177466884, 0.060041645324006514, 0.012534415092096575, 0.071028352188547264, 0.86905277971869588, 0.050137660368386301, 0.017748942303570738, 0.035497884607141476, 0.85194923057139538, 0.088744711517853694, 1.0008795729538578, 0.98654016879263107, 0.34153869246902341, 0.15369241161106054, 0.30738482322212107, 0.1707693462345117, 0.73699701499786552, 0.034823709022597349, 0.064672602470537938, 0.85566827884096341, 0.044773340171910876, 0.57146736791252317, 0.19048912263750772, 0.23811140329688466, 0.88149108497301054, 0.011832095100308866, 0.1064888559027798, 0.73699701485010383, 0.0044402027394319097, 0.10212466300693392, 0.44402027394319094, 0.45290067942205475, 0.081623360165230363, 0.89785696181753394, 0.020405840041307591, 0.73699701485997504, 0.97180820522518796, 0.8618671895982507, 0.0056331188862630763, 0.016899356658789229, 0.11829549661152459, 0.87140263892195602, 0.12448609127456516, 0.84925220851418171, 0.097990639443944039, 0.054439244135524464, 0.954170136335587, 0.032344750384257187, 0.10378824319721371, 0.11243726346364817, 0.28541766879233765, 0.073516672264693037, 0.42380199305528926, 0.73699701492207825, 0.79736153529739706, 0.18553564810244141, 0.015461304008536784, 0.10036976781290825, 0.77786570055003901, 0.12546220976613531, 0.98654016893696361, 0.95694346800541075, 0.37965243358680939, 0.15985365624707765, 0.19981707030884704, 0.2597621914015012, 0.082914777903314155, 0.86599879143461445, 0.046063765501841193, 0.024471313146501212, 0.39503691222209097, 0.0008739754695179004, 0.30851334073981884, 0.27180637102006705, 0.71147263459987897, 0.14978371254734293, 0.11233778441050721, 0.0048362759968749759, 0.0048362759968749759, 0.9769277513687451, 0.0048362759968749759, 0.0096725519937499518, 0.77299028720571561, 0.50566780049853111, 0.33711186699902079, 0.056185311166503459, 0.084277966749755198, 1.0003581765095191, 0.9972729471945212, 0.046939991593484595, 0.92941183355099499, 0.018775996637393839, 0.0083371571763184264, 0.080592519371078128, 0.002779052392106142, 0.075034414586865841, 0.8337157176318426, 0.96714717498679825, 0.73699701488787361, 0.96597054853354791, 0.51431073749909006, 0.0051689521356692468, 0.043936093153188596, 0.43419197939621673, 0.73699701475437707, 0.98695882029750615, 0.73699701488462521, 0.97745875166720042, 0.99621749250426794, 1.0003582411365675, 0.99647134975719065, 0.95580101933783868, 0.070657393415751543, 0.21740736435615862, 0.14674997094040707, 0.56525914732601235, 0.016537596396588312, 0.97571818739871041, 0.16135049666530293, 0.24910252116748521, 0.18682689087561391, 0.40196088642935118, 0.73699701488768399, 0.9367379787921315, 0.0059052855480010626, 0.041336998836007437, 0.011810571096002125, 0.94484568768016997, 0.73699701486618929, 0.9556456328388262, 0.0468511249253689, 0.9474338596019044, 0.011478237926623988, 0.075756370315718316, 0.018365180682598382, 0.8930069106913463, 0.017622292374009304, 0.55251069619687998, 0.34726282031135985, 0.081891829267455002, 0.97888226553195068, 0.02280480737310624, 0.20938959497124821, 0.022113752604224234, 0.16792630883832776, 0.57772178678535813, 0.73699701494493508, 0.010423756433136448, 0.15635634649704672, 0.0020847512866272897, 0.16469535164355589, 0.66712041172073266, 0.99121489999749235, 0.0074096416780315777, 0.10743980433145789, 0.0074096416780315777, 0.1333735502045684, 0.74466898864217357, 0.73699701494916336, 0.73699701480047519, 0.99423616082559396, 0.98732095344365089, 0.97901325629834246, 0.98695882021164905, 0.73699701487970837, 1.0079086508038178, 0.0059008772814512049, 0.035405263688707229, 0.0059008772814512049, 0.03343830459489016, 0.92250381500020506, 0.73699701489837766, 0.020891156629125489, 0.041782313258250978, 0.60584354224463921, 0.33127405511898989, 0.97890360318389791, 0.98596731557300665, 0.0034036959255250034, 0.20081805960597521, 0.10721642165403761, 0.68924842491881322, 0.96728258575904247, 0.12486977319095199, 0.2987080848881597, 0.33298606184253865, 0.24239426560596564, 0.84702273884936019, 1.0022403005257245, 0.021853401624206896, 0.24949300187636206, 0.0063739088070603441, 0.10380365771498275, 0.61872443348535777, 0.019419182239164633, 0.88357279188199078, 0.097095911195823156, 0.7369970149096261, 0.080801453836725087, 0.064641163069380073, 0.79185424759990586, 0.064641163069380073, 1.0034746257801139, 0.73699701491412151, 0.99037558588335761, 0.98640234853510989, 0.16496419652303854, 0.81451072033250282, 0.020620524565379817, 0.0088807698294979601, 0.12211058515559695, 0.01554134720162143, 0.12433077761297144, 0.72822312601883266, 0.22437779223968579, 0.60984733275401781, 0.16109174827464623, 0.73699701490154301, 0.9556456329013393, 0.063640160760212103, 0.31138221514818065, 0.0022728628842932893, 0.26137923169372829, 0.36138519860263302, 0.7369970149641184, 0.73699701483088531, 0.7369970148720919, 0.99900507552480511, 0.99769239850855918, 0.014556347246830555, 0.41242983866019906, 0.54343696388167406, 0.02911269449366111, 0.99032204672228752, 0.033569614227484126, 0.32992636482949245, 0.16837259635972507, 0.46787649829556005, 0.98912100487182797, 1.0027859922533222, 0.98165083759849914, 0.94381699956805354, 0.048400871772720691, 0.017676090733354306, 0.31375061051703895, 0.35352181466708615, 0.30933158783370041, 0.0044190226833385766, 0.73699701494723491, 0.73699701491328962, 0.92776908850358297, 0.99469447964791746, 0.549096348734325, 0.10237389552673856, 0.037226871100632206, 0.31642840435537373, 0.080975700039221638, 0.20744336639261274, 0.15376284614189276, 0.55773150701171759, 0.11139192163994963, 0.20856359796416099, 0.056880981262952997, 0.62332075300652667, 0.73699701493265524, 0.73699701487429592, 1.0049503785236937, 0.73699701483301938, 0.95893577256902274, 0.0067015518887761014, 0.060313966998984912, 0.40879466521534219, 0.52272104732453595, 0.0077091998604369837, 0.32378639413835331, 0.45484279176578202, 0.21585759609223554, 0.99037558007327642, 0.95695021288349646, 0.006258849487242671, 0.28790707641316288, 0.68221459410945107, 0.025035397948970684, 0.52571263876316054, 0.16175773500404941, 0.32351547000809883, 0.16746662639971882, 0.12908885784978325, 0.066288872949888694, 0.63846651314892799, 0.7369970149427757, 0.021375571819424424, 0.33132136320107858, 0.51835761662104229, 0.042751143638848847, 0.080158394322841595, 0.99173346566150333, 0.073229402279378991, 0.78721607450332409, 0.12815145398891323, 0.74723076920460696, 0.24366220734932834, 0.99037606348346785, 0.054653036644480416, 0.36818887844702597, 0.41996543947863896, 0.15820615870770646, 0.73699701494558922, 0.73699701494425529, 0.70244268982984159, 0.1756106724574604, 0.1936551077486223, 0.018443343595106885, 0.74695541560182888, 0.03688668719021377, 0.62781117347623572, 0.17122122912988247, 0.028536871521647078, 0.14268435760823539, 0.73699701496046433, 0.01433418081668004, 0.037627224643785102, 0.001791772602085005, 0.0035835452041700099, 0.94426416129879753, 0.010104162057204151, 0.03031248617161245, 0.069466114143278537, 0.020208324114408302, 0.87022095717670744, 1.0001490932508716, 0.73699701497375503, 0.054464629852635719, 0.22952951152182194, 0.23731017292934134, 0.39681373178348878, 0.085587275482713276, 0.98976554247187998, 0.41489212315916069, 0.48404081035235413, 0.092198249590924597, 0.73699701493538938, 0.78407139282145422, 0.019123692507840347, 0.19123692507840345, 0.011996598126818124, 0.095972785014544995, 0.06398185667636333, 0.82776527075045059, 0.99848850756864393, 0.9898257631954317, 0.0070755967067789123, 0.46875828182410295, 0.22818799379361993, 0.2971750616847143, 0.079519610972942856, 0.38220716241833824, 0.020521189928501382, 0.3488602287845235, 0.1692998169101364, 0.99253144277382976, 0.98695882022091552, 0.92993466994881258, 0.071533436149908661, 1.0023217936536264, 1.0013677272908319, 0.97901324628733111, 0.82771078126907194, 0.18393572917090487, 0.98695882052338746, 0.73699701489350056, 0.061235249930887442, 0.85729349903242413, 0.081646999907849918, 1.0020567585163638, 0.97901325230304936, 0.0047665209017208549, 0.83890767870287053, 0.071497813525812831, 0.0047665209017208549, 0.081030855329254539, 0.67537950419122839, 0.33018553538237833, 0.040206560475366901, 0.35180740415946038, 0.0080413120950733798, 0.34778674811192367, 0.2533013309948115, 0.73699701482934221, 0.98695882020966796, 0.97890360288776035, 0.73699701478266066, 0.082851131134328876, 0.24395055278441277, 0.22553919031011749, 0.3314045245373155, 0.11507101546434566, 0.73699701496230974, 0.002519957408007192, 0.07307876483220857, 0.0075598722240215761, 0.91726449651461794, 0.73699701491274117, 0.087099906262184723, 0.88841904387428416, 0.034839962504873888, 0.049919513134868136, 0.87359147986019237, 0.074879269702302201, 0.73699701493970493, 0.0051532501575670044, 0.061839001890804053, 0.020613000630268018, 0.57716401764750447, 0.34011451039942231, 0.98695378233225239, 0.010637558074722714, 0.12765069689667258, 0.80845441367892634, 0.053187790373613576, 0.0069353680197307187, 0.05548294415784575, 0.93704527911028379, 0.85058690562551453, 0.12151241508935921, 0.7369970148845556, 0.73699701480806978, 0.73699701496246428, 0.98186066477153411, 0.96408464262929505, 0.84702273895112146, 0.97180817786107809, 0.0084475866376488139, 0.016895173275297628, 0.50685519825892877, 0.10137103965178576, 0.37169381205654778, 0.77299028726101349, 0.73699701496443382, 0.89866398099082567, 0.98654016895948016, 0.013085790191500963, 0.094217689378806924, 0.005234316076600385, 0.03140589645960231, 0.85581067852416293, 0.30354267691932757, 0.18930618560560214, 0.50590446153221258, 0.16241872592348525, 0.83529630474935268, 0.026923593134843677, 0.026923593134843677, 0.033654491418554595, 0.61924264210140456, 0.29615952448328042, 0.18702894253332716, 0.38774292964226365, 0.013685044575609305, 0.2645775284617799, 0.15053549033170235, 0.0043434943044592424, 0.39742972885802069, 0.22151820952742138, 0.04777843734905167, 0.32576207283444319, 0.064115374964752017, 0.34077760940169566, 0.088707573581369242, 0.18005002558594746, 0.3267249244779144, 0.60101115280025152, 0.1639121325818868, 0.21854951010918239, 0.081130100647467937, 0.28105784867158534, 0.06374507908015338, 0.22021027318598438, 0.353495438535396, 0.9843390365994652, 0.77299028716997809, 0.066611153511613536, 0.28981694861193258, 0.0023372334565478431, 0.23255472892651041, 0.40901585489587255, 0.0058031838643333297, 0.011606367728666659, 0.011606367728666659, 0.97493488920799942, 0.99422069335818941, 0.7369970149120556, 1.0027859698045267, 0.96092507678177919, 0.73699701486422076, 1.0003240059612917, 0.34403594995157566, 0.33339566283967126, 0.0035467623706348008, 0.15605754430793123, 0.15960430667856604, 0.098774096841305781, 0.90307745683479568, 1.0008503914475602, 0.040949430666096097, 0.32597902043405441, 0.038255389174905563, 0.18912171268157538, 0.40626145687153231, 1.0027859699431489, 0.032787074093368632, 0.74754528932880482, 0.006557414818673727, 0.065574148186737263, 0.15082054082949573, 0.99749799326340149, 0.99752853424917209, 0.99423615891675432, 0.0085656843367171148, 0.4419893117746031, 0.0051394106020302682, 0.43684990117257283, 0.10621448577529222, 0.20739907786774051, 0.24014630068896267, 0.27835139398038855, 0.27289352351018487, 0.98571678163067611, 0.99469447927980648, 0.99149338340484827, 0.98041724529951568, 0.73699701490226621, 0.0074239083579019007, 0.49740185997942737, 0.081662991936920906, 0.15837671163524056, 0.25488752028796524, 0.022812615932736532, 0.3371197687837732, 0.017743145725461745, 0.27121665608920098, 0.35232817940559752, 0.085807309226034273, 0.83662126495383415, 0.064355481919525712, 0.98695882023623305, 0.97612953862791085, 0.0078701865243962967, 0.12854637989847287, 0.091818842784623475, 0.77127827939083715, 0.73699701490194769, 0.99037560114119416, 0.99037561045162015, 0.99562369275881091, 0.77299028737733222, 0.73699701490523495, 0.99060099678664826, 0.9589357711923826, 0.99090562919394509, 0.98695882023514447, 0.99037545165587881, 0.99749767572433978, 0.97082299977663689, 0.94764042285041206, 1.0022634404175657, 1.0008784211033861, 0.0099141275488839088, 0.98893422300116984, 1.0008795767068517, 0.99090562927438719, 0.73699701488253266, 0.98392388303226952, 0.61407278522515718, 0.34115154734730957, 0.94274565169208679, 0.97358445712099539, 0.99769240096383383], \"Term\": [\"!\", \"!\", \"%\", \"%\", \"%\", \"%\", \"%\", \"(\", \"(\", \"(\", \"(\", \"(\", \")\", \")\", \")\", \")\", \")\", \"+\", \"+\", \"+\", \",\", \",\", \",\", \",\", \",\", \".\", \".\", \".\", \".\", \".\", \"//dx.doi.org/10.1561/2000000039\", \"//www.cs.toronto\", \"0.00.51.01.52.02.53.0classif\", \"0.3\", \"0.628\", \"0.632\", \"0.891\", \"0.895\", \"0.92\", \"1.05\", \"1.25\", \"1.60\", \"1.62\", \"102103104105dataset\", \"1024-1024-2048-10\", \"12\", \"1|rj\", \"1\\u2212rj\", \"2.3\", \"2.3\", \"2.3\", \"2.4.2\", \"2.4.5\", \"2.4.6\", \"2.6\", \"2.7\", \"2.78\", \"23:469\\u2013477\", \"23rd\", \"25th\", \"28\\u00d728\", \"29th\", \"3.02\", \"3.1a\", \"3.1b\", \"3.2a\", \"3.2b\", \"3.95\", \"31.05\", \"36.7\", \"4.90\", \"402,738\", \"448\\u2013455\", \"767\\u2013774\", \"784-1024-1024-2048-10\", \"89.1\", \":\", \":\", \":\", \":\", \":\", \":1452\\u20131457\", \":195\\u2013198\", \":1\\u20131\", \":267\\u2013288\", \"=\", \"=\", \"=\", \"=\", \"[\", \"[\", \"[\", \"[\", \"[\", \"]\", \"]\", \"]\", \"]\", \"]\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a.\", \"a.\", \"a.\", \"a.\", \"a.\", \"ab\", \"acero\", \"acoust\", \"acoust\", \"acoust\", \"acoust.\", \"adversari\", \"akademii\", \"al\", \"al\", \"al\", \"al.\", \"approach\", \"approach\", \"approach\", \"approach\", \"arora\", \"asr\", \"averag\", \"averag\", \"averag\", \"averag\", \"averag\", \"baxter\", \"bibliographi\", \"bimod\", \"bio-chem\", \"biochemistri\", \"bird\", \"borman\", \"brainstorm\", \"bucharest\", \"burg\", \"can\", \"can\", \"can\", \"can\", \"can\", \"canon\", \"canon\", \"categori\", \"categori\", \"categori\", \"categori\", \"categori\", \"cca\", \"ccv\", \"cf\", \"cf\", \"cf\", \"cfp11srw-usb\", \"chapter\", \"chapter\", \"chapter\", \"christo\", \"cid:101\", \"cid:13\", \"cid:13\", \"cid:13\", \"cid:48\", \"cid:48\", \"cid:48\", \"cid:96\", \"citi\", \"class\", \"class\", \"class\", \"class\", \"class\", \"classi\", \"clue\", \"cnn\", \"cnn\", \"co-adapt\", \"co-adapt\", \"coarser\", \"confer\", \"confer\", \"confer\", \"confer\", \"context-depend\", \"conv\", \"conv\", \"convers\", \"convers\", \"corner\", \"correl\", \"correl\", \"correl\", \"crbm\", \"creativ\", \"credenc\", \"cuda-convnet\", \"curious\", \"d.\", \"d.\", \"d.\", \"d.\", \"d.\", \"data\", \"data\", \"data\", \"data\", \"data\", \"dataset\", \"dataset\", \"dataset\", \"dataset\", \"dataset\", \"day\", \"day\", \"dbn-pretrain\", \"dcca\", \"dccae\", \"dealt\", \"deep\", \"deep\", \"deep\", \"deep\", \"deform\", \"deng\", \"deng\", \"deng\", \"depth\", \"depth\", \"depth\", \"depth\", \"descript\", \"descript\", \"descript\", \"descript\", \"descript\", \"detect\", \"detect\", \"detect\", \"detect\", \"detector\", \"detector\", \"detector\", \"detector\", \"di\\u001ber\", \"diana\", \"di\\ufb00er\", \"di\\ufb00er\", \"di\\ufb00er\", \"di\\ufb00er\", \"dokladi\", \"domain\", \"domain\", \"domain\", \"domain\", \"drop\", \"drop\", \"drop\", \"dropout\", \"dropout\", \"dropout\", \"dropoutwithout\", \"e\", \"e\", \"e\", \"e\", \"edit\", \"edit\", \"edit\", \"edu/~nitish/dropout\", \"eko\", \"embed\", \"embed\", \"embed\", \"embed\", \"emot\", \"emot\", \"encod\", \"encod\", \"encod\", \"engag\", \"engag\", \"error\", \"error\", \"error\", \"error\", \"error\", \"error0.00.20.40.60.81.0prob\", \"et\", \"et\", \"et\", \"evid\", \"evid\", \"evid\", \"excerpt\", \"execut\", \"e\\ufb00ect\", \"e\\ufb00ect\", \"e\\ufb00ect\", \"e\\ufb00ect\", \"face\", \"face\", \"face\", \"featur\", \"featur\", \"featur\", \"featur\", \"featur\", \"feed\", \"feed\", \"feed\", \"feedback\", \"feedback\", \"feedback\", \"feedback\", \"feedback\", \"flickr-1m\", \"forward\", \"forward\", \"forward\", \"forward\", \"foundat\", \"fragment\", \"fusion\", \"fusion\", \"fusion\", \"g.\", \"g.\", \"g.\", \"g.\", \"g.\", \"gan\", \"ge\", \"gene\", \"generat\", \"generat\", \"generat\", \"generat\", \"genet\", \"geo\\ufb00rey\", \"gill\", \"gillian\", \"grasp\", \"graspabl\", \"gripper\", \"gus\", \"h\", \"h\", \"h\", \"h\", \"he\", \"he\", \"hidden\", \"hidden\", \"hidden\", \"hidden\", \"hj|rj\", \"hous\", \"http\", \"http\", \"http\", \"http\", \"human-level\", \"hyland\", \"icassp\", \"icassp\", \"ieee\", \"ieee\", \"ieee\", \"ieee\", \"imag\", \"imag\", \"imag\", \"imag\", \"image-sent\", \"in\", \"in\", \"in\", \"in\", \"in\", \"incid\", \"inform\", \"inform\", \"inform\", \"inform\", \"inform\", \"interspeech\", \"j.\", \"j.\", \"j.\", \"j.\", \"j.\", \"joell\", \"karel\", \"kcca\", \"keyword\", \"kingsburi\", \"kl-sparsiti\", \"kmean\", \"l+1\", \"l.\", \"l.\", \"l.\", \"l.\", \"l.\", \"l4\", \"label\", \"label\", \"label\", \"label\", \"lago\", \"lang\", \"languag\", \"languag\", \"languag\", \"languag\", \"lapkin\", \"layer\", \"layer\", \"layer\", \"layer\", \"layersdropout\", \"lbl\", \"learn\", \"learn\", \"learn\", \"learn\", \"learn\", \"learner\", \"learner\", \"learner\", \"livnat\", \"locat\", \"locat\", \"locat\", \"locat\", \"log-bilinear\", \"log-\\ufb01lter\", \"loop\", \"lres\", \"lstm\", \"lstm\", \"lstm\", \"m.\", \"m.\", \"m.\", \"m.\", \"m.\", \"map\", \"map\", \"map\", \"mcrbm-dbn-pretrain\", \"mere\", \"method\", \"method\", \"method\", \"method\", \"method\", \"minmin\", \"mix-\", \"mixabl\", \"ml\", \"mlbl-b\", \"modal\", \"modal\", \"modal\", \"modal\", \"mode\", \"model\", \"model\", \"model\", \"model\", \"monte-carlo\", \"mug\", \"multi-task\", \"multi-view\", \"multi-view\", \"n\", \"n\", \"n\", \"n\", \"n\", \"nagendra\", \"nauk\", \"nding\", \"neomi\", \"net\", \"net\", \"net\", \"net\", \"network\", \"network\", \"network\", \"network\", \"neural\", \"neural\", \"neural\", \"neural\", \"nightmar\", \"nikolayevich\", \"novelti\", \"n\\u00d7d\", \"n\\u22121\", \"o\", \"o\", \"o\", \"o\", \"object\", \"object\", \"object\", \"object\", \"occlus\", \"optic\", \"our\", \"our\", \"our\", \"our\", \"over\\ufb01t\", \"over\\ufb01t\", \"over\\ufb01t\", \"p\", \"p\", \"p\", \"p\", \"p.i\", \"pair\", \"pair\", \"pair\", \"pair\", \"pair\", \"paradigm\", \"pars\", \"pars\", \"pars\", \"particip\", \"particip\", \"pathway\", \"perform\", \"perform\", \"perform\", \"perform\", \"pineau\", \"pippeng\", \"pn\", \"pn\", \"pose\", \"pose\", \"pose\", \"pose\", \"pretrain\", \"pretrain\", \"pretrain\", \"pretrain\", \"prj\", \"proceed\", \"proceed\", \"proceed\", \"proceed\", \"proceed\", \"process\", \"process\", \"process\", \"process\", \"process\", \"process.\", \"python\", \"r\", \"r\", \"r\", \"r\", \"r\", \"rainfal\", \"rbms\", \"rbms\", \"rbms\", \"recall-weight\", \"receiv\", \"receiv\", \"receiv\", \"recognit\", \"recognit\", \"recognit\", \"recognit\", \"rectangl\", \"reformul\", \"represent\", \"represent\", \"represent\", \"represent\", \"result\", \"result\", \"result\", \"result\", \"result\", \"retent\", \"reuters-rcv1\", \"revis\", \"revis\", \"rewrit\", \"rgb-d\", \"risk\", \"rj\", \"rj\", \"rna\", \"rn\\u00d7d\", \"robot\", \"robot\", \"robot\", \"season\", \"seid\", \"sentenc\", \"sentenc\", \"sentenc\", \"sentenc\", \"sentenc\", \"session\", \"session\", \"set\", \"set\", \"set\", \"set\", \"set\", \"seventh\", \"sexual\", \"sherri\", \"shortcom\", \"show\", \"show\", \"show\", \"show\", \"show\", \"side-e\\ufb00ect\", \"signal\", \"signal\", \"signal\", \"signal\", \"silovski\", \"soft\", \"soft\", \"soft\", \"softmax\", \"softmax\", \"softmax\", \"soumith\", \"sourc\", \"sourc\", \"sourc\", \"sourc\", \"sourc\", \"sparser\", \"spatial\", \"spatial\", \"spatial\", \"spatial\", \"speech\", \"speech\", \"speech\", \"splice\", \"splice\", \"stabil\", \"steinkraus\", \"stemmer\", \"stock\", \"storch\", \"street\", \"student\", \"studi\", \"studi\", \"studi\", \"studi\", \"studi\", \"sub-model\", \"sub-sampl\", \"svhn\", \"swain\", \"system\", \"system\", \"system\", \"system\", \"system\", \"t\", \"t\", \"t\", \"talk\", \"talk\", \"target\", \"target\", \"target\", \"target\", \"target\", \"test\", \"test\", \"test\", \"test\", \"test\", \"text\", \"text\", \"text\", \"text\", \"text\", \"the\", \"the\", \"the\", \"the\", \"the\", \"thin\", \"thin\", \"thin\", \"this\", \"this\", \"this\", \"this\", \"this\", \"time-seri\", \"tissu\", \"train\", \"train\", \"train\", \"train\", \"train\", \"transact\", \"transact\", \"transact\", \"transact\", \"tray\", \"tuneabl\", \"two-stream\", \"ucf-101\", \"under\\ufb01t\", \"unimod\", \"unit\", \"unit\", \"unit\", \"unit\", \"unit\", \"unseen\", \"unseen\", \"uptak\", \"use\", \"use\", \"use\", \"use\", \"use\", \"video-level\", \"view\", \"view\", \"view\", \"view\", \"view\", \"vm\", \"vol\", \"w1\", \"we\", \"we\", \"we\", \"we\", \"we\", \"weight\", \"weight\", \"weight\", \"weight\", \"wf\", \"wigglesworth\", \"winter\", \"wn\", \"won\", \"word\", \"word\", \"word\", \"word\", \"word\", \"work\", \"work\", \"work\", \"work\", \"work\", \"write\", \"write\", \"write\", \"w||2\", \"xs\", \"y.\", \"y.\", \"y.\", \"y.\", \"yanmin\", \"ys\", \"yt\", \"yu\", \"||i\", \"||wi||2\", \"\\u00b1\", \"\\u02c6r\", \"\\u02c6\\u03c3\", \"\\u03b3\", \"\\u03b8c\", \"\\u03b8d\", \"\\u03b8repr\", \"\\u03c1dbn\", \"\\u2014\", \"\\u201cdeep\", \"\\u201d\", \"\\u201d\", \"\\u2202\", \"\\u22121/2\", \"\\uf8eb\\uf8edai\", \"\\ufb01\", \"\\ufb01netun\", \"\\ufb01netun\", \"\\ufb02\", \"\\ufb02ow\", \"\\ufb02ower\"]}, \"mdsDat\": {\"y\": [0.066483597467165242, 0.0064546810758621554, 0.028855391062811594, 0.0064546868540429729, -0.21867346328242621, 0.0064546803540406006, 0.036381337368779215, 0.061134402201356533, 0.0064546868983677286], \"cluster\": [1, 1, 1, 1, 1, 1, 1, 1, 1], \"Freq\": [3.8095882881688081, 0.0014097568072293264, 26.988851688825299, 0.0014097568072294737, 5.0544282324219081, 0.0014097568072295363, 18.348970998606131, 45.79252176474894, 0.0014097568072294691], \"topics\": [1, 2, 3, 4, 5, 6, 7, 8, 9], \"x\": [-0.12965873537302261, 0.206115471446707, -0.18035136244966471, 0.20611543913145794, -0.12292243571950576, 0.20611542572255928, -0.22456530880222822, -0.16696393370313317, 0.2061154397468298]}, \"R\": 30, \"lambda.step\": 0.01, \"tinfo\": {\"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\"], \"Term\": [\".\", \",\", \"(\", \")\", \"[\", \"]\", \"neural\", \"network\", \"the\", \"use\", \"speech\", \":\", \"model\", \"text\", \"process\", \"feedback\", \"unit\", \"imag\", \"train\", \"languag\", \"dropout\", \"hidden\", \"layer\", \"featur\", \"=\", \"e\", \"p\", \"et\", \"t\", \"pair\", \"l+1\", \"monte-carlo\", \"gene\", \"hous\", \"svhn\", \"cid:101\", \"street\", \"dbn-pretrain\", \"layersdropout\", \"sub-model\", \"tissu\", \"||i\", \"flickr-1m\", \"2.6\", \"bibliographi\", \"2.7\", \"dropout\", \"rna\", \"geo\\ufb00rey\", \"kl-sparsiti\", \"1.60\", \"w||2\", \"1.05\", \"sexual\", \"\\u03b3\", \"reuters-rcv1\", \"1.25\", \"sparser\", \"splice\", \"burg\", \"rj\", \"feed\", \"co-adapt\", \"pretrain\", \"net\", \"chapter\", \"conv\", \"thin\", \"unit\", \"\\ufb01netun\", \"forward\", \"pn\", \"drop\", \"over\\ufb01t\", \".\", \"rbms\", \"e\\ufb00ect\", \"2.3\", \"di\\ufb00er\", \"weight\", \"hidden\", \"neural\", \"p\", \"test\", \"network\", \"layer\", \"averag\", \",\", \"(\", \")\", \"the\", \"train\", \"use\", \"dataset\", \"model\", \"]\", \"[\", \"=\", \"learn\", \"result\", \"can\", \"this\", \"method\", \"in\", \"+\", \"featur\", \"sub-sampl\", \"p.i\", \"767\\u2013774\", \"stemmer\", \"coarser\", \"side-e\\ufb00ect\", \"joell\", \"pineau\", \"python\", \"nagendra\", \"biochemistri\", \"silovski\", \"1024-1024-2048-10\", \"akademii\", \"dokladi\", \"tuneabl\", \"incid\", \"23rd\", \":267\\u2013288\", \"12\", \"402,738\", \"//www.cs.toronto\", \"3.95\", \"prj\", \"recall-weight\", \"nightmar\", \"l4\", \"rn\\u00d7d\", \"0.92\", \"784-1024-1024-2048-10\", \"29th\", \":1452\\u20131457\", \"mcrbm-dbn-pretrain\", \"102103104105dataset\", \"3.1b\", \"cuda-convnet\", \"0.895\", \"31.05\", \"28\\u00d728\", \"0.632\", \"nikolayevich\", \"n\\u00d7d\", \"1.62\", \"3.2a\", \"genet\", \"karel\", \"2.78\", \"3.02\", \"448\\u2013455\", \".\", \",\", \"(\", \")\", \"[\", \"]\", \"model\", \"learn\", \"in\", \"use\", \"the\", \"imag\", \":\", \"network\", \"speech\", \"deep\", \"train\", \"can\", \"data\", \"featur\", \"label\", \"neural\", \"represent\", \"process\", \"a\", \"text\", \"%\", \"we\", \"work\", \"=\", \"al.\", \"cca\", \"fragment\", \"time-seri\", \"bird\", \"dcca\", \"kcca\", \"w1\", \"\\u02c6\\u03c3\", \"\\u22121/2\", \"wf\", \"classi\", \"stock\", \"wn\", \"bimod\", \"image-sent\", \"gan\", \"ab\", \"\\u02c6r\", \"n\\u22121\", \"log-bilinear\", \"adversari\", \"lbl\", \"arora\", \"\\u2202\", \"di\\u001ber\", \"dccae\", \"crbm\", \"\\ufb02ower\", \"mlbl-b\", \"canon\", \"descript\", \"multi-view\", \"embed\", \"et\", \"sentenc\", \"emot\", \"cid:48\", \"encod\", \"correl\", \"view\", \"imag\", \"(\", \")\", \",\", \"=\", \".\", \"data\", \"featur\", \"represent\", \"model\", \"generat\", \"use\", \"we\", \"+\", \"word\", \"the\", \":\", \"learn\", \"al\", \"text\", \"train\", \"in\", \"deep\", \"set\", \"can\", \"network\", \"neural\", \"result\", \"p.i\", \"12\", \"4.90\", \"python\", \"akademii\", \"dokladi\", \"soumith\", \":267\\u2013288\", \"prj\", \"error0.00.20.40.60.81.0prob\", \"sub-sampl\", \"nagendra\", \"minmin\", \"784-1024-1024-2048-10\", \"side-e\\ufb00ect\", \"cuda-convnet\", \"pippeng\", \"credenc\", \"1024-1024-2048-10\", \"3.2b\", \"under\\ufb01t\", \"biochemistri\", \"livnat\", \"||wi||2\", \"incid\", \"kmean\", \"2.4.5\", \"mcrbm-dbn-pretrain\", \"29th\", \"3.95\", \"human-level\", \".\", \",\", \")\", \"(\", \"[\", \"]\", \"learn\", \"model\", \"imag\", \"use\", \"featur\", \"train\", \"network\", \"the\", \":\", \"speech\", \"data\", \"in\", \"we\", \"deep\", \"neural\", \"class\", \"can\", \"recognit\", \"represent\", \"=\", \"a\", \"set\", \"layer\", \"languag\", \"reformul\", \"uptak\", \"lres\", \"retent\", \"rainfal\", \"\\ufb01\", \"rewrit\", \"winter\", \"lago\", \"sherri\", \"eko\", \"student\", \"storch\", \"citi\", \"hyland\", \"mere\", \"season\", \"wigglesworth\", \"neomi\", \"feedback\", \"excerpt\", \"swain\", \"diana\", \"gillian\", \"lapkin\", \"bucharest\", \"gus\", \"dealt\", \"\\ufb02\", \"nding\", \"engag\", \"day\", \"revis\", \"learner\", \"edit\", \"write\", \"talk\", \"receiv\", \"cf\", \"particip\", \"pair\", \"session\", \"evid\", \"e\", \"studi\", \"n\", \"o\", \"text\", \",\", \"(\", \")\", \".\", \"error\", \"r\", \"the\", \":\", \"t\", \"languag\", \"use\", \"process\", \"show\", \"coarser\", \"2.4.6\", \"1024-1024-2048-10\", \"hj|rj\", \"dokladi\", \"767\\u2013774\", \"3.95\", \"l4\", \"side-e\\ufb00ect\", \"python\", \"won\", \"minmin\", \"sub-sampl\", \"nauk\", \"akademii\", \"mixabl\", \"ge\", \"rn\\u00d7d\", \"christo\", \"soumith\", \"prj\", \"stemmer\", \"yanmin\", \"2.4.2\", \"livnat\", \"biochemistri\", \"//www.cs.toronto\", \"log-\\ufb01lter\", \"23:469\\u2013477\", \"0.92\", \"25th\", \":195\\u2013198\", \"0.628\", \"dropoutwithout\", \"36.7\", \"0.891\", \"mix-\", \":1\\u20131\", \"3.1a\", \"shortcom\", \",\", \".\", \"(\", \")\", \"[\", \"]\", \"use\", \":\", \"model\", \"in\", \"learn\", \"the\", \"deep\", \"featur\", \"imag\", \"data\", \"network\", \"feedback\", \"can\", \"train\", \"we\", \"=\", \"text\", \"%\", \"et\", \"speech\", \"process\", \"a\", \"j.\", \"represent\", \"\\u00b1\", \"cnn\", \"xs\", \"\\ufb02ow\", \"clue\", \"\\u03b8repr\", \"ccv\", \"deform\", \"ucf-101\", \"optic\", \"execut\", \"\\u03c1dbn\", \"novelti\", \"video-level\", \"two-stream\", \"mug\", \"unimod\", \"\\u03b8d\", \"cid:96\", \"vm\", \"tray\", \"yt\", \"ys\", \"occlus\", \"loop\", \"pathway\", \"\\u03b8c\", \"borman\", \"0.3\", \"corner\", \"89.1\", \"fusion\", \"cid:13\", \"detect\", \"unseen\", \"domain\", \"face\", \"!\", \"robot\", \"soft\", \"categori\", \"detector\", \"lstm\", \"spatial\", \"softmax\", \"pose\", \"class\", \"p\", \"label\", \"locat\", \"our\", \"depth\", \"pars\", \"map\", \"%\", \"target\", \"sourc\", \"we\", \"t\", \"h\", \"object\", \"imag\", \"modal\", \",\", \".\", \")\", \"(\", \"featur\", \"[\", \"]\", \"=\", \"use\", \"perform\", \"set\", \"model\", \"train\", \"in\", \"can\", \"approach\", \"the\", \"learn\", \":\", \"network\", \"data\", \"deep\", \"asr\", \"grasp\", \"yu\", \"vol\", \"ml\", \"//dx.doi.org/10.1561/2000000039\", \"paradigm\", \"interspeech\", \"process.\", \"acero\", \"rectangl\", \"gripper\", \"deng\", \"mode\", \"lang\", \"multi-task\", \"risk\", \"kingsburi\", \"seid\", \"\\u201d\", \"\\u2014\", \"acoust.\", \"rgb-d\", \"context-depend\", \"\\u201cdeep\", \"baxter\", \"foundat\", \"graspabl\", \"creativ\", \"brainstorm\", \"keyword\", \"transact\", \"speech\", \"proceed\", \"he\", \"convers\", \"icassp\", \"l.\", \"acoust\", \"signal\", \"http\", \"process\", \"ieee\", \"d.\", \"recognit\", \"]\", \"[\", \"system\", \"confer\", \".\", \",\", \"g.\", \"a.\", \"learn\", \"deep\", \"j.\", \"in\", \"y.\", \":\", \"neural\", \"languag\", \"model\", \"network\", \")\", \"(\", \"use\", \"m.\", \"inform\", \"the\", \"train\", \"joell\", \"23rd\", \"stabil\", \"767\\u2013774\", \"hj|rj\", \"minmin\", \"biochemistri\", \"recall-weight\", \"dokladi\", \"102103104105dataset\", \"edu/~nitish/dropout\", \"cfp11srw-usb\", \"pineau\", \"\\uf8eb\\uf8edai\", \"//www.cs.toronto\", \"bio-chem\", \"0.92\", \"nightmar\", \"akademii\", \"0.00.51.01.52.02.53.0classif\", \"l4\", \"gill\", \"tuneabl\", \"yanmin\", \"log-\\ufb01lter\", \"1.62\", \"curious\", \"coarser\", \"1\\u2212rj\", \"1|rj\", \"3.02\", \"steinkraus\", \"seventh\", \".\", \",\", \"(\", \"[\", \"]\", \")\", \"model\", \"use\", \"learn\", \"network\", \"in\", \"speech\", \"featur\", \"deep\", \"imag\", \"the\", \"can\", \"neural\", \":\", \"data\", \"train\", \"recognit\", \"set\", \"process\", \"=\", \"we\", \"work\", \"a\", \"represent\", \"method\"], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 3.2191999999999998, 3.2141000000000002, 3.2077, 3.1995, 3.1888000000000001, 3.1888000000000001, 3.1739999999999999, 3.1739999999999999, 3.1739999999999999, 3.1524999999999999, 3.1524999999999999, 3.1524999999999999, 3.1524999999999999, 3.1524999999999999, 3.1524999999999999, 3.1524999999999999, 3.1436000000000002, 3.1181999999999999, 3.1181999999999999, 3.1181999999999999, 3.1181999999999999, 3.1181999999999999, 3.1181999999999999, 3.1181999999999999, 3.1181999999999999, 3.1181999999999999, 3.1181999999999999, 3.1181999999999999, 3.0804, 3.0545, 3.0223, 2.9285000000000001, 2.9765000000000001, 2.8108, 2.6608000000000001, 2.7269999999999999, 2.9344000000000001, 2.7614000000000001, 2.1985999999999999, 2.8001999999999998, 2.6093999999999999, 2.9868999999999999, 2.6974999999999998, 2.6023999999999998, 0.49070000000000003, 2.3874, 2.3222999999999998, 2.8367, 2.2075999999999998, 1.7029000000000001, 1.4419, 1.0702, 1.4844999999999999, 1.5808, 0.75270000000000004, 1.1900999999999999, 1.7916000000000001, -0.86850000000000005, -0.24510000000000001, -0.25990000000000002, 0.51439999999999997, 0.5554, 0.067900000000000002, 1.2581, -0.1208, -0.4304, -0.4304, 0.64129999999999998, -0.56420000000000003, 0.73529999999999995, 0.36170000000000002, 0.76859999999999995, 0.51870000000000005, -0.49930000000000002, 0.997, -0.4546, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, -7.0049999999999999, -7.1208, -5.7976999999999999, -5.8151000000000002, -5.2824, -5.2824, -4.8517000000000001, -4.9932999999999996, -4.5759999999999996, -4.8247999999999998, -4.3361999999999998, -4.1704999999999997, -4.7301000000000002, -4.3009000000000004, -4.4669999999999996, -4.4349999999999996, -4.0506000000000002, -3.7038000000000002, -4.1018999999999997, -4.3411, -3.113, -4.0366999999999997, -3.6360999999999999, -3.9729000000000001, -3.4832000000000001, -3.4308999999999998, -3.1604000000000001, -3.6680999999999999, -3.2763, -3.5367000000000002, 1.3080000000000001, 1.3052999999999999, 1.3047, 1.3026, 1.3019000000000001, 1.3013999999999999, 1.3005, 1.3005, 1.2996000000000001, 1.2996000000000001, 1.2982, 1.2975000000000001, 1.2971999999999999, 1.2968, 1.2968, 1.2964, 1.2931999999999999, 1.2909999999999999, 1.2909999999999999, 1.2909999999999999, 1.2901, 1.2890999999999999, 1.2890999999999999, 1.288, 1.288, 1.288, 1.2868999999999999, 1.2868999999999999, 1.2855000000000001, 1.2855000000000001, 1.2817000000000001, 1.2050000000000001, 1.2536, 1.1598999999999999, 1.0832999999999999, 1.1317999999999999, 1.1718, 1.1812, 1.1442000000000001, 1.0129999999999999, 1.0173000000000001, 0.71679999999999999, 0.43659999999999999, 0.41620000000000001, 0.054800000000000001, 0.65820000000000001, -0.21299999999999999, 0.4824, 0.38129999999999997, 0.5524, 0.2011, 0.64470000000000005, 0.1888, 0.49440000000000001, 0.76649999999999996, 0.60940000000000005, 0.23219999999999999, 0.035900000000000001, -0.079100000000000004, 0.75280000000000002, 0.38919999999999999, 0.071199999999999999, -0.25530000000000003, -0.25030000000000002, 0.26650000000000001, 0.12809999999999999, -0.26279999999999998, -0.25840000000000002, 0.34660000000000002, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, -7.0049999999999999, -7.1207000000000003, -5.8151000000000002, -5.7976999999999999, -5.2824, -5.2824, -4.9932999999999996, -4.8517000000000001, -4.1704999999999997, -4.8247999999999998, -4.3411, -4.0506000000000002, -4.3009000000000004, -4.3361999999999998, -4.7301000000000002, -4.4669999999999996, -4.1018999999999997, -4.5759999999999996, -3.6680999999999999, -4.4349999999999996, -4.0366999999999997, -2.8567, -3.7038000000000002, -3.919, -3.6360999999999999, -3.5367000000000002, -3.4832000000000001, -3.5081000000000002, -3.3109999999999999, -3.6747000000000001, 2.9786999999999999, 2.9784999999999999, 2.9771000000000001, 2.9763999999999999, 2.9762, 2.9759000000000002, 2.968, 2.9666999999999999, 2.9651999999999998, 2.9651999999999998, 2.9643999999999999, 2.9643999999999999, 2.9634, 2.9624000000000001, 2.9624000000000001, 2.9624000000000001, 2.9613, 2.9601000000000002, 2.9601000000000002, 2.9592999999999998, 2.9588000000000001, 2.9588000000000001, 2.9588000000000001, 2.9573, 2.9556, 2.9556, 2.9537, 2.9537, 2.9514999999999998, 2.9489999999999998, 2.9409999999999998, 2.9323000000000001, 2.9039000000000001, 2.8616999999999999, 2.8706999999999998, 2.8142, 2.7984, 2.7469999999999999, 2.8003, 2.6898, 2.3311999999999999, 2.5838999999999999, 2.7216999999999998, 2.1695000000000002, 2.3007, 1.9439, 2.0846, 1.4763999999999999, -0.37430000000000002, 0.26500000000000001, 0.24790000000000001, -0.46379999999999999, 1.7385999999999999, 1.5495000000000001, 0.56379999999999997, 0.29949999999999999, 1.3213999999999999, 0.75309999999999999, -0.28199999999999997, 0.32369999999999999, 1.496, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, -7.1207000000000003, -7.0049999999999999, -5.7976999999999999, -5.8151000000000002, -5.2824, -5.2824, -4.8247999999999998, -4.7301000000000002, -4.8517000000000001, -4.5759999999999996, -4.9932999999999996, -4.3361999999999998, -4.4349999999999996, -4.3411, -4.1704999999999997, -4.1018999999999997, -4.3009000000000004, -2.6303000000000001, -3.7038000000000002, -4.0506000000000002, -3.6680999999999999, -3.5367000000000002, -3.4308999999999998, -3.1604000000000001, -3.4140000000000001, -4.4669999999999996, -3.9729000000000001, -3.4832000000000001, -3.5899000000000001, -3.6360999999999999, 1.6899999999999999, 1.6858, 1.6788000000000001, 1.6781999999999999, 1.6781999999999999, 1.6774, 1.6774, 1.6757, 1.6747000000000001, 1.6736, 1.6733, 1.6711, 1.6696, 1.6678999999999999, 1.6678999999999999, 1.6678999999999999, 1.6659999999999999, 1.6637999999999999, 1.6637999999999999, 1.6637, 1.6612, 1.6580999999999999, 1.6580999999999999, 1.6580999999999999, 1.6580999999999999, 1.6580999999999999, 1.6580999999999999, 1.6545000000000001, 1.6545000000000001, 1.6545000000000001, 1.6537999999999999, 1.6228, 1.6212, 1.5541, 1.5869, 1.5377000000000001, 1.5549999999999999, 1.6315, 1.5389999999999999, 1.5692999999999999, 1.4913000000000001, 1.5382, 1.4877, 1.4811000000000001, 1.5590999999999999, 1.4047000000000001, 1.2789999999999999, 1.2450000000000001, 1.1949000000000001, 1.4638, 1.3109, 1.4338, 1.4505999999999999, 1.2007000000000001, 1.0091000000000001, 1.2138, 1.1436999999999999, 0.86829999999999996, 1.0143, 1.1209, 0.90920000000000001, 0.63859999999999995, 1.0818000000000001, -0.2752, -0.29420000000000002, 0.055300000000000002, 0.058700000000000002, 0.51849999999999996, 0.075600000000000001, 0.075600000000000001, 0.64990000000000003, 0.030099999999999998, 0.82669999999999999, 0.63690000000000002, -0.085999999999999993, 0.2349, -0.087999999999999995, 0.43580000000000002, 0.66449999999999998, -0.017999999999999999, -0.56989999999999996, -0.45300000000000001, -0.17469999999999999, -0.0101, -0.439, 0.77949999999999997, 0.7792, 0.77910000000000001, 0.77880000000000005, 0.77810000000000001, 0.77739999999999998, 0.77690000000000003, 0.77680000000000005, 0.77649999999999997, 0.77610000000000001, 0.77600000000000002, 0.77549999999999997, 0.77400000000000002, 0.77380000000000004, 0.77270000000000005, 0.77149999999999996, 0.77080000000000004, 0.77080000000000004, 0.77080000000000004, 0.76970000000000005, 0.76929999999999998, 0.76859999999999995, 0.76859999999999995, 0.76839999999999997, 0.76829999999999998, 0.76829999999999998, 0.76780000000000004, 0.76780000000000004, 0.76690000000000003, 0.76649999999999996, 0.76619999999999999, 0.75349999999999995, 0.71599999999999997, 0.72299999999999998, 0.75800000000000001, 0.75660000000000005, 0.72919999999999996, 0.69950000000000001, 0.70430000000000004, 0.69340000000000002, 0.72130000000000005, 0.64170000000000005, 0.66700000000000004, 0.62849999999999995, 0.59199999999999997, 0.46429999999999999, 0.46429999999999999, 0.62480000000000002, 0.62480000000000002, 0.18529999999999999, 0.13639999999999999, 0.59960000000000002, 0.51600000000000001, 0.30099999999999999, 0.36330000000000001, 0.48549999999999999, 0.2326, 0.5212, 0.13270000000000001, 0.30930000000000002, 0.40810000000000002, 0.021399999999999999, 0.19670000000000001, -0.41389999999999999, -0.44969999999999999, -0.1203, 0.46339999999999998, 0.37540000000000001, -0.33729999999999999, -0.11169999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, 2.3961999999999999, -7.0049999999999999, -7.1207000000000003, -5.7976999999999999, -5.2824, -5.2824, -5.8151000000000002, -4.8517000000000001, -4.8247999999999998, -4.9932999999999996, -4.3009000000000004, -4.5759999999999996, -4.4669999999999996, -4.3411, -4.4349999999999996, -4.1704999999999997, -4.3361999999999998, -3.7038000000000002, -4.0366999999999997, -4.7301000000000002, -4.1018999999999997, -4.0506000000000002, -3.919, -3.5081000000000002, -3.9729000000000001, -3.5367000000000002, -3.6680999999999999, -3.2763, -3.4832000000000001, -3.6360999999999999, -3.3854000000000002], \"Freq\": [16421.0, 18436.0, 4909.0, 4996.0, 2932.0, 2932.0, 843.0, 1099.0, 1138.0, 1855.0, 1297.0, 1688.0, 1906.0, 460.0, 791.0, 206.0, 281.0, 964.0, 855.0, 587.0, 169.0, 353.0, 408.0, 1144.0, 511.0, 225.0, 286.0, 452.0, 306.0, 187.0, 8.5074024062735756, 7.6660109604266591, 6.8246195147838549, 5.9832280659720514, 5.1418366196096876, 5.1418366195419809, 4.3004451734523794, 4.3004451732088036, 4.3004451723937134, 3.4590537275170261, 3.4590537272791475, 3.4590537260250862, 3.4590537262277805, 3.4590537256820304, 3.459053725076854, 3.4590537250107634, 149.30984954810441, 2.6176622797582105, 2.6176622798138141, 2.6176622799334659, 2.6176622797244473, 2.6176622795178393, 2.6176622790765762, 2.6176622790965389, 2.617662278950462, 2.6176622789862165, 2.6176622782989827, 2.6175700966547453, 6.8246198089374985, 1.7762708331199826, 8.5074032982242418, 19.024217653685952, 8.183908704933252, 22.191388946506656, 58.569384426815105, 21.128277788960201, 5.9832286686849203, 11.031579282758985, 96.802056732372449, 9.1836617050114135, 18.430861963899044, 4.3004455917911661, 11.87297286306832, 12.714361830300534, 1021.841363940193, 17.991082244525572, 19.445497109998005, 6.6177463266411296, 20.286888816583787, 38.319502197518943, 56.908091701018897, 93.741802567865662, 48.182271416683058, 40.576766305127862, 88.883465904262223, 51.151975485211587, 22.262767374362706, 294.69085746047233, 146.39497272499617, 146.77255894206101, 72.546776705630947, 56.808105794807219, 75.675238339982798, 31.457040222406842, 64.366014363733299, 72.653422767622601, 72.653420926763047, 37.027330742521087, 47.595716130267832, 30.980863932210806, 33.088752636260423, 28.356281828898101, 28.15531422563765, 33.459082107797116, 25.559659442673219, 27.666732976638293, 0.00021004586578860019, 0.00021004586579180239, 0.00021004586577985301, 0.00021004586578528205, 0.00021004586578729883, 0.00021004586578445494, 0.00021004586578776341, 0.00021004586578777851, 0.00021004586577929975, 0.00021004586578593845, 0.00021004586577901089, 0.00021004586579520033, 0.00021004586578297595, 0.00021004586577447312, 0.00021004586576792188, 0.00021004586579044364, 0.00021004586578089661, 0.00021004586578250725, 0.00021004586577788115, 0.00021004586578125142, 0.00021004586578187128, 0.00021004586578365385, 0.00021004586577945682, 0.00021004586577249895, 0.00021004586577877553, 0.00021004586577940077, 0.00021004586578914918, 0.00021004586579027531, 0.00021004586578273507, 0.00021004586577594907, 0.00021004586578658423, 0.00021004586578865007, 0.00021004586578532257, 0.00021004586578446635, 0.0002100458657898698, 0.00021004586578791349, 0.00021004586579315119, 0.00021004586578834991, 0.00021004586579254246, 0.00021004586579015792, 0.00021004586578626406, 0.00021004586578869043, 0.00021004586578613041, 0.00021004586578803828, 0.00021004586579281216, 0.00021004586578907987, 0.00021004586578663882, 0.00021004586578602679, 0.00021004586578468509, 0.00021004633234709516, 0.00021004630773865766, 0.00021004604029291183, 0.00021004601882897172, 0.00021004597309125461, 0.00021004597144111734, 0.00021004594794583355, 0.00021004594626307672, 0.0002100459287099572, 0.00021004591610871974, 0.00021004590901121776, 0.00021004590666588396, 0.00021004590664448037, 0.0002100459046695789, 0.00021004590246988266, 0.0002100459024586361, 0.00021004589771804613, 0.00021004589665662109, 0.00021004589488561858, 0.00021004589440408377, 0.00021004589434946394, 0.00021004589368967466, 0.00021004589258558044, 0.00021004588960554002, 0.00021004588809399902, 0.00021004588798185549, 0.00021004588695656276, 0.00021004588665998655, 0.00021004588661938299, 0.00021004588635779875, 227.55339913200754, 92.447180856240493, 79.811244528220001, 56.483493653869431, 51.623488802451973, 48.707568881782997, 43.847604819123084, 43.847589616220283, 39.959646323277276, 39.959646307314898, 35.09968340768215, 33.155700091475509, 32.183708691235985, 31.211715586254652, 31.211483552114068, 30.239699149175351, 24.407782702887911, 21.491809778394341, 21.491804662274788, 21.491802770888846, 20.519811517489121, 19.547825757445331, 19.547822853368338, 18.575835581629054, 18.575835231470226, 18.575829084066893, 17.603844184860755, 17.603837675366599, 16.631851335355389, 16.631849542879973, 49.658821766940292, 134.99228298245325, 39.065606822242579, 152.81788324522589, 361.01578124559109, 175.5897215060811, 69.979060067799125, 63.416677800114428, 77.835842119830176, 125.96920590232776, 113.82545599446073, 533.18841394516755, 2050.6266525862729, 2044.4838909464859, 5255.8329088840319, 266.78568576564152, 3581.8728534478978, 393.8417852922949, 452.15761022627601, 265.08242822385074, 629.1316957905575, 198.9765857934953, 605.01115534391977, 258.28390555588999, 143.80964310310137, 200.5986636066113, 387.5847431568298, 472.32458040337212, 547.73686878450633, 127.39022803158103, 183.40037741569549, 248.00050425174061, 302.55985686187586, 264.08693697729069, 175.25696909407105, 185.58755420972022, 228.08359372042801, 175.88765320967624, 148.80411477909627, 0.00021004586197843295, 0.00021004586197088978, 0.00021004586196067914, 0.00021004586191678869, 0.00021004586188997965, 0.00021004586187266651, 0.00021004586187076636, 0.00021004586186543591, 0.00021004586185759014, 0.00021004586186457207, 0.00021004586184204455, 0.00021004586184675814, 0.000210045861823059, 0.00021004586182525557, 0.00021004586181773882, 0.00021004586184096224, 0.00021004586181047347, 0.00021004586181633839, 0.00021004586180096998, 0.00021004586180697386, 0.00021004586181637511, 0.00021004586177125379, 0.00021004586178059395, 0.0002100458617812767, 0.00021004586175766646, 0.00021004586177246788, 0.00021004586175770186, 0.00021004586176445866, 0.00021004586176318087, 0.00021004586174086675, 0.00021004586176088039, 0.00021005324647320209, 0.00021005031917525663, 0.0002100475488786898, 0.00021004747917435497, 0.00021004686470432122, 0.00021004671911489212, 0.00021004669529374559, 0.00021004664135965706, 0.000210046603867375, 0.00021004655020326107, 0.00021004641849711717, 0.00021004634705359317, 0.00021004633726577892, 0.00021004632699590168, 0.00021004630460817641, 0.00021004628727279073, 0.0002100462839897042, 0.00021004626956678274, 0.00021004621310253773, 0.00021004618765338472, 0.00021004617657566609, 0.0002100461708262598, 0.00021004616965992969, 0.00021004616731992755, 0.000210046127141557, 0.00021004610395329815, 0.00021004609760963473, 0.00021004608288010024, 0.00021004606137884201, 0.00021004606090702758, 67.270145403745872, 65.525386589614399, 53.312074873826461, 48.950177836051687, 48.077798434388193, 46.33296407752821, 24.523554447590314, 22.778795633829183, 21.034036814023008, 21.034036818162996, 20.161657411436689, 20.161652606564925, 19.28927800238128, 18.416898595724199, 18.416898593692704, 18.4168985899923, 17.544519186179485, 16.672139784409499, 16.672139776804809, 201.54757315767853, 15.799760378547868, 15.799760374372797, 15.799760375654909, 14.927380970468596, 14.055001562162364, 14.055001558217983, 13.182622151309246, 13.182622150109184, 12.31024274555099, 11.437863338470253, 59.177898241386075, 33.732761991907878, 51.567323416091021, 91.052179773742324, 43.715905598834297, 39.299703822430885, 35.763677414415639, 41.218927653885189, 30.63021505338719, 45.829578006621425, 97.324297375146443, 44.619168196395911, 30.630215189174361, 99.653520968366109, 59.720722604503443, 79.901356591852618, 60.646690950220062, 101.87640934442388, 640.88795999895274, 323.46498287999441, 323.57790896895256, 521.98110630819303, 66.493470484914624, 61.18033653637783, 101.13199559828202, 115.12540042453114, 58.049272811449534, 63.068047354476263, 70.753583172369218, 55.315164402192316, 49.018634633612834, 0.0002100458588586708, 0.00021004585885930921, 0.00021004585883593167, 0.00021004585885346904, 0.00021004585881927116, 0.00021004585881780204, 0.00021004585882642153, 0.00021004585883722184, 0.00021004585881532074, 0.0002100458588094822, 0.00021004585882738804, 0.00021004585880975331, 0.00021004585880928821, 0.00021004585882375285, 0.0002100458588053908, 0.00021004585883193971, 0.00021004585882713401, 0.00021004585882472644, 0.00021004585882643479, 0.00021004585881008895, 0.00021004585880376902, 0.00021004585880291925, 0.0002100458588199589, 0.00021004585885501086, 0.00021004585881039019, 0.00021004585879141202, 0.00021004585880218243, 0.0002100458588042387, 0.00021004585880893083, 0.00021004585880086634, 0.00021004585880282138, 0.00021004585880898439, 0.00021004585881860403, 0.00021004585881016273, 0.00021004585880647617, 0.00021004585880607778, 0.00021004585880797099, 0.00021004585880852106, 0.00021004585880668181, 0.00021004585880716298, 0.00021005591343796023, 0.00021005200012334457, 0.00021004908244946514, 0.00021004859454311096, 0.00021004720360663946, 0.00021004707631468313, 0.00021004698487372899, 0.00021004684280200457, 0.00021004682393879837, 0.0002100466555331374, 0.00021004654753190572, 0.00021004648626224824, 0.00021004643746986909, 0.00021004642765971249, 0.00021004638970191229, 0.00021004638603544583, 0.00021004636666519371, 0.00021004626818316838, 0.00021004624160912965, 0.00021004623051833467, 0.0002100462226428218, 0.00021004621903068286, 0.00021004621872480517, 0.00021004620239616501, 0.00021004619074136072, 0.00021004618406670418, 0.00021004616901088179, 0.00021004616758108662, 0.00021004615561602227, 0.00021004615266213329, 73.2828913654958, 127.33162269551275, 24.178373318212209, 23.215509936838068, 23.215509785756762, 22.252703348428668, 22.252702657601862, 20.32678573902718, 19.364130575678047, 18.401273204995238, 18.394940015899795, 16.475559218791524, 15.512687355735585, 14.549843796713224, 14.549843791818965, 14.549784786039325, 13.586892779913711, 12.624129776620624, 12.624123507624299, 12.624030029068336, 11.661255630832335, 10.698414941184563, 10.698408358325597, 10.698398840102799, 10.698398357482734, 10.698365177149693, 10.698240647959095, 9.7355577198254704, 9.7355577111600518, 9.7355358494209447, 10.652812847387445, 99.042122196921298, 28.987174052157496, 207.76164135862101, 63.57219150363666, 171.64754813109005, 94.307791692587386, 19.364131313098113, 83.778230318104178, 50.595548408005108, 119.58165093705477, 48.135905575290906, 78.78169986774698, 75.856285645732896, 34.952373352294202, 81.069951748621293, 170.95821969849692, 182.65675629996937, 203.08681809986339, 49.078571146486169, 108.74876099350182, 46.148690187030148, 42.752243101419204, 105.96611275814227, 176.83572789253074, 91.766639526989209, 111.74858859898636, 255.22840362300639, 155.01274802793864, 103.55650980106583, 177.25058528514705, 335.22342863148231, 111.55257679690635, 2569.1600477751563, 2245.165558702573, 968.89869404019623, 955.41304637781502, 352.59273943043519, 580.42351042125813, 580.42322274589969, 179.88094537820021, 350.94691826966817, 145.81420483840083, 172.55993685612324, 320.99003887829866, 198.58967628240885, 243.14430799344422, 171.62913373073593, 146.865889014544, 205.18544322741502, 227.93634067049402, 196.93927053703027, 169.34789935862722, 163.61686608352844, 148.67540774327838, 269.45507883195233, 214.40610119521261, 208.49960396672026, 178.03457489962597, 137.73119339940808, 110.20679979618289, 98.410625405794946, 96.444544201611549, 88.580489858066343, 82.681866049464361, 80.716189312669059, 72.852076240760653, 377.26252821931996, 56.140895965942462, 48.276889512933536, 42.378764317135285, 39.429784839118732, 39.429781816137968, 39.429781193339004, 398.92230360022887, 34.514712815314518, 32.548655583712055, 32.548498214535925, 41.273048578088257, 31.56562786252422, 31.565502103476422, 30.582542336965311, 30.58246994365599, 28.616528217922742, 27.633514151246885, 49.897428499893465, 167.64311663510676, 1215.9301166595883, 526.64137388916572, 59.088075546318294, 61.880904462941579, 182.40020983575593, 468.6082428451171, 293.2530105721587, 363.5396482576391, 159.52584001357147, 688.75912113288553, 388.63565428613265, 392.13491520569806, 621.0030489705083, 2136.5427187975806, 2136.5425760797298, 326.81228802788485, 271.8757978045079, 9050.4993518121537, 9675.9252861294663, 300.12228222667801, 422.87929700981863, 1359.0319821687704, 827.64051884442426, 401.69121733461247, 836.18772195463828, 293.96282853911777, 882.85842177930363, 526.49027694676249, 404.68666009538043, 891.90049195293125, 612.68242905293721, 1512.3962439908198, 1434.0149312061287, 753.55992412446722, 327.8378988976558, 319.72116901911181, 372.12265804742987, 350.45055216328484, 0.00021004586148150705, 0.00021004586147049345, 0.00021004586148391151, 0.0002100458614395549, 0.00021004586146233293, 0.0002100458614316656, 0.00021004586142211692, 0.00021004586142864972, 0.00021004586141028369, 0.00021004586143319657, 0.00021004586144577424, 0.00021004586143243218, 0.00021004586141431093, 0.00021004586143104768, 0.00021004586141249652, 0.00021004586143032585, 0.00021004586141092017, 0.00021004586140640276, 0.00021004586139311329, 0.00021004586142365172, 0.00021004586141094674, 0.00021004586141405314, 0.00021004586140474599, 0.0002100458614046553, 0.000210045861400945, 0.0002100458614063739, 0.00021004586140253894, 0.00021004586137975543, 0.00021004586139569432, 0.0002100458613885105, 0.00021004586140971861, 0.00021004586140217337, 0.00021004586139467934, 0.00021005214174642023, 0.00021005151081952226, 0.00021004736074272268, 0.00021004713299402006, 0.00021004706899511218, 0.00021004699518449823, 0.00021004673251820145, 0.00021004668371350508, 0.00021004650780413024, 0.0002100464721163439, 0.00021004646437940925, 0.0002100464512961052, 0.00021004633526242749, 0.00021004631584310802, 0.00021004631309355141, 0.00021004623959557859, 0.00021004623538875344, 0.00021004616546458648, 0.0002100461602009716, 0.00021004615960860639, 0.00021004609531345391, 0.00021004609006042307, 0.00021004607964188217, 0.00021004607921278347, 0.00021004607594389947, 0.00021004607474761972, 0.00021004607165187123, 0.00021004606386290206, 0.00021004606321586578, 0.00021004606114653448], \"Total\": [16421.0, 18436.0, 4909.0, 4996.0, 2932.0, 2932.0, 843.0, 1099.0, 1138.0, 1855.0, 1297.0, 1688.0, 1906.0, 460.0, 791.0, 206.0, 281.0, 964.0, 855.0, 587.0, 169.0, 353.0, 408.0, 1144.0, 511.0, 225.0, 286.0, 452.0, 306.0, 187.0, 8.9293806465718948, 8.0879891950496532, 7.2465977463047793, 6.4052062965746801, 5.5638148471102955, 5.5638148479329423, 4.7224233967475877, 4.7224233973753664, 4.7224233973149392, 3.8810319475424384, 3.8810319479995092, 3.8810319469584247, 3.8810319478200781, 3.8810319480996367, 3.8810319480017701, 3.88103194797463, 169.0317718920119, 3.0396404972692683, 3.0396404979649385, 3.0396404982293617, 3.0396404980969667, 3.039640498153648, 3.0396404979839136, 3.039640498235463, 3.0396404981570004, 3.0396404982008227, 3.0396404983972221, 3.039656013993639, 8.2296117583097033, 2.1982490482466912, 10.873363261260074, 26.705173292694948, 10.94985910266684, 35.042383648867563, 107.44926666512325, 36.278244802499486, 8.3491890403964142, 18.302488978363257, 281.94727909584191, 14.656243065225633, 35.596492365648039, 5.694414730074211, 20.998574326009265, 24.728338338193627, 16421.361074414734, 43.384771595422286, 50.045774290172055, 10.182909423487077, 58.558518964330645, 183.22164394690705, 353.26820293734721, 843.86729859849925, 286.62427273975703, 219.21740798322753, 1099.0951601145985, 408.42550360054179, 97.418104256825174, 18436.49790045213, 4909.9154259651705, 4996.1301370776737, 1138.5724569205495, 855.71254955166262, 1855.9476594365419, 234.67004413100369, 1906.4860134020216, 2932.7886292777935, 2932.7886250277325, 511.84750138556001, 2196.4543930236773, 389.84094138171758, 604.96305156005326, 345.12467970017093, 439.97374716730178, 1447.0633081916303, 247.57026539710839, 1144.1968737996924, 1.3568575987356724, 1.3568575987755462, 1.3568575987020197, 1.3568575987392983, 1.3568575987570175, 1.3568575987395828, 1.3568575987637861, 1.3568575987703664, 1.3568575987185114, 1.3568575987673366, 1.3568575987261415, 1.3568575988308418, 1.3568575987534652, 1.3568575987143752, 1.3568575986741225, 1.3568575988321039, 1.3568575987715707, 1.3568575987861218, 1.3568575987570752, 1.3568575987853089, 1.3568575987900837, 1.3568575988020499, 1.356857598777381, 1.3568575987429803, 1.3568575987891449, 1.3568575987941787, 1.3568575988572857, 1.3568575988662648, 1.3568575988209723, 1.3568575987793439, 1.356857598854978, 1.356857598876235, 1.3568575988514582, 1.3568575988461189, 1.3568575989023608, 1.3568575988929779, 1.3568575989712994, 1.3568575989058536, 1.356857598977621, 1.3568575989804936, 1.3568575989016218, 1.3568575989776144, 1.3568575989153939, 1.3568575989741047, 1.3568575991223999, 1.3568575990375304, 1.3568575990272351, 1.3568575989784353, 1.3568575989216931, 16421.361074414734, 18436.49790045213, 4909.9154259651705, 4996.1301370776737, 2932.7886250277325, 2932.7886292777935, 1906.4860134020216, 2196.4543930236773, 1447.0633081916303, 1855.9476594365419, 1138.5724569205495, 964.68720636328169, 1688.286236352606, 1099.0951601145985, 1297.6960954913313, 1256.8197210835951, 855.71254955166262, 604.96305156005326, 900.77962808934046, 1144.1968737996924, 335.07000709768852, 843.86729859849925, 565.3233452618523, 791.75293851270851, 485.1948512349789, 460.45875965503228, 351.32155330612591, 583.72452257752718, 394.51854300868808, 511.84750138556001, 227.96145969254857, 92.854649017645272, 80.218760796682631, 56.890967357608524, 52.03100907361042, 49.115036079109878, 44.255079158922641, 44.255079243888218, 40.367113498525697, 40.367113495248681, 35.50715646952802, 33.563173783658698, 32.591182382732455, 31.619190858408, 31.619193062331412, 30.64719941953101, 24.815251102116495, 21.899276919895669, 21.899276918085643, 21.899276886647225, 20.927285514244403, 19.955294111253163, 19.955294144038124, 18.983302721337722, 18.98330272910037, 18.983302800281979, 18.011311324036843, 18.011311401021672, 17.039319918220212, 17.039319960153186, 51.070276250776686, 149.89595890373644, 41.321569772369756, 177.52155070587983, 452.74318363671193, 209.7966253833001, 80.330259369652069, 72.118906410484755, 91.845507398168252, 169.49037412770178, 152.49912162827843, 964.68720636328169, 4909.9154259651705, 4996.1301370776737, 18436.49790045213, 511.84750138556001, 16421.361074414734, 900.77962808934046, 1144.1968737996924, 565.3233452618523, 1906.4860134020216, 386.92561809552359, 1855.9476594365419, 583.72452257752718, 247.57026539710839, 404.09981580751105, 1138.5724569205495, 1688.286236352606, 2196.4543930236773, 222.33687833191209, 460.45875965503228, 855.71254955166262, 1447.0633081916303, 1256.8197210835951, 497.43125906662107, 604.96305156005326, 1099.0951601145985, 843.86729859849925, 389.84094138171758, 1.3568575987755462, 1.3568575987853089, 1.3568575987854461, 1.3568575987185114, 1.3568575987143752, 1.3568575986741225, 1.3568575987811997, 1.3568575987570752, 1.3568575987429803, 1.3568575988136515, 1.3568575987356724, 1.3568575987673366, 1.356857598736253, 1.3568575987793439, 1.3568575987395828, 1.3568575988929779, 1.3568575987728222, 1.3568575988148299, 1.3568575987534652, 1.356857598807544, 1.3568575989201708, 1.3568575987261415, 1.3568575988365768, 1.3568575988446612, 1.3568575987715707, 1.3568575988916571, 1.3568575987988938, 1.3568575988514582, 1.356857598854978, 1.356857598777381, 1.3568575989165466, 16421.361074414734, 18436.49790045213, 4996.1301370776737, 4909.9154259651705, 2932.7886250277325, 2932.7886292777935, 2196.4543930236773, 1906.4860134020216, 964.68720636328169, 1855.9476594365419, 1144.1968737996924, 855.71254955166262, 1099.0951601145985, 1138.5724569205495, 1688.286236352606, 1297.6960954913313, 900.77962808934046, 1447.0633081916303, 583.72452257752718, 1256.8197210835951, 843.86729859849925, 259.30290694686113, 604.96305156005326, 750.21267736565278, 565.3233452618523, 511.84750138556001, 485.1948512349789, 497.43125906662107, 408.42550360054179, 587.59655496884898, 67.68868066608556, 65.943921852837775, 53.73061010926164, 49.368713058660987, 48.496333667186356, 46.751583931712887, 24.942089614624582, 23.197330799139181, 21.452571971026771, 21.452571977516595, 20.580192565224934, 20.580193144720624, 19.707813152362167, 18.835433743450249, 18.835433743917687, 18.835433742685577, 17.963054335016547, 17.090674929963011, 17.09067492363819, 206.77066417345978, 16.218295517799987, 16.218295517429826, 16.218295520172752, 15.345916105835958, 14.4735366956017, 14.473536696049345, 13.60115728795326, 13.601157287530395, 12.72877788241378, 11.856398468440155, 61.833836286875119, 35.554440183506067, 55.917906580321706, 102.99094860783568, 49.005578695887927, 46.616075437853077, 43.098478701881149, 52.291156615806244, 36.839065279442501, 61.560634138453509, 187.12949687573351, 66.629205832782574, 39.852637772920822, 225.21494145286314, 118.37700433201196, 226.29438037745007, 149.21916842497643, 460.45875965503228, 18436.49790045213, 4909.9154259651705, 4996.1301370776737, 16421.361074414734, 231.24006400605791, 257.04755614569729, 1138.5724569205495, 1688.286236352606, 306.38195901763288, 587.59655496884898, 1855.9476594365419, 791.75293851270851, 217.25714246213602, 1.3568575987570175, 1.3568575988545657, 1.3568575987534652, 1.3568575988769735, 1.3568575986741225, 1.3568575987020197, 1.356857598777381, 1.3568575988572857, 1.3568575987395828, 1.3568575987185114, 1.3568575988501268, 1.356857598736253, 1.3568575987356724, 1.3568575988298319, 1.3568575987143752, 1.3568575989056795, 1.3568575988766245, 1.3568575988662648, 1.3568575988815481, 1.3568575987811997, 1.3568575987429803, 1.3568575987392983, 1.3568575988507132, 1.3568575990824296, 1.3568575988365768, 1.3568575987261415, 1.3568575988020499, 1.3568575988283005, 1.3568575988637943, 1.3568575988209723, 1.3568575988353295, 1.3568575988926019, 1.3568575990176803, 1.3568575989461609, 1.3568575988927816, 1.3568575988856229, 1.3568575989815435, 1.3568575990739191, 1.3568575989454119, 1.356857599070328, 18436.49790045213, 16421.361074414734, 4909.9154259651705, 4996.1301370776737, 2932.7886250277325, 2932.7886292777935, 1855.9476594365419, 1688.286236352606, 1906.4860134020216, 1447.0633081916303, 2196.4543930236773, 1138.5724569205495, 1256.8197210835951, 1144.1968737996924, 964.68720636328169, 900.77962808934046, 1099.0951601145985, 206.77066417345978, 604.96305156005326, 855.71254955166262, 583.72452257752718, 511.84750138556001, 460.45875965503228, 351.32155330612591, 452.74318363671193, 1297.6960954913313, 791.75293851270851, 485.1948512349789, 539.83717078510972, 565.3233452618523, 73.692637335113091, 128.58342163285124, 24.586900662524176, 23.624041891561955, 23.624041892650002, 22.661185411822416, 22.66118542662403, 20.735474556872756, 19.772613348412797, 18.809755991131592, 18.809888568985169, 16.884041261002274, 15.921184112099686, 14.958326551826804, 14.958326553894599, 14.958326219031113, 13.995465385783953, 13.032611820934781, 13.032611950566912, 13.032607672191268, 12.069754814162515, 11.106897104406581, 11.106897208821454, 11.106897445094644, 11.10689737993555, 11.106892023732378, 11.106898885274591, 10.144039743579546, 10.144039742440949, 10.144040198641287, 11.107335621963875, 106.51897945150067, 31.225707752493424, 239.3410444729578, 70.868782644972868, 201.01247674271718, 108.54518612465903, 20.644992747987068, 97.982779637085514, 57.405343066032657, 146.6919075616986, 56.341385469421446, 96.990743065665612, 94.006537306360755, 40.064493309391388, 108.44020715043287, 259.30290694686113, 286.62427273975703, 335.07000709768852, 61.880074708846998, 159.77377344482986, 59.961852576444372, 54.622868349239262, 173.81399295675061, 351.32155330612591, 148.56857997988851, 194.05229116067758, 583.72452257752718, 306.38195901763288, 183.98640781308427, 389.14544366604991, 964.68720636328169, 206.09566047919122, 18436.49790045213, 16421.361074414734, 4996.1301370776737, 4909.9154259651705, 1144.1968737996924, 2932.7886250277325, 2932.7886292777935, 511.84750138556001, 1855.9476594365419, 347.64765448616424, 497.43125906662107, 1906.4860134020216, 855.71254955166262, 1447.0633081916303, 604.96305156005326, 411.82689635431285, 1138.5724569205495, 2196.4543930236773, 1688.286236352606, 1099.0951601145985, 900.77962808934046, 1256.8197210835951, 269.86132202905691, 214.81253000492077, 208.91427304591861, 178.4410108468511, 138.13743631633179, 110.61304230243192, 98.816873074493159, 96.850844352968124, 88.986732678740481, 83.088637055910979, 81.122616220428981, 73.258503636645301, 379.92183244356102, 56.547261757269432, 48.683155356021338, 42.785070201486697, 39.836028928002747, 39.836028520654907, 39.836028683223297, 403.46464984206335, 34.920958491131039, 32.954930177348572, 32.954926647456908, 41.799539777509239, 31.971915194977061, 31.971912937474041, 30.988900503783718, 30.988898501779751, 29.022865307248232, 28.039851234450737, 50.642093460699186, 172.31919983546481, 1297.6960954913313, 558.10653586082583, 60.468279429427781, 63.415565423697046, 192.09784214010813, 508.39898152605014, 316.64999069435959, 396.8321039167127, 169.3398213975444, 791.75293851270851, 435.60693130453467, 456.78134170230078, 750.21267736565278, 2932.7886292777935, 2932.7886250277325, 382.09385347225191, 317.84750277439127, 16421.361074414734, 18436.49790045213, 359.83488574756109, 551.2009619698797, 2196.4543930236773, 1256.8197210835951, 539.83717078510972, 1447.0633081916303, 381.18537479391222, 1688.286236352606, 843.86729859849925, 587.59655496884898, 1906.4860134020216, 1099.0951601145985, 4996.1301370776737, 4909.9154259651705, 1855.9476594365419, 450.41140315491373, 479.67352576517646, 1138.5724569205495, 855.71254955166262, 1.3568575987637861, 1.3568575987861218, 1.3568575988827332, 1.3568575987020197, 1.3568575988769735, 1.356857598736253, 1.3568575987261415, 1.3568575987891449, 1.3568575986741225, 1.3568575988461189, 1.3568575989279874, 1.356857598854704, 1.3568575987703664, 1.3568575988864575, 1.3568575988020499, 1.3568575989279472, 1.3568575988209723, 1.3568575987941787, 1.3568575987143752, 1.3568575989378089, 1.3568575988572857, 1.3568575988826048, 1.3568575988321039, 1.3568575988507132, 1.3568575988283005, 1.3568575989153939, 1.3568575988965401, 1.3568575987570175, 1.3568575988750835, 1.3568575988289246, 1.3568575989784353, 1.3568575990235483, 1.3568575989843843, 16421.361074414734, 18436.49790045213, 4909.9154259651705, 2932.7886250277325, 2932.7886292777935, 4996.1301370776737, 1906.4860134020216, 1855.9476594365419, 2196.4543930236773, 1099.0951601145985, 1447.0633081916303, 1297.6960954913313, 1144.1968737996924, 1256.8197210835951, 964.68720636328169, 1138.5724569205495, 604.96305156005326, 843.86729859849925, 1688.286236352606, 900.77962808934046, 855.71254955166262, 750.21267736565278, 497.43125906662107, 791.75293851270851, 511.84750138556001, 583.72452257752718, 394.51854300868808, 485.1948512349789, 565.3233452618523, 439.97374716730178], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -6.8593999999999999, -6.9634999999999998, -7.0797999999999996, -7.2112999999999996, -7.3628999999999998, -7.3628999999999998, -7.5415999999999999, -7.5415999999999999, -7.5415999999999999, -7.7592999999999996, -7.7592999999999996, -7.7592999999999996, -7.7592999999999996, -7.7592999999999996, -7.7592999999999996, -7.7592999999999996, -3.9943, -8.0380000000000003, -8.0380000000000003, -8.0380000000000003, -8.0380000000000003, -8.0380000000000003, -8.0380000000000003, -8.0380000000000003, -8.0380000000000003, -8.0380000000000003, -8.0380000000000003, -8.0380000000000003, -7.0797999999999996, -8.4258000000000006, -6.8593999999999999, -6.0545999999999998, -6.8981000000000003, -5.9005999999999998, -4.9301000000000004, -5.9497, -7.2112999999999996, -6.5994999999999999, -4.4276, -6.7828999999999997, -6.0862999999999996, -7.5415999999999999, -6.5259999999999998, -6.4576000000000002, -2.0709, -6.1104000000000003, -6.0327000000000002, -7.1105, -5.9903000000000004, -5.3543000000000003, -4.9588999999999999, -4.4596999999999998, -5.1253000000000002, -5.2971000000000004, -4.5129999999999999, -5.0655000000000001, -5.8974000000000002, -3.3144, -4.0140000000000002, -4.0114000000000001, -4.7161, -4.9606000000000003, -4.6738, -5.5517000000000003, -4.8357000000000001, -4.7145999999999999, -4.7145999999999999, -5.3886000000000003, -5.1375000000000002, -5.5669000000000004, -5.5011000000000001, -5.6554000000000002, -5.6626000000000003, -5.4900000000000002, -5.7592999999999996, -5.6801000000000004, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -5.5308000000000002, -6.4316000000000004, -6.5785, -6.9241999999999999, -7.0141999999999998, -7.0724, -7.1775000000000002, -7.1775000000000002, -7.2702999999999998, -7.2702999999999998, -7.4000000000000004, -7.4569999999999999, -7.4866999999999999, -7.5174000000000003, -7.5174000000000003, -7.5490000000000004, -7.7633000000000001, -7.8905000000000003, -7.8905000000000003, -7.8905000000000003, -7.9367999999999999, -7.9852999999999996, -7.9852999999999996, -8.0363000000000007, -8.0363000000000007, -8.0363000000000007, -8.0900999999999996, -8.0900999999999996, -8.1469000000000005, -8.1469000000000005, -7.0529999999999999, -6.0529999999999999, -7.2930000000000001, -5.9288999999999996, -5.0693000000000001, -5.79, -6.71, -6.8085000000000004, -6.6036000000000001, -6.1222000000000003, -6.2234999999999996, -4.6792999999999996, -3.3323, -3.3353000000000002, -2.3910999999999998, -5.3716999999999997, -2.7746, -4.9821999999999997, -4.8441999999999998, -5.3781999999999996, -4.5138999999999996, -5.665, -4.5529000000000002, -5.4040999999999997, -5.9897, -5.6569000000000003, -4.9983000000000004, -4.8005000000000004, -4.6524000000000001, -6.1109, -5.7465000000000002, -5.4447999999999999, -5.2458999999999998, -5.3818999999999999, -5.7919, -5.7347000000000001, -5.5285000000000002, -5.7882999999999996, -5.9555999999999996, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -5.0743, -5.1006, -5.3068999999999997, -5.3921999999999999, -5.4101999999999997, -5.4471999999999996, -6.0834000000000001, -6.1571999999999996, -6.2369000000000003, -6.2369000000000003, -6.2793000000000001, -6.2793000000000001, -6.3235000000000001, -6.3697999999999997, -6.3697999999999997, -6.3697999999999997, -6.4183000000000003, -6.4692999999999996, -6.4692999999999996, -3.9769999999999999, -6.5229999999999997, -6.5229999999999997, -6.5229999999999997, -6.5797999999999996, -6.6401000000000003, -6.6401000000000003, -6.7041000000000004, -6.7041000000000004, -6.7725999999999997, -6.8460999999999999, -5.2024999999999997, -5.7645999999999997, -5.3400999999999996, -4.7716000000000003, -5.5053000000000001, -5.6117999999999997, -5.7061000000000002, -5.5640999999999998, -5.8609999999999998, -5.4581, -4.7050000000000001, -5.4848999999999997, -5.8609999999999998, -4.6813000000000002, -5.1933999999999996, -4.9021999999999997, -5.1779999999999999, -4.6593, -2.8201999999999998, -3.5038999999999998, -3.5036, -3.0253999999999999, -5.0858999999999996, -5.1692, -4.6665999999999999, -4.5369999999999999, -5.2217000000000002, -5.1387999999999998, -5.0237999999999996, -5.2699999999999996, -5.3907999999999996, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -6.2779999999999996, -5.7255000000000003, -7.3868999999999998, -7.4275000000000002, -7.4275000000000002, -7.4699, -7.4699, -7.5603999999999996, -7.6089000000000002, -7.6599000000000004, -7.6603000000000003, -7.7705000000000002, -7.8307000000000002, -7.8948, -7.8948, -7.8948, -7.9631999999999996, -8.0366999999999997, -8.0366999999999997, -8.0366999999999997, -8.1160999999999994, -8.2021999999999995, -8.2021999999999995, -8.2021999999999995, -8.2021999999999995, -8.2022999999999993, -8.2022999999999993, -8.2965999999999998, -8.2965999999999998, -8.2965999999999998, -8.2065000000000001, -5.9767999999999999, -7.2054999999999998, -5.2359999999999998, -6.4202000000000004, -5.4268999999999998, -6.0258000000000003, -7.6089000000000002, -6.1441999999999997, -6.6485000000000003, -5.7882999999999996, -6.6982999999999997, -6.2057000000000002, -6.2435, -7.0183999999999997, -6.1769999999999996, -5.4309000000000003, -5.3647, -5.2587000000000002, -6.6788999999999996, -5.8833000000000002, -6.7404999999999999, -6.8169000000000004, -5.9092000000000002, -5.3971, -6.0530999999999997, -5.8560999999999996, -5.0301999999999998, -5.5288000000000004, -5.9321999999999999, -5.3948, -4.7575000000000003, -5.8578000000000001, -2.7210000000000001, -2.8557999999999999, -3.6962000000000002, -3.7101999999999999, -4.7069999999999999, -4.2085999999999997, -4.2085999999999997, -5.3799999999999999, -4.7117000000000004, -5.5899999999999999, -5.4215999999999998, -4.8009000000000004, -5.2811000000000003, -5.0787000000000004, -5.4269999999999996, -5.5827999999999998, -5.2484000000000002, -5.1433, -5.2893999999999997, -5.4404000000000003, -5.4748000000000001, -5.5705999999999998, -5.8905000000000003, -6.1189999999999998, -6.1470000000000002, -6.3048999999999999, -6.5616000000000003, -6.7845000000000004, -6.8977000000000004, -6.9179000000000004, -7.0030000000000001, -7.0719000000000003, -7.0960000000000001, -7.1985000000000001, -5.5538999999999996, -7.4589999999999996, -7.6098999999999997, -7.7401999999999997, -7.8124000000000002, -7.8124000000000002, -7.8124000000000002, -5.4981, -7.9455, -8.0042000000000009, -8.0042000000000009, -7.7667000000000002, -8.0348000000000006, -8.0348000000000006, -8.0664999999999996, -8.0664999999999996, -8.1328999999999994, -8.1678999999999995, -7.5769000000000002, -6.3651, -4.3836000000000004, -5.2203999999999997, -7.4078999999999997, -7.3616999999999999, -6.2807000000000004, -5.3371000000000004, -5.8059000000000003, -5.5910000000000002, -6.4146999999999998, -4.952, -5.5242000000000004, -5.5152999999999999, -5.0556000000000001, -3.8199000000000001, -3.8199000000000001, -5.6974999999999998, -5.8815, -2.3763000000000001, -2.3094999999999999, -5.7827000000000002, -5.4398, -4.2724000000000002, -4.7683, -5.4912000000000001, -4.758, -5.8033999999999999, -4.7037000000000004, -5.2206999999999999, -5.4837999999999996, -4.6935000000000002, -5.069, -4.1654, -4.2187000000000001, -4.8620999999999999, -5.6943999999999999, -5.7194000000000003, -5.5677000000000003, -5.6276999999999999, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993, -9.5665999999999993]}};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el169948234518562783493763\", ldavis_el169948234518562783493763_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el169948234518562783493763\", ldavis_el169948234518562783493763_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el169948234518562783493763\", ldavis_el169948234518562783493763_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_display = LDAvis('20top_100Tok.gensim')\n",
    "pyLDAvis.display(lda_display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = nltk.Text(textAnalytics[0]['tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pose estimation; information sources; ... ...; deep model; mixture\n",
      "type; mixture types; shown Fig; body locations; mod- els; human pose;\n",
      "body parts; Best viewed; IEEE Trans; viewed color; hidden layers; high\n",
      "response; 46.6 83.1; 85.8 76.5; L.leg U.arm; Method Torso\n"
     ]
    }
   ],
   "source": [
    "asd = a.collocations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "asd = ngrams(a, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####\n",
      "Multi-source\n",
      "####\n",
      "Deep\n",
      "####\n",
      "Learning\n",
      "####\n",
      "for\n",
      "####\n",
      "Human\n",
      "####\n",
      "Pose\n",
      "####\n",
      "Estimation\n",
      "####\n",
      "Wanli\n",
      "####\n",
      "Ouyang\n",
      "####\n",
      "Xiao\n",
      "####\n",
      "Chu\n",
      "####\n",
      "Xiaogang\n",
      "####\n",
      "Wang\n",
      "####\n",
      "Department\n",
      "####\n",
      "Electronic\n",
      "####\n",
      "Engineering\n",
      "####\n",
      "The\n",
      "####\n",
      "Chinese\n",
      "####\n",
      "University\n",
      "####\n",
      "Hong\n",
      "####\n",
      "Kong\n",
      "####\n",
      "wlouyang\n",
      "####\n",
      "ee.cuhk.edu.hk\n",
      "####\n",
      "xgwang\n",
      "####\n",
      "ee.cuhk.edu.hk\n",
      "####\n",
      "Abstract\n",
      "####\n",
      "Visual\n",
      "####\n",
      "appearance\n",
      "####\n",
      "score\n",
      "####\n",
      "appearance\n",
      "####\n",
      "mixture\n",
      "####\n",
      "type\n",
      "####\n",
      "and\n",
      "####\n",
      "deformation\n",
      "####\n",
      "are\n",
      "####\n",
      "three\n",
      "####\n",
      "important\n",
      "####\n",
      "information\n",
      "####\n",
      "sources\n",
      "####\n",
      "for\n",
      "####\n",
      "human\n",
      "####\n",
      "pose\n",
      "####\n",
      "estimation\n",
      "####\n",
      "This\n",
      "####\n",
      "paper\n",
      "####\n",
      "proposes\n",
      "####\n",
      "build\n",
      "####\n",
      "multi-source\n",
      "####\n",
      "deep\n",
      "####\n",
      "model\n",
      "####\n",
      "order\n",
      "####\n",
      "extract\n",
      "####\n",
      "non-linear\n",
      "####\n",
      "representation\n",
      "####\n",
      "from\n",
      "####\n",
      "these\n",
      "####\n",
      "different\n",
      "####\n",
      "aspects\n",
      "####\n",
      "information\n",
      "####\n",
      "sources\n",
      "####\n",
      "With\n",
      "####\n",
      "the\n",
      "####\n",
      "deep\n",
      "####\n",
      "model\n",
      "####\n",
      "the\n",
      "####\n",
      "global\n",
      "####\n",
      "high-order\n",
      "####\n",
      "hu-\n",
      "####\n",
      "man\n",
      "####\n",
      "body\n",
      "####\n",
      "articulation\n",
      "####\n",
      "patterns\n",
      "####\n",
      "these\n",
      "####\n",
      "information\n",
      "####\n",
      "sources\n",
      "####\n",
      "are\n",
      "####\n",
      "extracted\n",
      "####\n",
      "for\n",
      "####\n",
      "pose\n",
      "####\n",
      "estimation\n",
      "####\n",
      "The\n",
      "####\n",
      "task\n",
      "####\n",
      "for\n",
      "####\n",
      "estimat-\n",
      "####\n",
      "ing\n",
      "####\n",
      "body\n",
      "####\n",
      "locations\n",
      "####\n",
      "and\n",
      "####\n",
      "the\n",
      "####\n",
      "task\n",
      "####\n",
      "for\n",
      "####\n",
      "human\n",
      "####\n",
      "detection\n",
      "####\n",
      "are\n",
      "####\n",
      "jointly\n",
      "####\n",
      "learned\n",
      "####\n",
      "using\n",
      "####\n",
      "uniﬁed\n",
      "####\n",
      "deep\n",
      "####\n",
      "model\n",
      "####\n",
      "The\n",
      "####\n",
      "proposed\n",
      "####\n",
      "approach\n",
      "####\n",
      "can\n",
      "####\n",
      "viewed\n",
      "####\n",
      "post-processing\n",
      "####\n",
      "pose\n",
      "####\n",
      "esti-\n",
      "####\n",
      "mation\n",
      "####\n",
      "results\n",
      "####\n",
      "and\n",
      "####\n",
      "can\n",
      "####\n",
      "ﬂexibly\n",
      "####\n",
      "integrate\n",
      "####\n",
      "with\n",
      "####\n",
      "existing\n",
      "####\n",
      "meth-\n",
      "####\n",
      "ods\n",
      "####\n",
      "taking\n",
      "####\n",
      "their\n",
      "####\n",
      "information\n",
      "####\n",
      "sources\n",
      "####\n",
      "input\n",
      "####\n",
      "extract-\n",
      "####\n",
      "ing\n",
      "####\n",
      "the\n",
      "####\n",
      "non-linear\n",
      "####\n",
      "representation\n",
      "####\n",
      "from\n",
      "####\n",
      "multiple\n",
      "####\n",
      "information\n",
      "####\n",
      "sources\n",
      "####\n",
      "the\n",
      "####\n",
      "deep\n",
      "####\n",
      "model\n",
      "####\n",
      "outperforms\n",
      "####\n",
      "state-of-the-art\n",
      "####\n",
      "8.6\n",
      "####\n",
      "percent\n",
      "####\n",
      "three\n",
      "####\n",
      "public\n",
      "####\n",
      "benchmark\n",
      "####\n",
      "datasets\n",
      "####\n",
      "Introduction\n",
      "####\n",
      "Human\n",
      "####\n",
      "pose\n",
      "####\n",
      "estimation\n",
      "####\n",
      "the\n",
      "####\n",
      "process\n",
      "####\n",
      "determining\n",
      "####\n",
      "from\n",
      "####\n",
      "image\n",
      "####\n",
      "the\n",
      "####\n",
      "positions\n",
      "####\n",
      "human\n",
      "####\n",
      "body\n",
      "####\n",
      "parts\n",
      "####\n",
      "such\n",
      "####\n",
      "the\n",
      "####\n",
      "head\n",
      "####\n",
      "shoulder\n",
      "####\n",
      "elbow\n",
      "####\n",
      "wrist\n",
      "####\n",
      "hip\n",
      "####\n",
      "knee\n",
      "####\n",
      "and\n",
      "####\n",
      "ankle\n",
      "####\n",
      "fundamental\n",
      "####\n",
      "problem\n",
      "####\n",
      "computer\n",
      "####\n",
      "vision\n",
      "####\n",
      "and\n",
      "####\n",
      "has\n",
      "####\n",
      "abun-\n",
      "####\n",
      "dant\n",
      "####\n",
      "important\n",
      "####\n",
      "applications\n",
      "####\n",
      "such\n",
      "####\n",
      "sports\n",
      "####\n",
      "action\n",
      "####\n",
      "recogni-\n",
      "####\n",
      "tion\n",
      "####\n",
      "character\n",
      "####\n",
      "animation\n",
      "####\n",
      "clinical\n",
      "####\n",
      "analysis\n",
      "####\n",
      "gait\n",
      "####\n",
      "patholo-\n",
      "####\n",
      "gies\n",
      "####\n",
      "content-based\n",
      "####\n",
      "video\n",
      "####\n",
      "and\n",
      "####\n",
      "image\n",
      "####\n",
      "retrieval\n",
      "####\n",
      "and\n",
      "####\n",
      "intelli-\n",
      "####\n",
      "gent\n",
      "####\n",
      "video\n",
      "####\n",
      "surveillance\n",
      "####\n",
      "Despite\n",
      "####\n",
      "many\n",
      "####\n",
      "years\n",
      "####\n",
      "research\n",
      "####\n",
      "pose\n",
      "####\n",
      "estimation\n",
      "####\n",
      "remains\n",
      "####\n",
      "difﬁ-\n",
      "####\n",
      "cult\n",
      "####\n",
      "problem\n",
      "####\n",
      "One\n",
      "####\n",
      "the\n",
      "####\n",
      "most\n",
      "####\n",
      "signiﬁcant\n",
      "####\n",
      "challenges\n",
      "####\n",
      "pose\n",
      "####\n",
      "estimation\n",
      "####\n",
      "how\n",
      "####\n",
      "model\n",
      "####\n",
      "the\n",
      "####\n",
      "complex\n",
      "####\n",
      "human\n",
      "####\n",
      "articulation\n",
      "####\n",
      "Many\n",
      "####\n",
      "approaches\n",
      "####\n",
      "have\n",
      "####\n",
      "been\n",
      "####\n",
      "used\n",
      "####\n",
      "handle\n",
      "####\n",
      "the\n",
      "####\n",
      "com-\n",
      "####\n",
      "plex\n",
      "####\n",
      "human\n",
      "####\n",
      "articulation\n",
      "####\n",
      "using\n",
      "####\n",
      "three\n",
      "####\n",
      "information\n",
      "####\n",
      "sources\n",
      "####\n",
      "mixture\n",
      "####\n",
      "type\n",
      "####\n",
      "appearance\n",
      "####\n",
      "score\n",
      "####\n",
      "and\n",
      "####\n",
      "deformation\n",
      "####\n",
      "Inﬂuenced\n",
      "####\n",
      "human\n",
      "####\n",
      "body\n",
      "####\n",
      "articulation\n",
      "####\n",
      "clothing\n",
      "####\n",
      "occlusion\n",
      "####\n",
      "etc.\n",
      "####\n",
      "body\n",
      "####\n",
      "part\n",
      "####\n",
      "appearance\n",
      "####\n",
      "varies\n",
      "####\n",
      "handle\n",
      "####\n",
      "this\n",
      "####\n",
      "variation\n",
      "####\n",
      "the\n",
      "####\n",
      "appearance\n",
      "####\n",
      "part\n",
      "####\n",
      "clustered\n",
      "####\n",
      "into\n",
      "####\n",
      "multiple\n",
      "####\n",
      "mixture\n",
      "####\n",
      "types\n",
      "####\n",
      "shown\n",
      "####\n",
      "Fig\n",
      "####\n",
      "For\n",
      "####\n",
      "each\n",
      "####\n",
      "mixture\n",
      "####\n",
      "type\n",
      "####\n",
      "part\n",
      "####\n",
      "part\n",
      "####\n",
      "template\n",
      "####\n",
      "learned\n",
      "####\n",
      "capture\n",
      "####\n",
      "its\n",
      "####\n",
      "appearance\n",
      "####\n",
      "Then\n",
      "####\n",
      "the\n",
      "####\n",
      "appearance\n",
      "####\n",
      "scores\n",
      "####\n",
      "log-likelihoods\n",
      "####\n",
      "body\n",
      "####\n",
      "parts\n",
      "####\n",
      "Figure\n",
      "####\n",
      "The\n",
      "####\n",
      "motivation\n",
      "####\n",
      "this\n",
      "####\n",
      "paper\n",
      "####\n",
      "using\n",
      "####\n",
      "multi-source\n",
      "####\n",
      "deep\n",
      "####\n",
      "model\n",
      "####\n",
      "for\n",
      "####\n",
      "constructing\n",
      "####\n",
      "the\n",
      "####\n",
      "non-linear\n",
      "####\n",
      "representation\n",
      "####\n",
      "from\n",
      "####\n",
      "three\n",
      "####\n",
      "in-\n",
      "####\n",
      "formation\n",
      "####\n",
      "sources\n",
      "####\n",
      "mixture\n",
      "####\n",
      "type\n",
      "####\n",
      "appearance\n",
      "####\n",
      "score\n",
      "####\n",
      "and\n",
      "####\n",
      "deforma-\n",
      "####\n",
      "tion\n",
      "####\n",
      "Best\n",
      "####\n",
      "viewed\n",
      "####\n",
      "color\n",
      "####\n",
      "being\n",
      "####\n",
      "different\n",
      "####\n",
      "locations\n",
      "####\n",
      "are\n",
      "####\n",
      "obtained\n",
      "####\n",
      "convolving\n",
      "####\n",
      "the\n",
      "####\n",
      "part\n",
      "####\n",
      "templates\n",
      "####\n",
      "with\n",
      "####\n",
      "the\n",
      "####\n",
      "visual\n",
      "####\n",
      "features\n",
      "####\n",
      "the\n",
      "####\n",
      "input\n",
      "####\n",
      "image\n",
      "####\n",
      "e.g\n",
      "####\n",
      "HOG\n",
      "####\n",
      "The\n",
      "####\n",
      "appearance\n",
      "####\n",
      "scores\n",
      "####\n",
      "are\n",
      "####\n",
      "inaccurate\n",
      "####\n",
      "for\n",
      "####\n",
      "well-locating\n",
      "####\n",
      "body\n",
      "####\n",
      "parts\n",
      "####\n",
      "because\n",
      "####\n",
      "the\n",
      "####\n",
      "part\n",
      "####\n",
      "template\n",
      "####\n",
      "imper-\n",
      "####\n",
      "fect\n",
      "####\n",
      "Therefore\n",
      "####\n",
      "the\n",
      "####\n",
      "deformations\n",
      "####\n",
      "relative\n",
      "####\n",
      "locations\n",
      "####\n",
      "among\n",
      "####\n",
      "body\n",
      "####\n",
      "parts\n",
      "####\n",
      "are\n",
      "####\n",
      "used\n",
      "####\n",
      "for\n",
      "####\n",
      "encoding\n",
      "####\n",
      "likely\n",
      "####\n",
      "pairwise\n",
      "####\n",
      "poses\n",
      "####\n",
      "for\n",
      "####\n",
      "example\n",
      "####\n",
      "the\n",
      "####\n",
      "head\n",
      "####\n",
      "should\n",
      "####\n",
      "not\n",
      "####\n",
      "far\n",
      "####\n",
      "from\n",
      "####\n",
      "the\n",
      "####\n",
      "neck\n",
      "####\n",
      "Existing\n",
      "####\n",
      "approaches\n",
      "####\n",
      "use\n",
      "####\n",
      "log-linear\n",
      "####\n",
      "models\n",
      "####\n",
      "with\n",
      "####\n",
      "pairwise\n",
      "####\n",
      "potentials\n",
      "####\n",
      "these\n",
      "####\n",
      "three\n",
      "####\n",
      "information\n",
      "####\n",
      "sources\n",
      "####\n",
      "determine\n",
      "####\n",
      "whether\n",
      "####\n",
      "estimated\n",
      "####\n",
      "location\n",
      "####\n",
      "correct\n",
      "####\n",
      "However\n",
      "####\n",
      "these\n",
      "####\n",
      "information\n",
      "####\n",
      "sources\n",
      "####\n",
      "are\n",
      "####\n",
      "not\n",
      "####\n",
      "log-linearly\n",
      "####\n",
      "cor-\n",
      "####\n",
      "related\n",
      "####\n",
      "when\n",
      "####\n",
      "choosing\n",
      "####\n",
      "the\n",
      "####\n",
      "correct\n",
      "####\n",
      "candidate\n",
      "####\n",
      "For\n",
      "####\n",
      "the\n",
      "####\n",
      "exam-\n",
      "####\n",
      "ple\n",
      "####\n",
      "Fig\n",
      "####\n",
      "linear\n",
      "####\n",
      "models\n",
      "####\n",
      "may\n",
      "####\n",
      "ﬁnd\n",
      "####\n",
      "that\n",
      "####\n",
      "the\n",
      "####\n",
      "estimated\n",
      "####\n",
      "result\n",
      "####\n",
      "the\n",
      "####\n",
      "left\n",
      "####\n",
      "and\n",
      "####\n",
      "the\n",
      "####\n",
      "result\n",
      "####\n",
      "the\n",
      "####\n",
      "right\n",
      "####\n",
      "have\n",
      "####\n",
      "the\n",
      "####\n",
      "same\n",
      "####\n",
      "deformation\n",
      "####\n",
      "score\n",
      "####\n",
      "because\n",
      "####\n",
      "they\n",
      "####\n",
      "simply\n",
      "####\n",
      "linearly\n",
      "####\n",
      "add\n",
      "####\n",
      "local\n",
      "####\n",
      "deformation\n",
      "####\n",
      "cost\n",
      "####\n",
      "While\n",
      "####\n",
      "obvious\n",
      "####\n",
      "for\n",
      "####\n",
      "human\n",
      "####\n",
      "ﬁnd\n",
      "####\n",
      "that\n",
      "####\n",
      "the\n",
      "####\n",
      "result\n",
      "####\n",
      "the\n",
      "####\n",
      "left\n",
      "####\n",
      "not\n",
      "####\n",
      "reasonable\n",
      "####\n",
      "Similar\n",
      "####\n",
      "situations\n",
      "####\n",
      "also\n",
      "####\n",
      "occur\n",
      "####\n",
      "for\n",
      "####\n",
      "mixture\n",
      "####\n",
      "type\n",
      "####\n",
      "and\n",
      "####\n",
      "appearance\n",
      "####\n",
      "score\n",
      "####\n",
      "There-\n",
      "####\n",
      "fore\n",
      "####\n",
      "desirable\n",
      "####\n",
      "construct\n",
      "####\n",
      "the\n",
      "####\n",
      "non-linear\n",
      "####\n",
      "representation\n",
      "####\n",
      "that\n",
      "####\n",
      "identiﬁes\n",
      "####\n",
      "reasonable\n",
      "####\n",
      "conﬁgurations\n",
      "####\n",
      "deformation\n",
      "####\n",
      "ap-\n",
      "####\n",
      "pearance\n",
      "####\n",
      "score\n",
      "####\n",
      "and\n",
      "####\n",
      "mixture\n",
      "####\n",
      "type\n",
      "####\n",
      "order\n",
      "####\n",
      "construct\n",
      "####\n",
      "useful\n",
      "####\n",
      "representation\n",
      "####\n",
      "from\n",
      "####\n",
      "multi-\n",
      "####\n",
      "ple\n",
      "####\n",
      "information\n",
      "####\n",
      "sources\n",
      "####\n",
      "for\n",
      "####\n",
      "pose\n",
      "####\n",
      "estimation\n",
      "####\n",
      "model\n",
      "####\n",
      "should\n",
      "####\n",
      "Multi-sourcedeep\n",
      "####\n",
      "modelEstimated\n",
      "####\n",
      "resultType\n",
      "####\n",
      "1Type\n",
      "####\n",
      "2Non-Linearmodel\n",
      "####\n",
      "YesNoEstimated\n",
      "####\n",
      "resultMixture\n",
      "####\n",
      "typeHead\n",
      "####\n",
      "topNeckDeformationAppearance\n",
      "####\n",
      "scoreTemplateLinear\n",
      "####\n",
      "model1389\n",
      "####\n",
      "satisfy\n",
      "####\n",
      "certain\n",
      "####\n",
      "properties\n",
      "####\n",
      "First\n",
      "####\n",
      "the\n",
      "####\n",
      "model\n",
      "####\n",
      "should\n",
      "####\n",
      "capture\n",
      "####\n",
      "the\n",
      "####\n",
      "global\n",
      "####\n",
      "complex\n",
      "####\n",
      "relationships\n",
      "####\n",
      "among\n",
      "####\n",
      "body\n",
      "####\n",
      "parts\n",
      "####\n",
      "For\n",
      "####\n",
      "the\n",
      "####\n",
      "example\n",
      "####\n",
      "Fig\n",
      "####\n",
      "the\n",
      "####\n",
      "result\n",
      "####\n",
      "the\n",
      "####\n",
      "left\n",
      "####\n",
      "unreasonable\n",
      "####\n",
      "because\n",
      "####\n",
      "its\n",
      "####\n",
      "global\n",
      "####\n",
      "conﬁguration\n",
      "####\n",
      "arm\n",
      "####\n",
      "torso\n",
      "####\n",
      "and\n",
      "####\n",
      "leg\n",
      "####\n",
      "Second\n",
      "####\n",
      "since\n",
      "####\n",
      "reasonable\n",
      "####\n",
      "conﬁguration\n",
      "####\n",
      "very\n",
      "####\n",
      "abstract\n",
      "####\n",
      "concept\n",
      "####\n",
      "while\n",
      "####\n",
      "the\n",
      "####\n",
      "information\n",
      "####\n",
      "sources\n",
      "####\n",
      "are\n",
      "####\n",
      "less\n",
      "####\n",
      "abstract\n",
      "####\n",
      "con-\n",
      "####\n",
      "cepts\n",
      "####\n",
      "the\n",
      "####\n",
      "model\n",
      "####\n",
      "should\n",
      "####\n",
      "construct\n",
      "####\n",
      "more\n",
      "####\n",
      "abstract\n",
      "####\n",
      "representa-\n",
      "####\n",
      "tion\n",
      "####\n",
      "from\n",
      "####\n",
      "the\n",
      "####\n",
      "less\n",
      "####\n",
      "abstract\n",
      "####\n",
      "representation\n",
      "####\n",
      "Third\n",
      "####\n",
      "since\n",
      "####\n",
      "dif-\n",
      "####\n",
      "ferent\n",
      "####\n",
      "information\n",
      "####\n",
      "sources\n",
      "####\n",
      "describe\n",
      "####\n",
      "different\n",
      "####\n",
      "aspects\n",
      "####\n",
      "hu-\n",
      "####\n",
      "man\n",
      "####\n",
      "pose\n",
      "####\n",
      "and\n",
      "####\n",
      "have\n",
      "####\n",
      "different\n",
      "####\n",
      "statistical\n",
      "####\n",
      "properties\n",
      "####\n",
      "the\n",
      "####\n",
      "model\n",
      "####\n",
      "should\n",
      "####\n",
      "learn\n",
      "####\n",
      "useful\n",
      "####\n",
      "representation\n",
      "####\n",
      "from\n",
      "####\n",
      "these\n",
      "####\n",
      "sources\n",
      "####\n",
      "and\n",
      "####\n",
      "fuse\n",
      "####\n",
      "them\n",
      "####\n",
      "into\n",
      "####\n",
      "joint\n",
      "####\n",
      "representation\n",
      "####\n",
      "for\n",
      "####\n",
      "pose\n",
      "####\n",
      "estimation\n",
      "####\n",
      "The\n",
      "####\n",
      "multi-source\n",
      "####\n",
      "deep\n",
      "####\n",
      "architecture\n",
      "####\n",
      "propose\n",
      "####\n",
      "satisﬁes\n",
      "####\n",
      "the\n",
      "####\n",
      "above\n",
      "####\n",
      "requirement\n",
      "####\n",
      "There\n",
      "####\n",
      "are\n",
      "####\n",
      "three\n",
      "####\n",
      "contributions\n",
      "####\n",
      "this\n",
      "####\n",
      "paper\n",
      "####\n",
      "propose\n",
      "####\n",
      "deep\n",
      "####\n",
      "architecture\n",
      "####\n",
      "construct\n",
      "####\n",
      "the\n",
      "####\n",
      "non-\n",
      "####\n",
      "linear\n",
      "####\n",
      "representation\n",
      "####\n",
      "from\n",
      "####\n",
      "different\n",
      "####\n",
      "aspects\n",
      "####\n",
      "informa-\n",
      "####\n",
      "tion\n",
      "####\n",
      "sources\n",
      "####\n",
      "the\n",
      "####\n",
      "best\n",
      "####\n",
      "our\n",
      "####\n",
      "knowledge\n",
      "####\n",
      "this\n",
      "####\n",
      "paper\n",
      "####\n",
      "the\n",
      "####\n",
      "ﬁrst\n",
      "####\n",
      "use\n",
      "####\n",
      "deep\n",
      "####\n",
      "model\n",
      "####\n",
      "for\n",
      "####\n",
      "pose\n",
      "####\n",
      "estimation\n",
      "####\n",
      "The\n",
      "####\n",
      "body\n",
      "####\n",
      "articulation\n",
      "####\n",
      "patterns\n",
      "####\n",
      "global\n",
      "####\n",
      "and\n",
      "####\n",
      "more\n",
      "####\n",
      "abstract\n",
      "####\n",
      "representations\n",
      "####\n",
      "are\n",
      "####\n",
      "captured\n",
      "####\n",
      "the\n",
      "####\n",
      "deep\n",
      "####\n",
      "model\n",
      "####\n",
      "from\n",
      "####\n",
      "the\n",
      "####\n",
      "information\n",
      "####\n",
      "sources\n",
      "####\n",
      "local\n",
      "####\n",
      "and\n",
      "####\n",
      "less\n",
      "####\n",
      "abstract\n",
      "####\n",
      "representa-\n",
      "####\n",
      "tions\n",
      "####\n",
      "For\n",
      "####\n",
      "each\n",
      "####\n",
      "information\n",
      "####\n",
      "source\n",
      "####\n",
      "more\n",
      "####\n",
      "abstract\n",
      "####\n",
      "repre-\n",
      "####\n",
      "sentation\n",
      "####\n",
      "the\n",
      "####\n",
      "higher\n",
      "####\n",
      "layer\n",
      "####\n",
      "composed\n",
      "####\n",
      "the\n",
      "####\n",
      "less\n",
      "####\n",
      "ab-\n",
      "####\n",
      "stract\n",
      "####\n",
      "representation\n",
      "####\n",
      "all\n",
      "####\n",
      "body\n",
      "####\n",
      "parts\n",
      "####\n",
      "the\n",
      "####\n",
      "lower\n",
      "####\n",
      "layer\n",
      "####\n",
      "Then\n",
      "####\n",
      "representations\n",
      "####\n",
      "all\n",
      "####\n",
      "information\n",
      "####\n",
      "sources\n",
      "####\n",
      "the\n",
      "####\n",
      "higher\n",
      "####\n",
      "layer\n",
      "####\n",
      "are\n",
      "####\n",
      "fused\n",
      "####\n",
      "for\n",
      "####\n",
      "pose\n",
      "####\n",
      "estimation\n",
      "####\n",
      "Both\n",
      "####\n",
      "the\n",
      "####\n",
      "task\n",
      "####\n",
      "for\n",
      "####\n",
      "detecting\n",
      "####\n",
      "human\n",
      "####\n",
      "and\n",
      "####\n",
      "the\n",
      "####\n",
      "task\n",
      "####\n",
      "for\n",
      "####\n",
      "esti-\n",
      "####\n",
      "mating\n",
      "####\n",
      "body\n",
      "####\n",
      "locations\n",
      "####\n",
      "are\n",
      "####\n",
      "jointly\n",
      "####\n",
      "learned\n",
      "####\n",
      "using\n",
      "####\n",
      "single\n",
      "####\n",
      "deep\n",
      "####\n",
      "model\n",
      "####\n",
      "Joint\n",
      "####\n",
      "learning\n",
      "####\n",
      "these\n",
      "####\n",
      "tasks\n",
      "####\n",
      "with\n",
      "####\n",
      "shared\n",
      "####\n",
      "representation\n",
      "####\n",
      "improves\n",
      "####\n",
      "pose\n",
      "####\n",
      "estimation\n",
      "####\n",
      "accuracy\n",
      "####\n",
      "Related\n",
      "####\n",
      "work\n",
      "####\n",
      "Human\n",
      "####\n",
      "pose\n",
      "####\n",
      "estimation\n",
      "####\n",
      "Pose\n",
      "####\n",
      "estimation\n",
      "####\n",
      "considered\n",
      "####\n",
      "holistic\n",
      "####\n",
      "recognition\n",
      "####\n",
      "the\n",
      "####\n",
      "other\n",
      "####\n",
      "hand\n",
      "####\n",
      "many\n",
      "####\n",
      "recent\n",
      "####\n",
      "works\n",
      "####\n",
      "use\n",
      "####\n",
      "local\n",
      "####\n",
      "body\n",
      "####\n",
      "parts\n",
      "####\n",
      "order\n",
      "####\n",
      "handle\n",
      "####\n",
      "the\n",
      "####\n",
      "many\n",
      "####\n",
      "degrees\n",
      "####\n",
      "freedom\n",
      "####\n",
      "body\n",
      "####\n",
      "part\n",
      "####\n",
      "articulation\n",
      "####\n",
      "Since\n",
      "####\n",
      "the\n",
      "####\n",
      "ﬁrst\n",
      "####\n",
      "work\n",
      "####\n",
      "some\n",
      "####\n",
      "approaches\n",
      "####\n",
      "have\n",
      "####\n",
      "clustered\n",
      "####\n",
      "part\n",
      "####\n",
      "appearance\n",
      "####\n",
      "into\n",
      "####\n",
      "mixture\n",
      "####\n",
      "types\n",
      "####\n",
      "shown\n",
      "####\n",
      "Fig\n",
      "####\n",
      "There\n",
      "####\n",
      "are\n",
      "####\n",
      "also\n",
      "####\n",
      "approaches\n",
      "####\n",
      "that\n",
      "####\n",
      "warp\n",
      "####\n",
      "the\n",
      "####\n",
      "part\n",
      "####\n",
      "template\n",
      "####\n",
      "ﬂexible\n",
      "####\n",
      "sizes\n",
      "####\n",
      "and\n",
      "####\n",
      "orientations\n",
      "####\n",
      "The\n",
      "####\n",
      "appearance\n",
      "####\n",
      "score\n",
      "####\n",
      "rotation\n",
      "####\n",
      "size\n",
      "####\n",
      "and\n",
      "####\n",
      "location\n",
      "####\n",
      "used\n",
      "####\n",
      "these\n",
      "####\n",
      "approaches\n",
      "####\n",
      "can\n",
      "####\n",
      "treated\n",
      "####\n",
      "multiple\n",
      "####\n",
      "information\n",
      "####\n",
      "sources\n",
      "####\n",
      "and\n",
      "####\n",
      "used\n",
      "####\n",
      "our\n",
      "####\n",
      "deep\n",
      "####\n",
      "model\n",
      "####\n",
      "for\n",
      "####\n",
      "pose\n",
      "####\n",
      "estimation\n",
      "####\n",
      "existing\n",
      "####\n",
      "pose\n",
      "####\n",
      "estimation\n",
      "####\n",
      "approaches\n",
      "####\n",
      "the\n",
      "####\n",
      "pair-wise\n",
      "####\n",
      "part\n",
      "####\n",
      "deformation\n",
      "####\n",
      "relationships\n",
      "####\n",
      "are\n",
      "####\n",
      "arranged\n",
      "####\n",
      "tree\n",
      "####\n",
      "models\n",
      "####\n",
      "multi-tree\n",
      "####\n",
      "model\n",
      "####\n",
      "loopy\n",
      "####\n",
      "mod-\n",
      "####\n",
      "els\n",
      "####\n",
      "Tree\n",
      "####\n",
      "models\n",
      "####\n",
      "allow\n",
      "####\n",
      "for\n",
      "####\n",
      "efﬁcient\n",
      "####\n",
      "and\n",
      "####\n",
      "exact\n",
      "####\n",
      "inference\n",
      "####\n",
      "but\n",
      "####\n",
      "are\n",
      "####\n",
      "insufﬁcient\n",
      "####\n",
      "modeling\n",
      "####\n",
      "the\n",
      "####\n",
      "complex\n",
      "####\n",
      "re-\n",
      "####\n",
      "lationships\n",
      "####\n",
      "among\n",
      "####\n",
      "body\n",
      "####\n",
      "parts\n",
      "####\n",
      "Hence\n",
      "####\n",
      "tree\n",
      "####\n",
      "models\n",
      "####\n",
      "often\n",
      "####\n",
      "suffer\n",
      "####\n",
      "from\n",
      "####\n",
      "double\n",
      "####\n",
      "counting\n",
      "####\n",
      "for\n",
      "####\n",
      "example\n",
      "####\n",
      "given\n",
      "####\n",
      "the\n",
      "####\n",
      "posi-\n",
      "####\n",
      "tion\n",
      "####\n",
      "torso\n",
      "####\n",
      "the\n",
      "####\n",
      "positions\n",
      "####\n",
      "two\n",
      "####\n",
      "legs\n",
      "####\n",
      "are\n",
      "####\n",
      "independent\n",
      "####\n",
      "and\n",
      "####\n",
      "often\n",
      "####\n",
      "respond\n",
      "####\n",
      "the\n",
      "####\n",
      "same\n",
      "####\n",
      "visual\n",
      "####\n",
      "cue\n",
      "####\n",
      "Loopy\n",
      "####\n",
      "models\n",
      "####\n",
      "allow\n",
      "####\n",
      "more\n",
      "####\n",
      "complex\n",
      "####\n",
      "relationships\n",
      "####\n",
      "among\n",
      "####\n",
      "parts\n",
      "####\n",
      "but\n",
      "####\n",
      "require\n",
      "####\n",
      "approximate\n",
      "####\n",
      "inference\n",
      "####\n",
      "Our\n",
      "####\n",
      "deep\n",
      "####\n",
      "architecture\n",
      "####\n",
      "models\n",
      "####\n",
      "the\n",
      "####\n",
      "complex\n",
      "####\n",
      "relationships\n",
      "####\n",
      "among\n",
      "####\n",
      "parts\n",
      "####\n",
      "and\n",
      "####\n",
      "computationally\n",
      "####\n",
      "efﬁcient\n",
      "####\n",
      "both\n",
      "####\n",
      "training\n",
      "####\n",
      "and\n",
      "####\n",
      "testing\n",
      "####\n",
      "Deep\n",
      "####\n",
      "learning\n",
      "####\n",
      "Since\n",
      "####\n",
      "the\n",
      "####\n",
      "breakthrough\n",
      "####\n",
      "deep\n",
      "####\n",
      "learning\n",
      "####\n",
      "initiated\n",
      "####\n",
      "Hinton\n",
      "####\n",
      "deep\n",
      "####\n",
      "learning\n",
      "####\n",
      "gain-\n",
      "####\n",
      "ing\n",
      "####\n",
      "more\n",
      "####\n",
      "and\n",
      "####\n",
      "more\n",
      "####\n",
      "attention\n",
      "####\n",
      "Bengio\n",
      "####\n",
      "proved\n",
      "####\n",
      "that\n",
      "####\n",
      "exist-\n",
      "####\n",
      "ing\n",
      "####\n",
      "commonly\n",
      "####\n",
      "used\n",
      "####\n",
      "machine\n",
      "####\n",
      "learning\n",
      "####\n",
      "tools\n",
      "####\n",
      "such\n",
      "####\n",
      "SVM\n",
      "####\n",
      "and\n",
      "####\n",
      "Boosting\n",
      "####\n",
      "are\n",
      "####\n",
      "shallow\n",
      "####\n",
      "models\n",
      "####\n",
      "and\n",
      "####\n",
      "they\n",
      "####\n",
      "may\n",
      "####\n",
      "require\n",
      "####\n",
      "many\n",
      "####\n",
      "more\n",
      "####\n",
      "computational\n",
      "####\n",
      "elements\n",
      "####\n",
      "potentially\n",
      "####\n",
      "exponen-\n",
      "####\n",
      "tially\n",
      "####\n",
      "more\n",
      "####\n",
      "with\n",
      "####\n",
      "respect\n",
      "####\n",
      "input\n",
      "####\n",
      "size\n",
      "####\n",
      "than\n",
      "####\n",
      "deep\n",
      "####\n",
      "models\n",
      "####\n",
      "whose\n",
      "####\n",
      "depth\n",
      "####\n",
      "matched\n",
      "####\n",
      "the\n",
      "####\n",
      "task\n",
      "####\n",
      "Deep\n",
      "####\n",
      "architecture\n",
      "####\n",
      "found\n",
      "####\n",
      "yield\n",
      "####\n",
      "better\n",
      "####\n",
      "data\n",
      "####\n",
      "representation\n",
      "####\n",
      "for\n",
      "####\n",
      "example\n",
      "####\n",
      "terms\n",
      "####\n",
      "classiﬁcation\n",
      "####\n",
      "error\n",
      "####\n",
      "invariance\n",
      "####\n",
      "input\n",
      "####\n",
      "trans-\n",
      "####\n",
      "formations\n",
      "####\n",
      "modeling\n",
      "####\n",
      "multi-modal\n",
      "####\n",
      "data\n",
      "####\n",
      "Deep\n",
      "####\n",
      "learning\n",
      "####\n",
      "has\n",
      "####\n",
      "achieved\n",
      "####\n",
      "spectacular\n",
      "####\n",
      "progress\n",
      "####\n",
      "computer\n",
      "####\n",
      "vi-\n",
      "####\n",
      "sion\n",
      "####\n",
      "Recent\n",
      "####\n",
      "progress\n",
      "####\n",
      "deep\n",
      "####\n",
      "learning\n",
      "####\n",
      "reviewed\n",
      "####\n",
      "Krizhevsky\n",
      "####\n",
      "pro-\n",
      "####\n",
      "posed\n",
      "####\n",
      "large-scale\n",
      "####\n",
      "deep\n",
      "####\n",
      "convolutional\n",
      "####\n",
      "network\n",
      "####\n",
      "with\n",
      "####\n",
      "breakthrough\n",
      "####\n",
      "the\n",
      "####\n",
      "large-scale\n",
      "####\n",
      "ImageNet\n",
      "####\n",
      "object\n",
      "####\n",
      "recogni-\n",
      "####\n",
      "tion\n",
      "####\n",
      "dataset\n",
      "####\n",
      "attaining\n",
      "####\n",
      "signiﬁcant\n",
      "####\n",
      "gap\n",
      "####\n",
      "compared\n",
      "####\n",
      "with\n",
      "####\n",
      "existing\n",
      "####\n",
      "approaches\n",
      "####\n",
      "that\n",
      "####\n",
      "use\n",
      "####\n",
      "shallow\n",
      "####\n",
      "models\n",
      "####\n",
      "and\n",
      "####\n",
      "bringing\n",
      "####\n",
      "high\n",
      "####\n",
      "impact\n",
      "####\n",
      "research\n",
      "####\n",
      "computer\n",
      "####\n",
      "vision\n",
      "####\n",
      "Our\n",
      "####\n",
      "approaches\n",
      "####\n",
      "learns\n",
      "####\n",
      "feature\n",
      "####\n",
      "learning\n",
      "####\n",
      "translational\n",
      "####\n",
      "de-\n",
      "####\n",
      "formation\n",
      "####\n",
      "and\n",
      "####\n",
      "occlusion\n",
      "####\n",
      "relationship\n",
      "####\n",
      "pedestrian\n",
      "####\n",
      "detec-\n",
      "####\n",
      "tion\n",
      "####\n",
      "the\n",
      "####\n",
      "approach\n",
      "####\n",
      "learns\n",
      "####\n",
      "relational\n",
      "####\n",
      "ﬁlter\n",
      "####\n",
      "pairs\n",
      "####\n",
      "face\n",
      "####\n",
      "veriﬁcation\n",
      "####\n",
      "the\n",
      "####\n",
      "best\n",
      "####\n",
      "our\n",
      "####\n",
      "knowledge\n",
      "####\n",
      "however\n",
      "####\n",
      "deep\n",
      "####\n",
      "model\n",
      "####\n",
      "for\n",
      "####\n",
      "human\n",
      "####\n",
      "pose\n",
      "####\n",
      "estimation\n",
      "####\n",
      "has\n",
      "####\n",
      "not\n",
      "####\n",
      "yet\n",
      "####\n",
      "been\n",
      "####\n",
      "explored\n",
      "####\n",
      "Our\n",
      "####\n",
      "work\n",
      "####\n",
      "inspired\n",
      "####\n",
      "multi-modality\n",
      "####\n",
      "models\n",
      "####\n",
      "that\n",
      "####\n",
      "learn\n",
      "####\n",
      "from\n",
      "####\n",
      "multiple\n",
      "####\n",
      "modalities\n",
      "####\n",
      "such\n",
      "####\n",
      "audio\n",
      "####\n",
      "visual\n",
      "####\n",
      "text\n",
      "####\n",
      "data\n",
      "####\n",
      "contrast\n",
      "####\n",
      "these\n",
      "####\n",
      "works\n",
      "####\n",
      "investigate\n",
      "####\n",
      "multi-\n",
      "####\n",
      "source\n",
      "####\n",
      "learning\n",
      "####\n",
      "from\n",
      "####\n",
      "single\n",
      "####\n",
      "modality\n",
      "####\n",
      "which\n",
      "####\n",
      "image\n",
      "####\n",
      "data\n",
      "####\n",
      "pose\n",
      "####\n",
      "estimation\n",
      "####\n",
      "Pictorial\n",
      "####\n",
      "structure\n",
      "####\n",
      "model\n",
      "####\n",
      "for\n",
      "####\n",
      "pose\n",
      "####\n",
      "estimation\n",
      "####\n",
      "The\n",
      "####\n",
      "model\n",
      "####\n",
      "introduced\n",
      "####\n",
      "this\n",
      "####\n",
      "section\n",
      "####\n",
      "used\n",
      "####\n",
      "provide\n",
      "####\n",
      "our\n",
      "####\n",
      "deep\n",
      "####\n",
      "model\n",
      "####\n",
      "with\n",
      "####\n",
      "information\n",
      "####\n",
      "sources\n",
      "####\n",
      "Pictorial\n",
      "####\n",
      "struc-\n",
      "####\n",
      "ture\n",
      "####\n",
      "model\n",
      "####\n",
      "considers\n",
      "####\n",
      "human\n",
      "####\n",
      "body\n",
      "####\n",
      "parts\n",
      "####\n",
      "nodes\n",
      "####\n",
      "tied\n",
      "####\n",
      "to-\n",
      "####\n",
      "gether\n",
      "####\n",
      "conditional\n",
      "####\n",
      "random\n",
      "####\n",
      "ﬁeld\n",
      "####\n",
      "Let\n",
      "####\n",
      "for\n",
      "####\n",
      "the\n",
      "####\n",
      "conﬁguration\n",
      "####\n",
      "the\n",
      "####\n",
      "pth\n",
      "####\n",
      "part\n",
      "####\n",
      "The\n",
      "####\n",
      "posterior\n",
      "####\n",
      "con-\n",
      "####\n",
      "ﬁguration\n",
      "####\n",
      "parts\n",
      "####\n",
      "lp|p\n",
      "####\n",
      "given\n",
      "####\n",
      "image\n",
      "####\n",
      "L|I\n",
      "####\n",
      "exp\n",
      "####\n",
      "cid:0\n",
      "####\n",
      "cid:88\n",
      "####\n",
      "cid:88\n",
      "####\n",
      "cid:1\n",
      "####\n",
      "I|lp\n",
      "####\n",
      "p=1\n",
      "####\n",
      "the\n",
      "####\n",
      "pair-wise\n",
      "####\n",
      "term\n",
      "####\n",
      "that\n",
      "####\n",
      "models\n",
      "####\n",
      "the\n",
      "####\n",
      "geometric\n",
      "####\n",
      "deformation\n",
      "####\n",
      "constraint\n",
      "####\n",
      "the\n",
      "####\n",
      "pth\n",
      "####\n",
      "and\n",
      "####\n",
      "qth\n",
      "####\n",
      "parts\n",
      "####\n",
      "for\n",
      "####\n",
      "exam-\n",
      "####\n",
      "ple\n",
      "####\n",
      "head\n",
      "####\n",
      "shall\n",
      "####\n",
      "not\n",
      "####\n",
      "too\n",
      "####\n",
      "far\n",
      "####\n",
      "from\n",
      "####\n",
      "torso\n",
      "####\n",
      "The\n",
      "####\n",
      "edge\n",
      "####\n",
      "set\n",
      "####\n",
      "de-\n",
      "####\n",
      "noted\n",
      "####\n",
      "arranged\n",
      "####\n",
      "tree\n",
      "####\n",
      "models\n",
      "####\n",
      "loopy\n",
      "####\n",
      "models\n",
      "####\n",
      "I|lp\n",
      "####\n",
      "the\n",
      "####\n",
      "unary\n",
      "####\n",
      "term\n",
      "####\n",
      "that\n",
      "####\n",
      "models\n",
      "####\n",
      "the\n",
      "####\n",
      "appearance\n",
      "####\n",
      "The\n",
      "####\n",
      "appearance\n",
      "####\n",
      "varies\n",
      "####\n",
      "body\n",
      "####\n",
      "articulates\n",
      "####\n",
      "model\n",
      "####\n",
      "this\n",
      "####\n",
      "variation\n",
      "####\n",
      "and\n",
      "####\n",
      "I|lp\n",
      "####\n",
      "speciﬁes\n",
      "####\n",
      "the\n",
      "####\n",
      "part\n",
      "####\n",
      "appearance\n",
      "####\n",
      "warped\n",
      "####\n",
      "size\n",
      "####\n",
      "orientation\n",
      "####\n",
      "loca-\n",
      "####\n",
      "tion\n",
      "####\n",
      "Alternatively\n",
      "####\n",
      "Yang\n",
      "####\n",
      "and\n",
      "####\n",
      "Ramanan\n",
      "####\n",
      "propose\n",
      "####\n",
      "use\n",
      "####\n",
      "appearance\n",
      "####\n",
      "mixture\n",
      "####\n",
      "type\n",
      "####\n",
      "for\n",
      "####\n",
      "approximat-\n",
      "####\n",
      "ing\n",
      "####\n",
      "the\n",
      "####\n",
      "variation\n",
      "####\n",
      "rotation\n",
      "####\n",
      "and\n",
      "####\n",
      "size\n",
      "####\n",
      "this\n",
      "####\n",
      "model\n",
      "####\n",
      "and\n",
      "####\n",
      "I|lp\n",
      "####\n",
      "speciﬁes\n",
      "####\n",
      "the\n",
      "####\n",
      "part\n",
      "####\n",
      "appear-\n",
      "####\n",
      "ance\n",
      "####\n",
      "with\n",
      "####\n",
      "mixture\n",
      "####\n",
      "type\n",
      "####\n",
      "location\n",
      "####\n",
      "The\n",
      "####\n",
      "appearance\n",
      "####\n",
      "part\n",
      "####\n",
      "clustered\n",
      "####\n",
      "into\n",
      "####\n",
      "multiple\n",
      "####\n",
      "appearance\n",
      "####\n",
      "mixture\n",
      "####\n",
      "types\n",
      "####\n",
      "shown\n",
      "####\n",
      "Fig\n",
      "####\n",
      "The\n",
      "####\n",
      "overall\n",
      "####\n",
      "model\n",
      "####\n",
      "follows\n",
      "####\n",
      "L|I\n",
      "####\n",
      "exp\n",
      "####\n",
      "cid:0\n",
      "####\n",
      "cid:1\n",
      "####\n",
      "cid:88\n",
      "####\n",
      "cid:88\n",
      "####\n",
      "cid:88\n",
      "####\n",
      "btp\n",
      "####\n",
      "btp\n",
      "####\n",
      "where\n",
      "####\n",
      "cid:88\n",
      "####\n",
      "wtp\n",
      "####\n",
      "wtp\n",
      "####\n",
      "compatibility/co-occurrence\n",
      "####\n",
      "mixture\n",
      "####\n",
      "types\n",
      "####\n",
      "the\n",
      "####\n",
      "pair-wise\n",
      "####\n",
      "compatibility\n",
      "####\n",
      "term\n",
      "####\n",
      "that\n",
      "####\n",
      "models\n",
      "####\n",
      "the\n",
      "####\n",
      "the\n",
      "####\n",
      "pair-wise\n",
      "####\n",
      "deformation\n",
      "####\n",
      "term\n",
      "####\n",
      "that\n",
      "####\n",
      "mod-\n",
      "####\n",
      "els\n",
      "####\n",
      "the\n",
      "####\n",
      "geometric\n",
      "####\n",
      "deformation\n",
      "####\n",
      "constraints\n",
      "####\n",
      "the\n",
      "####\n",
      "pth\n",
      "####\n",
      "and\n",
      "####\n",
      "qth\n",
      "####\n",
      "parts\n",
      "####\n",
      "dx2dy2\n",
      "####\n",
      "the\n",
      "####\n",
      "unary\n",
      "####\n",
      "appearance\n",
      "####\n",
      "term\n",
      "####\n",
      "that\n",
      "####\n",
      "computes\n",
      "####\n",
      "location\n",
      "####\n",
      "the\n",
      "####\n",
      "the\n",
      "####\n",
      "score\n",
      "####\n",
      "placing\n",
      "####\n",
      "template\n",
      "####\n",
      "wtp\n",
      "####\n",
      "HOG\n",
      "####\n",
      "feature\n",
      "####\n",
      "map\n",
      "####\n",
      "for\n",
      "####\n",
      "image\n",
      "####\n",
      "denoted\n",
      "####\n",
      "wtp\n",
      "####\n",
      "Linear\n",
      "####\n",
      "SVM\n",
      "####\n",
      "used\n",
      "####\n",
      "learn\n",
      "####\n",
      "the\n",
      "####\n",
      "linear\n",
      "####\n",
      "weights\n",
      "####\n",
      "wtp\n",
      "####\n",
      "and\n",
      "####\n",
      "compatibility\n",
      "####\n",
      "biases\n",
      "####\n",
      "btp\n",
      "####\n",
      "The\n",
      "####\n",
      "model\n",
      "####\n",
      "used\n",
      "####\n",
      "many\n",
      "####\n",
      "approaches\n",
      "####\n",
      "with\n",
      "####\n",
      "different\n",
      "####\n",
      "implementations\n",
      "####\n",
      "edge\n",
      "####\n",
      "set\n",
      "####\n",
      "part\n",
      "####\n",
      "size\n",
      "####\n",
      "and\n",
      "####\n",
      "part\n",
      "####\n",
      "locations\n",
      "####\n",
      "btp\n",
      "####\n",
      "The\n",
      "####\n",
      "multi-source\n",
      "####\n",
      "deep\n",
      "####\n",
      "model\n",
      "####\n",
      "overview\n",
      "####\n",
      "our\n",
      "####\n",
      "framework\n",
      "####\n",
      "the\n",
      "####\n",
      "testing\n",
      "####\n",
      "stage\n",
      "####\n",
      "shown\n",
      "####\n",
      "Fig\n",
      "####\n",
      "this\n",
      "####\n",
      "framework\n",
      "####\n",
      "existing\n",
      "####\n",
      "approach\n",
      "####\n",
      "used\n",
      "####\n",
      "generate\n",
      "####\n",
      "candidate\n",
      "####\n",
      "body\n",
      "####\n",
      "locations\n",
      "####\n",
      "with\n",
      "####\n",
      "conserva-\n",
      "####\n",
      "tive\n",
      "####\n",
      "thresholding\n",
      "####\n",
      "the\n",
      "####\n",
      "experiment\n",
      "####\n",
      "the\n",
      "####\n",
      "existing\n",
      "####\n",
      "approach\n",
      "####\n",
      "the\n",
      "####\n",
      "off-the-shelf\n",
      "####\n",
      "approach\n",
      "####\n",
      "multi-source\n",
      "####\n",
      "deep\n",
      "####\n",
      "model\n",
      "####\n",
      "then\n",
      "####\n",
      "applied\n",
      "####\n",
      "candidate\n",
      "####\n",
      "all\n",
      "####\n",
      "body\n",
      "####\n",
      "locations\n",
      "####\n",
      "order\n",
      "####\n",
      "determine\n",
      "####\n",
      "whether\n",
      "####\n",
      "its\n",
      "####\n",
      "body\n",
      "####\n",
      "locations\n",
      "####\n",
      "are\n",
      "####\n",
      "correct\n",
      "####\n",
      "Simultaneously\n",
      "####\n",
      "the\n",
      "####\n",
      "body\n",
      "####\n",
      "locations\n",
      "####\n",
      "this\n",
      "####\n",
      "candidate\n",
      "####\n",
      "esti-\n",
      "####\n",
      "mated\n",
      "####\n",
      "One\n",
      "####\n",
      "direct\n",
      "####\n",
      "approach\n",
      "####\n",
      "with\n",
      "####\n",
      "which\n",
      "####\n",
      "train\n",
      "####\n",
      "multi-source\n",
      "####\n",
      "model\n",
      "####\n",
      "train\n",
      "####\n",
      "deep\n",
      "####\n",
      "model\n",
      "####\n",
      "over\n",
      "####\n",
      "the\n",
      "####\n",
      "concatenated\n",
      "####\n",
      "infor-\n",
      "####\n",
      "mation\n",
      "####\n",
      "sources\n",
      "####\n",
      "shown\n",
      "####\n",
      "Fig\n",
      "####\n",
      "This\n",
      "####\n",
      "approach\n",
      "####\n",
      "lim-\n",
      "####\n",
      "ited\n",
      "####\n",
      "because\n",
      "####\n",
      "information\n",
      "####\n",
      "sources\n",
      "####\n",
      "with\n",
      "####\n",
      "different\n",
      "####\n",
      "statistical\n",
      "####\n",
      "properties\n",
      "####\n",
      "are\n",
      "####\n",
      "mixed\n",
      "####\n",
      "the\n",
      "####\n",
      "ﬁrst\n",
      "####\n",
      "hidden\n",
      "####\n",
      "layer\n",
      "####\n",
      "better\n",
      "####\n",
      "so-\n",
      "####\n",
      "lution\n",
      "####\n",
      "have\n",
      "####\n",
      "their\n",
      "####\n",
      "high-level\n",
      "####\n",
      "representations\n",
      "####\n",
      "constructed\n",
      "####\n",
      "before\n",
      "####\n",
      "they\n",
      "####\n",
      "are\n",
      "####\n",
      "mixed\n",
      "####\n",
      "Therefore\n",
      "####\n",
      "use\n",
      "####\n",
      "the\n",
      "####\n",
      "architecture\n",
      "####\n",
      "shown\n",
      "####\n",
      "Fig\n",
      "####\n",
      "which\n",
      "####\n",
      "each\n",
      "####\n",
      "information\n",
      "####\n",
      "source\n",
      "####\n",
      "Figure\n",
      "####\n",
      "Framework\n",
      "####\n",
      "the\n",
      "####\n",
      "testing\n",
      "####\n",
      "stage\n",
      "####\n",
      "The\n",
      "####\n",
      "existing\n",
      "####\n",
      "approach\n",
      "####\n",
      "used\n",
      "####\n",
      "generate\n",
      "####\n",
      "multiple\n",
      "####\n",
      "candidate\n",
      "####\n",
      "locations\n",
      "####\n",
      "candidate\n",
      "####\n",
      "used\n",
      "####\n",
      "the\n",
      "####\n",
      "input\n",
      "####\n",
      "deep\n",
      "####\n",
      "model\n",
      "####\n",
      "determine\n",
      "####\n",
      "whether\n",
      "####\n",
      "the\n",
      "####\n",
      "candidate\n",
      "####\n",
      "correct\n",
      "####\n",
      "and\n",
      "####\n",
      "estimate\n",
      "####\n",
      "body\n",
      "####\n",
      "locations\n",
      "####\n",
      "Best\n",
      "####\n",
      "viewed\n",
      "####\n",
      "color\n",
      "####\n",
      "Figure\n",
      "####\n",
      "Direct\n",
      "####\n",
      "use\n",
      "####\n",
      "deep\n",
      "####\n",
      "model\n",
      "####\n",
      "and\n",
      "####\n",
      "the\n",
      "####\n",
      "deep\n",
      "####\n",
      "architecture\n",
      "####\n",
      "propose\n",
      "####\n",
      "for\n",
      "####\n",
      "part\n",
      "####\n",
      "score\n",
      "####\n",
      "deformation\n",
      "####\n",
      "and\n",
      "####\n",
      "mixture\n",
      "####\n",
      "type\n",
      "####\n",
      "Best\n",
      "####\n",
      "viewed\n",
      "####\n",
      "color\n",
      "####\n",
      "connected\n",
      "####\n",
      "two\n",
      "####\n",
      "layers\n",
      "####\n",
      "for\n",
      "####\n",
      "constructing\n",
      "####\n",
      "high\n",
      "####\n",
      "level\n",
      "####\n",
      "rep-\n",
      "####\n",
      "resentation\n",
      "####\n",
      "individually\n",
      "####\n",
      "High-level\n",
      "####\n",
      "representations\n",
      "####\n",
      "dif-\n",
      "####\n",
      "ferent\n",
      "####\n",
      "information\n",
      "####\n",
      "sources\n",
      "####\n",
      "are\n",
      "####\n",
      "then\n",
      "####\n",
      "fused\n",
      "####\n",
      "using\n",
      "####\n",
      "other\n",
      "####\n",
      "two\n",
      "####\n",
      "layers\n",
      "####\n",
      "for\n",
      "####\n",
      "pose\n",
      "####\n",
      "estimation\n",
      "####\n",
      "4.1\n",
      "####\n",
      "Inference\n",
      "####\n",
      "The\n",
      "####\n",
      "mixture\n",
      "####\n",
      "type\n",
      "####\n",
      "information\n",
      "####\n",
      "Fig\n",
      "####\n",
      "taken\n",
      "####\n",
      "from\n",
      "####\n",
      "the\n",
      "####\n",
      "The\n",
      "####\n",
      "relative\n",
      "####\n",
      "positions\n",
      "####\n",
      "among\n",
      "####\n",
      "parts\n",
      "####\n",
      "denoted\n",
      "####\n",
      "comes\n",
      "####\n",
      "from\n",
      "####\n",
      "the\n",
      "####\n",
      "deformation\n",
      "####\n",
      "information\n",
      "####\n",
      "The\n",
      "####\n",
      "appearance\n",
      "####\n",
      "scores\n",
      "####\n",
      "denoted\n",
      "####\n",
      "obtained\n",
      "####\n",
      "from\n",
      "####\n",
      "the\n",
      "####\n",
      "unary\n",
      "####\n",
      "appearance\n",
      "####\n",
      "term\n",
      "####\n",
      "our\n",
      "####\n",
      "experiment\n",
      "####\n",
      "and\n",
      "####\n",
      "are\n",
      "####\n",
      "obtained\n",
      "####\n",
      "using\n",
      "####\n",
      "the\n",
      "####\n",
      "approach\n",
      "####\n",
      "the\n",
      "####\n",
      "inference\n",
      "####\n",
      "stage\n",
      "####\n",
      "the\n",
      "####\n",
      "model\n",
      "####\n",
      "follows\n",
      "####\n",
      "h1,1\n",
      "####\n",
      "sTW1,1\n",
      "####\n",
      "b1,1\n",
      "####\n",
      "h1,2\n",
      "####\n",
      "dTW1,2\n",
      "####\n",
      "b1,2\n",
      "####\n",
      "h1,3\n",
      "####\n",
      "tTW1,3\n",
      "####\n",
      "b1,3\n",
      "####\n",
      "h2,1T\n",
      "####\n",
      "h2T\n",
      "####\n",
      "˜ycls\n",
      "####\n",
      "h3T\n",
      "####\n",
      "˜ypst\n",
      "####\n",
      "h3T\n",
      "####\n",
      "Wpst\n",
      "####\n",
      "bpst\n",
      "####\n",
      "wcls\n",
      "####\n",
      "bcls\n",
      "####\n",
      "h2,2T\n",
      "####\n",
      "h2,3T\n",
      "####\n",
      "exp\n",
      "####\n",
      "the\n",
      "####\n",
      "sigmoid\n",
      "####\n",
      "function\n",
      "####\n",
      "the\n",
      "####\n",
      "point-wise\n",
      "####\n",
      "non-linear\n",
      "####\n",
      "activation\n",
      "####\n",
      "function\n",
      "####\n",
      "for\n",
      "####\n",
      "and\n",
      "####\n",
      "wcls\n",
      "####\n",
      "connect\n",
      "####\n",
      "nodes\n",
      "####\n",
      "between\n",
      "####\n",
      "adjacent\n",
      "####\n",
      "layers\n",
      "####\n",
      "and\n",
      "####\n",
      "bcls\n",
      "####\n",
      "are\n",
      "####\n",
      "biases\n",
      "####\n",
      "which\n",
      "####\n",
      "sigmoid\n",
      "####\n",
      "function\n",
      "####\n",
      "can\n",
      "####\n",
      "used\n",
      "####\n",
      "Our\n",
      "####\n",
      "deep\n",
      "####\n",
      "modelExisting\n",
      "####\n",
      "approachInputCandidatesResultsExisting\n",
      "####\n",
      "approachDeep\n",
      "####\n",
      "model\n",
      "####\n",
      "...\n",
      "####\n",
      "...\n",
      "####\n",
      "...\n",
      "####\n",
      "...\n",
      "####\n",
      "h3h1,3\n",
      "####\n",
      "...\n",
      "####\n",
      "...\n",
      "####\n",
      "sh2,3\n",
      "####\n",
      "...\n",
      "####\n",
      "...\n",
      "####\n",
      "...\n",
      "####\n",
      "...\n",
      "####\n",
      "...\n",
      "####\n",
      "...\n",
      "####\n",
      "...\n",
      "####\n",
      "...\n",
      "####\n",
      "...\n",
      "####\n",
      "...\n",
      "####\n",
      "ypstW1\n",
      "####\n",
      "1W1\n",
      "####\n",
      "2W1\n",
      "####\n",
      "3W2\n",
      "####\n",
      "1W2\n",
      "####\n",
      "2W2\n",
      "####\n",
      "3W3Wpstyclswclsh3h1\n",
      "####\n",
      "...\n",
      "####\n",
      "sh2\n",
      "####\n",
      "...\n",
      "####\n",
      "...\n",
      "####\n",
      "...\n",
      "####\n",
      "...\n",
      "####\n",
      "...\n",
      "####\n",
      "...\n",
      "####\n",
      "ypstycls\n",
      "####\n",
      "...\n",
      "####\n",
      "...\n",
      "####\n",
      "...\n",
      "####\n",
      "...\n",
      "####\n",
      "...\n",
      "####\n",
      "...\n",
      "####\n",
      "h1,1h1,2h2,2h2,1\n",
      "####\n",
      "ing\n",
      "####\n",
      "non-linear\n",
      "####\n",
      "representations\n",
      "####\n",
      "from\n",
      "####\n",
      "and\n",
      "####\n",
      "are\n",
      "####\n",
      "hidden\n",
      "####\n",
      "nodes\n",
      "####\n",
      "different\n",
      "####\n",
      "layers\n",
      "####\n",
      "used\n",
      "####\n",
      "for\n",
      "####\n",
      "extract-\n",
      "####\n",
      "˜ycls\n",
      "####\n",
      "the\n",
      "####\n",
      "estimated\n",
      "####\n",
      "label\n",
      "####\n",
      "indicating\n",
      "####\n",
      "whether\n",
      "####\n",
      "the\n",
      "####\n",
      "candi-\n",
      "####\n",
      "date\n",
      "####\n",
      "body\n",
      "####\n",
      "locations\n",
      "####\n",
      "correct\n",
      "####\n",
      "For\n",
      "####\n",
      "pose\n",
      "####\n",
      "estimatio\n",
      "####\n",
      "single\n",
      "####\n",
      "human\n",
      "####\n",
      "the\n",
      "####\n",
      "candidate\n",
      "####\n",
      "with\n",
      "####\n",
      "the\n",
      "####\n",
      "largest\n",
      "####\n",
      "˜ycls\n",
      "####\n",
      "used\n",
      "####\n",
      "the\n",
      "####\n",
      "ﬁnal\n",
      "####\n",
      "output\n",
      "####\n",
      "our\n",
      "####\n",
      "experiments\n",
      "####\n",
      "˜ypst\n",
      "####\n",
      "contains\n",
      "####\n",
      "the\n",
      "####\n",
      "estimated\n",
      "####\n",
      "part\n",
      "####\n",
      "locations\n",
      "####\n",
      "Through\n",
      "####\n",
      "the\n",
      "####\n",
      "ﬁrst\n",
      "####\n",
      "two\n",
      "####\n",
      "separate\n",
      "####\n",
      "layers\n",
      "####\n",
      "each\n",
      "####\n",
      "information\n",
      "####\n",
      "source\n",
      "####\n",
      "has\n",
      "####\n",
      "its\n",
      "####\n",
      "individual\n",
      "####\n",
      "representation\n",
      "####\n",
      "con-\n",
      "####\n",
      "structed\n",
      "####\n",
      "Then\n",
      "####\n",
      "all\n",
      "####\n",
      "high-order\n",
      "####\n",
      "representations\n",
      "####\n",
      "are\n",
      "####\n",
      "combined\n",
      "####\n",
      "two\n",
      "####\n",
      "layers\n",
      "####\n",
      "4.2\n",
      "####\n",
      "Training\n",
      "####\n",
      "method\n",
      "####\n",
      "Denote\n",
      "####\n",
      "the\n",
      "####\n",
      "parameter\n",
      "####\n",
      "set\n",
      "####\n",
      "for\n",
      "####\n",
      "the\n",
      "####\n",
      "model\n",
      "####\n",
      "wcls\n",
      "####\n",
      "bcls\n",
      "####\n",
      "The\n",
      "####\n",
      "objective\n",
      "####\n",
      "function\n",
      "####\n",
      "for\n",
      "####\n",
      "backpropagating\n",
      "####\n",
      "error\n",
      "####\n",
      "derivatives\n",
      "####\n",
      "follows\n",
      "####\n",
      "cid:88\n",
      "####\n",
      "cid:16\n",
      "####\n",
      "ycls\n",
      "####\n",
      "˜ycls\n",
      "####\n",
      "wcls\n",
      "####\n",
      "ycls\n",
      "####\n",
      "ypst\n",
      "####\n",
      "˜ypst\n",
      "####\n",
      "log\n",
      "####\n",
      "˜ycls\n",
      "####\n",
      "˜ycls\n",
      "####\n",
      "˜ypst\n",
      "####\n",
      "ycls\n",
      "####\n",
      "ypst\n",
      "####\n",
      "wcls\n",
      "####\n",
      "ycls\n",
      "####\n",
      "ycls\n",
      "####\n",
      "log\n",
      "####\n",
      "˜ycls\n",
      "####\n",
      "cid:88\n",
      "####\n",
      "cid:88\n",
      "####\n",
      "˜ypst\n",
      "####\n",
      "=||ypst\n",
      "####\n",
      "||2\n",
      "####\n",
      "|w∗\n",
      "####\n",
      "|wcls\n",
      "####\n",
      "cid:17\n",
      "####\n",
      "where\n",
      "####\n",
      "denotes\n",
      "####\n",
      "the\n",
      "####\n",
      "nth\n",
      "####\n",
      "sample\n",
      "####\n",
      "˜ycls\n",
      "####\n",
      "are\n",
      "####\n",
      "computed\n",
      "####\n",
      "using\n",
      "####\n",
      "ycls\n",
      "####\n",
      "and\n",
      "####\n",
      "˜ypst\n",
      "####\n",
      "the\n",
      "####\n",
      "ground\n",
      "####\n",
      "truth\n",
      "####\n",
      "classiﬁcation\n",
      "####\n",
      "label\n",
      "####\n",
      "in-\n",
      "####\n",
      "dicating\n",
      "####\n",
      "whether\n",
      "####\n",
      "the\n",
      "####\n",
      "current\n",
      "####\n",
      "body\n",
      "####\n",
      "location\n",
      "####\n",
      "estimation\n",
      "####\n",
      "correct\n",
      "####\n",
      "not\n",
      "####\n",
      "Positive\n",
      "####\n",
      "training\n",
      "####\n",
      "samples\n",
      "####\n",
      "have\n",
      "####\n",
      "their\n",
      "####\n",
      "part\n",
      "####\n",
      "templates\n",
      "####\n",
      "placed\n",
      "####\n",
      "around\n",
      "####\n",
      "annotated\n",
      "####\n",
      "body\n",
      "####\n",
      "locations\n",
      "####\n",
      "negative\n",
      "####\n",
      "training\n",
      "####\n",
      "samples\n",
      "####\n",
      "have\n",
      "####\n",
      "their\n",
      "####\n",
      "part\n",
      "####\n",
      "templates\n",
      "####\n",
      "placed\n",
      "####\n",
      "images\n",
      "####\n",
      "without\n",
      "####\n",
      "human\n",
      "####\n",
      "Therefore\n",
      "####\n",
      "ycls\n",
      "####\n",
      "can\n",
      "####\n",
      "used\n",
      "####\n",
      "for\n",
      "####\n",
      "human\n",
      "####\n",
      "detection\n",
      "####\n",
      "considering\n",
      "####\n",
      "indi-\n",
      "####\n",
      "cator\n",
      "####\n",
      "whether\n",
      "####\n",
      "the\n",
      "####\n",
      "rectangle\n",
      "####\n",
      "covering\n",
      "####\n",
      "body\n",
      "####\n",
      "locations\n",
      "####\n",
      "contains\n",
      "####\n",
      "human\n",
      "####\n",
      "ycls\n",
      "####\n",
      "ypst\n",
      "####\n",
      "ypst\n",
      "####\n",
      "˜ycls\n",
      "####\n",
      "the\n",
      "####\n",
      "cross-entropy\n",
      "####\n",
      "error\n",
      "####\n",
      "classiﬁcation\n",
      "####\n",
      "contains\n",
      "####\n",
      "the\n",
      "####\n",
      "ground\n",
      "####\n",
      "truth\n",
      "####\n",
      "body\n",
      "####\n",
      "locations\n",
      "####\n",
      "˜ypst\n",
      "####\n",
      "the\n",
      "####\n",
      "sum\n",
      "####\n",
      "square\n",
      "####\n",
      "error\n",
      "####\n",
      "body\n",
      "####\n",
      "loca-\n",
      "####\n",
      "tion\n",
      "####\n",
      "estimation\n",
      "####\n",
      "Since\n",
      "####\n",
      "negative\n",
      "####\n",
      "background\n",
      "####\n",
      "samples\n",
      "####\n",
      "not\n",
      "####\n",
      "have\n",
      "####\n",
      "ground\n",
      "####\n",
      "truth\n",
      "####\n",
      "body\n",
      "####\n",
      "location\n",
      "####\n",
      "the\n",
      "####\n",
      "ycls\n",
      "####\n",
      "multi-\n",
      "####\n",
      "plied\n",
      "####\n",
      "ensure\n",
      "####\n",
      "that\n",
      "####\n",
      "only\n",
      "####\n",
      "positive\n",
      "####\n",
      "samples\n",
      "####\n",
      "are\n",
      "####\n",
      "used\n",
      "####\n",
      "learn\n",
      "####\n",
      "location\n",
      "####\n",
      "estimation\n",
      "####\n",
      "wcls\n",
      "####\n",
      "the\n",
      "####\n",
      "norm\n",
      "####\n",
      "regularization\n",
      "####\n",
      "term\n",
      "####\n",
      "the\n",
      "####\n",
      "element\n",
      "####\n",
      "and\n",
      "####\n",
      "wcls\n",
      "####\n",
      "the\n",
      "####\n",
      "ith\n",
      "####\n",
      "and\n",
      "####\n",
      "element\n",
      "####\n",
      "wcls\n",
      "####\n",
      "The\n",
      "####\n",
      "information\n",
      "####\n",
      "sources\n",
      "####\n",
      "and\n",
      "####\n",
      "hidden\n",
      "####\n",
      "nodes\n",
      "####\n",
      "may\n",
      "####\n",
      "have\n",
      "####\n",
      "different\n",
      "####\n",
      "purpose\n",
      "####\n",
      "For\n",
      "####\n",
      "example\n",
      "####\n",
      "node\n",
      "####\n",
      "may\n",
      "####\n",
      "use\n",
      "####\n",
      "the\n",
      "####\n",
      "information\n",
      "####\n",
      "source\n",
      "####\n",
      "mixture\n",
      "####\n",
      "type\n",
      "####\n",
      "Hence\n",
      "####\n",
      "wcls\n",
      "####\n",
      "used\n",
      "####\n",
      "encourage\n",
      "####\n",
      "sparsity\n",
      "####\n",
      "the\n",
      "####\n",
      "weights\n",
      "####\n",
      "Body\n",
      "####\n",
      "part\n",
      "####\n",
      "location\n",
      "####\n",
      "estimation\n",
      "####\n",
      "and\n",
      "####\n",
      "human\n",
      "####\n",
      "detection\n",
      "####\n",
      "are\n",
      "####\n",
      "both\n",
      "####\n",
      "learned\n",
      "####\n",
      "through\n",
      "####\n",
      "shared\n",
      "####\n",
      "representation\n",
      "####\n",
      "this\n",
      "####\n",
      "model\n",
      "####\n",
      "They\n",
      "####\n",
      "are\n",
      "####\n",
      "jointly\n",
      "####\n",
      "learned\n",
      "####\n",
      "because\n",
      "####\n",
      "they\n",
      "####\n",
      "are\n",
      "####\n",
      "dependent\n",
      "####\n",
      "tasks\n",
      "####\n",
      "4.3\n",
      "####\n",
      "Analysis\n",
      "####\n",
      "The\n",
      "####\n",
      "mixture\n",
      "####\n",
      "type\n",
      "####\n",
      "used\n",
      "####\n",
      "example\n",
      "####\n",
      "for\n",
      "####\n",
      "analysis\n",
      "####\n",
      "the\n",
      "####\n",
      "layer-wise\n",
      "####\n",
      "pre-training\n",
      "####\n",
      "stage\n",
      "####\n",
      "and\n",
      "####\n",
      "hidden\n",
      "####\n",
      "vector\n",
      "####\n",
      "h1,3\n",
      "####\n",
      "are\n",
      "####\n",
      "considered\n",
      "####\n",
      "restricted\n",
      "####\n",
      "Boltzmann\n",
      "####\n",
      "machine\n",
      "####\n",
      "with\n",
      "####\n",
      "the\n",
      "####\n",
      "following\n",
      "####\n",
      "distribution\n",
      "####\n",
      "h1,3\n",
      "####\n",
      "exp\n",
      "####\n",
      "tTW1,3h1,3\n",
      "####\n",
      "b1,3T\n",
      "####\n",
      "h1,3\n",
      "####\n",
      "cTt\n",
      "####\n",
      "Denote\n",
      "####\n",
      "the\n",
      "####\n",
      "jth\n",
      "####\n",
      "column\n",
      "####\n",
      "W1,3\n",
      "####\n",
      "w1,3∗\n",
      "####\n",
      "Denote\n",
      "####\n",
      "the\n",
      "####\n",
      "jth\n",
      "####\n",
      "element\n",
      "####\n",
      "b1,3\n",
      "####\n",
      "The\n",
      "####\n",
      "marginal\n",
      "####\n",
      "distribution\n",
      "####\n",
      "can\n",
      "####\n",
      "obtained\n",
      "####\n",
      "follows\n",
      "####\n",
      "h1,3\n",
      "####\n",
      "cid:88\n",
      "####\n",
      "cid:88\n",
      "####\n",
      "h1,3\n",
      "####\n",
      "h1,3\n",
      "####\n",
      "exp\n",
      "####\n",
      "cTt\n",
      "####\n",
      "cid:89\n",
      "####\n",
      "cid:16\n",
      "####\n",
      "exp\n",
      "####\n",
      "tTW1,3h1,3\n",
      "####\n",
      "b1,3T\n",
      "####\n",
      "h1,3\n",
      "####\n",
      "cTt\n",
      "####\n",
      "exp\n",
      "####\n",
      "tTw1,3∗\n",
      "####\n",
      "cid:17\n",
      "####\n",
      "cid:89\n",
      "####\n",
      "where\n",
      "####\n",
      "exp\n",
      "####\n",
      "tTw1,3∗\n",
      "####\n",
      "and\n",
      "####\n",
      "fully\n",
      "####\n",
      "connected\n",
      "####\n",
      "graphical\n",
      "####\n",
      "model\n",
      "####\n",
      "because\n",
      "####\n",
      "can\n",
      "####\n",
      "not\n",
      "####\n",
      "factorized\n",
      "####\n",
      "can\n",
      "####\n",
      "considered\n",
      "####\n",
      "factor\n",
      "####\n",
      "that\n",
      "####\n",
      "explains\n",
      "####\n",
      "fac-\n",
      "####\n",
      "tor\n",
      "####\n",
      "graph\n",
      "####\n",
      "pose\n",
      "####\n",
      "estimation\n",
      "####\n",
      "can\n",
      "####\n",
      "con-\n",
      "####\n",
      "sidered\n",
      "####\n",
      "global\n",
      "####\n",
      "pattern\n",
      "####\n",
      "explaining\n",
      "####\n",
      "the\n",
      "####\n",
      "mixture\n",
      "####\n",
      "type\n",
      "####\n",
      "for\n",
      "####\n",
      "all\n",
      "####\n",
      "parts\n",
      "####\n",
      "both\n",
      "####\n",
      "training\n",
      "####\n",
      "and\n",
      "####\n",
      "inference\n",
      "####\n",
      "stages\n",
      "####\n",
      "every\n",
      "####\n",
      "node\n",
      "####\n",
      "h1,3\n",
      "####\n",
      "connected\n",
      "####\n",
      "the\n",
      "####\n",
      "mixture\n",
      "####\n",
      "types\n",
      "####\n",
      "all\n",
      "####\n",
      "parts\n",
      "####\n",
      "Therefore\n",
      "####\n",
      "h1,3\n",
      "####\n",
      "nonlinearly\n",
      "####\n",
      "extracts\n",
      "####\n",
      "the\n",
      "####\n",
      "global\n",
      "####\n",
      "representa-\n",
      "####\n",
      "tion\n",
      "####\n",
      "from\n",
      "####\n",
      "t1,3\n",
      "####\n",
      "Similarly\n",
      "####\n",
      "the\n",
      "####\n",
      "h2,3\n",
      "####\n",
      "extracts\n",
      "####\n",
      "higher-level\n",
      "####\n",
      "rep-\n",
      "####\n",
      "resentation\n",
      "####\n",
      "from\n",
      "####\n",
      "h1,3\n",
      "####\n",
      "Therefore\n",
      "####\n",
      "the\n",
      "####\n",
      "stack\n",
      "####\n",
      "hidden\n",
      "####\n",
      "layers\n",
      "####\n",
      "extracts\n",
      "####\n",
      "global\n",
      "####\n",
      "high-level\n",
      "####\n",
      "representation\n",
      "####\n",
      "from\n",
      "####\n",
      "the\n",
      "####\n",
      "informa-\n",
      "####\n",
      "tion\n",
      "####\n",
      "source\n",
      "####\n",
      "The\n",
      "####\n",
      "analysis\n",
      "####\n",
      "mixture\n",
      "####\n",
      "type\n",
      "####\n",
      "applica-\n",
      "####\n",
      "ble\n",
      "####\n",
      "deformation\n",
      "####\n",
      "and\n",
      "####\n",
      "appearance\n",
      "####\n",
      "score\n",
      "####\n",
      "shown\n",
      "####\n",
      "Fig\n",
      "####\n",
      "h2,3\n",
      "####\n",
      "captures\n",
      "####\n",
      "the\n",
      "####\n",
      "global\n",
      "####\n",
      "articulation\n",
      "####\n",
      "patterns\n",
      "####\n",
      "human\n",
      "####\n",
      "body\n",
      "####\n",
      "One\n",
      "####\n",
      "the\n",
      "####\n",
      "nodes\n",
      "####\n",
      "h2,3\n",
      "####\n",
      "has\n",
      "####\n",
      "high\n",
      "####\n",
      "response\n",
      "####\n",
      "people\n",
      "####\n",
      "squat\n",
      "####\n",
      "Another\n",
      "####\n",
      "node\n",
      "####\n",
      "has\n",
      "####\n",
      "high\n",
      "####\n",
      "response\n",
      "####\n",
      "people\n",
      "####\n",
      "standing\n",
      "####\n",
      "upright\n",
      "####\n",
      "Yet\n",
      "####\n",
      "another\n",
      "####\n",
      "node\n",
      "####\n",
      "concisely\n",
      "####\n",
      "captures\n",
      "####\n",
      "two\n",
      "####\n",
      "clusters\n",
      "####\n",
      "pose\n",
      "####\n",
      "patterns\n",
      "####\n",
      "our\n",
      "####\n",
      "deep\n",
      "####\n",
      "model\n",
      "####\n",
      "the\n",
      "####\n",
      "ﬁrst\n",
      "####\n",
      "hidden\n",
      "####\n",
      "layer\n",
      "####\n",
      "has\n",
      "####\n",
      "hidden\n",
      "####\n",
      "nodes\n",
      "####\n",
      "the\n",
      "####\n",
      "second\n",
      "####\n",
      "layer\n",
      "####\n",
      "i.e\n",
      "####\n",
      "has\n",
      "####\n",
      "hidden\n",
      "####\n",
      "nodes\n",
      "####\n",
      "and\n",
      "####\n",
      "the\n",
      "####\n",
      "third\n",
      "####\n",
      "layer\n",
      "####\n",
      "i.e\n",
      "####\n",
      "has\n",
      "####\n",
      "hidden\n",
      "####\n",
      "nodes\n",
      "####\n",
      "Since\n",
      "####\n",
      "the\n",
      "####\n",
      "dimensions\n",
      "####\n",
      "and\n",
      "####\n",
      "are\n",
      "####\n",
      "small\n",
      "####\n",
      "train-\n",
      "####\n",
      "ing\n",
      "####\n",
      "the\n",
      "####\n",
      "deep\n",
      "####\n",
      "model\n",
      "####\n",
      "fast\n",
      "####\n",
      "Unlike\n",
      "####\n",
      "loopy\n",
      "####\n",
      "graphical\n",
      "####\n",
      "mod-\n",
      "####\n",
      "els\n",
      "####\n",
      "the\n",
      "####\n",
      "deep\n",
      "####\n",
      "model\n",
      "####\n",
      "fast\n",
      "####\n",
      "the\n",
      "####\n",
      "inference\n",
      "####\n",
      "stage\n",
      "####\n",
      "because\n",
      "####\n",
      "does\n",
      "####\n",
      "not\n",
      "####\n",
      "require\n",
      "####\n",
      "loopy\n",
      "####\n",
      "belief\n",
      "####\n",
      "propagation\n",
      "####\n",
      "sampling\n",
      "####\n",
      "The\n",
      "####\n",
      "extra\n",
      "####\n",
      "testing\n",
      "####\n",
      "time\n",
      "####\n",
      "required\n",
      "####\n",
      "our\n",
      "####\n",
      "deep\n",
      "####\n",
      "model\n",
      "####\n",
      "less\n",
      "####\n",
      "than\n",
      "####\n",
      "percent\n",
      "####\n",
      "the\n",
      "####\n",
      "testing\n",
      "####\n",
      "time\n",
      "####\n",
      "required\n",
      "####\n",
      "the\n",
      "####\n",
      "approach\n",
      "####\n",
      "Experimental\n",
      "####\n",
      "results\n",
      "####\n",
      "The\n",
      "####\n",
      "proposed\n",
      "####\n",
      "approach\n",
      "####\n",
      "evaluated\n",
      "####\n",
      "three\n",
      "####\n",
      "datasets\n",
      "####\n",
      "LSP\n",
      "####\n",
      "PARSE\n",
      "####\n",
      "and\n",
      "####\n",
      "UIUC\n",
      "####\n",
      "people\n",
      "####\n",
      "The\n",
      "####\n",
      "train-\n",
      "####\n",
      "ing\n",
      "####\n",
      "procedure\n",
      "####\n",
      "and\n",
      "####\n",
      "training\n",
      "####\n",
      "set\n",
      "####\n",
      "are\n",
      "####\n",
      "the\n",
      "####\n",
      "same\n",
      "####\n",
      "Positive\n",
      "####\n",
      "pose\n",
      "####\n",
      "estimation\n",
      "####\n",
      "for\n",
      "####\n",
      "future\n",
      "####\n",
      "applications\n",
      "####\n",
      "For\n",
      "####\n",
      "example\n",
      "####\n",
      "character\n",
      "####\n",
      "animation\n",
      "####\n",
      "the\n",
      "####\n",
      "rendering\n",
      "####\n",
      "limb\n",
      "####\n",
      "possible\n",
      "####\n",
      "only\n",
      "####\n",
      "when\n",
      "####\n",
      "both\n",
      "####\n",
      "end\n",
      "####\n",
      "points\n",
      "####\n",
      "the\n",
      "####\n",
      "limb\n",
      "####\n",
      "are\n",
      "####\n",
      "correct\n",
      "####\n",
      "follow\n",
      "####\n",
      "and\n",
      "####\n",
      "use\n",
      "####\n",
      "the\n",
      "####\n",
      "observer-centric\n",
      "####\n",
      "anno-\n",
      "####\n",
      "tations\n",
      "####\n",
      "for\n",
      "####\n",
      "all\n",
      "####\n",
      "approaches\n",
      "####\n",
      "when\n",
      "####\n",
      "evaluate\n",
      "####\n",
      "the\n",
      "####\n",
      "LSP\n",
      "####\n",
      "dataset\n",
      "####\n",
      "5.2\n",
      "####\n",
      "Overall\n",
      "####\n",
      "experimental\n",
      "####\n",
      "results\n",
      "####\n",
      "Table\n",
      "####\n",
      "shows\n",
      "####\n",
      "the\n",
      "####\n",
      "experimental\n",
      "####\n",
      "results\n",
      "####\n",
      "from\n",
      "####\n",
      "the\n",
      "####\n",
      "three\n",
      "####\n",
      "datasets\n",
      "####\n",
      "Pishchulin’s\n",
      "####\n",
      "approach\n",
      "####\n",
      "used\n",
      "####\n",
      "the\n",
      "####\n",
      "LSP+PARSE\n",
      "####\n",
      "training\n",
      "####\n",
      "set\n",
      "####\n",
      "when\n",
      "####\n",
      "evaluated\n",
      "####\n",
      "the\n",
      "####\n",
      "PARSE\n",
      "####\n",
      "dataset\n",
      "####\n",
      "and\n",
      "####\n",
      "used\n",
      "####\n",
      "the\n",
      "####\n",
      "UIUC+LSP\n",
      "####\n",
      "training\n",
      "####\n",
      "set\n",
      "####\n",
      "when\n",
      "####\n",
      "evaluated\n",
      "####\n",
      "the\n",
      "####\n",
      "UIUC\n",
      "####\n",
      "dataset\n",
      "####\n",
      "evaluate\n",
      "####\n",
      "the\n",
      "####\n",
      "PARSE\n",
      "####\n",
      "dataset\n",
      "####\n",
      "Pishchulin’s\n",
      "####\n",
      "approach\n",
      "####\n",
      "included\n",
      "####\n",
      "LSP+PARSE\n",
      "####\n",
      "and\n",
      "####\n",
      "ex-\n",
      "####\n",
      "tra\n",
      "####\n",
      "animated\n",
      "####\n",
      "samples\n",
      "####\n",
      "for\n",
      "####\n",
      "training\n",
      "####\n",
      "Johnson’s\n",
      "####\n",
      "approach\n",
      "####\n",
      "included\n",
      "####\n",
      "10,000\n",
      "####\n",
      "extra\n",
      "####\n",
      "training\n",
      "####\n",
      "samples\n",
      "####\n",
      "when\n",
      "####\n",
      "evaluated\n",
      "####\n",
      "the\n",
      "####\n",
      "PARSE\n",
      "####\n",
      "dataset\n",
      "####\n",
      "all\n",
      "####\n",
      "experiments\n",
      "####\n",
      "Andriluka’s\n",
      "####\n",
      "ap-\n",
      "####\n",
      "proach\n",
      "####\n",
      "Yang\n",
      "####\n",
      "and\n",
      "####\n",
      "Ramanan’s\n",
      "####\n",
      "approach\n",
      "####\n",
      "and\n",
      "####\n",
      "our\n",
      "####\n",
      "approach\n",
      "####\n",
      "are\n",
      "####\n",
      "trained\n",
      "####\n",
      "the\n",
      "####\n",
      "training\n",
      "####\n",
      "images\n",
      "####\n",
      "the\n",
      "####\n",
      "LSP\n",
      "####\n",
      "dataset\n",
      "####\n",
      "shown\n",
      "####\n",
      "Table\n",
      "####\n",
      "our\n",
      "####\n",
      "deep\n",
      "####\n",
      "model\n",
      "####\n",
      "obviously\n",
      "####\n",
      "improves\n",
      "####\n",
      "the\n",
      "####\n",
      "pose\n",
      "####\n",
      "estimation\n",
      "####\n",
      "accuracy\n",
      "####\n",
      "and\n",
      "####\n",
      "outperforms\n",
      "####\n",
      "all\n",
      "####\n",
      "the\n",
      "####\n",
      "state-\n",
      "####\n",
      "of-the-art\n",
      "####\n",
      "these\n",
      "####\n",
      "three\n",
      "####\n",
      "datasets\n",
      "####\n",
      "Speciﬁcally\n",
      "####\n",
      "our\n",
      "####\n",
      "approach\n",
      "####\n",
      "better\n",
      "####\n",
      "detecting\n",
      "####\n",
      "legs\n",
      "####\n",
      "arms\n",
      "####\n",
      "and\n",
      "####\n",
      "head\n",
      "####\n",
      "compared\n",
      "####\n",
      "with\n",
      "####\n",
      "ex-\n",
      "####\n",
      "isting\n",
      "####\n",
      "approaches\n",
      "####\n",
      "The\n",
      "####\n",
      "approach\n",
      "####\n",
      "Pishchulin\n",
      "####\n",
      "better\n",
      "####\n",
      "than\n",
      "####\n",
      "our\n",
      "####\n",
      "approach\n",
      "####\n",
      "locating\n",
      "####\n",
      "torso\n",
      "####\n",
      "possibly\n",
      "####\n",
      "because\n",
      "####\n",
      "the\n",
      "####\n",
      "torso\n",
      "####\n",
      "region\n",
      "####\n",
      "included\n",
      "####\n",
      "many\n",
      "####\n",
      "poslets\n",
      "####\n",
      "which\n",
      "####\n",
      "helps\n",
      "####\n",
      "increase\n",
      "####\n",
      "the\n",
      "####\n",
      "accuracy\n",
      "####\n",
      "their\n",
      "####\n",
      "approach\n",
      "####\n",
      "locating\n",
      "####\n",
      "torso\n",
      "####\n",
      "Our\n",
      "####\n",
      "approach\n",
      "####\n",
      "complementary\n",
      "####\n",
      "existing\n",
      "####\n",
      "approaches\n",
      "####\n",
      "because\n",
      "####\n",
      "the\n",
      "####\n",
      "information\n",
      "####\n",
      "sources\n",
      "####\n",
      "provided\n",
      "####\n",
      "these\n",
      "####\n",
      "ap-\n",
      "####\n",
      "proaches\n",
      "####\n",
      "can\n",
      "####\n",
      "used\n",
      "####\n",
      "our\n",
      "####\n",
      "model\n",
      "####\n",
      "improve\n",
      "####\n",
      "their\n",
      "####\n",
      "re-\n",
      "####\n",
      "sults\n",
      "####\n",
      "Currently\n",
      "####\n",
      "our\n",
      "####\n",
      "model\n",
      "####\n",
      "uses\n",
      "####\n",
      "the\n",
      "####\n",
      "approach\n",
      "####\n",
      "obtain\n",
      "####\n",
      "information\n",
      "####\n",
      "sources\n",
      "####\n",
      "Compared\n",
      "####\n",
      "with\n",
      "####\n",
      "the\n",
      "####\n",
      "approach\n",
      "####\n",
      "our\n",
      "####\n",
      "approach\n",
      "####\n",
      "improves\n",
      "####\n",
      "the\n",
      "####\n",
      "pose\n",
      "####\n",
      "estimation\n",
      "####\n",
      "accuracy\n",
      "####\n",
      "5.8\n",
      "####\n",
      "62.8\n",
      "####\n",
      "vs.\n",
      "####\n",
      "68.6\n",
      "####\n",
      "PCP\n",
      "####\n",
      "7.4\n",
      "####\n",
      "63.6\n",
      "####\n",
      "vs.\n",
      "####\n",
      "71.0\n",
      "####\n",
      "PCP\n",
      "####\n",
      "and\n",
      "####\n",
      "8.6\n",
      "####\n",
      "57.0\n",
      "####\n",
      "vs.\n",
      "####\n",
      "65.6\n",
      "####\n",
      "PCP\n",
      "####\n",
      "respectively\n",
      "####\n",
      "the\n",
      "####\n",
      "LSP\n",
      "####\n",
      "PARSE\n",
      "####\n",
      "and\n",
      "####\n",
      "UIUC\n",
      "####\n",
      "datasets\n",
      "####\n",
      "Fig\n",
      "####\n",
      "shows\n",
      "####\n",
      "the\n",
      "####\n",
      "compar-\n",
      "####\n",
      "ison\n",
      "####\n",
      "between\n",
      "####\n",
      "our\n",
      "####\n",
      "approach\n",
      "####\n",
      "left\n",
      "####\n",
      "and\n",
      "####\n",
      "the\n",
      "####\n",
      "approach\n",
      "####\n",
      "right\n",
      "####\n",
      "5.3\n",
      "####\n",
      "Results\n",
      "####\n",
      "different\n",
      "####\n",
      "designs\n",
      "####\n",
      "deep\n",
      "####\n",
      "models\n",
      "####\n",
      "this\n",
      "####\n",
      "section\n",
      "####\n",
      "evaluate\n",
      "####\n",
      "different\n",
      "####\n",
      "designs\n",
      "####\n",
      "deep\n",
      "####\n",
      "models\n",
      "####\n",
      "Yang\n",
      "####\n",
      "and\n",
      "####\n",
      "Ramanan’s\n",
      "####\n",
      "approach\n",
      "####\n",
      "used\n",
      "####\n",
      "the\n",
      "####\n",
      "baseline\n",
      "####\n",
      "because\n",
      "####\n",
      "this\n",
      "####\n",
      "approach\n",
      "####\n",
      "used\n",
      "####\n",
      "our\n",
      "####\n",
      "model\n",
      "####\n",
      "for\n",
      "####\n",
      "obtaining\n",
      "####\n",
      "information\n",
      "####\n",
      "sources\n",
      "####\n",
      "concise\n",
      "####\n",
      "only\n",
      "####\n",
      "refer\n",
      "####\n",
      "the\n",
      "####\n",
      "PCP\n",
      "####\n",
      "results\n",
      "####\n",
      "the\n",
      "####\n",
      "LSP\n",
      "####\n",
      "dataset\n",
      "####\n",
      "Depth\n",
      "####\n",
      "model\n",
      "####\n",
      "investigated\n",
      "####\n",
      "Table\n",
      "####\n",
      "The\n",
      "####\n",
      "approach\n",
      "####\n",
      "uses\n",
      "####\n",
      "linear-SVM\n",
      "####\n",
      "for\n",
      "####\n",
      "combining\n",
      "####\n",
      "information\n",
      "####\n",
      "sources\n",
      "####\n",
      "also\n",
      "####\n",
      "trained\n",
      "####\n",
      "Kernel-SVM\n",
      "####\n",
      "with\n",
      "####\n",
      "RBF\n",
      "####\n",
      "kernel\n",
      "####\n",
      "for\n",
      "####\n",
      "learn-\n",
      "####\n",
      "ing\n",
      "####\n",
      "non-linear\n",
      "####\n",
      "model\n",
      "####\n",
      "using\n",
      "####\n",
      "the\n",
      "####\n",
      "off-the-shelf\n",
      "####\n",
      "tool\n",
      "####\n",
      "Libsvm\n",
      "####\n",
      "The\n",
      "####\n",
      "difference\n",
      "####\n",
      "PCP\n",
      "####\n",
      "between\n",
      "####\n",
      "Linear\n",
      "####\n",
      "SVM\n",
      "####\n",
      "and\n",
      "####\n",
      "kernel-\n",
      "####\n",
      "SVM\n",
      "####\n",
      "within\n",
      "####\n",
      "62.8\n",
      "####\n",
      "vs.\n",
      "####\n",
      "64.2\n",
      "####\n",
      "LSP\n",
      "####\n",
      "Bengio\n",
      "####\n",
      "Figure\n",
      "####\n",
      "Visualization\n",
      "####\n",
      "mixture-type\n",
      "####\n",
      "patterns\n",
      "####\n",
      "extracted\n",
      "####\n",
      "hid-\n",
      "####\n",
      "den\n",
      "####\n",
      "nodes\n",
      "####\n",
      "h2,3\n",
      "####\n",
      "use\n",
      "####\n",
      "the\n",
      "####\n",
      "approach\n",
      "####\n",
      "and\n",
      "####\n",
      "visualize\n",
      "####\n",
      "train-\n",
      "####\n",
      "ing\n",
      "####\n",
      "samples\n",
      "####\n",
      "with\n",
      "####\n",
      "the\n",
      "####\n",
      "largest\n",
      "####\n",
      "responses\n",
      "####\n",
      "each\n",
      "####\n",
      "hidden\n",
      "####\n",
      "node\n",
      "####\n",
      "Sam-\n",
      "####\n",
      "ples\n",
      "####\n",
      "with\n",
      "####\n",
      "the\n",
      "####\n",
      "highest\n",
      "####\n",
      "responses\n",
      "####\n",
      "are\n",
      "####\n",
      "placed\n",
      "####\n",
      "the\n",
      "####\n",
      "upper-left\n",
      "####\n",
      "corner\n",
      "####\n",
      "Hidden\n",
      "####\n",
      "node\n",
      "####\n",
      "has\n",
      "####\n",
      "high\n",
      "####\n",
      "response\n",
      "####\n",
      "people\n",
      "####\n",
      "squat\n",
      "####\n",
      "Node\n",
      "####\n",
      "has\n",
      "####\n",
      "high\n",
      "####\n",
      "response\n",
      "####\n",
      "standing\n",
      "####\n",
      "people\n",
      "####\n",
      "Node\n",
      "####\n",
      "has\n",
      "####\n",
      "high\n",
      "####\n",
      "response\n",
      "####\n",
      "two\n",
      "####\n",
      "clusters\n",
      "####\n",
      "pose\n",
      "####\n",
      "patterns\n",
      "####\n",
      "Best\n",
      "####\n",
      "viewed\n",
      "####\n",
      "color\n",
      "####\n",
      "training\n",
      "####\n",
      "samples\n",
      "####\n",
      "are\n",
      "####\n",
      "constrained\n",
      "####\n",
      "have\n",
      "####\n",
      "estimated\n",
      "####\n",
      "part\n",
      "####\n",
      "lo-\n",
      "####\n",
      "cations\n",
      "####\n",
      "near\n",
      "####\n",
      "the\n",
      "####\n",
      "ground\n",
      "####\n",
      "truth\n",
      "####\n",
      "Part\n",
      "####\n",
      "the\n",
      "####\n",
      "training\n",
      "####\n",
      "data\n",
      "####\n",
      "used\n",
      "####\n",
      "for\n",
      "####\n",
      "validation\n",
      "####\n",
      "5.1\n",
      "####\n",
      "Evaluation\n",
      "####\n",
      "criteria\n",
      "####\n",
      "all\n",
      "####\n",
      "experiments\n",
      "####\n",
      "use\n",
      "####\n",
      "the\n",
      "####\n",
      "most\n",
      "####\n",
      "popular\n",
      "####\n",
      "criterion\n",
      "####\n",
      "which\n",
      "####\n",
      "the\n",
      "####\n",
      "percentage\n",
      "####\n",
      "correctly\n",
      "####\n",
      "localized\n",
      "####\n",
      "parts\n",
      "####\n",
      "PCP\n",
      "####\n",
      "introduced\n",
      "####\n",
      "stated\n",
      "####\n",
      "the\n",
      "####\n",
      "PCP\n",
      "####\n",
      "scoring\n",
      "####\n",
      "metric\n",
      "####\n",
      "has\n",
      "####\n",
      "been\n",
      "####\n",
      "implemented\n",
      "####\n",
      "different\n",
      "####\n",
      "ways\n",
      "####\n",
      "different\n",
      "####\n",
      "papers\n",
      "####\n",
      "These\n",
      "####\n",
      "differences\n",
      "####\n",
      "have\n",
      "####\n",
      "two\n",
      "####\n",
      "dimensions\n",
      "####\n",
      "There\n",
      "####\n",
      "are\n",
      "####\n",
      "two\n",
      "####\n",
      "ways\n",
      "####\n",
      "compute\n",
      "####\n",
      "the\n",
      "####\n",
      "ﬁnal\n",
      "####\n",
      "PCP\n",
      "####\n",
      "score\n",
      "####\n",
      "across\n",
      "####\n",
      "the\n",
      "####\n",
      "dataset\n",
      "####\n",
      "the\n",
      "####\n",
      "single\n",
      "####\n",
      "way\n",
      "####\n",
      "only\n",
      "####\n",
      "single\n",
      "####\n",
      "candi-\n",
      "####\n",
      "date\n",
      "####\n",
      "given\n",
      "####\n",
      "the\n",
      "####\n",
      "maximum\n",
      "####\n",
      "scoring\n",
      "####\n",
      "candidate\n",
      "####\n",
      "al-\n",
      "####\n",
      "gorithm\n",
      "####\n",
      "for\n",
      "####\n",
      "one\n",
      "####\n",
      "image\n",
      "####\n",
      "used\n",
      "####\n",
      "The\n",
      "####\n",
      "match\n",
      "####\n",
      "way\n",
      "####\n",
      "matches\n",
      "####\n",
      "multiple\n",
      "####\n",
      "candidates\n",
      "####\n",
      "without\n",
      "####\n",
      "penalizing\n",
      "####\n",
      "false\n",
      "####\n",
      "positives\n",
      "####\n",
      "There\n",
      "####\n",
      "are\n",
      "####\n",
      "two\n",
      "####\n",
      "deﬁnitions\n",
      "####\n",
      "correct\n",
      "####\n",
      "part\n",
      "####\n",
      "localization\n",
      "####\n",
      "For\n",
      "####\n",
      "the\n",
      "####\n",
      "deﬁnition\n",
      "####\n",
      "both\n",
      "####\n",
      "requires\n",
      "####\n",
      "both\n",
      "####\n",
      "end\n",
      "####\n",
      "points\n",
      "####\n",
      "part\n",
      "####\n",
      "for\n",
      "####\n",
      "example\n",
      "####\n",
      "end\n",
      "####\n",
      "points\n",
      "####\n",
      "wrist\n",
      "####\n",
      "and\n",
      "####\n",
      "elbow\n",
      "####\n",
      "for\n",
      "####\n",
      "the\n",
      "####\n",
      "part\n",
      "####\n",
      "lower\n",
      "####\n",
      "arm\n",
      "####\n",
      "correct\n",
      "####\n",
      "For\n",
      "####\n",
      "the\n",
      "####\n",
      "deﬁnition\n",
      "####\n",
      "avg\n",
      "####\n",
      "requires\n",
      "####\n",
      "only\n",
      "####\n",
      "the\n",
      "####\n",
      "average\n",
      "####\n",
      "the\n",
      "####\n",
      "endpoints\n",
      "####\n",
      "correct\n",
      "####\n",
      "The\n",
      "####\n",
      "paper\n",
      "####\n",
      "used\n",
      "####\n",
      "‘match+avg’\n",
      "####\n",
      "The\n",
      "####\n",
      "paper\n",
      "####\n",
      "used\n",
      "####\n",
      "‘single+both’\n",
      "####\n",
      "which\n",
      "####\n",
      "the\n",
      "####\n",
      "strictest\n",
      "####\n",
      "case\n",
      "####\n",
      "and\n",
      "####\n",
      "generally\n",
      "####\n",
      "has\n",
      "####\n",
      "lower\n",
      "####\n",
      "PCP\n",
      "####\n",
      "value\n",
      "####\n",
      "The\n",
      "####\n",
      "paper\n",
      "####\n",
      "pro-\n",
      "####\n",
      "vides\n",
      "####\n",
      "results\n",
      "####\n",
      "for\n",
      "####\n",
      "‘match+both’\n",
      "####\n",
      "and\n",
      "####\n",
      "‘match+avg’\n",
      "####\n",
      "follow\n",
      "####\n",
      "and\n",
      "####\n",
      "evaluate\n",
      "####\n",
      "all\n",
      "####\n",
      "approaches\n",
      "####\n",
      "using\n",
      "####\n",
      "the\n",
      "####\n",
      "strictest\n",
      "####\n",
      "‘sin-\n",
      "####\n",
      "gle+both’\n",
      "####\n",
      "criterion\n",
      "####\n",
      "This\n",
      "####\n",
      "used\n",
      "####\n",
      "because\n",
      "####\n",
      "the\n",
      "####\n",
      "following\n",
      "####\n",
      "reasons\n",
      "####\n",
      "For\n",
      "####\n",
      "‘single’\n",
      "####\n",
      "and\n",
      "####\n",
      "‘match’\n",
      "####\n",
      "discussed\n",
      "####\n",
      "the\n",
      "####\n",
      "‘match’\n",
      "####\n",
      "way\n",
      "####\n",
      "gives\n",
      "####\n",
      "unfair\n",
      "####\n",
      "advantage\n",
      "####\n",
      "approaches\n",
      "####\n",
      "that\n",
      "####\n",
      "produce\n",
      "####\n",
      "large\n",
      "####\n",
      "number\n",
      "####\n",
      "candidates\n",
      "####\n",
      "because\n",
      "####\n",
      "mis-\n",
      "####\n",
      "matched\n",
      "####\n",
      "candidates\n",
      "####\n",
      "false\n",
      "####\n",
      "positives\n",
      "####\n",
      "are\n",
      "####\n",
      "not\n",
      "####\n",
      "penalized\n",
      "####\n",
      "For\n",
      "####\n",
      "‘both’\n",
      "####\n",
      "and\n",
      "####\n",
      "‘avg’\n",
      "####\n",
      "‘both’\n",
      "####\n",
      "better\n",
      "####\n",
      "describing\n",
      "####\n",
      "the\n",
      "####\n",
      "orientation\n",
      "####\n",
      "body\n",
      "####\n",
      "parts\n",
      "####\n",
      "and\n",
      "####\n",
      "will\n",
      "####\n",
      "facilitate\n",
      "####\n",
      "the\n",
      "####\n",
      "use\n",
      "####\n",
      "Table\n",
      "####\n",
      "Pose\n",
      "####\n",
      "estimation\n",
      "####\n",
      "results\n",
      "####\n",
      "PCP\n",
      "####\n",
      "LSP\n",
      "####\n",
      "UIUC\n",
      "####\n",
      "people\n",
      "####\n",
      "and\n",
      "####\n",
      "PARSE\n",
      "####\n",
      "Method\n",
      "####\n",
      "Torso\n",
      "####\n",
      "U.leg\n",
      "####\n",
      "L.leg\n",
      "####\n",
      "U.arm\n",
      "####\n",
      "L.arm\n",
      "####\n",
      "head\n",
      "####\n",
      "Total\n",
      "####\n",
      "LSP\n",
      "####\n",
      "Andriluka\n",
      "####\n",
      "80.9\n",
      "####\n",
      "67.1\n",
      "####\n",
      "60.7\n",
      "####\n",
      "46.5\n",
      "####\n",
      "Yang\n",
      "####\n",
      "Ramanan\n",
      "####\n",
      "81.0\n",
      "####\n",
      "69.5\n",
      "####\n",
      "65.9\n",
      "####\n",
      "53.5\n",
      "####\n",
      "Yang\n",
      "####\n",
      "Ramanan\n",
      "####\n",
      "82.9\n",
      "####\n",
      "70.3\n",
      "####\n",
      "67.0\n",
      "####\n",
      "56.0\n",
      "####\n",
      "Pishchulin\n",
      "####\n",
      "87.5\n",
      "####\n",
      "75.7\n",
      "####\n",
      "68.0\n",
      "####\n",
      "54.2\n",
      "####\n",
      "26.4\n",
      "####\n",
      "74.9\n",
      "####\n",
      "55.7\n",
      "####\n",
      "35.8\n",
      "####\n",
      "76.8\n",
      "####\n",
      "60.7\n",
      "####\n",
      "39.8\n",
      "####\n",
      "79.3\n",
      "####\n",
      "62.8\n",
      "####\n",
      "33.9\n",
      "####\n",
      "78.1\n",
      "####\n",
      "62.9\n",
      "####\n",
      "Eichner\n",
      "####\n",
      "Ferrari\n",
      "####\n",
      "Ours\n",
      "####\n",
      "86.2\n",
      "####\n",
      "74.3\n",
      "####\n",
      "69.3\n",
      "####\n",
      "56.5\n",
      "####\n",
      "85.8\n",
      "####\n",
      "76.5\n",
      "####\n",
      "72.2\n",
      "####\n",
      "63.3\n",
      "####\n",
      "37.4\n",
      "####\n",
      "80.1\n",
      "####\n",
      "64.3\n",
      "####\n",
      "46.6\n",
      "####\n",
      "83.1\n",
      "####\n",
      "68.6\n",
      "####\n",
      "PARSE\n",
      "####\n",
      "Andriluka\n",
      "####\n",
      "86.3\n",
      "####\n",
      "66.3\n",
      "####\n",
      "60.0\n",
      "####\n",
      "54.6\n",
      "####\n",
      "Yang\n",
      "####\n",
      "Ramanan\n",
      "####\n",
      "83.4\n",
      "####\n",
      "68.8\n",
      "####\n",
      "60.7\n",
      "####\n",
      "59.8\n",
      "####\n",
      "Yang\n",
      "####\n",
      "Ramanan\n",
      "####\n",
      "82.9\n",
      "####\n",
      "68.8\n",
      "####\n",
      "60.5\n",
      "####\n",
      "63.4\n",
      "####\n",
      "Pishchulin\n",
      "####\n",
      "88.8\n",
      "####\n",
      "77.3\n",
      "####\n",
      "67.1\n",
      "####\n",
      "53.7\n",
      "####\n",
      "Pishchulin\n",
      "####\n",
      "92.2\n",
      "####\n",
      "74.6\n",
      "####\n",
      "63.7\n",
      "####\n",
      "54.9\n",
      "####\n",
      "90.7\n",
      "####\n",
      "80.0\n",
      "####\n",
      "70.0\n",
      "####\n",
      "59.3\n",
      "####\n",
      "Johnson\n",
      "####\n",
      "35.6\n",
      "####\n",
      "72.7\n",
      "####\n",
      "59.2\n",
      "####\n",
      "40.7\n",
      "####\n",
      "83.4\n",
      "####\n",
      "62.7\n",
      "####\n",
      "42.4\n",
      "####\n",
      "82.4\n",
      "####\n",
      "63.6\n",
      "####\n",
      "36.1\n",
      "####\n",
      "73.7\n",
      "####\n",
      "63.1\n",
      "####\n",
      "39.8\n",
      "####\n",
      "70.7\n",
      "####\n",
      "62.9\n",
      "####\n",
      "37.1\n",
      "####\n",
      "77.6\n",
      "####\n",
      "66.1\n",
      "####\n",
      "Everingham\n",
      "####\n",
      "Ours\n",
      "####\n",
      "87.6\n",
      "####\n",
      "74.7\n",
      "####\n",
      "67.1\n",
      "####\n",
      "67.3\n",
      "####\n",
      "89.3\n",
      "####\n",
      "78.0\n",
      "####\n",
      "72.0\n",
      "####\n",
      "67.8\n",
      "####\n",
      "45.8\n",
      "####\n",
      "76.8\n",
      "####\n",
      "67.4\n",
      "####\n",
      "47.8\n",
      "####\n",
      "89.3\n",
      "####\n",
      "71.0\n",
      "####\n",
      "UIUC\n",
      "####\n",
      "People\n",
      "####\n",
      "Andriluka\n",
      "####\n",
      "88.3\n",
      "####\n",
      "64.0\n",
      "####\n",
      "50.6\n",
      "####\n",
      "42.3\n",
      "####\n",
      "Yang\n",
      "####\n",
      "Ramanan\n",
      "####\n",
      "78.1\n",
      "####\n",
      "60.9\n",
      "####\n",
      "53.2\n",
      "####\n",
      "41.3\n",
      "####\n",
      "Yang\n",
      "####\n",
      "Ramanan\n",
      "####\n",
      "81.8\n",
      "####\n",
      "65.0\n",
      "####\n",
      "55.1\n",
      "####\n",
      "46.8\n",
      "####\n",
      "Pishchulin\n",
      "####\n",
      "91.5\n",
      "####\n",
      "66.8\n",
      "####\n",
      "54.7\n",
      "####\n",
      "38.3\n",
      "####\n",
      "86.8\n",
      "####\n",
      "56.3\n",
      "####\n",
      "50.2\n",
      "####\n",
      "30.8\n",
      "####\n",
      "89.1\n",
      "####\n",
      "72.9\n",
      "####\n",
      "62.4\n",
      "####\n",
      "56.3\n",
      "####\n",
      "Wang\n",
      "####\n",
      "Ours\n",
      "####\n",
      "21.3\n",
      "####\n",
      "81.8\n",
      "####\n",
      "52.6\n",
      "####\n",
      "32.2\n",
      "####\n",
      "76.1\n",
      "####\n",
      "53.0\n",
      "####\n",
      "37.7\n",
      "####\n",
      "79.8\n",
      "####\n",
      "57.0\n",
      "####\n",
      "23.9\n",
      "####\n",
      "85.0\n",
      "####\n",
      "54.4\n",
      "####\n",
      "20.3\n",
      "####\n",
      "68.8\n",
      "####\n",
      "47.0\n",
      "####\n",
      "47.6\n",
      "####\n",
      "89.1\n",
      "####\n",
      "65.6\n",
      "####\n",
      "Table\n",
      "####\n",
      "Results\n",
      "####\n",
      "PCP\n",
      "####\n",
      "investigating\n",
      "####\n",
      "model\n",
      "####\n",
      "depth\n",
      "####\n",
      "Method\n",
      "####\n",
      "Torso\n",
      "####\n",
      "U.leg\n",
      "####\n",
      "L.leg\n",
      "####\n",
      "U.arm\n",
      "####\n",
      "L.arm\n",
      "####\n",
      "Head\n",
      "####\n",
      "Total\n",
      "####\n",
      "LSP\n",
      "####\n",
      "82.9\n",
      "####\n",
      "70.3\n",
      "####\n",
      "67.0\n",
      "####\n",
      "Kernel\n",
      "####\n",
      "SVM\n",
      "####\n",
      "81.9\n",
      "####\n",
      "72.2\n",
      "####\n",
      "67.6\n",
      "####\n",
      "hidden\n",
      "####\n",
      "layer\n",
      "####\n",
      "84.9\n",
      "####\n",
      "73.9\n",
      "####\n",
      "69.5\n",
      "####\n",
      "hidden\n",
      "####\n",
      "layers\n",
      "####\n",
      "85.0\n",
      "####\n",
      "74.6\n",
      "####\n",
      "70.7\n",
      "####\n",
      "Ours\n",
      "####\n",
      "85.8\n",
      "####\n",
      "76.5\n",
      "####\n",
      "72.2\n",
      "####\n",
      "PARSE\n",
      "####\n",
      "82.9\n",
      "####\n",
      "68.8\n",
      "####\n",
      "60.5\n",
      "####\n",
      "Kernel\n",
      "####\n",
      "SVM\n",
      "####\n",
      "81.0\n",
      "####\n",
      "67.8\n",
      "####\n",
      "61.2\n",
      "####\n",
      "hidden\n",
      "####\n",
      "layer\n",
      "####\n",
      "84.4\n",
      "####\n",
      "71.2\n",
      "####\n",
      "63.2\n",
      "####\n",
      "hidden\n",
      "####\n",
      "layers\n",
      "####\n",
      "85.9\n",
      "####\n",
      "74.4\n",
      "####\n",
      "68.3\n",
      "####\n",
      "Ours\n",
      "####\n",
      "89.3\n",
      "####\n",
      "78.0\n",
      "####\n",
      "72.0\n",
      "####\n",
      "UIUC\n",
      "####\n",
      "81.8\n",
      "####\n",
      "65.0\n",
      "####\n",
      "55.1\n",
      "####\n",
      "Kernel\n",
      "####\n",
      "SVM\n",
      "####\n",
      "82.2\n",
      "####\n",
      "65.0\n",
      "####\n",
      "54.9\n",
      "####\n",
      "hidden\n",
      "####\n",
      "layer\n",
      "####\n",
      "83.0\n",
      "####\n",
      "65.6\n",
      "####\n",
      "55.9\n",
      "####\n",
      "hidden\n",
      "####\n",
      "layers\n",
      "####\n",
      "84.2\n",
      "####\n",
      "68.4\n",
      "####\n",
      "59.3\n",
      "####\n",
      "Ours\n",
      "####\n",
      "89.1\n",
      "####\n",
      "72.9\n",
      "####\n",
      "62.3\n",
      "####\n",
      "58.8\n",
      "####\n",
      "57.5\n",
      "####\n",
      "61.2\n",
      "####\n",
      "63.3\n",
      "####\n",
      "63.4\n",
      "####\n",
      "63.2\n",
      "####\n",
      "62.4\n",
      "####\n",
      "64.6\n",
      "####\n",
      "67.8\n",
      "####\n",
      "46.8\n",
      "####\n",
      "50.2\n",
      "####\n",
      "50.6\n",
      "####\n",
      "53.0\n",
      "####\n",
      "56.3\n",
      "####\n",
      "39.8\n",
      "####\n",
      "79.3\n",
      "####\n",
      "62.8\n",
      "####\n",
      "42.8\n",
      "####\n",
      "77.5\n",
      "####\n",
      "64.2\n",
      "####\n",
      "42.9\n",
      "####\n",
      "50.7\n",
      "####\n",
      "62.3\n",
      "####\n",
      "45.2\n",
      "####\n",
      "82.2\n",
      "####\n",
      "67.1\n",
      "####\n",
      "46.6\n",
      "####\n",
      "83.1\n",
      "####\n",
      "68.6\n",
      "####\n",
      "42.4\n",
      "####\n",
      "82.4\n",
      "####\n",
      "63.6\n",
      "####\n",
      "44.1\n",
      "####\n",
      "78.0\n",
      "####\n",
      "63.2\n",
      "####\n",
      "44.4\n",
      "####\n",
      "70.2\n",
      "####\n",
      "63.7\n",
      "####\n",
      "46.3\n",
      "####\n",
      "85.4\n",
      "####\n",
      "67.9\n",
      "####\n",
      "47.8\n",
      "####\n",
      "89.3\n",
      "####\n",
      "71.0\n",
      "####\n",
      "37.7\n",
      "####\n",
      "79.8\n",
      "####\n",
      "57.0\n",
      "####\n",
      "43.1\n",
      "####\n",
      "80.6\n",
      "####\n",
      "58.9\n",
      "####\n",
      "42.3\n",
      "####\n",
      "79.8\n",
      "####\n",
      "59.2\n",
      "####\n",
      "45.3\n",
      "####\n",
      "83.4\n",
      "####\n",
      "62.0\n",
      "####\n",
      "47.6\n",
      "####\n",
      "89.1\n",
      "####\n",
      "65.6\n",
      "####\n",
      "proved\n",
      "####\n",
      "that\n",
      "####\n",
      "linear-SVM\n",
      "####\n",
      "and\n",
      "####\n",
      "kernel-SVM\n",
      "####\n",
      "are\n",
      "####\n",
      "shallow\n",
      "####\n",
      "mod-\n",
      "####\n",
      "els\n",
      "####\n",
      "With\n",
      "####\n",
      "the\n",
      "####\n",
      "deep\n",
      "####\n",
      "model\n",
      "####\n",
      "our\n",
      "####\n",
      "approach\n",
      "####\n",
      "performs\n",
      "####\n",
      "better\n",
      "####\n",
      "the\n",
      "####\n",
      "number\n",
      "####\n",
      "hidden\n",
      "####\n",
      "layers\n",
      "####\n",
      "increases\n",
      "####\n",
      "from\n",
      "####\n",
      "hidden\n",
      "####\n",
      "layer\n",
      "####\n",
      "hidden\n",
      "####\n",
      "layers\n",
      "####\n",
      "the\n",
      "####\n",
      "estimation\n",
      "####\n",
      "accuracy\n",
      "####\n",
      "increases\n",
      "####\n",
      "from\n",
      "####\n",
      "62.3\n",
      "####\n",
      "67.1\n",
      "####\n",
      "With\n",
      "####\n",
      "PCP\n",
      "####\n",
      "68.6\n",
      "####\n",
      "our\n",
      "####\n",
      "ﬁnal\n",
      "####\n",
      "model\n",
      "####\n",
      "Fig\n",
      "####\n",
      "uses\n",
      "####\n",
      "three\n",
      "####\n",
      "hidden\n",
      "####\n",
      "layers\n",
      "####\n",
      "and\n",
      "####\n",
      "better\n",
      "####\n",
      "than\n",
      "####\n",
      "SVM\n",
      "####\n",
      "and\n",
      "####\n",
      "deep\n",
      "####\n",
      "models\n",
      "####\n",
      "with\n",
      "####\n",
      "fewer\n",
      "####\n",
      "layers\n",
      "####\n",
      "Table\n",
      "####\n",
      "Results\n",
      "####\n",
      "PCP\n",
      "####\n",
      "investigating\n",
      "####\n",
      "deep\n",
      "####\n",
      "model\n",
      "####\n",
      "structures\n",
      "####\n",
      "Method\n",
      "####\n",
      "Torso\n",
      "####\n",
      "U.leg\n",
      "####\n",
      "L.leg\n",
      "####\n",
      "U.arm\n",
      "####\n",
      "L.arm\n",
      "####\n",
      "Head\n",
      "####\n",
      "Total\n",
      "####\n",
      "LSP\n",
      "####\n",
      "DBN\n",
      "####\n",
      "Fig\n",
      "####\n",
      "82.9\n",
      "####\n",
      "73.2\n",
      "####\n",
      "69.5\n",
      "####\n",
      "Ours\n",
      "####\n",
      "85.8\n",
      "####\n",
      "76.5\n",
      "####\n",
      "72.2\n",
      "####\n",
      "PARSE\n",
      "####\n",
      "DBN\n",
      "####\n",
      "Fig\n",
      "####\n",
      "82.0\n",
      "####\n",
      "70.0\n",
      "####\n",
      "64.6\n",
      "####\n",
      "Ours\n",
      "####\n",
      "89.3\n",
      "####\n",
      "78.0\n",
      "####\n",
      "72.0\n",
      "####\n",
      "DBN\n",
      "####\n",
      "Fig\n",
      "####\n",
      "87.4\n",
      "####\n",
      "68.4\n",
      "####\n",
      "58.3\n",
      "####\n",
      "Ours\n",
      "####\n",
      "89.1\n",
      "####\n",
      "72.9\n",
      "####\n",
      "62.3\n",
      "####\n",
      "UIUC\n",
      "####\n",
      "59.8\n",
      "####\n",
      "63.3\n",
      "####\n",
      "62.9\n",
      "####\n",
      "67.8\n",
      "####\n",
      "52.2\n",
      "####\n",
      "56.3\n",
      "####\n",
      "43.8\n",
      "####\n",
      "79.2\n",
      "####\n",
      "65.5\n",
      "####\n",
      "46.6\n",
      "####\n",
      "83.1\n",
      "####\n",
      "68.6\n",
      "####\n",
      "46.3\n",
      "####\n",
      "80.5\n",
      "####\n",
      "65.0\n",
      "####\n",
      "47.8\n",
      "####\n",
      "89.3\n",
      "####\n",
      "71.0\n",
      "####\n",
      "44.3\n",
      "####\n",
      "84.6\n",
      "####\n",
      "61.8\n",
      "####\n",
      "47.6\n",
      "####\n",
      "89.1\n",
      "####\n",
      "65.6\n",
      "####\n",
      "Deep\n",
      "####\n",
      "model\n",
      "####\n",
      "structure\n",
      "####\n",
      "design\n",
      "####\n",
      "investigated\n",
      "####\n",
      "Table\n",
      "####\n",
      "The\n",
      "####\n",
      "DBN\n",
      "####\n",
      "Fig\n",
      "####\n",
      "trains\n",
      "####\n",
      "three-layer\n",
      "####\n",
      "deep\n",
      "####\n",
      "model\n",
      "####\n",
      "over\n",
      "####\n",
      "the\n",
      "####\n",
      "concatenated\n",
      "####\n",
      "informations\n",
      "####\n",
      "with\n",
      "####\n",
      "three\n",
      "####\n",
      "hidden\n",
      "####\n",
      "layers\n",
      "####\n",
      "The\n",
      "####\n",
      "model\n",
      "####\n",
      "learns\n",
      "####\n",
      "high-order\n",
      "####\n",
      "representations\n",
      "####\n",
      "individu-\n",
      "####\n",
      "ally\n",
      "####\n",
      "The\n",
      "####\n",
      "model\n",
      "####\n",
      "with\n",
      "####\n",
      "PCP\n",
      "####\n",
      "68.6\n",
      "####\n",
      "better\n",
      "####\n",
      "con-\n",
      "####\n",
      "structing\n",
      "####\n",
      "the\n",
      "####\n",
      "high-order\n",
      "####\n",
      "representations\n",
      "####\n",
      "and\n",
      "####\n",
      "therefore\n",
      "####\n",
      "has\n",
      "####\n",
      "higher\n",
      "####\n",
      "estimation\n",
      "####\n",
      "accuracy\n",
      "####\n",
      "compared\n",
      "####\n",
      "with\n",
      "####\n",
      "the\n",
      "####\n",
      "DBN\n",
      "####\n",
      "Fig\n",
      "####\n",
      "with\n",
      "####\n",
      "PCP\n",
      "####\n",
      "65.5\n",
      "####\n",
      "Classiﬁcation\n",
      "####\n",
      "label\n",
      "####\n",
      "and\n",
      "####\n",
      "location\n",
      "####\n",
      "learning\n",
      "####\n",
      "investigated\n",
      "####\n",
      "Table\n",
      "####\n",
      "There\n",
      "####\n",
      "are\n",
      "####\n",
      "two\n",
      "####\n",
      "sets\n",
      "####\n",
      "labels\n",
      "####\n",
      "estimated\n",
      "####\n",
      "our\n",
      "####\n",
      "deep\n",
      "####\n",
      "model\n",
      "####\n",
      "classiﬁcation\n",
      "####\n",
      "label\n",
      "####\n",
      "ycls\n",
      "####\n",
      "and\n",
      "####\n",
      "part\n",
      "####\n",
      "posi-\n",
      "####\n",
      "tions\n",
      "####\n",
      "ypst\n",
      "####\n",
      "the\n",
      "####\n",
      "experiments\n",
      "####\n",
      "evaluate\n",
      "####\n",
      "different\n",
      "####\n",
      "ways\n",
      "####\n",
      "estimating\n",
      "####\n",
      "these\n",
      "####\n",
      "labels\n",
      "####\n",
      "The\n",
      "####\n",
      "Only\n",
      "####\n",
      "ycls\n",
      "####\n",
      "Table\n",
      "####\n",
      "with\n",
      "####\n",
      "PCP\n",
      "####\n",
      "63.7\n",
      "####\n",
      "only\n",
      "####\n",
      "estimates\n",
      "####\n",
      "class\n",
      "####\n",
      "label\n",
      "####\n",
      "with\n",
      "####\n",
      "part\n",
      "####\n",
      "location\n",
      "####\n",
      "directly\n",
      "####\n",
      "obtained\n",
      "####\n",
      "the\n",
      "####\n",
      "approach\n",
      "####\n",
      "The\n",
      "####\n",
      "Only\n",
      "####\n",
      "ypst\n",
      "####\n",
      "with\n",
      "####\n",
      "PCP\n",
      "####\n",
      "64.1\n",
      "####\n",
      "only\n",
      "####\n",
      "reﬁnes\n",
      "####\n",
      "the\n",
      "####\n",
      "part\n",
      "####\n",
      "location\n",
      "####\n",
      "with\n",
      "####\n",
      "class\n",
      "####\n",
      "label\n",
      "####\n",
      "directly\n",
      "####\n",
      "obtained\n",
      "####\n",
      "the\n",
      "####\n",
      "approach\n",
      "####\n",
      "Separate\n",
      "####\n",
      "ycls+ypst\n",
      "####\n",
      "with\n",
      "####\n",
      "PCP\n",
      "####\n",
      "64.7\n",
      "####\n",
      "uses\n",
      "####\n",
      "two\n",
      "####\n",
      "deep\n",
      "####\n",
      "models\n",
      "####\n",
      "for\n",
      "####\n",
      "esti-\n",
      "####\n",
      "mating\n",
      "####\n",
      "ycls\n",
      "####\n",
      "and\n",
      "####\n",
      "ypst\n",
      "####\n",
      "separately\n",
      "####\n",
      "can\n",
      "####\n",
      "seen\n",
      "####\n",
      "that\n",
      "####\n",
      "both\n",
      "####\n",
      "ycls\n",
      "####\n",
      "and\n",
      "####\n",
      "ypst\n",
      "####\n",
      "are\n",
      "####\n",
      "helpful\n",
      "####\n",
      "for\n",
      "####\n",
      "improving\n",
      "####\n",
      "accuracy\n",
      "####\n",
      "Our\n",
      "####\n",
      "model\n",
      "####\n",
      "uses\n",
      "####\n",
      "the\n",
      "####\n",
      "single\n",
      "####\n",
      "deep\n",
      "####\n",
      "model\n",
      "####\n",
      "jointly\n",
      "####\n",
      "learn\n",
      "####\n",
      "both\n",
      "####\n",
      "ycls\n",
      "####\n",
      "and\n",
      "####\n",
      "ypst\n",
      "####\n",
      "PCP\n",
      "####\n",
      "68.6\n",
      "####\n",
      "and\n",
      "####\n",
      "performs\n",
      "####\n",
      "better\n",
      "####\n",
      "than\n",
      "####\n",
      "using\n",
      "####\n",
      "two\n",
      "####\n",
      "mod-\n",
      "####\n",
      "els\n",
      "####\n",
      "learn\n",
      "####\n",
      "them\n",
      "####\n",
      "separately\n",
      "####\n",
      "PCP\n",
      "####\n",
      "64.7\n",
      "####\n",
      "because\n",
      "####\n",
      "body\n",
      "####\n",
      "lo-\n",
      "####\n",
      "cation\n",
      "####\n",
      "and\n",
      "####\n",
      "the\n",
      "####\n",
      "correctness\n",
      "####\n",
      "candidate\n",
      "####\n",
      "body\n",
      "####\n",
      "location\n",
      "####\n",
      "are\n",
      "####\n",
      "dependent\n",
      "####\n",
      "Analysis\n",
      "####\n",
      "Our\n",
      "####\n",
      "model\n",
      "####\n",
      "extracts\n",
      "####\n",
      "high-order\n",
      "####\n",
      "representations\n",
      "####\n",
      "appearance\n",
      "####\n",
      "deformation\n",
      "####\n",
      "and\n",
      "####\n",
      "mixture\n",
      "####\n",
      "types\n",
      "####\n",
      "and\n",
      "####\n",
      "better\n",
      "####\n",
      "models\n",
      "####\n",
      "their\n",
      "####\n",
      "dependence\n",
      "####\n",
      "the\n",
      "####\n",
      "top\n",
      "####\n",
      "layer\n",
      "####\n",
      "For\n",
      "####\n",
      "example\n",
      "####\n",
      "the\n",
      "####\n",
      "mixture\n",
      "####\n",
      "types\n",
      "####\n",
      "are\n",
      "####\n",
      "upright\n",
      "####\n",
      "upper-\n",
      "####\n",
      "and\n",
      "####\n",
      "lower-arms\n",
      "####\n",
      "the\n",
      "####\n",
      "weighted\n",
      "####\n",
      "combination\n",
      "####\n",
      "the\n",
      "####\n",
      "locations\n",
      "####\n",
      "wrist\n",
      "####\n",
      "and\n",
      "####\n",
      "shoul-\n",
      "####\n",
      "der\n",
      "####\n",
      "good\n",
      "####\n",
      "estimation\n",
      "####\n",
      "the\n",
      "####\n",
      "location\n",
      "####\n",
      "elbow\n",
      "####\n",
      "the\n",
      "####\n",
      "mixture\n",
      "####\n",
      "types\n",
      "####\n",
      "change\n",
      "####\n",
      "such\n",
      "####\n",
      "estimation\n",
      "####\n",
      "should\n",
      "####\n",
      "change\n",
      "####\n",
      "cor-\n",
      "####\n",
      "respondingly\n",
      "####\n",
      "Such\n",
      "####\n",
      "complex\n",
      "####\n",
      "dependence\n",
      "####\n",
      "can\n",
      "####\n",
      "not\n",
      "####\n",
      "mod-\n",
      "####\n",
      "eled\n",
      "####\n",
      "linearly\n",
      "####\n",
      "and\n",
      "####\n",
      "deep\n",
      "####\n",
      "model\n",
      "####\n",
      "better\n",
      "####\n",
      "solution\n",
      "####\n",
      "When\n",
      "####\n",
      "different\n",
      "####\n",
      "information\n",
      "####\n",
      "sources\n",
      "####\n",
      "are\n",
      "####\n",
      "extracted\n",
      "####\n",
      "separately\n",
      "####\n",
      "with\n",
      "####\n",
      "the\n",
      "####\n",
      "ﬁrst\n",
      "####\n",
      "several\n",
      "####\n",
      "layers\n",
      "####\n",
      "the\n",
      "####\n",
      "connections\n",
      "####\n",
      "across\n",
      "####\n",
      "sources\n",
      "####\n",
      "are\n",
      "####\n",
      "removed\n",
      "####\n",
      "and\n",
      "####\n",
      "the\n",
      "####\n",
      "number\n",
      "####\n",
      "parameters\n",
      "####\n",
      "reduced\n",
      "####\n",
      "helps\n",
      "####\n",
      "regularize\n",
      "####\n",
      "optimization\n",
      "####\n",
      "when\n",
      "####\n",
      "training\n",
      "####\n",
      "samples\n",
      "####\n",
      "are\n",
      "####\n",
      "lim-\n",
      "####\n",
      "ited\n",
      "####\n",
      "Existing\n",
      "####\n",
      "methods\n",
      "####\n",
      "only\n",
      "####\n",
      "use\n",
      "####\n",
      "ycls\n",
      "####\n",
      "for\n",
      "####\n",
      "supervision\n",
      "####\n",
      "while\n",
      "####\n",
      "use\n",
      "####\n",
      "both\n",
      "####\n",
      "ypts\n",
      "####\n",
      "and\n",
      "####\n",
      "ycls\n",
      "####\n",
      "shown\n",
      "####\n",
      "Fig\n",
      "####\n",
      "reﬁning\n",
      "####\n",
      "ypts\n",
      "####\n",
      "does\n",
      "####\n",
      "help\n",
      "####\n",
      "rectify\n",
      "####\n",
      "incorrect\n",
      "####\n",
      "part\n",
      "####\n",
      "locations\n",
      "####\n",
      "based\n",
      "####\n",
      "the\n",
      "####\n",
      "high\n",
      "####\n",
      "order\n",
      "####\n",
      "prior\n",
      "####\n",
      "model\n",
      "####\n",
      "body\n",
      "####\n",
      "pose\n",
      "####\n",
      "Jointly\n",
      "####\n",
      "learning\n",
      "####\n",
      "Table\n",
      "####\n",
      "PCP\n",
      "####\n",
      "results\n",
      "####\n",
      "classiﬁcation\n",
      "####\n",
      "label\n",
      "####\n",
      "and\n",
      "####\n",
      "location\n",
      "####\n",
      "learning\n",
      "####\n",
      "Method\n",
      "####\n",
      "Torso\n",
      "####\n",
      "U.leg\n",
      "####\n",
      "L.leg\n",
      "####\n",
      "U.arm\n",
      "####\n",
      "L.arm\n",
      "####\n",
      "Head\n",
      "####\n",
      "Total\n",
      "####\n",
      "LSP\n",
      "####\n",
      "82.9\n",
      "####\n",
      "70.3\n",
      "####\n",
      "67.0\n",
      "####\n",
      "Only\n",
      "####\n",
      "ycls\n",
      "####\n",
      "82.0\n",
      "####\n",
      "71.5\n",
      "####\n",
      "68.0\n",
      "####\n",
      "Only\n",
      "####\n",
      "ypst\n",
      "####\n",
      "80.4\n",
      "####\n",
      "72.0\n",
      "####\n",
      "68.0\n",
      "####\n",
      "Separate\n",
      "####\n",
      "ycls+ypst\n",
      "####\n",
      "81.1\n",
      "####\n",
      "72.8\n",
      "####\n",
      "69.0\n",
      "####\n",
      "Ours\n",
      "####\n",
      "85.8\n",
      "####\n",
      "76.5\n",
      "####\n",
      "72.2\n",
      "####\n",
      "PARSE\n",
      "####\n",
      "82.9\n",
      "####\n",
      "68.8\n",
      "####\n",
      "60.5\n",
      "####\n",
      "Only\n",
      "####\n",
      "ycls\n",
      "####\n",
      "81.0\n",
      "####\n",
      "69.8\n",
      "####\n",
      "66.1\n",
      "####\n",
      "Only\n",
      "####\n",
      "ypst\n",
      "####\n",
      "80.5\n",
      "####\n",
      "71.2\n",
      "####\n",
      "65.4\n",
      "####\n",
      "Separate\n",
      "####\n",
      "ycls+ypst\n",
      "####\n",
      "83.4\n",
      "####\n",
      "73.7\n",
      "####\n",
      "67.6\n",
      "####\n",
      "89.3\n",
      "####\n",
      "78.0\n",
      "####\n",
      "72.0\n",
      "####\n",
      "Ours\n",
      "####\n",
      "UIUC\n",
      "####\n",
      "81.8\n",
      "####\n",
      "65.0\n",
      "####\n",
      "55.1\n",
      "####\n",
      "Only\n",
      "####\n",
      "ycls\n",
      "####\n",
      "85.4\n",
      "####\n",
      "68.8\n",
      "####\n",
      "59.3\n",
      "####\n",
      "Only\n",
      "####\n",
      "ypst\n",
      "####\n",
      "82.6\n",
      "####\n",
      "66.6\n",
      "####\n",
      "58.3\n",
      "####\n",
      "Separate\n",
      "####\n",
      "ycls+ypst\n",
      "####\n",
      "87.9\n",
      "####\n",
      "69.6\n",
      "####\n",
      "60.3\n",
      "####\n",
      "Ours\n",
      "####\n",
      "89.1\n",
      "####\n",
      "72.9\n",
      "####\n",
      "62.3\n",
      "####\n",
      "56.0\n",
      "####\n",
      "57.6\n",
      "####\n",
      "59.2\n",
      "####\n",
      "59.5\n",
      "####\n",
      "63.3\n",
      "####\n",
      "63.4\n",
      "####\n",
      "60.5\n",
      "####\n",
      "62.2\n",
      "####\n",
      "64.4\n",
      "####\n",
      "67.8\n",
      "####\n",
      "46.8\n",
      "####\n",
      "49.2\n",
      "####\n",
      "52.2\n",
      "####\n",
      "53.0\n",
      "####\n",
      "56.3\n",
      "####\n",
      "39.8\n",
      "####\n",
      "79.3\n",
      "####\n",
      "62.8\n",
      "####\n",
      "42.0\n",
      "####\n",
      "77.2\n",
      "####\n",
      "63.7\n",
      "####\n",
      "42.8\n",
      "####\n",
      "76.8\n",
      "####\n",
      "64.1\n",
      "####\n",
      "43.0\n",
      "####\n",
      "77.7\n",
      "####\n",
      "64.7\n",
      "####\n",
      "46.6\n",
      "####\n",
      "83.1\n",
      "####\n",
      "68.6\n",
      "####\n",
      "42.4\n",
      "####\n",
      "82.4\n",
      "####\n",
      "63.6\n",
      "####\n",
      "43.9\n",
      "####\n",
      "76.1\n",
      "####\n",
      "63.8\n",
      "####\n",
      "44.4\n",
      "####\n",
      "79.5\n",
      "####\n",
      "64.6\n",
      "####\n",
      "47.1\n",
      "####\n",
      "82.0\n",
      "####\n",
      "67.1\n",
      "####\n",
      "47.8\n",
      "####\n",
      "89.3\n",
      "####\n",
      "71.0\n",
      "####\n",
      "37.7\n",
      "####\n",
      "79.8\n",
      "####\n",
      "57.0\n",
      "####\n",
      "40.5\n",
      "####\n",
      "83.4\n",
      "####\n",
      "60.4\n",
      "####\n",
      "44.7\n",
      "####\n",
      "81.8\n",
      "####\n",
      "60.8\n",
      "####\n",
      "44.3\n",
      "####\n",
      "85.4\n",
      "####\n",
      "62.8\n",
      "####\n",
      "47.6\n",
      "####\n",
      "89.1\n",
      "####\n",
      "65.6\n",
      "####\n",
      "ypst\n",
      "####\n",
      "and\n",
      "####\n",
      "ycls\n",
      "####\n",
      "helps\n",
      "####\n",
      "ﬁnd\n",
      "####\n",
      "their\n",
      "####\n",
      "shared\n",
      "####\n",
      "representation\n",
      "####\n",
      "under\n",
      "####\n",
      "multi-task\n",
      "####\n",
      "learning\n",
      "####\n",
      "framework\n",
      "####\n",
      "for\n",
      "####\n",
      "which\n",
      "####\n",
      "deep\n",
      "####\n",
      "model\n",
      "####\n",
      "ideal\n",
      "####\n",
      "choice\n",
      "####\n",
      "Conclusion\n",
      "####\n",
      "This\n",
      "####\n",
      "paper\n",
      "####\n",
      "has\n",
      "####\n",
      "proposed\n",
      "####\n",
      "multi-source\n",
      "####\n",
      "deep\n",
      "####\n",
      "model\n",
      "####\n",
      "for\n",
      "####\n",
      "pose\n",
      "####\n",
      "estimation\n",
      "####\n",
      "non-linearly\n",
      "####\n",
      "integrates\n",
      "####\n",
      "three\n",
      "####\n",
      "information\n",
      "####\n",
      "sources\n",
      "####\n",
      "appearance\n",
      "####\n",
      "score\n",
      "####\n",
      "deformation\n",
      "####\n",
      "and\n",
      "####\n",
      "appearance\n",
      "####\n",
      "mixture\n",
      "####\n",
      "type\n",
      "####\n",
      "These\n",
      "####\n",
      "information\n",
      "####\n",
      "sources\n",
      "####\n",
      "are\n",
      "####\n",
      "used\n",
      "####\n",
      "for\n",
      "####\n",
      "de-\n",
      "####\n",
      "scribing\n",
      "####\n",
      "different\n",
      "####\n",
      "aspects\n",
      "####\n",
      "the\n",
      "####\n",
      "single\n",
      "####\n",
      "modality\n",
      "####\n",
      "data\n",
      "####\n",
      "which\n",
      "####\n",
      "the\n",
      "####\n",
      "image\n",
      "####\n",
      "data\n",
      "####\n",
      "our\n",
      "####\n",
      "pose\n",
      "####\n",
      "estimation\n",
      "####\n",
      "approach\n",
      "####\n",
      "Exten-\n",
      "####\n",
      "sive\n",
      "####\n",
      "experimental\n",
      "####\n",
      "comparisons\n",
      "####\n",
      "three\n",
      "####\n",
      "public\n",
      "####\n",
      "benchmark\n",
      "####\n",
      "datasets\n",
      "####\n",
      "show\n",
      "####\n",
      "that\n",
      "####\n",
      "the\n",
      "####\n",
      "proposed\n",
      "####\n",
      "model\n",
      "####\n",
      "obviously\n",
      "####\n",
      "improves\n",
      "####\n",
      "the\n",
      "####\n",
      "pose\n",
      "####\n",
      "estimation\n",
      "####\n",
      "accuracy\n",
      "####\n",
      "and\n",
      "####\n",
      "outperforms\n",
      "####\n",
      "the\n",
      "####\n",
      "state\n",
      "####\n",
      "the\n",
      "####\n",
      "art\n",
      "####\n",
      "Since\n",
      "####\n",
      "this\n",
      "####\n",
      "model\n",
      "####\n",
      "post-processing\n",
      "####\n",
      "informa-\n",
      "####\n",
      "tion\n",
      "####\n",
      "sources\n",
      "####\n",
      "very\n",
      "####\n",
      "ﬂexible\n",
      "####\n",
      "terms\n",
      "####\n",
      "integrating\n",
      "####\n",
      "with\n",
      "####\n",
      "existing\n",
      "####\n",
      "approaches\n",
      "####\n",
      "that\n",
      "####\n",
      "use\n",
      "####\n",
      "different\n",
      "####\n",
      "information\n",
      "####\n",
      "sources\n",
      "####\n",
      "features\n",
      "####\n",
      "articulation\n",
      "####\n",
      "models\n",
      "####\n",
      "Learning\n",
      "####\n",
      "deep\n",
      "####\n",
      "model\n",
      "####\n",
      "from\n",
      "####\n",
      "pixels\n",
      "####\n",
      "for\n",
      "####\n",
      "pose\n",
      "####\n",
      "estimation\n",
      "####\n",
      "and\n",
      "####\n",
      "analyzing\n",
      "####\n",
      "the\n",
      "####\n",
      "inﬂuence\n",
      "####\n",
      "training\n",
      "####\n",
      "data\n",
      "####\n",
      "number\n",
      "####\n",
      "will\n",
      "####\n",
      "the\n",
      "####\n",
      "future\n",
      "####\n",
      "work\n",
      "####\n",
      "Acknowledgement\n",
      "####\n",
      "This\n",
      "####\n",
      "work\n",
      "####\n",
      "supported\n",
      "####\n",
      "the\n",
      "####\n",
      "General\n",
      "####\n",
      "Research\n",
      "####\n",
      "Fund\n",
      "####\n",
      "sponsored\n",
      "####\n",
      "the\n",
      "####\n",
      "Research\n",
      "####\n",
      "Grants\n",
      "####\n",
      "Council\n",
      "####\n",
      "Hong\n",
      "####\n",
      "Kong\n",
      "####\n",
      "Project\n",
      "####\n",
      "CUHK\n",
      "####\n",
      "CUHK\n",
      "####\n",
      "CUHK\n",
      "####\n",
      "National\n",
      "####\n",
      "Natural\n",
      "####\n",
      "Science\n",
      "####\n",
      "Foun-\n",
      "####\n",
      "dation\n",
      "####\n",
      "China\n",
      "####\n",
      "Shenzhen\n",
      "####\n",
      "Basic\n",
      "####\n",
      "Research\n",
      "####\n",
      "Program\n",
      "####\n",
      "JC201005270350A\n",
      "####\n",
      "JCYJ20120903092050890\n",
      "####\n",
      "JCYJ20120617114614438\n",
      "####\n",
      "and\n",
      "####\n",
      "Guangdong\n",
      "####\n",
      "Innovative\n",
      "####\n",
      "Research\n",
      "####\n",
      "Team\n",
      "####\n",
      "Program\n",
      "####\n",
      "No.201001D0104648280\n",
      "####\n",
      "Figure\n",
      "####\n",
      "Comparison\n",
      "####\n",
      "between\n",
      "####\n",
      "our\n",
      "####\n",
      "method\n",
      "####\n",
      "left\n",
      "####\n",
      "and\n",
      "####\n",
      "the\n",
      "####\n",
      "approach\n",
      "####\n",
      "right\n",
      "####\n",
      "the\n",
      "####\n",
      "LSP\n",
      "####\n",
      "PARSE\n",
      "####\n",
      "and\n",
      "####\n",
      "UIUC\n",
      "####\n",
      "dataset\n",
      "####\n",
      "Our\n",
      "####\n",
      "ap-\n",
      "####\n",
      "proach\n",
      "####\n",
      "obtains\n",
      "####\n",
      "more\n",
      "####\n",
      "reasonable\n",
      "####\n",
      "articulation\n",
      "####\n",
      "patterns\n",
      "####\n",
      "and\n",
      "####\n",
      "better\n",
      "####\n",
      "solving\n",
      "####\n",
      "the\n",
      "####\n",
      "double\n",
      "####\n",
      "counting\n",
      "####\n",
      "problem\n",
      "####\n",
      "Best\n",
      "####\n",
      "viewed\n",
      "####\n",
      "color\n",
      "####\n",
      "References\n",
      "####\n",
      "Andriluka\n",
      "####\n",
      "Pishchulin\n",
      "####\n",
      "Gehler\n",
      "####\n",
      "and\n",
      "####\n",
      "Schiele\n",
      "####\n",
      "human\n",
      "####\n",
      "pose\n",
      "####\n",
      "estimation\n",
      "####\n",
      "New\n",
      "####\n",
      "benchmark\n",
      "####\n",
      "and\n",
      "####\n",
      "state\n",
      "####\n",
      "the\n",
      "####\n",
      "art\n",
      "####\n",
      "analysis\n",
      "####\n",
      "CVPR\n",
      "####\n",
      "Andriluka\n",
      "####\n",
      "Roth\n",
      "####\n",
      "and\n",
      "####\n",
      "Schiele\n",
      "####\n",
      "Pictorial\n",
      "####\n",
      "structures\n",
      "####\n",
      "revisited\n",
      "####\n",
      "people\n",
      "####\n",
      "detection\n",
      "####\n",
      "and\n",
      "####\n",
      "articulated\n",
      "####\n",
      "pose\n",
      "####\n",
      "estimation\n",
      "####\n",
      "CVPR\n",
      "####\n",
      "Bengio\n",
      "####\n",
      "Learning\n",
      "####\n",
      "deep\n",
      "####\n",
      "architectures\n",
      "####\n",
      "for\n",
      "####\n",
      "Foundations\n",
      "####\n",
      "and\n",
      "####\n",
      "Trends\n",
      "####\n",
      "Machine\n",
      "####\n",
      "Learning\n",
      "####\n",
      ":1–127\n",
      "####\n",
      "Bengio\n",
      "####\n",
      "Courville\n",
      "####\n",
      "and\n",
      "####\n",
      "Vincent\n",
      "####\n",
      "Representation\n",
      "####\n",
      "learning\n",
      "####\n",
      "review\n",
      "####\n",
      "and\n",
      "####\n",
      "new\n",
      "####\n",
      "perspectives\n",
      "####\n",
      "IEEE\n",
      "####\n",
      "Trans\n",
      "####\n",
      "PAMI\n",
      "####\n",
      ":1798–1828\n",
      "####\n",
      "Bishop\n",
      "####\n",
      "and\n",
      "####\n",
      "Nasrabadi\n",
      "####\n",
      "Pattern\n",
      "####\n",
      "recognition\n",
      "####\n",
      "and\n",
      "####\n",
      "machine\n",
      "####\n",
      "learning\n",
      "####\n",
      "springer\n",
      "####\n",
      "Bourdev\n",
      "####\n",
      "and\n",
      "####\n",
      "Malik\n",
      "####\n",
      "Poselets\n",
      "####\n",
      "body\n",
      "####\n",
      "part\n",
      "####\n",
      "detectors\n",
      "####\n",
      "trained\n",
      "####\n",
      "using\n",
      "####\n",
      "human\n",
      "####\n",
      "pose\n",
      "####\n",
      "annotations\n",
      "####\n",
      "ICCV\n",
      "####\n",
      "Dalal\n",
      "####\n",
      "and\n",
      "####\n",
      "Triggs\n",
      "####\n",
      "Histograms\n",
      "####\n",
      "oriented\n",
      "####\n",
      "gradients\n",
      "####\n",
      "for\n",
      "####\n",
      "human\n",
      "####\n",
      "detection\n",
      "####\n",
      "CVPR\n",
      "####\n",
      "Deng\n",
      "####\n",
      "Dong\n",
      "####\n",
      "Socher\n",
      "####\n",
      "L.-J\n",
      "####\n",
      "and\n",
      "####\n",
      "Fei-\n",
      "####\n",
      "Fei\n",
      "####\n",
      "Imagenet\n",
      "####\n",
      "large-scale\n",
      "####\n",
      "hierarchical\n",
      "####\n",
      "image\n",
      "####\n",
      "database\n",
      "####\n",
      "CVPR\n",
      "####\n",
      "Desai\n",
      "####\n",
      "and\n",
      "####\n",
      "Ramanan\n",
      "####\n",
      "Detecting\n",
      "####\n",
      "actions\n",
      "####\n",
      "poses\n",
      "####\n",
      "and\n",
      "####\n",
      "objects\n",
      "####\n",
      "with\n",
      "####\n",
      "relational\n",
      "####\n",
      "phraselets\n",
      "####\n",
      "ECCV\n",
      "####\n",
      "Duan\n",
      "####\n",
      "Batra\n",
      "####\n",
      "and\n",
      "####\n",
      "Crandall\n",
      "####\n",
      "multi-layer\n",
      "####\n",
      "com-\n",
      "####\n",
      "posite\n",
      "####\n",
      "model\n",
      "####\n",
      "for\n",
      "####\n",
      "human\n",
      "####\n",
      "pose\n",
      "####\n",
      "estimation\n",
      "####\n",
      "BMVC\n",
      "####\n",
      "Eichner\n",
      "####\n",
      "and\n",
      "####\n",
      "Ferrari\n",
      "####\n",
      "Appearance\n",
      "####\n",
      "sharing\n",
      "####\n",
      "for\n",
      "####\n",
      "collective\n",
      "####\n",
      "human\n",
      "####\n",
      "pose\n",
      "####\n",
      "estimation\n",
      "####\n",
      "ACCV\n",
      "####\n",
      "Farabet\n",
      "####\n",
      "Couprie\n",
      "####\n",
      "Najman\n",
      "####\n",
      "and\n",
      "####\n",
      "LeCun\n",
      "####\n",
      "Learning\n",
      "####\n",
      "hierarchical\n",
      "####\n",
      "features\n",
      "####\n",
      "for\n",
      "####\n",
      "scene\n",
      "####\n",
      "labeling\n",
      "####\n",
      "IEEE\n",
      "####\n",
      "Trans\n",
      "####\n",
      "PAMI\n",
      "####\n",
      "30:1915–1929\n",
      "####\n",
      "Felzenszwalb\n",
      "####\n",
      "and\n",
      "####\n",
      "Huttenlocher\n",
      "####\n",
      "Pictorial\n",
      "####\n",
      "struc-\n",
      "####\n",
      "tures\n",
      "####\n",
      "for\n",
      "####\n",
      "object\n",
      "####\n",
      "recognition\n",
      "####\n",
      "IJCV\n",
      "####\n",
      "61:55–79\n",
      "####\n",
      "Ferrari\n",
      "####\n",
      "Marin-Jimenez\n",
      "####\n",
      "and\n",
      "####\n",
      "Zisserman\n",
      "####\n",
      "Progressive\n",
      "####\n",
      "search\n",
      "####\n",
      "space\n",
      "####\n",
      "reduction\n",
      "####\n",
      "for\n",
      "####\n",
      "human\n",
      "####\n",
      "pose\n",
      "####\n",
      "estimation\n",
      "####\n",
      "CVPR\n",
      "####\n",
      "Left-OursRight-Yang\n",
      "####\n",
      "Ramanan\n",
      "####\n",
      "Gkioxari\n",
      "####\n",
      "Arbel´aez\n",
      "####\n",
      "Bourdev\n",
      "####\n",
      "and\n",
      "####\n",
      "Malik\n",
      "####\n",
      "Articu-\n",
      "####\n",
      "lated\n",
      "####\n",
      "pose\n",
      "####\n",
      "estimation\n",
      "####\n",
      "using\n",
      "####\n",
      "discriminative\n",
      "####\n",
      "armlet\n",
      "####\n",
      "classiﬁers\n",
      "####\n",
      "CVPR\n",
      "####\n",
      "Goodfellow\n",
      "####\n",
      "Lee\n",
      "####\n",
      "Saxe\n",
      "####\n",
      "and\n",
      "####\n",
      "Measuring\n",
      "####\n",
      "invariances\n",
      "####\n",
      "deep\n",
      "####\n",
      "networks\n",
      "####\n",
      "NIPS\n",
      "####\n",
      "Guillaumin\n",
      "####\n",
      "Verbeek\n",
      "####\n",
      "and\n",
      "####\n",
      "Schmid\n",
      "####\n",
      "Multimodal\n",
      "####\n",
      "semi-supervised\n",
      "####\n",
      "learning\n",
      "####\n",
      "for\n",
      "####\n",
      "image\n",
      "####\n",
      "classiﬁcation\n",
      "####\n",
      "CVPR\n",
      "####\n",
      "Hinton\n",
      "####\n",
      "Osindero\n",
      "####\n",
      "and\n",
      "####\n",
      "Teh\n",
      "####\n",
      "fast\n",
      "####\n",
      "learning\n",
      "####\n",
      "al-\n",
      "####\n",
      "gorithm\n",
      "####\n",
      "for\n",
      "####\n",
      "deep\n",
      "####\n",
      "belief\n",
      "####\n",
      "nets\n",
      "####\n",
      "Neural\n",
      "####\n",
      "Computation\n",
      "####\n",
      "18:1527–\n",
      "####\n",
      "Hinton\n",
      "####\n",
      "and\n",
      "####\n",
      "Salakhutdinov\n",
      "####\n",
      "dimensionality\n",
      "####\n",
      "data\n",
      "####\n",
      "with\n",
      "####\n",
      "neural\n",
      "####\n",
      "networks\n",
      "####\n",
      ":504\n",
      "####\n",
      "July\n",
      "####\n",
      "Reducing\n",
      "####\n",
      "the\n",
      "####\n",
      "Science\n",
      "####\n",
      "Jarrett\n",
      "####\n",
      "Kavukcuoglu\n",
      "####\n",
      "Ranzato\n",
      "####\n",
      "and\n",
      "####\n",
      "LeCun\n",
      "####\n",
      "What\n",
      "####\n",
      "the\n",
      "####\n",
      "best\n",
      "####\n",
      "multi-stage\n",
      "####\n",
      "architecture\n",
      "####\n",
      "for\n",
      "####\n",
      "object\n",
      "####\n",
      "recog-\n",
      "####\n",
      "nition\n",
      "####\n",
      "CVPR\n",
      "####\n",
      "Johnson\n",
      "####\n",
      "and\n",
      "####\n",
      "Everingham\n",
      "####\n",
      "Clustered\n",
      "####\n",
      "pose\n",
      "####\n",
      "and\n",
      "####\n",
      "nonlin-\n",
      "####\n",
      "ear\n",
      "####\n",
      "appearance\n",
      "####\n",
      "models\n",
      "####\n",
      "for\n",
      "####\n",
      "human\n",
      "####\n",
      "pose\n",
      "####\n",
      "estimation\n",
      "####\n",
      "BMVC\n",
      "####\n",
      "Johnson\n",
      "####\n",
      "and\n",
      "####\n",
      "Everingham\n",
      "####\n",
      "Learning\n",
      "####\n",
      "effective\n",
      "####\n",
      "human\n",
      "####\n",
      "pose\n",
      "####\n",
      "estimation\n",
      "####\n",
      "from\n",
      "####\n",
      "inaccurate\n",
      "####\n",
      "annotation\n",
      "####\n",
      "CVPR\n",
      "####\n",
      "Krizhevsky\n",
      "####\n",
      "Sutskever\n",
      "####\n",
      "and\n",
      "####\n",
      "Hinton\n",
      "####\n",
      "Imagenet\n",
      "####\n",
      "clas-\n",
      "####\n",
      "siﬁcation\n",
      "####\n",
      "with\n",
      "####\n",
      "deep\n",
      "####\n",
      "convolutional\n",
      "####\n",
      "neural\n",
      "####\n",
      "networks\n",
      "####\n",
      "NIPS\n",
      "####\n",
      "Kschischang\n",
      "####\n",
      "Frey\n",
      "####\n",
      "and\n",
      "####\n",
      "H.-A\n",
      "####\n",
      "Loeliger\n",
      "####\n",
      "Factor\n",
      "####\n",
      "graphs\n",
      "####\n",
      "and\n",
      "####\n",
      "the\n",
      "####\n",
      "sum-product\n",
      "####\n",
      "algorithm\n",
      "####\n",
      "IEEE\n",
      "####\n",
      "Trans\n",
      "####\n",
      "Inf\n",
      "####\n",
      "The-\n",
      "####\n",
      "ory\n",
      "####\n",
      ":498–519\n",
      "####\n",
      "Larochelle\n",
      "####\n",
      "Bengio\n",
      "####\n",
      "Louradour\n",
      "####\n",
      "and\n",
      "####\n",
      "Lamblin\n",
      "####\n",
      "Ex-\n",
      "####\n",
      "ploring\n",
      "####\n",
      "strategies\n",
      "####\n",
      "for\n",
      "####\n",
      "training\n",
      "####\n",
      "deep\n",
      "####\n",
      "neural\n",
      "####\n",
      "networks\n",
      "####\n",
      "Ma-\n",
      "####\n",
      "chine\n",
      "####\n",
      "Learning\n",
      "####\n",
      "Research\n",
      "####\n",
      "10:1–40\n",
      "####\n",
      "Ranzato\n",
      "####\n",
      "Monga\n",
      "####\n",
      "Devin\n",
      "####\n",
      "Chen\n",
      "####\n",
      "Corrado\n",
      "####\n",
      "Dean\n",
      "####\n",
      "and\n",
      "####\n",
      "Building\n",
      "####\n",
      "high-level\n",
      "####\n",
      "features\n",
      "####\n",
      "using\n",
      "####\n",
      "large\n",
      "####\n",
      "scale\n",
      "####\n",
      "unsupervised\n",
      "####\n",
      "learning\n",
      "####\n",
      "ICML\n",
      "####\n",
      "LeCun\n",
      "####\n",
      "Bottou\n",
      "####\n",
      "Bengio\n",
      "####\n",
      "and\n",
      "####\n",
      "Haffner\n",
      "####\n",
      "Gradient-\n",
      "####\n",
      "based\n",
      "####\n",
      "learning\n",
      "####\n",
      "applied\n",
      "####\n",
      "document\n",
      "####\n",
      "recognition\n",
      "####\n",
      "Proceed-\n",
      "####\n",
      "ings\n",
      "####\n",
      "the\n",
      "####\n",
      "IEEE\n",
      "####\n",
      ":2278–2324\n",
      "####\n",
      "Zhao\n",
      "####\n",
      "Xiao\n",
      "####\n",
      "and\n",
      "####\n",
      "Wang\n",
      "####\n",
      "Deepreid\n",
      "####\n",
      "Deep\n",
      "####\n",
      "ﬁlter\n",
      "####\n",
      "pairing\n",
      "####\n",
      "neural\n",
      "####\n",
      "network\n",
      "####\n",
      "for\n",
      "####\n",
      "person\n",
      "####\n",
      "re-identiﬁcation\n",
      "####\n",
      "CVPR\n",
      "####\n",
      "Luo\n",
      "####\n",
      "Tian\n",
      "####\n",
      "Wang\n",
      "####\n",
      "and\n",
      "####\n",
      "Tang\n",
      "####\n",
      "Switchable\n",
      "####\n",
      "deep\n",
      "####\n",
      "network\n",
      "####\n",
      "for\n",
      "####\n",
      "pedestrian\n",
      "####\n",
      "detection\n",
      "####\n",
      "CVPR\n",
      "####\n",
      "Luo\n",
      "####\n",
      "Wang\n",
      "####\n",
      "and\n",
      "####\n",
      "Tang\n",
      "####\n",
      "Hierarchical\n",
      "####\n",
      "face\n",
      "####\n",
      "parsing\n",
      "####\n",
      "via\n",
      "####\n",
      "deep\n",
      "####\n",
      "learning\n",
      "####\n",
      "CVPR\n",
      "####\n",
      "Luo\n",
      "####\n",
      "Wang\n",
      "####\n",
      "and\n",
      "####\n",
      "Tang\n",
      "####\n",
      "deep\n",
      "####\n",
      "sum-product\n",
      "####\n",
      "archi-\n",
      "####\n",
      "tecture\n",
      "####\n",
      "for\n",
      "####\n",
      "robust\n",
      "####\n",
      "facial\n",
      "####\n",
      "attributes\n",
      "####\n",
      "analysis\n",
      "####\n",
      "ICCV\n",
      "####\n",
      "Luo\n",
      "####\n",
      "Wang\n",
      "####\n",
      "and\n",
      "####\n",
      "Tang\n",
      "####\n",
      "Pedestrian\n",
      "####\n",
      "parsing\n",
      "####\n",
      "via\n",
      "####\n",
      "deep\n",
      "####\n",
      "decompositional\n",
      "####\n",
      "neural\n",
      "####\n",
      "network\n",
      "####\n",
      "ICCV\n",
      "####\n",
      "Mori\n",
      "####\n",
      "and\n",
      "####\n",
      "Malik\n",
      "####\n",
      "Estimating\n",
      "####\n",
      "human\n",
      "####\n",
      "body\n",
      "####\n",
      "conﬁgurations\n",
      "####\n",
      "using\n",
      "####\n",
      "shape\n",
      "####\n",
      "context\n",
      "####\n",
      "matching\n",
      "####\n",
      "ECCV\n",
      "####\n",
      "Mori\n",
      "####\n",
      "and\n",
      "####\n",
      "Malik\n",
      "####\n",
      "Recovering\n",
      "####\n",
      "human\n",
      "####\n",
      "body\n",
      "####\n",
      "conﬁgura-\n",
      "####\n",
      "tions\n",
      "####\n",
      "using\n",
      "####\n",
      "shape\n",
      "####\n",
      "contexts\n",
      "####\n",
      "IEEE\n",
      "####\n",
      "Trans\n",
      "####\n",
      "PAMI\n",
      "####\n",
      ":1052–\n",
      "####\n",
      "Ngiam\n",
      "####\n",
      "Khosla\n",
      "####\n",
      "Kim\n",
      "####\n",
      "Nam\n",
      "####\n",
      "Lee\n",
      "####\n",
      "and\n",
      "####\n",
      "Multimodal\n",
      "####\n",
      "deep\n",
      "####\n",
      "learning\n",
      "####\n",
      "ICML\n",
      "####\n",
      "Norouzi\n",
      "####\n",
      "Ranjbar\n",
      "####\n",
      "and\n",
      "####\n",
      "Mori\n",
      "####\n",
      "Stacks\n",
      "####\n",
      "convolu-\n",
      "####\n",
      "tional\n",
      "####\n",
      "restricted\n",
      "####\n",
      "boltzmann\n",
      "####\n",
      "machines\n",
      "####\n",
      "for\n",
      "####\n",
      "shift-invariant\n",
      "####\n",
      "fea-\n",
      "####\n",
      "ture\n",
      "####\n",
      "learning\n",
      "####\n",
      "CVPR\n",
      "####\n",
      "Ouyang\n",
      "####\n",
      "and\n",
      "####\n",
      "Wang\n",
      "####\n",
      "discriminative\n",
      "####\n",
      "deep\n",
      "####\n",
      "model\n",
      "####\n",
      "for\n",
      "####\n",
      "pedestrian\n",
      "####\n",
      "detection\n",
      "####\n",
      "with\n",
      "####\n",
      "occlusion\n",
      "####\n",
      "handling\n",
      "####\n",
      "CVPR\n",
      "####\n",
      "Ouyang\n",
      "####\n",
      "and\n",
      "####\n",
      "Wang\n",
      "####\n",
      "Joint\n",
      "####\n",
      "deep\n",
      "####\n",
      "learning\n",
      "####\n",
      "for\n",
      "####\n",
      "pedestrian\n",
      "####\n",
      "detection\n",
      "####\n",
      "ICCV\n",
      "####\n",
      "Ouyang\n",
      "####\n",
      "Zeng\n",
      "####\n",
      "and\n",
      "####\n",
      "Wang\n",
      "####\n",
      "Modeling\n",
      "####\n",
      "mutual\n",
      "####\n",
      "visi-\n",
      "####\n",
      "bility\n",
      "####\n",
      "relationship\n",
      "####\n",
      "pedestrian\n",
      "####\n",
      "detection\n",
      "####\n",
      "CVPR\n",
      "####\n",
      "Pishchulin\n",
      "####\n",
      "Andriluka\n",
      "####\n",
      "Gehler\n",
      "####\n",
      "and\n",
      "####\n",
      "Schiele\n",
      "####\n",
      "Pose-\n",
      "####\n",
      "let\n",
      "####\n",
      "conditioned\n",
      "####\n",
      "pictorial\n",
      "####\n",
      "structures\n",
      "####\n",
      "CVPR\n",
      "####\n",
      "Pishchulin\n",
      "####\n",
      "Andriluka\n",
      "####\n",
      "Gehler\n",
      "####\n",
      "and\n",
      "####\n",
      "Schiele\n",
      "####\n",
      "Strong\n",
      "####\n",
      "appearance\n",
      "####\n",
      "and\n",
      "####\n",
      "expressive\n",
      "####\n",
      "spatial\n",
      "####\n",
      "models\n",
      "####\n",
      "for\n",
      "####\n",
      "human\n",
      "####\n",
      "pose\n",
      "####\n",
      "estimation\n",
      "####\n",
      "ICCV\n",
      "####\n",
      "December\n",
      "####\n",
      "Pishchulin\n",
      "####\n",
      "Jain\n",
      "####\n",
      "Andriluka\n",
      "####\n",
      "Thormahlen\n",
      "####\n",
      "and\n",
      "####\n",
      "Schiele\n",
      "####\n",
      "Articulated\n",
      "####\n",
      "people\n",
      "####\n",
      "detection\n",
      "####\n",
      "and\n",
      "####\n",
      "pose\n",
      "####\n",
      "estimation\n",
      "####\n",
      "Reshaping\n",
      "####\n",
      "the\n",
      "####\n",
      "future\n",
      "####\n",
      "CVPR\n",
      "####\n",
      "Poon\n",
      "####\n",
      "and\n",
      "####\n",
      "Domingos\n",
      "####\n",
      "Sum-product\n",
      "####\n",
      "networks\n",
      "####\n",
      "new\n",
      "####\n",
      "deep\n",
      "####\n",
      "architecture\n",
      "####\n",
      "UAI\n",
      "####\n",
      "Ramanan\n",
      "####\n",
      "Learning\n",
      "####\n",
      "parse\n",
      "####\n",
      "images\n",
      "####\n",
      "articulated\n",
      "####\n",
      "bodies\n",
      "####\n",
      "NIPS\n",
      "####\n",
      "Ranzato\n",
      "####\n",
      "Huang\n",
      "####\n",
      "Y.-L.\n",
      "####\n",
      "Boureau\n",
      "####\n",
      "and\n",
      "####\n",
      "Lecun\n",
      "####\n",
      "Un-\n",
      "####\n",
      "supervised\n",
      "####\n",
      "learning\n",
      "####\n",
      "invariant\n",
      "####\n",
      "feature\n",
      "####\n",
      "hierarchies\n",
      "####\n",
      "with\n",
      "####\n",
      "ap-\n",
      "####\n",
      "plications\n",
      "####\n",
      "object\n",
      "####\n",
      "recognition\n",
      "####\n",
      "CVPR\n",
      "####\n",
      "Sapp\n",
      "####\n",
      "Toshev\n",
      "####\n",
      "and\n",
      "####\n",
      "Taskar\n",
      "####\n",
      "Cascaded\n",
      "####\n",
      "models\n",
      "####\n",
      "for\n",
      "####\n",
      "articulated\n",
      "####\n",
      "pose\n",
      "####\n",
      "estimation\n",
      "####\n",
      "ECCV\n",
      "####\n",
      "Srivastava\n",
      "####\n",
      "and\n",
      "####\n",
      "Salakhutdinov\n",
      "####\n",
      "Multimodal\n",
      "####\n",
      "learning\n",
      "####\n",
      "with\n",
      "####\n",
      "deep\n",
      "####\n",
      "boltzmann\n",
      "####\n",
      "machines\n",
      "####\n",
      "NIPS\n",
      "####\n",
      "Sun\n",
      "####\n",
      "and\n",
      "####\n",
      "Savarese\n",
      "####\n",
      "Articulated\n",
      "####\n",
      "part-based\n",
      "####\n",
      "model\n",
      "####\n",
      "for\n",
      "####\n",
      "joint\n",
      "####\n",
      "object\n",
      "####\n",
      "detection\n",
      "####\n",
      "and\n",
      "####\n",
      "pose\n",
      "####\n",
      "estimation\n",
      "####\n",
      "ICCV\n",
      "####\n",
      "Sun\n",
      "####\n",
      "Wang\n",
      "####\n",
      "and\n",
      "####\n",
      "Tang\n",
      "####\n",
      "Deep\n",
      "####\n",
      "convolutional\n",
      "####\n",
      "network\n",
      "####\n",
      "cascade\n",
      "####\n",
      "for\n",
      "####\n",
      "facial\n",
      "####\n",
      "point\n",
      "####\n",
      "detection\n",
      "####\n",
      "CVPR\n",
      "####\n",
      "Sun\n",
      "####\n",
      "Wang\n",
      "####\n",
      "and\n",
      "####\n",
      "Tang\n",
      "####\n",
      "Hybrid\n",
      "####\n",
      "deep\n",
      "####\n",
      "learning\n",
      "####\n",
      "for\n",
      "####\n",
      "computing\n",
      "####\n",
      "face\n",
      "####\n",
      "similarities\n",
      "####\n",
      "ICCV\n",
      "####\n",
      "Sun\n",
      "####\n",
      "Wang\n",
      "####\n",
      "and\n",
      "####\n",
      "Tang\n",
      "####\n",
      "Deep\n",
      "####\n",
      "learning\n",
      "####\n",
      "face\n",
      "####\n",
      "represen-\n",
      "####\n",
      "tation\n",
      "####\n",
      "from\n",
      "####\n",
      "predicting\n",
      "####\n",
      "10,000\n",
      "####\n",
      "classes\n",
      "####\n",
      "CVPR\n",
      "####\n",
      "Tian\n",
      "####\n",
      "Zitnick\n",
      "####\n",
      "and\n",
      "####\n",
      "Narasimhan\n",
      "####\n",
      "Exploring\n",
      "####\n",
      "the\n",
      "####\n",
      "spatial\n",
      "####\n",
      "hierarchy\n",
      "####\n",
      "mixture\n",
      "####\n",
      "models\n",
      "####\n",
      "for\n",
      "####\n",
      "human\n",
      "####\n",
      "pose\n",
      "####\n",
      "estima-\n",
      "####\n",
      "tion\n",
      "####\n",
      "ECCV\n",
      "####\n",
      "Tran\n",
      "####\n",
      "and\n",
      "####\n",
      "Forsyth\n",
      "####\n",
      "Improved\n",
      "####\n",
      "human\n",
      "####\n",
      "parsing\n",
      "####\n",
      "with\n",
      "####\n",
      "full\n",
      "####\n",
      "relational\n",
      "####\n",
      "model\n",
      "####\n",
      "ECCV\n",
      "####\n",
      "Wang\n",
      "####\n",
      "and\n",
      "####\n",
      "Beyond\n",
      "####\n",
      "physical\n",
      "####\n",
      "connections\n",
      "####\n",
      "Tree\n",
      "####\n",
      "mod-\n",
      "####\n",
      "els\n",
      "####\n",
      "human\n",
      "####\n",
      "pose\n",
      "####\n",
      "estimation\n",
      "####\n",
      "CVPR\n",
      "####\n",
      "Wang\n",
      "####\n",
      "and\n",
      "####\n",
      "Mori\n",
      "####\n",
      "Multiple\n",
      "####\n",
      "tree\n",
      "####\n",
      "models\n",
      "####\n",
      "for\n",
      "####\n",
      "occlusion\n",
      "####\n",
      "and\n",
      "####\n",
      "spatial\n",
      "####\n",
      "constraints\n",
      "####\n",
      "human\n",
      "####\n",
      "pose\n",
      "####\n",
      "estimation\n",
      "####\n",
      "ECCV\n",
      "####\n",
      "Wang\n",
      "####\n",
      "Tran\n",
      "####\n",
      "and\n",
      "####\n",
      "Liao\n",
      "####\n",
      "Learning\n",
      "####\n",
      "hierarchical\n",
      "####\n",
      "pose-\n",
      "####\n",
      "lets\n",
      "####\n",
      "for\n",
      "####\n",
      "human\n",
      "####\n",
      "parsing\n",
      "####\n",
      "CVPR\n",
      "####\n",
      "Yang\n",
      "####\n",
      "and\n",
      "####\n",
      "Ramanan\n",
      "####\n",
      "Articulated\n",
      "####\n",
      "pose\n",
      "####\n",
      "estimation\n",
      "####\n",
      "with\n",
      "####\n",
      "ﬂexible\n",
      "####\n",
      "mixtures-of-parts\n",
      "####\n",
      "CVPR\n",
      "####\n",
      "Yang\n",
      "####\n",
      "and\n",
      "####\n",
      "Ramanan\n",
      "####\n",
      "Articulated\n",
      "####\n",
      "human\n",
      "####\n",
      "detection\n",
      "####\n",
      "with\n",
      "####\n",
      "ﬂexible\n",
      "####\n",
      "mixtures-of-parts\n",
      "####\n",
      "IEEE\n",
      "####\n",
      "Trans\n",
      "####\n",
      "PAMI\n",
      "####\n",
      "appear\n",
      "####\n",
      "Zeiler\n",
      "####\n",
      "Taylor\n",
      "####\n",
      "and\n",
      "####\n",
      "Fergus\n",
      "####\n",
      "Adaptive\n",
      "####\n",
      "decon-\n",
      "####\n",
      "volutional\n",
      "####\n",
      "networks\n",
      "####\n",
      "for\n",
      "####\n",
      "mid\n",
      "####\n",
      "and\n",
      "####\n",
      "high\n",
      "####\n",
      "level\n",
      "####\n",
      "feature\n",
      "####\n",
      "learning\n",
      "####\n",
      "ICCV\n",
      "####\n",
      "Zeng\n",
      "####\n",
      "Ouyang\n",
      "####\n",
      "and\n",
      "####\n",
      "Wang\n",
      "####\n",
      "Multi-stage\n",
      "####\n",
      "contextual\n",
      "####\n",
      "deep\n",
      "####\n",
      "learning\n",
      "####\n",
      "for\n",
      "####\n",
      "pedestrian\n",
      "####\n",
      "detection\n",
      "####\n",
      "ICCV\n",
      "####\n",
      "Zhu\n",
      "####\n",
      "Luo\n",
      "####\n",
      "Wang\n",
      "####\n",
      "and\n",
      "####\n",
      "Tang\n",
      "####\n",
      "Deep\n",
      "####\n",
      "learning\n",
      "####\n",
      "identity\n",
      "####\n",
      "preserving\n",
      "####\n",
      "face\n",
      "####\n",
      "space\n",
      "####\n",
      "ICCV\n"
     ]
    }
   ],
   "source": [
    "for each in textAnalytics[0]['tokens']:\n",
    "    if each is not each.isdigit():\n",
    "        print \"####\"\n",
    "        print each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/agonzamart/anaconda/lib/python2.7/site-packages/IPython/core/formatters.py:92: DeprecationWarning: DisplayFormatter._ipython_display_formatter_default is deprecated: use @default decorator instead.\n",
      "  def _ipython_display_formatter_default(self):\n",
      "/Users/agonzamart/anaconda/lib/python2.7/site-packages/IPython/core/formatters.py:98: DeprecationWarning: DisplayFormatter._formatters_default is deprecated: use @default decorator instead.\n",
      "  def _formatters_default(self):\n",
      "/Users/agonzamart/anaconda/lib/python2.7/site-packages/IPython/core/formatters.py:677: DeprecationWarning: PlainTextFormatter._deferred_printers_default is deprecated: use @default decorator instead.\n",
      "  def _deferred_printers_default(self):\n",
      "/Users/agonzamart/anaconda/lib/python2.7/site-packages/IPython/core/formatters.py:669: DeprecationWarning: PlainTextFormatter._singleton_printers_default is deprecated: use @default decorator instead.\n",
      "  def _singleton_printers_default(self):\n",
      "/Users/agonzamart/anaconda/lib/python2.7/site-packages/IPython/core/formatters.py:672: DeprecationWarning: PlainTextFormatter._type_printers_default is deprecated: use @default decorator instead.\n",
      "  def _type_printers_default(self):\n",
      "/Users/agonzamart/anaconda/lib/python2.7/site-packages/IPython/core/formatters.py:669: DeprecationWarning: PlainTextFormatter._singleton_printers_default is deprecated: use @default decorator instead.\n",
      "  def _singleton_printers_default(self):\n",
      "/Users/agonzamart/anaconda/lib/python2.7/site-packages/IPython/core/formatters.py:672: DeprecationWarning: PlainTextFormatter._type_printers_default is deprecated: use @default decorator instead.\n",
      "  def _type_printers_default(self):\n",
      "/Users/agonzamart/anaconda/lib/python2.7/site-packages/IPython/core/formatters.py:677: DeprecationWarning: PlainTextFormatter._deferred_printers_default is deprecated: use @default decorator instead.\n",
      "  def _deferred_printers_default(self):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a =\"60.3\"\n",
    "a.isdigit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'Multi-source', u'Deep', u'Learning')\n",
      "(u'Deep', u'Learning', u'for')\n",
      "(u'Learning', u'for', u'Human')\n",
      "(u'for', u'Human', u'Pose')\n",
      "(u'Human', u'Pose', u'Estimation')\n",
      "(u'Pose', u'Estimation', u'Wanli')\n",
      "(u'Estimation', u'Wanli', u'Ouyang')\n",
      "(u'Wanli', u'Ouyang', u'Xiao')\n",
      "(u'Ouyang', u'Xiao', u'Chu')\n",
      "(u'Xiao', u'Chu', u'Xiaogang')\n",
      "(u'Chu', u'Xiaogang', u'Wang')\n",
      "(u'Xiaogang', u'Wang', u'Department')\n",
      "(u'Wang', u'Department', u'Electronic')\n",
      "(u'Department', u'Electronic', u'Engineering')\n",
      "(u'Electronic', u'Engineering', u'The')\n",
      "(u'Engineering', u'The', u'Chinese')\n",
      "(u'The', u'Chinese', u'University')\n",
      "(u'Chinese', u'University', u'Hong')\n",
      "(u'University', u'Hong', u'Kong')\n",
      "(u'Hong', u'Kong', u'wlouyang')\n",
      "(u'Kong', u'wlouyang', u'ee.cuhk.edu.hk')\n",
      "(u'wlouyang', u'ee.cuhk.edu.hk', u'xgwang')\n",
      "(u'ee.cuhk.edu.hk', u'xgwang', u'ee.cuhk.edu.hk')\n",
      "(u'xgwang', u'ee.cuhk.edu.hk', u'Abstract')\n",
      "(u'ee.cuhk.edu.hk', u'Abstract', u'Visual')\n",
      "(u'Abstract', u'Visual', u'appearance')\n",
      "(u'Visual', u'appearance', u'score')\n",
      "(u'appearance', u'score', u'appearance')\n",
      "(u'score', u'appearance', u'mixture')\n",
      "(u'appearance', u'mixture', u'type')\n",
      "(u'mixture', u'type', u'and')\n",
      "(u'type', u'and', u'deformation')\n",
      "(u'and', u'deformation', u'are')\n",
      "(u'deformation', u'are', u'three')\n",
      "(u'are', u'three', u'important')\n",
      "(u'three', u'important', u'information')\n",
      "(u'important', u'information', u'sources')\n",
      "(u'information', u'sources', u'for')\n",
      "(u'sources', u'for', u'human')\n",
      "(u'for', u'human', u'pose')\n",
      "(u'human', u'pose', u'estimation')\n",
      "(u'pose', u'estimation', u'This')\n",
      "(u'estimation', u'This', u'paper')\n",
      "(u'This', u'paper', u'proposes')\n",
      "(u'paper', u'proposes', u'build')\n",
      "(u'proposes', u'build', u'multi-source')\n",
      "(u'build', u'multi-source', u'deep')\n",
      "(u'multi-source', u'deep', u'model')\n",
      "(u'deep', u'model', u'order')\n",
      "(u'model', u'order', u'extract')\n",
      "(u'order', u'extract', u'non-linear')\n",
      "(u'extract', u'non-linear', u'representation')\n",
      "(u'non-linear', u'representation', u'from')\n",
      "(u'representation', u'from', u'these')\n",
      "(u'from', u'these', u'different')\n",
      "(u'these', u'different', u'aspects')\n",
      "(u'different', u'aspects', u'information')\n",
      "(u'aspects', u'information', u'sources')\n",
      "(u'information', u'sources', u'With')\n",
      "(u'sources', u'With', u'the')\n",
      "(u'With', u'the', u'deep')\n",
      "(u'the', u'deep', u'model')\n",
      "(u'deep', u'model', u'the')\n",
      "(u'model', u'the', u'global')\n",
      "(u'the', u'global', u'high-order')\n",
      "(u'global', u'high-order', u'hu-')\n",
      "(u'high-order', u'hu-', u'man')\n",
      "(u'hu-', u'man', u'body')\n",
      "(u'man', u'body', u'articulation')\n",
      "(u'body', u'articulation', u'patterns')\n",
      "(u'articulation', u'patterns', u'these')\n",
      "(u'patterns', u'these', u'information')\n",
      "(u'these', u'information', u'sources')\n",
      "(u'information', u'sources', u'are')\n",
      "(u'sources', u'are', u'extracted')\n",
      "(u'are', u'extracted', u'for')\n",
      "(u'extracted', u'for', u'pose')\n",
      "(u'for', u'pose', u'estimation')\n",
      "(u'pose', u'estimation', u'The')\n",
      "(u'estimation', u'The', u'task')\n",
      "(u'The', u'task', u'for')\n",
      "(u'task', u'for', u'estimat-')\n",
      "(u'for', u'estimat-', u'ing')\n",
      "(u'estimat-', u'ing', u'body')\n",
      "(u'ing', u'body', u'locations')\n",
      "(u'body', u'locations', u'and')\n",
      "(u'locations', u'and', u'the')\n",
      "(u'and', u'the', u'task')\n",
      "(u'the', u'task', u'for')\n",
      "(u'task', u'for', u'human')\n",
      "(u'for', u'human', u'detection')\n",
      "(u'human', u'detection', u'are')\n",
      "(u'detection', u'are', u'jointly')\n",
      "(u'are', u'jointly', u'learned')\n",
      "(u'jointly', u'learned', u'using')\n",
      "(u'learned', u'using', u'uni\\ufb01ed')\n",
      "(u'using', u'uni\\ufb01ed', u'deep')\n",
      "(u'uni\\ufb01ed', u'deep', u'model')\n",
      "(u'deep', u'model', u'The')\n",
      "(u'model', u'The', u'proposed')\n",
      "(u'The', u'proposed', u'approach')\n",
      "(u'proposed', u'approach', u'can')\n",
      "(u'approach', u'can', u'viewed')\n",
      "(u'can', u'viewed', u'post-processing')\n",
      "(u'viewed', u'post-processing', u'pose')\n",
      "(u'post-processing', u'pose', u'esti-')\n",
      "(u'pose', u'esti-', u'mation')\n",
      "(u'esti-', u'mation', u'results')\n",
      "(u'mation', u'results', u'and')\n",
      "(u'results', u'and', u'can')\n",
      "(u'and', u'can', u'\\ufb02exibly')\n",
      "(u'can', u'\\ufb02exibly', u'integrate')\n",
      "(u'\\ufb02exibly', u'integrate', u'with')\n",
      "(u'integrate', u'with', u'existing')\n",
      "(u'with', u'existing', u'meth-')\n",
      "(u'existing', u'meth-', u'ods')\n",
      "(u'meth-', u'ods', u'taking')\n",
      "(u'ods', u'taking', u'their')\n",
      "(u'taking', u'their', u'information')\n",
      "(u'their', u'information', u'sources')\n",
      "(u'information', u'sources', u'input')\n",
      "(u'sources', u'input', u'extract-')\n",
      "(u'input', u'extract-', u'ing')\n",
      "(u'extract-', u'ing', u'the')\n",
      "(u'ing', u'the', u'non-linear')\n",
      "(u'the', u'non-linear', u'representation')\n",
      "(u'non-linear', u'representation', u'from')\n",
      "(u'representation', u'from', u'multiple')\n",
      "(u'from', u'multiple', u'information')\n",
      "(u'multiple', u'information', u'sources')\n",
      "(u'information', u'sources', u'the')\n",
      "(u'sources', u'the', u'deep')\n",
      "(u'the', u'deep', u'model')\n",
      "(u'deep', u'model', u'outperforms')\n",
      "(u'model', u'outperforms', u'state-of-the-art')\n",
      "(u'outperforms', u'state-of-the-art', u'8.6')\n",
      "(u'state-of-the-art', u'8.6', u'percent')\n",
      "(u'8.6', u'percent', u'three')\n",
      "(u'percent', u'three', u'public')\n",
      "(u'three', u'public', u'benchmark')\n",
      "(u'public', u'benchmark', u'datasets')\n",
      "(u'benchmark', u'datasets', u'Introduction')\n",
      "(u'datasets', u'Introduction', u'Human')\n",
      "(u'Introduction', u'Human', u'pose')\n",
      "(u'Human', u'pose', u'estimation')\n",
      "(u'pose', u'estimation', u'the')\n",
      "(u'estimation', u'the', u'process')\n",
      "(u'the', u'process', u'determining')\n",
      "(u'process', u'determining', u'from')\n",
      "(u'determining', u'from', u'image')\n",
      "(u'from', u'image', u'the')\n",
      "(u'image', u'the', u'positions')\n",
      "(u'the', u'positions', u'human')\n",
      "(u'positions', u'human', u'body')\n",
      "(u'human', u'body', u'parts')\n",
      "(u'body', u'parts', u'such')\n",
      "(u'parts', u'such', u'the')\n",
      "(u'such', u'the', u'head')\n",
      "(u'the', u'head', u'shoulder')\n",
      "(u'head', u'shoulder', u'elbow')\n",
      "(u'shoulder', u'elbow', u'wrist')\n",
      "(u'elbow', u'wrist', u'hip')\n",
      "(u'wrist', u'hip', u'knee')\n",
      "(u'hip', u'knee', u'and')\n",
      "(u'knee', u'and', u'ankle')\n",
      "(u'and', u'ankle', u'fundamental')\n",
      "(u'ankle', u'fundamental', u'problem')\n",
      "(u'fundamental', u'problem', u'computer')\n",
      "(u'problem', u'computer', u'vision')\n",
      "(u'computer', u'vision', u'and')\n",
      "(u'vision', u'and', u'has')\n",
      "(u'and', u'has', u'abun-')\n",
      "(u'has', u'abun-', u'dant')\n",
      "(u'abun-', u'dant', u'important')\n",
      "(u'dant', u'important', u'applications')\n",
      "(u'important', u'applications', u'such')\n",
      "(u'applications', u'such', u'sports')\n",
      "(u'such', u'sports', u'action')\n",
      "(u'sports', u'action', u'recogni-')\n",
      "(u'action', u'recogni-', u'tion')\n",
      "(u'recogni-', u'tion', u'character')\n",
      "(u'tion', u'character', u'animation')\n",
      "(u'character', u'animation', u'clinical')\n",
      "(u'animation', u'clinical', u'analysis')\n",
      "(u'clinical', u'analysis', u'gait')\n",
      "(u'analysis', u'gait', u'patholo-')\n",
      "(u'gait', u'patholo-', u'gies')\n",
      "(u'patholo-', u'gies', u'content-based')\n",
      "(u'gies', u'content-based', u'video')\n",
      "(u'content-based', u'video', u'and')\n",
      "(u'video', u'and', u'image')\n",
      "(u'and', u'image', u'retrieval')\n",
      "(u'image', u'retrieval', u'and')\n",
      "(u'retrieval', u'and', u'intelli-')\n",
      "(u'and', u'intelli-', u'gent')\n",
      "(u'intelli-', u'gent', u'video')\n",
      "(u'gent', u'video', u'surveillance')\n",
      "(u'video', u'surveillance', u'Despite')\n",
      "(u'surveillance', u'Despite', u'many')\n",
      "(u'Despite', u'many', u'years')\n",
      "(u'many', u'years', u'research')\n",
      "(u'years', u'research', u'pose')\n",
      "(u'research', u'pose', u'estimation')\n",
      "(u'pose', u'estimation', u'remains')\n",
      "(u'estimation', u'remains', u'dif\\ufb01-')\n",
      "(u'remains', u'dif\\ufb01-', u'cult')\n",
      "(u'dif\\ufb01-', u'cult', u'problem')\n",
      "(u'cult', u'problem', u'One')\n",
      "(u'problem', u'One', u'the')\n",
      "(u'One', u'the', u'most')\n",
      "(u'the', u'most', u'signi\\ufb01cant')\n",
      "(u'most', u'signi\\ufb01cant', u'challenges')\n",
      "(u'signi\\ufb01cant', u'challenges', u'pose')\n",
      "(u'challenges', u'pose', u'estimation')\n",
      "(u'pose', u'estimation', u'how')\n",
      "(u'estimation', u'how', u'model')\n",
      "(u'how', u'model', u'the')\n",
      "(u'model', u'the', u'complex')\n",
      "(u'the', u'complex', u'human')\n",
      "(u'complex', u'human', u'articulation')\n",
      "(u'human', u'articulation', u'Many')\n",
      "(u'articulation', u'Many', u'approaches')\n",
      "(u'Many', u'approaches', u'have')\n",
      "(u'approaches', u'have', u'been')\n",
      "(u'have', u'been', u'used')\n",
      "(u'been', u'used', u'handle')\n",
      "(u'used', u'handle', u'the')\n",
      "(u'handle', u'the', u'com-')\n",
      "(u'the', u'com-', u'plex')\n",
      "(u'com-', u'plex', u'human')\n",
      "(u'plex', u'human', u'articulation')\n",
      "(u'human', u'articulation', u'using')\n",
      "(u'articulation', u'using', u'three')\n",
      "(u'using', u'three', u'information')\n",
      "(u'three', u'information', u'sources')\n",
      "(u'information', u'sources', u'mixture')\n",
      "(u'sources', u'mixture', u'type')\n",
      "(u'mixture', u'type', u'appearance')\n",
      "(u'type', u'appearance', u'score')\n",
      "(u'appearance', u'score', u'and')\n",
      "(u'score', u'and', u'deformation')\n",
      "(u'and', u'deformation', u'In\\ufb02uenced')\n",
      "(u'deformation', u'In\\ufb02uenced', u'human')\n",
      "(u'In\\ufb02uenced', u'human', u'body')\n",
      "(u'human', u'body', u'articulation')\n",
      "(u'body', u'articulation', u'clothing')\n",
      "(u'articulation', u'clothing', u'occlusion')\n",
      "(u'clothing', u'occlusion', u'etc.')\n",
      "(u'occlusion', u'etc.', u'body')\n",
      "(u'etc.', u'body', u'part')\n",
      "(u'body', u'part', u'appearance')\n",
      "(u'part', u'appearance', u'varies')\n",
      "(u'appearance', u'varies', u'handle')\n",
      "(u'varies', u'handle', u'this')\n",
      "(u'handle', u'this', u'variation')\n",
      "(u'this', u'variation', u'the')\n",
      "(u'variation', u'the', u'appearance')\n",
      "(u'the', u'appearance', u'part')\n",
      "(u'appearance', u'part', u'clustered')\n",
      "(u'part', u'clustered', u'into')\n",
      "(u'clustered', u'into', u'multiple')\n",
      "(u'into', u'multiple', u'mixture')\n",
      "(u'multiple', u'mixture', u'types')\n",
      "(u'mixture', u'types', u'shown')\n",
      "(u'types', u'shown', u'Fig')\n",
      "(u'shown', u'Fig', u'For')\n",
      "(u'Fig', u'For', u'each')\n",
      "(u'For', u'each', u'mixture')\n",
      "(u'each', u'mixture', u'type')\n",
      "(u'mixture', u'type', u'part')\n",
      "(u'type', u'part', u'part')\n",
      "(u'part', u'part', u'template')\n",
      "(u'part', u'template', u'learned')\n",
      "(u'template', u'learned', u'capture')\n",
      "(u'learned', u'capture', u'its')\n",
      "(u'capture', u'its', u'appearance')\n",
      "(u'its', u'appearance', u'Then')\n",
      "(u'appearance', u'Then', u'the')\n",
      "(u'Then', u'the', u'appearance')\n",
      "(u'the', u'appearance', u'scores')\n",
      "(u'appearance', u'scores', u'log-likelihoods')\n",
      "(u'scores', u'log-likelihoods', u'body')\n",
      "(u'log-likelihoods', u'body', u'parts')\n",
      "(u'body', u'parts', u'Figure')\n",
      "(u'parts', u'Figure', u'The')\n",
      "(u'Figure', u'The', u'motivation')\n",
      "(u'The', u'motivation', u'this')\n",
      "(u'motivation', u'this', u'paper')\n",
      "(u'this', u'paper', u'using')\n",
      "(u'paper', u'using', u'multi-source')\n",
      "(u'using', u'multi-source', u'deep')\n",
      "(u'multi-source', u'deep', u'model')\n",
      "(u'deep', u'model', u'for')\n",
      "(u'model', u'for', u'constructing')\n",
      "(u'for', u'constructing', u'the')\n",
      "(u'constructing', u'the', u'non-linear')\n",
      "(u'the', u'non-linear', u'representation')\n",
      "(u'non-linear', u'representation', u'from')\n",
      "(u'representation', u'from', u'three')\n",
      "(u'from', u'three', u'in-')\n",
      "(u'three', u'in-', u'formation')\n",
      "(u'in-', u'formation', u'sources')\n",
      "(u'formation', u'sources', u'mixture')\n",
      "(u'sources', u'mixture', u'type')\n",
      "(u'mixture', u'type', u'appearance')\n",
      "(u'type', u'appearance', u'score')\n",
      "(u'appearance', u'score', u'and')\n",
      "(u'score', u'and', u'deforma-')\n",
      "(u'and', u'deforma-', u'tion')\n",
      "(u'deforma-', u'tion', u'Best')\n",
      "(u'tion', u'Best', u'viewed')\n",
      "(u'Best', u'viewed', u'color')\n",
      "(u'viewed', u'color', u'being')\n",
      "(u'color', u'being', u'different')\n",
      "(u'being', u'different', u'locations')\n",
      "(u'different', u'locations', u'are')\n",
      "(u'locations', u'are', u'obtained')\n",
      "(u'are', u'obtained', u'convolving')\n",
      "(u'obtained', u'convolving', u'the')\n",
      "(u'convolving', u'the', u'part')\n",
      "(u'the', u'part', u'templates')\n",
      "(u'part', u'templates', u'with')\n",
      "(u'templates', u'with', u'the')\n",
      "(u'with', u'the', u'visual')\n",
      "(u'the', u'visual', u'features')\n",
      "(u'visual', u'features', u'the')\n",
      "(u'features', u'the', u'input')\n",
      "(u'the', u'input', u'image')\n",
      "(u'input', u'image', u'e.g')\n",
      "(u'image', u'e.g', u'HOG')\n",
      "(u'e.g', u'HOG', u'The')\n",
      "(u'HOG', u'The', u'appearance')\n",
      "(u'The', u'appearance', u'scores')\n",
      "(u'appearance', u'scores', u'are')\n",
      "(u'scores', u'are', u'inaccurate')\n",
      "(u'are', u'inaccurate', u'for')\n",
      "(u'inaccurate', u'for', u'well-locating')\n",
      "(u'for', u'well-locating', u'body')\n",
      "(u'well-locating', u'body', u'parts')\n",
      "(u'body', u'parts', u'because')\n",
      "(u'parts', u'because', u'the')\n",
      "(u'because', u'the', u'part')\n",
      "(u'the', u'part', u'template')\n",
      "(u'part', u'template', u'imper-')\n",
      "(u'template', u'imper-', u'fect')\n",
      "(u'imper-', u'fect', u'Therefore')\n",
      "(u'fect', u'Therefore', u'the')\n",
      "(u'Therefore', u'the', u'deformations')\n",
      "(u'the', u'deformations', u'relative')\n",
      "(u'deformations', u'relative', u'locations')\n",
      "(u'relative', u'locations', u'among')\n",
      "(u'locations', u'among', u'body')\n",
      "(u'among', u'body', u'parts')\n",
      "(u'body', u'parts', u'are')\n",
      "(u'parts', u'are', u'used')\n",
      "(u'are', u'used', u'for')\n",
      "(u'used', u'for', u'encoding')\n",
      "(u'for', u'encoding', u'likely')\n",
      "(u'encoding', u'likely', u'pairwise')\n",
      "(u'likely', u'pairwise', u'poses')\n",
      "(u'pairwise', u'poses', u'for')\n",
      "(u'poses', u'for', u'example')\n",
      "(u'for', u'example', u'the')\n",
      "(u'example', u'the', u'head')\n",
      "(u'the', u'head', u'should')\n",
      "(u'head', u'should', u'not')\n",
      "(u'should', u'not', u'far')\n",
      "(u'not', u'far', u'from')\n",
      "(u'far', u'from', u'the')\n",
      "(u'from', u'the', u'neck')\n",
      "(u'the', u'neck', u'Existing')\n",
      "(u'neck', u'Existing', u'approaches')\n",
      "(u'Existing', u'approaches', u'use')\n",
      "(u'approaches', u'use', u'log-linear')\n",
      "(u'use', u'log-linear', u'models')\n",
      "(u'log-linear', u'models', u'with')\n",
      "(u'models', u'with', u'pairwise')\n",
      "(u'with', u'pairwise', u'potentials')\n",
      "(u'pairwise', u'potentials', u'these')\n",
      "(u'potentials', u'these', u'three')\n",
      "(u'these', u'three', u'information')\n",
      "(u'three', u'information', u'sources')\n",
      "(u'information', u'sources', u'determine')\n",
      "(u'sources', u'determine', u'whether')\n",
      "(u'determine', u'whether', u'estimated')\n",
      "(u'whether', u'estimated', u'location')\n",
      "(u'estimated', u'location', u'correct')\n",
      "(u'location', u'correct', u'However')\n",
      "(u'correct', u'However', u'these')\n",
      "(u'However', u'these', u'information')\n",
      "(u'these', u'information', u'sources')\n",
      "(u'information', u'sources', u'are')\n",
      "(u'sources', u'are', u'not')\n",
      "(u'are', u'not', u'log-linearly')\n",
      "(u'not', u'log-linearly', u'cor-')\n",
      "(u'log-linearly', u'cor-', u'related')\n",
      "(u'cor-', u'related', u'when')\n",
      "(u'related', u'when', u'choosing')\n",
      "(u'when', u'choosing', u'the')\n",
      "(u'choosing', u'the', u'correct')\n",
      "(u'the', u'correct', u'candidate')\n",
      "(u'correct', u'candidate', u'For')\n",
      "(u'candidate', u'For', u'the')\n",
      "(u'For', u'the', u'exam-')\n",
      "(u'the', u'exam-', u'ple')\n",
      "(u'exam-', u'ple', u'Fig')\n",
      "(u'ple', u'Fig', u'linear')\n",
      "(u'Fig', u'linear', u'models')\n",
      "(u'linear', u'models', u'may')\n",
      "(u'models', u'may', u'\\ufb01nd')\n",
      "(u'may', u'\\ufb01nd', u'that')\n",
      "(u'\\ufb01nd', u'that', u'the')\n",
      "(u'that', u'the', u'estimated')\n",
      "(u'the', u'estimated', u'result')\n",
      "(u'estimated', u'result', u'the')\n",
      "(u'result', u'the', u'left')\n",
      "(u'the', u'left', u'and')\n",
      "(u'left', u'and', u'the')\n",
      "(u'and', u'the', u'result')\n",
      "(u'the', u'result', u'the')\n",
      "(u'result', u'the', u'right')\n",
      "(u'the', u'right', u'have')\n",
      "(u'right', u'have', u'the')\n",
      "(u'have', u'the', u'same')\n",
      "(u'the', u'same', u'deformation')\n",
      "(u'same', u'deformation', u'score')\n",
      "(u'deformation', u'score', u'because')\n",
      "(u'score', u'because', u'they')\n",
      "(u'because', u'they', u'simply')\n",
      "(u'they', u'simply', u'linearly')\n",
      "(u'simply', u'linearly', u'add')\n",
      "(u'linearly', u'add', u'local')\n",
      "(u'add', u'local', u'deformation')\n",
      "(u'local', u'deformation', u'cost')\n",
      "(u'deformation', u'cost', u'While')\n",
      "(u'cost', u'While', u'obvious')\n",
      "(u'While', u'obvious', u'for')\n",
      "(u'obvious', u'for', u'human')\n",
      "(u'for', u'human', u'\\ufb01nd')\n",
      "(u'human', u'\\ufb01nd', u'that')\n",
      "(u'\\ufb01nd', u'that', u'the')\n",
      "(u'that', u'the', u'result')\n",
      "(u'the', u'result', u'the')\n",
      "(u'result', u'the', u'left')\n",
      "(u'the', u'left', u'not')\n",
      "(u'left', u'not', u'reasonable')\n",
      "(u'not', u'reasonable', u'Similar')\n",
      "(u'reasonable', u'Similar', u'situations')\n",
      "(u'Similar', u'situations', u'also')\n",
      "(u'situations', u'also', u'occur')\n",
      "(u'also', u'occur', u'for')\n",
      "(u'occur', u'for', u'mixture')\n",
      "(u'for', u'mixture', u'type')\n",
      "(u'mixture', u'type', u'and')\n",
      "(u'type', u'and', u'appearance')\n",
      "(u'and', u'appearance', u'score')\n",
      "(u'appearance', u'score', u'There-')\n",
      "(u'score', u'There-', u'fore')\n",
      "(u'There-', u'fore', u'desirable')\n",
      "(u'fore', u'desirable', u'construct')\n",
      "(u'desirable', u'construct', u'the')\n",
      "(u'construct', u'the', u'non-linear')\n",
      "(u'the', u'non-linear', u'representation')\n",
      "(u'non-linear', u'representation', u'that')\n",
      "(u'representation', u'that', u'identi\\ufb01es')\n",
      "(u'that', u'identi\\ufb01es', u'reasonable')\n",
      "(u'identi\\ufb01es', u'reasonable', u'con\\ufb01gurations')\n",
      "(u'reasonable', u'con\\ufb01gurations', u'deformation')\n",
      "(u'con\\ufb01gurations', u'deformation', u'ap-')\n",
      "(u'deformation', u'ap-', u'pearance')\n",
      "(u'ap-', u'pearance', u'score')\n",
      "(u'pearance', u'score', u'and')\n",
      "(u'score', u'and', u'mixture')\n",
      "(u'and', u'mixture', u'type')\n",
      "(u'mixture', u'type', u'order')\n",
      "(u'type', u'order', u'construct')\n",
      "(u'order', u'construct', u'useful')\n",
      "(u'construct', u'useful', u'representation')\n",
      "(u'useful', u'representation', u'from')\n",
      "(u'representation', u'from', u'multi-')\n",
      "(u'from', u'multi-', u'ple')\n",
      "(u'multi-', u'ple', u'information')\n",
      "(u'ple', u'information', u'sources')\n",
      "(u'information', u'sources', u'for')\n",
      "(u'sources', u'for', u'pose')\n",
      "(u'for', u'pose', u'estimation')\n",
      "(u'pose', u'estimation', u'model')\n",
      "(u'estimation', u'model', u'should')\n",
      "(u'model', u'should', u'Multi-sourcedeep')\n",
      "(u'should', u'Multi-sourcedeep', u'modelEstimated')\n",
      "(u'Multi-sourcedeep', u'modelEstimated', u'resultType')\n",
      "(u'modelEstimated', u'resultType', u'1Type')\n",
      "(u'resultType', u'1Type', u'2Non-Linearmodel')\n",
      "(u'1Type', u'2Non-Linearmodel', u'YesNoEstimated')\n",
      "(u'2Non-Linearmodel', u'YesNoEstimated', u'resultMixture')\n",
      "(u'YesNoEstimated', u'resultMixture', u'typeHead')\n",
      "(u'resultMixture', u'typeHead', u'topNeckDeformationAppearance')\n",
      "(u'typeHead', u'topNeckDeformationAppearance', u'scoreTemplateLinear')\n",
      "(u'topNeckDeformationAppearance', u'scoreTemplateLinear', u'model1389')\n",
      "(u'scoreTemplateLinear', u'model1389', u'satisfy')\n",
      "(u'model1389', u'satisfy', u'certain')\n",
      "(u'satisfy', u'certain', u'properties')\n",
      "(u'certain', u'properties', u'First')\n",
      "(u'properties', u'First', u'the')\n",
      "(u'First', u'the', u'model')\n",
      "(u'the', u'model', u'should')\n",
      "(u'model', u'should', u'capture')\n",
      "(u'should', u'capture', u'the')\n",
      "(u'capture', u'the', u'global')\n",
      "(u'the', u'global', u'complex')\n",
      "(u'global', u'complex', u'relationships')\n",
      "(u'complex', u'relationships', u'among')\n",
      "(u'relationships', u'among', u'body')\n",
      "(u'among', u'body', u'parts')\n",
      "(u'body', u'parts', u'For')\n",
      "(u'parts', u'For', u'the')\n",
      "(u'For', u'the', u'example')\n",
      "(u'the', u'example', u'Fig')\n",
      "(u'example', u'Fig', u'the')\n",
      "(u'Fig', u'the', u'result')\n",
      "(u'the', u'result', u'the')\n",
      "(u'result', u'the', u'left')\n",
      "(u'the', u'left', u'unreasonable')\n",
      "(u'left', u'unreasonable', u'because')\n",
      "(u'unreasonable', u'because', u'its')\n",
      "(u'because', u'its', u'global')\n",
      "(u'its', u'global', u'con\\ufb01guration')\n",
      "(u'global', u'con\\ufb01guration', u'arm')\n",
      "(u'con\\ufb01guration', u'arm', u'torso')\n",
      "(u'arm', u'torso', u'and')\n",
      "(u'torso', u'and', u'leg')\n",
      "(u'and', u'leg', u'Second')\n",
      "(u'leg', u'Second', u'since')\n",
      "(u'Second', u'since', u'reasonable')\n",
      "(u'since', u'reasonable', u'con\\ufb01guration')\n",
      "(u'reasonable', u'con\\ufb01guration', u'very')\n",
      "(u'con\\ufb01guration', u'very', u'abstract')\n",
      "(u'very', u'abstract', u'concept')\n",
      "(u'abstract', u'concept', u'while')\n",
      "(u'concept', u'while', u'the')\n",
      "(u'while', u'the', u'information')\n",
      "(u'the', u'information', u'sources')\n",
      "(u'information', u'sources', u'are')\n",
      "(u'sources', u'are', u'less')\n",
      "(u'are', u'less', u'abstract')\n",
      "(u'less', u'abstract', u'con-')\n",
      "(u'abstract', u'con-', u'cepts')\n",
      "(u'con-', u'cepts', u'the')\n",
      "(u'cepts', u'the', u'model')\n",
      "(u'the', u'model', u'should')\n",
      "(u'model', u'should', u'construct')\n",
      "(u'should', u'construct', u'more')\n",
      "(u'construct', u'more', u'abstract')\n",
      "(u'more', u'abstract', u'representa-')\n",
      "(u'abstract', u'representa-', u'tion')\n",
      "(u'representa-', u'tion', u'from')\n",
      "(u'tion', u'from', u'the')\n",
      "(u'from', u'the', u'less')\n",
      "(u'the', u'less', u'abstract')\n",
      "(u'less', u'abstract', u'representation')\n",
      "(u'abstract', u'representation', u'Third')\n",
      "(u'representation', u'Third', u'since')\n",
      "(u'Third', u'since', u'dif-')\n",
      "(u'since', u'dif-', u'ferent')\n",
      "(u'dif-', u'ferent', u'information')\n",
      "(u'ferent', u'information', u'sources')\n",
      "(u'information', u'sources', u'describe')\n",
      "(u'sources', u'describe', u'different')\n",
      "(u'describe', u'different', u'aspects')\n",
      "(u'different', u'aspects', u'hu-')\n",
      "(u'aspects', u'hu-', u'man')\n",
      "(u'hu-', u'man', u'pose')\n",
      "(u'man', u'pose', u'and')\n",
      "(u'pose', u'and', u'have')\n",
      "(u'and', u'have', u'different')\n",
      "(u'have', u'different', u'statistical')\n",
      "(u'different', u'statistical', u'properties')\n",
      "(u'statistical', u'properties', u'the')\n",
      "(u'properties', u'the', u'model')\n",
      "(u'the', u'model', u'should')\n",
      "(u'model', u'should', u'learn')\n",
      "(u'should', u'learn', u'useful')\n",
      "(u'learn', u'useful', u'representation')\n",
      "(u'useful', u'representation', u'from')\n",
      "(u'representation', u'from', u'these')\n",
      "(u'from', u'these', u'sources')\n",
      "(u'these', u'sources', u'and')\n",
      "(u'sources', u'and', u'fuse')\n",
      "(u'and', u'fuse', u'them')\n",
      "(u'fuse', u'them', u'into')\n",
      "(u'them', u'into', u'joint')\n",
      "(u'into', u'joint', u'representation')\n",
      "(u'joint', u'representation', u'for')\n",
      "(u'representation', u'for', u'pose')\n",
      "(u'for', u'pose', u'estimation')\n",
      "(u'pose', u'estimation', u'The')\n",
      "(u'estimation', u'The', u'multi-source')\n",
      "(u'The', u'multi-source', u'deep')\n",
      "(u'multi-source', u'deep', u'architecture')\n",
      "(u'deep', u'architecture', u'propose')\n",
      "(u'architecture', u'propose', u'satis\\ufb01es')\n",
      "(u'propose', u'satis\\ufb01es', u'the')\n",
      "(u'satis\\ufb01es', u'the', u'above')\n",
      "(u'the', u'above', u'requirement')\n",
      "(u'above', u'requirement', u'There')\n",
      "(u'requirement', u'There', u'are')\n",
      "(u'There', u'are', u'three')\n",
      "(u'are', u'three', u'contributions')\n",
      "(u'three', u'contributions', u'this')\n",
      "(u'contributions', u'this', u'paper')\n",
      "(u'this', u'paper', u'propose')\n",
      "(u'paper', u'propose', u'deep')\n",
      "(u'propose', u'deep', u'architecture')\n",
      "(u'deep', u'architecture', u'construct')\n",
      "(u'architecture', u'construct', u'the')\n",
      "(u'construct', u'the', u'non-')\n",
      "(u'the', u'non-', u'linear')\n",
      "(u'non-', u'linear', u'representation')\n",
      "(u'linear', u'representation', u'from')\n",
      "(u'representation', u'from', u'different')\n",
      "(u'from', u'different', u'aspects')\n",
      "(u'different', u'aspects', u'informa-')\n",
      "(u'aspects', u'informa-', u'tion')\n",
      "(u'informa-', u'tion', u'sources')\n",
      "(u'tion', u'sources', u'the')\n",
      "(u'sources', u'the', u'best')\n",
      "(u'the', u'best', u'our')\n",
      "(u'best', u'our', u'knowledge')\n",
      "(u'our', u'knowledge', u'this')\n",
      "(u'knowledge', u'this', u'paper')\n",
      "(u'this', u'paper', u'the')\n",
      "(u'paper', u'the', u'\\ufb01rst')\n",
      "(u'the', u'\\ufb01rst', u'use')\n",
      "(u'\\ufb01rst', u'use', u'deep')\n",
      "(u'use', u'deep', u'model')\n",
      "(u'deep', u'model', u'for')\n",
      "(u'model', u'for', u'pose')\n",
      "(u'for', u'pose', u'estimation')\n",
      "(u'pose', u'estimation', u'The')\n",
      "(u'estimation', u'The', u'body')\n",
      "(u'The', u'body', u'articulation')\n",
      "(u'body', u'articulation', u'patterns')\n",
      "(u'articulation', u'patterns', u'global')\n",
      "(u'patterns', u'global', u'and')\n",
      "(u'global', u'and', u'more')\n",
      "(u'and', u'more', u'abstract')\n",
      "(u'more', u'abstract', u'representations')\n",
      "(u'abstract', u'representations', u'are')\n",
      "(u'representations', u'are', u'captured')\n",
      "(u'are', u'captured', u'the')\n",
      "(u'captured', u'the', u'deep')\n",
      "(u'the', u'deep', u'model')\n",
      "(u'deep', u'model', u'from')\n",
      "(u'model', u'from', u'the')\n",
      "(u'from', u'the', u'information')\n",
      "(u'the', u'information', u'sources')\n",
      "(u'information', u'sources', u'local')\n",
      "(u'sources', u'local', u'and')\n",
      "(u'local', u'and', u'less')\n",
      "(u'and', u'less', u'abstract')\n",
      "(u'less', u'abstract', u'representa-')\n",
      "(u'abstract', u'representa-', u'tions')\n",
      "(u'representa-', u'tions', u'For')\n",
      "(u'tions', u'For', u'each')\n",
      "(u'For', u'each', u'information')\n",
      "(u'each', u'information', u'source')\n",
      "(u'information', u'source', u'more')\n",
      "(u'source', u'more', u'abstract')\n",
      "(u'more', u'abstract', u'repre-')\n",
      "(u'abstract', u'repre-', u'sentation')\n",
      "(u'repre-', u'sentation', u'the')\n",
      "(u'sentation', u'the', u'higher')\n",
      "(u'the', u'higher', u'layer')\n",
      "(u'higher', u'layer', u'composed')\n",
      "(u'layer', u'composed', u'the')\n",
      "(u'composed', u'the', u'less')\n",
      "(u'the', u'less', u'ab-')\n",
      "(u'less', u'ab-', u'stract')\n",
      "(u'ab-', u'stract', u'representation')\n",
      "(u'stract', u'representation', u'all')\n",
      "(u'representation', u'all', u'body')\n",
      "(u'all', u'body', u'parts')\n",
      "(u'body', u'parts', u'the')\n",
      "(u'parts', u'the', u'lower')\n",
      "(u'the', u'lower', u'layer')\n",
      "(u'lower', u'layer', u'Then')\n",
      "(u'layer', u'Then', u'representations')\n",
      "(u'Then', u'representations', u'all')\n",
      "(u'representations', u'all', u'information')\n",
      "(u'all', u'information', u'sources')\n",
      "(u'information', u'sources', u'the')\n",
      "(u'sources', u'the', u'higher')\n",
      "(u'the', u'higher', u'layer')\n",
      "(u'higher', u'layer', u'are')\n",
      "(u'layer', u'are', u'fused')\n",
      "(u'are', u'fused', u'for')\n",
      "(u'fused', u'for', u'pose')\n",
      "(u'for', u'pose', u'estimation')\n",
      "(u'pose', u'estimation', u'Both')\n",
      "(u'estimation', u'Both', u'the')\n",
      "(u'Both', u'the', u'task')\n",
      "(u'the', u'task', u'for')\n",
      "(u'task', u'for', u'detecting')\n",
      "(u'for', u'detecting', u'human')\n",
      "(u'detecting', u'human', u'and')\n",
      "(u'human', u'and', u'the')\n",
      "(u'and', u'the', u'task')\n",
      "(u'the', u'task', u'for')\n",
      "(u'task', u'for', u'esti-')\n",
      "(u'for', u'esti-', u'mating')\n",
      "(u'esti-', u'mating', u'body')\n",
      "(u'mating', u'body', u'locations')\n",
      "(u'body', u'locations', u'are')\n",
      "(u'locations', u'are', u'jointly')\n",
      "(u'are', u'jointly', u'learned')\n",
      "(u'jointly', u'learned', u'using')\n",
      "(u'learned', u'using', u'single')\n",
      "(u'using', u'single', u'deep')\n",
      "(u'single', u'deep', u'model')\n",
      "(u'deep', u'model', u'Joint')\n",
      "(u'model', u'Joint', u'learning')\n",
      "(u'Joint', u'learning', u'these')\n",
      "(u'learning', u'these', u'tasks')\n",
      "(u'these', u'tasks', u'with')\n",
      "(u'tasks', u'with', u'shared')\n",
      "(u'with', u'shared', u'representation')\n",
      "(u'shared', u'representation', u'improves')\n",
      "(u'representation', u'improves', u'pose')\n",
      "(u'improves', u'pose', u'estimation')\n",
      "(u'pose', u'estimation', u'accuracy')\n",
      "(u'estimation', u'accuracy', u'Related')\n",
      "(u'accuracy', u'Related', u'work')\n",
      "(u'Related', u'work', u'Human')\n",
      "(u'work', u'Human', u'pose')\n",
      "(u'Human', u'pose', u'estimation')\n",
      "(u'pose', u'estimation', u'Pose')\n",
      "(u'estimation', u'Pose', u'estimation')\n",
      "(u'Pose', u'estimation', u'considered')\n",
      "(u'estimation', u'considered', u'holistic')\n",
      "(u'considered', u'holistic', u'recognition')\n",
      "(u'holistic', u'recognition', u'the')\n",
      "(u'recognition', u'the', u'other')\n",
      "(u'the', u'other', u'hand')\n",
      "(u'other', u'hand', u'many')\n",
      "(u'hand', u'many', u'recent')\n",
      "(u'many', u'recent', u'works')\n",
      "(u'recent', u'works', u'use')\n",
      "(u'works', u'use', u'local')\n",
      "(u'use', u'local', u'body')\n",
      "(u'local', u'body', u'parts')\n",
      "(u'body', u'parts', u'order')\n",
      "(u'parts', u'order', u'handle')\n",
      "(u'order', u'handle', u'the')\n",
      "(u'handle', u'the', u'many')\n",
      "(u'the', u'many', u'degrees')\n",
      "(u'many', u'degrees', u'freedom')\n",
      "(u'degrees', u'freedom', u'body')\n",
      "(u'freedom', u'body', u'part')\n",
      "(u'body', u'part', u'articulation')\n",
      "(u'part', u'articulation', u'Since')\n",
      "(u'articulation', u'Since', u'the')\n",
      "(u'Since', u'the', u'\\ufb01rst')\n",
      "(u'the', u'\\ufb01rst', u'work')\n",
      "(u'\\ufb01rst', u'work', u'some')\n",
      "(u'work', u'some', u'approaches')\n",
      "(u'some', u'approaches', u'have')\n",
      "(u'approaches', u'have', u'clustered')\n",
      "(u'have', u'clustered', u'part')\n",
      "(u'clustered', u'part', u'appearance')\n",
      "(u'part', u'appearance', u'into')\n",
      "(u'appearance', u'into', u'mixture')\n",
      "(u'into', u'mixture', u'types')\n",
      "(u'mixture', u'types', u'shown')\n",
      "(u'types', u'shown', u'Fig')\n",
      "(u'shown', u'Fig', u'There')\n",
      "(u'Fig', u'There', u'are')\n",
      "(u'There', u'are', u'also')\n",
      "(u'are', u'also', u'approaches')\n",
      "(u'also', u'approaches', u'that')\n",
      "(u'approaches', u'that', u'warp')\n",
      "(u'that', u'warp', u'the')\n",
      "(u'warp', u'the', u'part')\n",
      "(u'the', u'part', u'template')\n",
      "(u'part', u'template', u'\\ufb02exible')\n",
      "(u'template', u'\\ufb02exible', u'sizes')\n",
      "(u'\\ufb02exible', u'sizes', u'and')\n",
      "(u'sizes', u'and', u'orientations')\n",
      "(u'and', u'orientations', u'The')\n",
      "(u'orientations', u'The', u'appearance')\n",
      "(u'The', u'appearance', u'score')\n",
      "(u'appearance', u'score', u'rotation')\n",
      "(u'score', u'rotation', u'size')\n",
      "(u'rotation', u'size', u'and')\n",
      "(u'size', u'and', u'location')\n",
      "(u'and', u'location', u'used')\n",
      "(u'location', u'used', u'these')\n",
      "(u'used', u'these', u'approaches')\n",
      "(u'these', u'approaches', u'can')\n",
      "(u'approaches', u'can', u'treated')\n",
      "(u'can', u'treated', u'multiple')\n",
      "(u'treated', u'multiple', u'information')\n",
      "(u'multiple', u'information', u'sources')\n",
      "(u'information', u'sources', u'and')\n",
      "(u'sources', u'and', u'used')\n",
      "(u'and', u'used', u'our')\n",
      "(u'used', u'our', u'deep')\n",
      "(u'our', u'deep', u'model')\n",
      "(u'deep', u'model', u'for')\n",
      "(u'model', u'for', u'pose')\n",
      "(u'for', u'pose', u'estimation')\n",
      "(u'pose', u'estimation', u'existing')\n",
      "(u'estimation', u'existing', u'pose')\n",
      "(u'existing', u'pose', u'estimation')\n",
      "(u'pose', u'estimation', u'approaches')\n",
      "(u'estimation', u'approaches', u'the')\n",
      "(u'approaches', u'the', u'pair-wise')\n",
      "(u'the', u'pair-wise', u'part')\n",
      "(u'pair-wise', u'part', u'deformation')\n",
      "(u'part', u'deformation', u'relationships')\n",
      "(u'deformation', u'relationships', u'are')\n",
      "(u'relationships', u'are', u'arranged')\n",
      "(u'are', u'arranged', u'tree')\n",
      "(u'arranged', u'tree', u'models')\n",
      "(u'tree', u'models', u'multi-tree')\n",
      "(u'models', u'multi-tree', u'model')\n",
      "(u'multi-tree', u'model', u'loopy')\n",
      "(u'model', u'loopy', u'mod-')\n",
      "(u'loopy', u'mod-', u'els')\n",
      "(u'mod-', u'els', u'Tree')\n",
      "(u'els', u'Tree', u'models')\n",
      "(u'Tree', u'models', u'allow')\n",
      "(u'models', u'allow', u'for')\n",
      "(u'allow', u'for', u'ef\\ufb01cient')\n",
      "(u'for', u'ef\\ufb01cient', u'and')\n",
      "(u'ef\\ufb01cient', u'and', u'exact')\n",
      "(u'and', u'exact', u'inference')\n",
      "(u'exact', u'inference', u'but')\n",
      "(u'inference', u'but', u'are')\n",
      "(u'but', u'are', u'insuf\\ufb01cient')\n",
      "(u'are', u'insuf\\ufb01cient', u'modeling')\n",
      "(u'insuf\\ufb01cient', u'modeling', u'the')\n",
      "(u'modeling', u'the', u'complex')\n",
      "(u'the', u'complex', u're-')\n",
      "(u'complex', u're-', u'lationships')\n",
      "(u're-', u'lationships', u'among')\n",
      "(u'lationships', u'among', u'body')\n",
      "(u'among', u'body', u'parts')\n",
      "(u'body', u'parts', u'Hence')\n",
      "(u'parts', u'Hence', u'tree')\n",
      "(u'Hence', u'tree', u'models')\n",
      "(u'tree', u'models', u'often')\n",
      "(u'models', u'often', u'suffer')\n",
      "(u'often', u'suffer', u'from')\n",
      "(u'suffer', u'from', u'double')\n",
      "(u'from', u'double', u'counting')\n",
      "(u'double', u'counting', u'for')\n",
      "(u'counting', u'for', u'example')\n",
      "(u'for', u'example', u'given')\n",
      "(u'example', u'given', u'the')\n",
      "(u'given', u'the', u'posi-')\n",
      "(u'the', u'posi-', u'tion')\n",
      "(u'posi-', u'tion', u'torso')\n",
      "(u'tion', u'torso', u'the')\n",
      "(u'torso', u'the', u'positions')\n",
      "(u'the', u'positions', u'two')\n",
      "(u'positions', u'two', u'legs')\n",
      "(u'two', u'legs', u'are')\n",
      "(u'legs', u'are', u'independent')\n",
      "(u'are', u'independent', u'and')\n",
      "(u'independent', u'and', u'often')\n",
      "(u'and', u'often', u'respond')\n",
      "(u'often', u'respond', u'the')\n",
      "(u'respond', u'the', u'same')\n",
      "(u'the', u'same', u'visual')\n",
      "(u'same', u'visual', u'cue')\n",
      "(u'visual', u'cue', u'Loopy')\n",
      "(u'cue', u'Loopy', u'models')\n",
      "(u'Loopy', u'models', u'allow')\n",
      "(u'models', u'allow', u'more')\n",
      "(u'allow', u'more', u'complex')\n",
      "(u'more', u'complex', u'relationships')\n",
      "(u'complex', u'relationships', u'among')\n",
      "(u'relationships', u'among', u'parts')\n",
      "(u'among', u'parts', u'but')\n",
      "(u'parts', u'but', u'require')\n",
      "(u'but', u'require', u'approximate')\n",
      "(u'require', u'approximate', u'inference')\n",
      "(u'approximate', u'inference', u'Our')\n",
      "(u'inference', u'Our', u'deep')\n",
      "(u'Our', u'deep', u'architecture')\n",
      "(u'deep', u'architecture', u'models')\n",
      "(u'architecture', u'models', u'the')\n",
      "(u'models', u'the', u'complex')\n",
      "(u'the', u'complex', u'relationships')\n",
      "(u'complex', u'relationships', u'among')\n",
      "(u'relationships', u'among', u'parts')\n",
      "(u'among', u'parts', u'and')\n",
      "(u'parts', u'and', u'computationally')\n",
      "(u'and', u'computationally', u'ef\\ufb01cient')\n",
      "(u'computationally', u'ef\\ufb01cient', u'both')\n",
      "(u'ef\\ufb01cient', u'both', u'training')\n",
      "(u'both', u'training', u'and')\n",
      "(u'training', u'and', u'testing')\n",
      "(u'and', u'testing', u'Deep')\n",
      "(u'testing', u'Deep', u'learning')\n",
      "(u'Deep', u'learning', u'Since')\n",
      "(u'learning', u'Since', u'the')\n",
      "(u'Since', u'the', u'breakthrough')\n",
      "(u'the', u'breakthrough', u'deep')\n",
      "(u'breakthrough', u'deep', u'learning')\n",
      "(u'deep', u'learning', u'initiated')\n",
      "(u'learning', u'initiated', u'Hinton')\n",
      "(u'initiated', u'Hinton', u'deep')\n",
      "(u'Hinton', u'deep', u'learning')\n",
      "(u'deep', u'learning', u'gain-')\n",
      "(u'learning', u'gain-', u'ing')\n",
      "(u'gain-', u'ing', u'more')\n",
      "(u'ing', u'more', u'and')\n",
      "(u'more', u'and', u'more')\n",
      "(u'and', u'more', u'attention')\n",
      "(u'more', u'attention', u'Bengio')\n",
      "(u'attention', u'Bengio', u'proved')\n",
      "(u'Bengio', u'proved', u'that')\n",
      "(u'proved', u'that', u'exist-')\n",
      "(u'that', u'exist-', u'ing')\n",
      "(u'exist-', u'ing', u'commonly')\n",
      "(u'ing', u'commonly', u'used')\n",
      "(u'commonly', u'used', u'machine')\n",
      "(u'used', u'machine', u'learning')\n",
      "(u'machine', u'learning', u'tools')\n",
      "(u'learning', u'tools', u'such')\n",
      "(u'tools', u'such', u'SVM')\n",
      "(u'such', u'SVM', u'and')\n",
      "(u'SVM', u'and', u'Boosting')\n",
      "(u'and', u'Boosting', u'are')\n",
      "(u'Boosting', u'are', u'shallow')\n",
      "(u'are', u'shallow', u'models')\n",
      "(u'shallow', u'models', u'and')\n",
      "(u'models', u'and', u'they')\n",
      "(u'and', u'they', u'may')\n",
      "(u'they', u'may', u'require')\n",
      "(u'may', u'require', u'many')\n",
      "(u'require', u'many', u'more')\n",
      "(u'many', u'more', u'computational')\n",
      "(u'more', u'computational', u'elements')\n",
      "(u'computational', u'elements', u'potentially')\n",
      "(u'elements', u'potentially', u'exponen-')\n",
      "(u'potentially', u'exponen-', u'tially')\n",
      "(u'exponen-', u'tially', u'more')\n",
      "(u'tially', u'more', u'with')\n",
      "(u'more', u'with', u'respect')\n",
      "(u'with', u'respect', u'input')\n",
      "(u'respect', u'input', u'size')\n",
      "(u'input', u'size', u'than')\n",
      "(u'size', u'than', u'deep')\n",
      "(u'than', u'deep', u'models')\n",
      "(u'deep', u'models', u'whose')\n",
      "(u'models', u'whose', u'depth')\n",
      "(u'whose', u'depth', u'matched')\n",
      "(u'depth', u'matched', u'the')\n",
      "(u'matched', u'the', u'task')\n",
      "(u'the', u'task', u'Deep')\n",
      "(u'task', u'Deep', u'architecture')\n",
      "(u'Deep', u'architecture', u'found')\n",
      "(u'architecture', u'found', u'yield')\n",
      "(u'found', u'yield', u'better')\n",
      "(u'yield', u'better', u'data')\n",
      "(u'better', u'data', u'representation')\n",
      "(u'data', u'representation', u'for')\n",
      "(u'representation', u'for', u'example')\n",
      "(u'for', u'example', u'terms')\n",
      "(u'example', u'terms', u'classi\\ufb01cation')\n",
      "(u'terms', u'classi\\ufb01cation', u'error')\n",
      "(u'classi\\ufb01cation', u'error', u'invariance')\n",
      "(u'error', u'invariance', u'input')\n",
      "(u'invariance', u'input', u'trans-')\n",
      "(u'input', u'trans-', u'formations')\n",
      "(u'trans-', u'formations', u'modeling')\n",
      "(u'formations', u'modeling', u'multi-modal')\n",
      "(u'modeling', u'multi-modal', u'data')\n",
      "(u'multi-modal', u'data', u'Deep')\n",
      "(u'data', u'Deep', u'learning')\n",
      "(u'Deep', u'learning', u'has')\n",
      "(u'learning', u'has', u'achieved')\n",
      "(u'has', u'achieved', u'spectacular')\n",
      "(u'achieved', u'spectacular', u'progress')\n",
      "(u'spectacular', u'progress', u'computer')\n",
      "(u'progress', u'computer', u'vi-')\n",
      "(u'computer', u'vi-', u'sion')\n",
      "(u'vi-', u'sion', u'Recent')\n",
      "(u'sion', u'Recent', u'progress')\n",
      "(u'Recent', u'progress', u'deep')\n",
      "(u'progress', u'deep', u'learning')\n",
      "(u'deep', u'learning', u'reviewed')\n",
      "(u'learning', u'reviewed', u'Krizhevsky')\n",
      "(u'reviewed', u'Krizhevsky', u'pro-')\n",
      "(u'Krizhevsky', u'pro-', u'posed')\n",
      "(u'pro-', u'posed', u'large-scale')\n",
      "(u'posed', u'large-scale', u'deep')\n",
      "(u'large-scale', u'deep', u'convolutional')\n",
      "(u'deep', u'convolutional', u'network')\n",
      "(u'convolutional', u'network', u'with')\n",
      "(u'network', u'with', u'breakthrough')\n",
      "(u'with', u'breakthrough', u'the')\n",
      "(u'breakthrough', u'the', u'large-scale')\n",
      "(u'the', u'large-scale', u'ImageNet')\n",
      "(u'large-scale', u'ImageNet', u'object')\n",
      "(u'ImageNet', u'object', u'recogni-')\n",
      "(u'object', u'recogni-', u'tion')\n",
      "(u'recogni-', u'tion', u'dataset')\n",
      "(u'tion', u'dataset', u'attaining')\n",
      "(u'dataset', u'attaining', u'signi\\ufb01cant')\n",
      "(u'attaining', u'signi\\ufb01cant', u'gap')\n",
      "(u'signi\\ufb01cant', u'gap', u'compared')\n",
      "(u'gap', u'compared', u'with')\n",
      "(u'compared', u'with', u'existing')\n",
      "(u'with', u'existing', u'approaches')\n",
      "(u'existing', u'approaches', u'that')\n",
      "(u'approaches', u'that', u'use')\n",
      "(u'that', u'use', u'shallow')\n",
      "(u'use', u'shallow', u'models')\n",
      "(u'shallow', u'models', u'and')\n",
      "(u'models', u'and', u'bringing')\n",
      "(u'and', u'bringing', u'high')\n",
      "(u'bringing', u'high', u'impact')\n",
      "(u'high', u'impact', u'research')\n",
      "(u'impact', u'research', u'computer')\n",
      "(u'research', u'computer', u'vision')\n",
      "(u'computer', u'vision', u'Our')\n",
      "(u'vision', u'Our', u'approaches')\n",
      "(u'Our', u'approaches', u'learns')\n",
      "(u'approaches', u'learns', u'feature')\n",
      "(u'learns', u'feature', u'learning')\n",
      "(u'feature', u'learning', u'translational')\n",
      "(u'learning', u'translational', u'de-')\n",
      "(u'translational', u'de-', u'formation')\n",
      "(u'de-', u'formation', u'and')\n",
      "(u'formation', u'and', u'occlusion')\n",
      "(u'and', u'occlusion', u'relationship')\n",
      "(u'occlusion', u'relationship', u'pedestrian')\n",
      "(u'relationship', u'pedestrian', u'detec-')\n",
      "(u'pedestrian', u'detec-', u'tion')\n",
      "(u'detec-', u'tion', u'the')\n",
      "(u'tion', u'the', u'approach')\n",
      "(u'the', u'approach', u'learns')\n",
      "(u'approach', u'learns', u'relational')\n",
      "(u'learns', u'relational', u'\\ufb01lter')\n",
      "(u'relational', u'\\ufb01lter', u'pairs')\n",
      "(u'\\ufb01lter', u'pairs', u'face')\n",
      "(u'pairs', u'face', u'veri\\ufb01cation')\n",
      "(u'face', u'veri\\ufb01cation', u'the')\n",
      "(u'veri\\ufb01cation', u'the', u'best')\n",
      "(u'the', u'best', u'our')\n",
      "(u'best', u'our', u'knowledge')\n",
      "(u'our', u'knowledge', u'however')\n",
      "(u'knowledge', u'however', u'deep')\n",
      "(u'however', u'deep', u'model')\n",
      "(u'deep', u'model', u'for')\n",
      "(u'model', u'for', u'human')\n",
      "(u'for', u'human', u'pose')\n",
      "(u'human', u'pose', u'estimation')\n",
      "(u'pose', u'estimation', u'has')\n",
      "(u'estimation', u'has', u'not')\n",
      "(u'has', u'not', u'yet')\n",
      "(u'not', u'yet', u'been')\n",
      "(u'yet', u'been', u'explored')\n",
      "(u'been', u'explored', u'Our')\n",
      "(u'explored', u'Our', u'work')\n",
      "(u'Our', u'work', u'inspired')\n",
      "(u'work', u'inspired', u'multi-modality')\n",
      "(u'inspired', u'multi-modality', u'models')\n",
      "(u'multi-modality', u'models', u'that')\n",
      "(u'models', u'that', u'learn')\n",
      "(u'that', u'learn', u'from')\n",
      "(u'learn', u'from', u'multiple')\n",
      "(u'from', u'multiple', u'modalities')\n",
      "(u'multiple', u'modalities', u'such')\n",
      "(u'modalities', u'such', u'audio')\n",
      "(u'such', u'audio', u'visual')\n",
      "(u'audio', u'visual', u'text')\n",
      "(u'visual', u'text', u'data')\n",
      "(u'text', u'data', u'contrast')\n",
      "(u'data', u'contrast', u'these')\n",
      "(u'contrast', u'these', u'works')\n",
      "(u'these', u'works', u'investigate')\n",
      "(u'works', u'investigate', u'multi-')\n",
      "(u'investigate', u'multi-', u'source')\n",
      "(u'multi-', u'source', u'learning')\n",
      "(u'source', u'learning', u'from')\n",
      "(u'learning', u'from', u'single')\n",
      "(u'from', u'single', u'modality')\n",
      "(u'single', u'modality', u'which')\n",
      "(u'modality', u'which', u'image')\n",
      "(u'which', u'image', u'data')\n",
      "(u'image', u'data', u'pose')\n",
      "(u'data', u'pose', u'estimation')\n",
      "(u'pose', u'estimation', u'Pictorial')\n",
      "(u'estimation', u'Pictorial', u'structure')\n",
      "(u'Pictorial', u'structure', u'model')\n",
      "(u'structure', u'model', u'for')\n",
      "(u'model', u'for', u'pose')\n",
      "(u'for', u'pose', u'estimation')\n",
      "(u'pose', u'estimation', u'The')\n",
      "(u'estimation', u'The', u'model')\n",
      "(u'The', u'model', u'introduced')\n",
      "(u'model', u'introduced', u'this')\n",
      "(u'introduced', u'this', u'section')\n",
      "(u'this', u'section', u'used')\n",
      "(u'section', u'used', u'provide')\n",
      "(u'used', u'provide', u'our')\n",
      "(u'provide', u'our', u'deep')\n",
      "(u'our', u'deep', u'model')\n",
      "(u'deep', u'model', u'with')\n",
      "(u'model', u'with', u'information')\n",
      "(u'with', u'information', u'sources')\n",
      "(u'information', u'sources', u'Pictorial')\n",
      "(u'sources', u'Pictorial', u'struc-')\n",
      "(u'Pictorial', u'struc-', u'ture')\n",
      "(u'struc-', u'ture', u'model')\n",
      "(u'ture', u'model', u'considers')\n",
      "(u'model', u'considers', u'human')\n",
      "(u'considers', u'human', u'body')\n",
      "(u'human', u'body', u'parts')\n",
      "(u'body', u'parts', u'nodes')\n",
      "(u'parts', u'nodes', u'tied')\n",
      "(u'nodes', u'tied', u'to-')\n",
      "(u'tied', u'to-', u'gether')\n",
      "(u'to-', u'gether', u'conditional')\n",
      "(u'gether', u'conditional', u'random')\n",
      "(u'conditional', u'random', u'\\ufb01eld')\n",
      "(u'random', u'\\ufb01eld', u'Let')\n",
      "(u'\\ufb01eld', u'Let', u'for')\n",
      "(u'Let', u'for', u'the')\n",
      "(u'for', u'the', u'con\\ufb01guration')\n",
      "(u'the', u'con\\ufb01guration', u'the')\n",
      "(u'con\\ufb01guration', u'the', u'pth')\n",
      "(u'the', u'pth', u'part')\n",
      "(u'pth', u'part', u'The')\n",
      "(u'part', u'The', u'posterior')\n",
      "(u'The', u'posterior', u'con-')\n",
      "(u'posterior', u'con-', u'\\ufb01guration')\n",
      "(u'con-', u'\\ufb01guration', u'parts')\n",
      "(u'\\ufb01guration', u'parts', u'lp|p')\n",
      "(u'parts', u'lp|p', u'given')\n",
      "(u'lp|p', u'given', u'image')\n",
      "(u'given', u'image', u'L|I')\n",
      "(u'image', u'L|I', u'exp')\n",
      "(u'L|I', u'exp', u'cid:0')\n",
      "(u'exp', u'cid:0', u'cid:88')\n",
      "(u'cid:0', u'cid:88', u'cid:88')\n",
      "(u'cid:88', u'cid:88', u'cid:1')\n",
      "(u'cid:88', u'cid:1', u'I|lp')\n",
      "(u'cid:1', u'I|lp', u'p=1')\n",
      "(u'I|lp', u'p=1', u'the')\n",
      "(u'p=1', u'the', u'pair-wise')\n",
      "(u'the', u'pair-wise', u'term')\n",
      "(u'pair-wise', u'term', u'that')\n",
      "(u'term', u'that', u'models')\n",
      "(u'that', u'models', u'the')\n",
      "(u'models', u'the', u'geometric')\n",
      "(u'the', u'geometric', u'deformation')\n",
      "(u'geometric', u'deformation', u'constraint')\n",
      "(u'deformation', u'constraint', u'the')\n",
      "(u'constraint', u'the', u'pth')\n",
      "(u'the', u'pth', u'and')\n",
      "(u'pth', u'and', u'qth')\n",
      "(u'and', u'qth', u'parts')\n",
      "(u'qth', u'parts', u'for')\n",
      "(u'parts', u'for', u'exam-')\n",
      "(u'for', u'exam-', u'ple')\n",
      "(u'exam-', u'ple', u'head')\n",
      "(u'ple', u'head', u'shall')\n",
      "(u'head', u'shall', u'not')\n",
      "(u'shall', u'not', u'too')\n",
      "(u'not', u'too', u'far')\n",
      "(u'too', u'far', u'from')\n",
      "(u'far', u'from', u'torso')\n",
      "(u'from', u'torso', u'The')\n",
      "(u'torso', u'The', u'edge')\n",
      "(u'The', u'edge', u'set')\n",
      "(u'edge', u'set', u'de-')\n",
      "(u'set', u'de-', u'noted')\n",
      "(u'de-', u'noted', u'arranged')\n",
      "(u'noted', u'arranged', u'tree')\n",
      "(u'arranged', u'tree', u'models')\n",
      "(u'tree', u'models', u'loopy')\n",
      "(u'models', u'loopy', u'models')\n",
      "(u'loopy', u'models', u'I|lp')\n",
      "(u'models', u'I|lp', u'the')\n",
      "(u'I|lp', u'the', u'unary')\n",
      "(u'the', u'unary', u'term')\n",
      "(u'unary', u'term', u'that')\n",
      "(u'term', u'that', u'models')\n",
      "(u'that', u'models', u'the')\n",
      "(u'models', u'the', u'appearance')\n",
      "(u'the', u'appearance', u'The')\n",
      "(u'appearance', u'The', u'appearance')\n",
      "(u'The', u'appearance', u'varies')\n",
      "(u'appearance', u'varies', u'body')\n",
      "(u'varies', u'body', u'articulates')\n",
      "(u'body', u'articulates', u'model')\n",
      "(u'articulates', u'model', u'this')\n",
      "(u'model', u'this', u'variation')\n",
      "(u'this', u'variation', u'and')\n",
      "(u'variation', u'and', u'I|lp')\n",
      "(u'and', u'I|lp', u'speci\\ufb01es')\n",
      "(u'I|lp', u'speci\\ufb01es', u'the')\n",
      "(u'speci\\ufb01es', u'the', u'part')\n",
      "(u'the', u'part', u'appearance')\n",
      "(u'part', u'appearance', u'warped')\n",
      "(u'appearance', u'warped', u'size')\n",
      "(u'warped', u'size', u'orientation')\n",
      "(u'size', u'orientation', u'loca-')\n",
      "(u'orientation', u'loca-', u'tion')\n",
      "(u'loca-', u'tion', u'Alternatively')\n",
      "(u'tion', u'Alternatively', u'Yang')\n",
      "(u'Alternatively', u'Yang', u'and')\n",
      "(u'Yang', u'and', u'Ramanan')\n",
      "(u'and', u'Ramanan', u'propose')\n",
      "(u'Ramanan', u'propose', u'use')\n",
      "(u'propose', u'use', u'appearance')\n",
      "(u'use', u'appearance', u'mixture')\n",
      "(u'appearance', u'mixture', u'type')\n",
      "(u'mixture', u'type', u'for')\n",
      "(u'type', u'for', u'approximat-')\n",
      "(u'for', u'approximat-', u'ing')\n",
      "(u'approximat-', u'ing', u'the')\n",
      "(u'ing', u'the', u'variation')\n",
      "(u'the', u'variation', u'rotation')\n",
      "(u'variation', u'rotation', u'and')\n",
      "(u'rotation', u'and', u'size')\n",
      "(u'and', u'size', u'this')\n",
      "(u'size', u'this', u'model')\n",
      "(u'this', u'model', u'and')\n",
      "(u'model', u'and', u'I|lp')\n",
      "(u'and', u'I|lp', u'speci\\ufb01es')\n",
      "(u'I|lp', u'speci\\ufb01es', u'the')\n",
      "(u'speci\\ufb01es', u'the', u'part')\n",
      "(u'the', u'part', u'appear-')\n",
      "(u'part', u'appear-', u'ance')\n",
      "(u'appear-', u'ance', u'with')\n",
      "(u'ance', u'with', u'mixture')\n",
      "(u'with', u'mixture', u'type')\n",
      "(u'mixture', u'type', u'location')\n",
      "(u'type', u'location', u'The')\n",
      "(u'location', u'The', u'appearance')\n",
      "(u'The', u'appearance', u'part')\n",
      "(u'appearance', u'part', u'clustered')\n",
      "(u'part', u'clustered', u'into')\n",
      "(u'clustered', u'into', u'multiple')\n",
      "(u'into', u'multiple', u'appearance')\n",
      "(u'multiple', u'appearance', u'mixture')\n",
      "(u'appearance', u'mixture', u'types')\n",
      "(u'mixture', u'types', u'shown')\n",
      "(u'types', u'shown', u'Fig')\n",
      "(u'shown', u'Fig', u'The')\n",
      "(u'Fig', u'The', u'overall')\n",
      "(u'The', u'overall', u'model')\n",
      "(u'overall', u'model', u'follows')\n",
      "(u'model', u'follows', u'L|I')\n",
      "(u'follows', u'L|I', u'exp')\n",
      "(u'L|I', u'exp', u'cid:0')\n",
      "(u'exp', u'cid:0', u'cid:1')\n",
      "(u'cid:0', u'cid:1', u'cid:88')\n",
      "(u'cid:1', u'cid:88', u'cid:88')\n",
      "(u'cid:88', u'cid:88', u'cid:88')\n",
      "(u'cid:88', u'cid:88', u'btp')\n",
      "(u'cid:88', u'btp', u'btp')\n",
      "(u'btp', u'btp', u'where')\n",
      "(u'btp', u'where', u'cid:88')\n",
      "(u'where', u'cid:88', u'wtp')\n",
      "(u'cid:88', u'wtp', u'wtp')\n",
      "(u'wtp', u'wtp', u'compatibility/co-occurrence')\n",
      "(u'wtp', u'compatibility/co-occurrence', u'mixture')\n",
      "(u'compatibility/co-occurrence', u'mixture', u'types')\n",
      "(u'mixture', u'types', u'the')\n",
      "(u'types', u'the', u'pair-wise')\n",
      "(u'the', u'pair-wise', u'compatibility')\n",
      "(u'pair-wise', u'compatibility', u'term')\n",
      "(u'compatibility', u'term', u'that')\n",
      "(u'term', u'that', u'models')\n",
      "(u'that', u'models', u'the')\n",
      "(u'models', u'the', u'the')\n",
      "(u'the', u'the', u'pair-wise')\n",
      "(u'the', u'pair-wise', u'deformation')\n",
      "(u'pair-wise', u'deformation', u'term')\n",
      "(u'deformation', u'term', u'that')\n",
      "(u'term', u'that', u'mod-')\n",
      "(u'that', u'mod-', u'els')\n",
      "(u'mod-', u'els', u'the')\n",
      "(u'els', u'the', u'geometric')\n",
      "(u'the', u'geometric', u'deformation')\n",
      "(u'geometric', u'deformation', u'constraints')\n",
      "(u'deformation', u'constraints', u'the')\n",
      "(u'constraints', u'the', u'pth')\n",
      "(u'the', u'pth', u'and')\n",
      "(u'pth', u'and', u'qth')\n",
      "(u'and', u'qth', u'parts')\n",
      "(u'qth', u'parts', u'dx2dy2')\n",
      "(u'parts', u'dx2dy2', u'the')\n",
      "(u'dx2dy2', u'the', u'unary')\n",
      "(u'the', u'unary', u'appearance')\n",
      "(u'unary', u'appearance', u'term')\n",
      "(u'appearance', u'term', u'that')\n",
      "(u'term', u'that', u'computes')\n",
      "(u'that', u'computes', u'location')\n",
      "(u'computes', u'location', u'the')\n",
      "(u'location', u'the', u'the')\n",
      "(u'the', u'the', u'score')\n",
      "(u'the', u'score', u'placing')\n",
      "(u'score', u'placing', u'template')\n",
      "(u'placing', u'template', u'wtp')\n",
      "(u'template', u'wtp', u'HOG')\n",
      "(u'wtp', u'HOG', u'feature')\n",
      "(u'HOG', u'feature', u'map')\n",
      "(u'feature', u'map', u'for')\n",
      "(u'map', u'for', u'image')\n",
      "(u'for', u'image', u'denoted')\n",
      "(u'image', u'denoted', u'wtp')\n",
      "(u'denoted', u'wtp', u'Linear')\n",
      "(u'wtp', u'Linear', u'SVM')\n",
      "(u'Linear', u'SVM', u'used')\n",
      "(u'SVM', u'used', u'learn')\n",
      "(u'used', u'learn', u'the')\n",
      "(u'learn', u'the', u'linear')\n",
      "(u'the', u'linear', u'weights')\n",
      "(u'linear', u'weights', u'wtp')\n",
      "(u'weights', u'wtp', u'and')\n",
      "(u'wtp', u'and', u'compatibility')\n",
      "(u'and', u'compatibility', u'biases')\n",
      "(u'compatibility', u'biases', u'btp')\n",
      "(u'biases', u'btp', u'The')\n",
      "(u'btp', u'The', u'model')\n",
      "(u'The', u'model', u'used')\n",
      "(u'model', u'used', u'many')\n",
      "(u'used', u'many', u'approaches')\n",
      "(u'many', u'approaches', u'with')\n",
      "(u'approaches', u'with', u'different')\n",
      "(u'with', u'different', u'implementations')\n",
      "(u'different', u'implementations', u'edge')\n",
      "(u'implementations', u'edge', u'set')\n",
      "(u'edge', u'set', u'part')\n",
      "(u'set', u'part', u'size')\n",
      "(u'part', u'size', u'and')\n",
      "(u'size', u'and', u'part')\n",
      "(u'and', u'part', u'locations')\n",
      "(u'part', u'locations', u'btp')\n",
      "(u'locations', u'btp', u'The')\n",
      "(u'btp', u'The', u'multi-source')\n",
      "(u'The', u'multi-source', u'deep')\n",
      "(u'multi-source', u'deep', u'model')\n",
      "(u'deep', u'model', u'overview')\n",
      "(u'model', u'overview', u'our')\n",
      "(u'overview', u'our', u'framework')\n",
      "(u'our', u'framework', u'the')\n",
      "(u'framework', u'the', u'testing')\n",
      "(u'the', u'testing', u'stage')\n",
      "(u'testing', u'stage', u'shown')\n",
      "(u'stage', u'shown', u'Fig')\n",
      "(u'shown', u'Fig', u'this')\n",
      "(u'Fig', u'this', u'framework')\n",
      "(u'this', u'framework', u'existing')\n",
      "(u'framework', u'existing', u'approach')\n",
      "(u'existing', u'approach', u'used')\n",
      "(u'approach', u'used', u'generate')\n",
      "(u'used', u'generate', u'candidate')\n",
      "(u'generate', u'candidate', u'body')\n",
      "(u'candidate', u'body', u'locations')\n",
      "(u'body', u'locations', u'with')\n",
      "(u'locations', u'with', u'conserva-')\n",
      "(u'with', u'conserva-', u'tive')\n",
      "(u'conserva-', u'tive', u'thresholding')\n",
      "(u'tive', u'thresholding', u'the')\n",
      "(u'thresholding', u'the', u'experiment')\n",
      "(u'the', u'experiment', u'the')\n",
      "(u'experiment', u'the', u'existing')\n",
      "(u'the', u'existing', u'approach')\n",
      "(u'existing', u'approach', u'the')\n",
      "(u'approach', u'the', u'off-the-shelf')\n",
      "(u'the', u'off-the-shelf', u'approach')\n",
      "(u'off-the-shelf', u'approach', u'multi-source')\n",
      "(u'approach', u'multi-source', u'deep')\n",
      "(u'multi-source', u'deep', u'model')\n",
      "(u'deep', u'model', u'then')\n",
      "(u'model', u'then', u'applied')\n",
      "(u'then', u'applied', u'candidate')\n",
      "(u'applied', u'candidate', u'all')\n",
      "(u'candidate', u'all', u'body')\n",
      "(u'all', u'body', u'locations')\n",
      "(u'body', u'locations', u'order')\n",
      "(u'locations', u'order', u'determine')\n",
      "(u'order', u'determine', u'whether')\n",
      "(u'determine', u'whether', u'its')\n",
      "(u'whether', u'its', u'body')\n",
      "(u'its', u'body', u'locations')\n",
      "(u'body', u'locations', u'are')\n",
      "(u'locations', u'are', u'correct')\n",
      "(u'are', u'correct', u'Simultaneously')\n",
      "(u'correct', u'Simultaneously', u'the')\n",
      "(u'Simultaneously', u'the', u'body')\n",
      "(u'the', u'body', u'locations')\n",
      "(u'body', u'locations', u'this')\n",
      "(u'locations', u'this', u'candidate')\n",
      "(u'this', u'candidate', u'esti-')\n",
      "(u'candidate', u'esti-', u'mated')\n",
      "(u'esti-', u'mated', u'One')\n",
      "(u'mated', u'One', u'direct')\n",
      "(u'One', u'direct', u'approach')\n",
      "(u'direct', u'approach', u'with')\n",
      "(u'approach', u'with', u'which')\n",
      "(u'with', u'which', u'train')\n",
      "(u'which', u'train', u'multi-source')\n",
      "(u'train', u'multi-source', u'model')\n",
      "(u'multi-source', u'model', u'train')\n",
      "(u'model', u'train', u'deep')\n",
      "(u'train', u'deep', u'model')\n",
      "(u'deep', u'model', u'over')\n",
      "(u'model', u'over', u'the')\n",
      "(u'over', u'the', u'concatenated')\n",
      "(u'the', u'concatenated', u'infor-')\n",
      "(u'concatenated', u'infor-', u'mation')\n",
      "(u'infor-', u'mation', u'sources')\n",
      "(u'mation', u'sources', u'shown')\n",
      "(u'sources', u'shown', u'Fig')\n",
      "(u'shown', u'Fig', u'This')\n",
      "(u'Fig', u'This', u'approach')\n",
      "(u'This', u'approach', u'lim-')\n",
      "(u'approach', u'lim-', u'ited')\n",
      "(u'lim-', u'ited', u'because')\n",
      "(u'ited', u'because', u'information')\n",
      "(u'because', u'information', u'sources')\n",
      "(u'information', u'sources', u'with')\n",
      "(u'sources', u'with', u'different')\n",
      "(u'with', u'different', u'statistical')\n",
      "(u'different', u'statistical', u'properties')\n",
      "(u'statistical', u'properties', u'are')\n",
      "(u'properties', u'are', u'mixed')\n",
      "(u'are', u'mixed', u'the')\n",
      "(u'mixed', u'the', u'\\ufb01rst')\n",
      "(u'the', u'\\ufb01rst', u'hidden')\n",
      "(u'\\ufb01rst', u'hidden', u'layer')\n",
      "(u'hidden', u'layer', u'better')\n",
      "(u'layer', u'better', u'so-')\n",
      "(u'better', u'so-', u'lution')\n",
      "(u'so-', u'lution', u'have')\n",
      "(u'lution', u'have', u'their')\n",
      "(u'have', u'their', u'high-level')\n",
      "(u'their', u'high-level', u'representations')\n",
      "(u'high-level', u'representations', u'constructed')\n",
      "(u'representations', u'constructed', u'before')\n",
      "(u'constructed', u'before', u'they')\n",
      "(u'before', u'they', u'are')\n",
      "(u'they', u'are', u'mixed')\n",
      "(u'are', u'mixed', u'Therefore')\n",
      "(u'mixed', u'Therefore', u'use')\n",
      "(u'Therefore', u'use', u'the')\n",
      "(u'use', u'the', u'architecture')\n",
      "(u'the', u'architecture', u'shown')\n",
      "(u'architecture', u'shown', u'Fig')\n",
      "(u'shown', u'Fig', u'which')\n",
      "(u'Fig', u'which', u'each')\n",
      "(u'which', u'each', u'information')\n",
      "(u'each', u'information', u'source')\n",
      "(u'information', u'source', u'Figure')\n",
      "(u'source', u'Figure', u'Framework')\n",
      "(u'Figure', u'Framework', u'the')\n",
      "(u'Framework', u'the', u'testing')\n",
      "(u'the', u'testing', u'stage')\n",
      "(u'testing', u'stage', u'The')\n",
      "(u'stage', u'The', u'existing')\n",
      "(u'The', u'existing', u'approach')\n",
      "(u'existing', u'approach', u'used')\n",
      "(u'approach', u'used', u'generate')\n",
      "(u'used', u'generate', u'multiple')\n",
      "(u'generate', u'multiple', u'candidate')\n",
      "(u'multiple', u'candidate', u'locations')\n",
      "(u'candidate', u'locations', u'candidate')\n",
      "(u'locations', u'candidate', u'used')\n",
      "(u'candidate', u'used', u'the')\n",
      "(u'used', u'the', u'input')\n",
      "(u'the', u'input', u'deep')\n",
      "(u'input', u'deep', u'model')\n",
      "(u'deep', u'model', u'determine')\n",
      "(u'model', u'determine', u'whether')\n",
      "(u'determine', u'whether', u'the')\n",
      "(u'whether', u'the', u'candidate')\n",
      "(u'the', u'candidate', u'correct')\n",
      "(u'candidate', u'correct', u'and')\n",
      "(u'correct', u'and', u'estimate')\n",
      "(u'and', u'estimate', u'body')\n",
      "(u'estimate', u'body', u'locations')\n",
      "(u'body', u'locations', u'Best')\n",
      "(u'locations', u'Best', u'viewed')\n",
      "(u'Best', u'viewed', u'color')\n",
      "(u'viewed', u'color', u'Figure')\n",
      "(u'color', u'Figure', u'Direct')\n",
      "(u'Figure', u'Direct', u'use')\n",
      "(u'Direct', u'use', u'deep')\n",
      "(u'use', u'deep', u'model')\n",
      "(u'deep', u'model', u'and')\n",
      "(u'model', u'and', u'the')\n",
      "(u'and', u'the', u'deep')\n",
      "(u'the', u'deep', u'architecture')\n",
      "(u'deep', u'architecture', u'propose')\n",
      "(u'architecture', u'propose', u'for')\n",
      "(u'propose', u'for', u'part')\n",
      "(u'for', u'part', u'score')\n",
      "(u'part', u'score', u'deformation')\n",
      "(u'score', u'deformation', u'and')\n",
      "(u'deformation', u'and', u'mixture')\n",
      "(u'and', u'mixture', u'type')\n",
      "(u'mixture', u'type', u'Best')\n",
      "(u'type', u'Best', u'viewed')\n",
      "(u'Best', u'viewed', u'color')\n",
      "(u'viewed', u'color', u'connected')\n",
      "(u'color', u'connected', u'two')\n",
      "(u'connected', u'two', u'layers')\n",
      "(u'two', u'layers', u'for')\n",
      "(u'layers', u'for', u'constructing')\n",
      "(u'for', u'constructing', u'high')\n",
      "(u'constructing', u'high', u'level')\n",
      "(u'high', u'level', u'rep-')\n",
      "(u'level', u'rep-', u'resentation')\n",
      "(u'rep-', u'resentation', u'individually')\n",
      "(u'resentation', u'individually', u'High-level')\n",
      "(u'individually', u'High-level', u'representations')\n",
      "(u'High-level', u'representations', u'dif-')\n",
      "(u'representations', u'dif-', u'ferent')\n",
      "(u'dif-', u'ferent', u'information')\n",
      "(u'ferent', u'information', u'sources')\n",
      "(u'information', u'sources', u'are')\n",
      "(u'sources', u'are', u'then')\n",
      "(u'are', u'then', u'fused')\n",
      "(u'then', u'fused', u'using')\n",
      "(u'fused', u'using', u'other')\n",
      "(u'using', u'other', u'two')\n",
      "(u'other', u'two', u'layers')\n",
      "(u'two', u'layers', u'for')\n",
      "(u'layers', u'for', u'pose')\n",
      "(u'for', u'pose', u'estimation')\n",
      "(u'pose', u'estimation', u'4.1')\n",
      "(u'estimation', u'4.1', u'Inference')\n",
      "(u'4.1', u'Inference', u'The')\n",
      "(u'Inference', u'The', u'mixture')\n",
      "(u'The', u'mixture', u'type')\n",
      "(u'mixture', u'type', u'information')\n",
      "(u'type', u'information', u'Fig')\n",
      "(u'information', u'Fig', u'taken')\n",
      "(u'Fig', u'taken', u'from')\n",
      "(u'taken', u'from', u'the')\n",
      "(u'from', u'the', u'The')\n",
      "(u'the', u'The', u'relative')\n",
      "(u'The', u'relative', u'positions')\n",
      "(u'relative', u'positions', u'among')\n",
      "(u'positions', u'among', u'parts')\n",
      "(u'among', u'parts', u'denoted')\n",
      "(u'parts', u'denoted', u'comes')\n",
      "(u'denoted', u'comes', u'from')\n",
      "(u'comes', u'from', u'the')\n",
      "(u'from', u'the', u'deformation')\n",
      "(u'the', u'deformation', u'information')\n",
      "(u'deformation', u'information', u'The')\n",
      "(u'information', u'The', u'appearance')\n",
      "(u'The', u'appearance', u'scores')\n",
      "(u'appearance', u'scores', u'denoted')\n",
      "(u'scores', u'denoted', u'obtained')\n",
      "(u'denoted', u'obtained', u'from')\n",
      "(u'obtained', u'from', u'the')\n",
      "(u'from', u'the', u'unary')\n",
      "(u'the', u'unary', u'appearance')\n",
      "(u'unary', u'appearance', u'term')\n",
      "(u'appearance', u'term', u'our')\n",
      "(u'term', u'our', u'experiment')\n",
      "(u'our', u'experiment', u'and')\n",
      "(u'experiment', u'and', u'are')\n",
      "(u'and', u'are', u'obtained')\n",
      "(u'are', u'obtained', u'using')\n",
      "(u'obtained', u'using', u'the')\n",
      "(u'using', u'the', u'approach')\n",
      "(u'the', u'approach', u'the')\n",
      "(u'approach', u'the', u'inference')\n",
      "(u'the', u'inference', u'stage')\n",
      "(u'inference', u'stage', u'the')\n",
      "(u'stage', u'the', u'model')\n",
      "(u'the', u'model', u'follows')\n",
      "(u'model', u'follows', u'h1,1')\n",
      "(u'follows', u'h1,1', u'sTW1,1')\n",
      "(u'h1,1', u'sTW1,1', u'b1,1')\n",
      "(u'sTW1,1', u'b1,1', u'h1,2')\n",
      "(u'b1,1', u'h1,2', u'dTW1,2')\n",
      "(u'h1,2', u'dTW1,2', u'b1,2')\n",
      "(u'dTW1,2', u'b1,2', u'h1,3')\n",
      "(u'b1,2', u'h1,3', u'tTW1,3')\n",
      "(u'h1,3', u'tTW1,3', u'b1,3')\n",
      "(u'tTW1,3', u'b1,3', u'h2,1T')\n",
      "(u'b1,3', u'h2,1T', u'h2T')\n",
      "(u'h2,1T', u'h2T', u'\\u02dcycls')\n",
      "(u'h2T', u'\\u02dcycls', u'h3T')\n",
      "(u'\\u02dcycls', u'h3T', u'\\u02dcypst')\n",
      "(u'h3T', u'\\u02dcypst', u'h3T')\n",
      "(u'\\u02dcypst', u'h3T', u'Wpst')\n",
      "(u'h3T', u'Wpst', u'bpst')\n",
      "(u'Wpst', u'bpst', u'wcls')\n",
      "(u'bpst', u'wcls', u'bcls')\n",
      "(u'wcls', u'bcls', u'h2,2T')\n",
      "(u'bcls', u'h2,2T', u'h2,3T')\n",
      "(u'h2,2T', u'h2,3T', u'exp')\n",
      "(u'h2,3T', u'exp', u'the')\n",
      "(u'exp', u'the', u'sigmoid')\n",
      "(u'the', u'sigmoid', u'function')\n",
      "(u'sigmoid', u'function', u'the')\n",
      "(u'function', u'the', u'point-wise')\n",
      "(u'the', u'point-wise', u'non-linear')\n",
      "(u'point-wise', u'non-linear', u'activation')\n",
      "(u'non-linear', u'activation', u'function')\n",
      "(u'activation', u'function', u'for')\n",
      "(u'function', u'for', u'and')\n",
      "(u'for', u'and', u'wcls')\n",
      "(u'and', u'wcls', u'connect')\n",
      "(u'wcls', u'connect', u'nodes')\n",
      "(u'connect', u'nodes', u'between')\n",
      "(u'nodes', u'between', u'adjacent')\n",
      "(u'between', u'adjacent', u'layers')\n",
      "(u'adjacent', u'layers', u'and')\n",
      "(u'layers', u'and', u'bcls')\n",
      "(u'and', u'bcls', u'are')\n",
      "(u'bcls', u'are', u'biases')\n",
      "(u'are', u'biases', u'which')\n",
      "(u'biases', u'which', u'sigmoid')\n",
      "(u'which', u'sigmoid', u'function')\n",
      "(u'sigmoid', u'function', u'can')\n",
      "(u'function', u'can', u'used')\n",
      "(u'can', u'used', u'Our')\n",
      "(u'used', u'Our', u'deep')\n",
      "(u'Our', u'deep', u'modelExisting')\n",
      "(u'deep', u'modelExisting', u'approachInputCandidatesResultsExisting')\n",
      "(u'modelExisting', u'approachInputCandidatesResultsExisting', u'approachDeep')\n",
      "(u'approachInputCandidatesResultsExisting', u'approachDeep', u'model')\n",
      "(u'approachDeep', u'model', u'...')\n",
      "(u'model', u'...', u'...')\n",
      "(u'...', u'...', u'...')\n",
      "(u'...', u'...', u'...')\n",
      "(u'...', u'...', u'h3h1,3')\n",
      "(u'...', u'h3h1,3', u'...')\n",
      "(u'h3h1,3', u'...', u'...')\n",
      "(u'...', u'...', u'sh2,3')\n",
      "(u'...', u'sh2,3', u'...')\n",
      "(u'sh2,3', u'...', u'...')\n",
      "(u'...', u'...', u'...')\n",
      "(u'...', u'...', u'...')\n",
      "(u'...', u'...', u'...')\n",
      "(u'...', u'...', u'...')\n",
      "(u'...', u'...', u'...')\n",
      "(u'...', u'...', u'...')\n",
      "(u'...', u'...', u'...')\n",
      "(u'...', u'...', u'...')\n",
      "(u'...', u'...', u'ypstW1')\n",
      "(u'...', u'ypstW1', u'1W1')\n",
      "(u'ypstW1', u'1W1', u'2W1')\n",
      "(u'1W1', u'2W1', u'3W2')\n",
      "(u'2W1', u'3W2', u'1W2')\n",
      "(u'3W2', u'1W2', u'2W2')\n",
      "(u'1W2', u'2W2', u'3W3Wpstyclswclsh3h1')\n",
      "(u'2W2', u'3W3Wpstyclswclsh3h1', u'...')\n",
      "(u'3W3Wpstyclswclsh3h1', u'...', u'sh2')\n",
      "(u'...', u'sh2', u'...')\n",
      "(u'sh2', u'...', u'...')\n",
      "(u'...', u'...', u'...')\n",
      "(u'...', u'...', u'...')\n",
      "(u'...', u'...', u'...')\n",
      "(u'...', u'...', u'...')\n",
      "(u'...', u'...', u'ypstycls')\n",
      "(u'...', u'ypstycls', u'...')\n",
      "(u'ypstycls', u'...', u'...')\n",
      "(u'...', u'...', u'...')\n",
      "(u'...', u'...', u'...')\n",
      "(u'...', u'...', u'...')\n",
      "(u'...', u'...', u'...')\n",
      "(u'...', u'...', u'h1,1h1,2h2,2h2,1')\n",
      "(u'...', u'h1,1h1,2h2,2h2,1', u'ing')\n",
      "(u'h1,1h1,2h2,2h2,1', u'ing', u'non-linear')\n",
      "(u'ing', u'non-linear', u'representations')\n",
      "(u'non-linear', u'representations', u'from')\n",
      "(u'representations', u'from', u'and')\n",
      "(u'from', u'and', u'are')\n",
      "(u'and', u'are', u'hidden')\n",
      "(u'are', u'hidden', u'nodes')\n",
      "(u'hidden', u'nodes', u'different')\n",
      "(u'nodes', u'different', u'layers')\n",
      "(u'different', u'layers', u'used')\n",
      "(u'layers', u'used', u'for')\n",
      "(u'used', u'for', u'extract-')\n",
      "(u'for', u'extract-', u'\\u02dcycls')\n",
      "(u'extract-', u'\\u02dcycls', u'the')\n",
      "(u'\\u02dcycls', u'the', u'estimated')\n",
      "(u'the', u'estimated', u'label')\n",
      "(u'estimated', u'label', u'indicating')\n",
      "(u'label', u'indicating', u'whether')\n",
      "(u'indicating', u'whether', u'the')\n",
      "(u'whether', u'the', u'candi-')\n",
      "(u'the', u'candi-', u'date')\n",
      "(u'candi-', u'date', u'body')\n",
      "(u'date', u'body', u'locations')\n",
      "(u'body', u'locations', u'correct')\n",
      "(u'locations', u'correct', u'For')\n",
      "(u'correct', u'For', u'pose')\n",
      "(u'For', u'pose', u'estimatio')\n",
      "(u'pose', u'estimatio', u'single')\n",
      "(u'estimatio', u'single', u'human')\n",
      "(u'single', u'human', u'the')\n",
      "(u'human', u'the', u'candidate')\n",
      "(u'the', u'candidate', u'with')\n",
      "(u'candidate', u'with', u'the')\n",
      "(u'with', u'the', u'largest')\n",
      "(u'the', u'largest', u'\\u02dcycls')\n",
      "(u'largest', u'\\u02dcycls', u'used')\n",
      "(u'\\u02dcycls', u'used', u'the')\n",
      "(u'used', u'the', u'\\ufb01nal')\n",
      "(u'the', u'\\ufb01nal', u'output')\n",
      "(u'\\ufb01nal', u'output', u'our')\n",
      "(u'output', u'our', u'experiments')\n",
      "(u'our', u'experiments', u'\\u02dcypst')\n",
      "(u'experiments', u'\\u02dcypst', u'contains')\n",
      "(u'\\u02dcypst', u'contains', u'the')\n",
      "(u'contains', u'the', u'estimated')\n",
      "(u'the', u'estimated', u'part')\n",
      "(u'estimated', u'part', u'locations')\n",
      "(u'part', u'locations', u'Through')\n",
      "(u'locations', u'Through', u'the')\n",
      "(u'Through', u'the', u'\\ufb01rst')\n",
      "(u'the', u'\\ufb01rst', u'two')\n",
      "(u'\\ufb01rst', u'two', u'separate')\n",
      "(u'two', u'separate', u'layers')\n",
      "(u'separate', u'layers', u'each')\n",
      "(u'layers', u'each', u'information')\n",
      "(u'each', u'information', u'source')\n",
      "(u'information', u'source', u'has')\n",
      "(u'source', u'has', u'its')\n",
      "(u'has', u'its', u'individual')\n",
      "(u'its', u'individual', u'representation')\n",
      "(u'individual', u'representation', u'con-')\n",
      "(u'representation', u'con-', u'structed')\n",
      "(u'con-', u'structed', u'Then')\n",
      "(u'structed', u'Then', u'all')\n",
      "(u'Then', u'all', u'high-order')\n",
      "(u'all', u'high-order', u'representations')\n",
      "(u'high-order', u'representations', u'are')\n",
      "(u'representations', u'are', u'combined')\n",
      "(u'are', u'combined', u'two')\n",
      "(u'combined', u'two', u'layers')\n",
      "(u'two', u'layers', u'4.2')\n",
      "(u'layers', u'4.2', u'Training')\n",
      "(u'4.2', u'Training', u'method')\n",
      "(u'Training', u'method', u'Denote')\n",
      "(u'method', u'Denote', u'the')\n",
      "(u'Denote', u'the', u'parameter')\n",
      "(u'the', u'parameter', u'set')\n",
      "(u'parameter', u'set', u'for')\n",
      "(u'set', u'for', u'the')\n",
      "(u'for', u'the', u'model')\n",
      "(u'the', u'model', u'wcls')\n",
      "(u'model', u'wcls', u'bcls')\n",
      "(u'wcls', u'bcls', u'The')\n",
      "(u'bcls', u'The', u'objective')\n",
      "(u'The', u'objective', u'function')\n",
      "(u'objective', u'function', u'for')\n",
      "(u'function', u'for', u'backpropagating')\n",
      "(u'for', u'backpropagating', u'error')\n",
      "(u'backpropagating', u'error', u'derivatives')\n",
      "(u'error', u'derivatives', u'follows')\n",
      "(u'derivatives', u'follows', u'cid:88')\n",
      "(u'follows', u'cid:88', u'cid:16')\n",
      "(u'cid:88', u'cid:16', u'ycls')\n",
      "(u'cid:16', u'ycls', u'\\u02dcycls')\n",
      "(u'ycls', u'\\u02dcycls', u'wcls')\n",
      "(u'\\u02dcycls', u'wcls', u'ycls')\n",
      "(u'wcls', u'ycls', u'ypst')\n",
      "(u'ycls', u'ypst', u'\\u02dcypst')\n",
      "(u'ypst', u'\\u02dcypst', u'log')\n",
      "(u'\\u02dcypst', u'log', u'\\u02dcycls')\n",
      "(u'log', u'\\u02dcycls', u'\\u02dcycls')\n",
      "(u'\\u02dcycls', u'\\u02dcycls', u'\\u02dcypst')\n",
      "(u'\\u02dcycls', u'\\u02dcypst', u'ycls')\n",
      "(u'\\u02dcypst', u'ycls', u'ypst')\n",
      "(u'ycls', u'ypst', u'wcls')\n",
      "(u'ypst', u'wcls', u'ycls')\n",
      "(u'wcls', u'ycls', u'ycls')\n",
      "(u'ycls', u'ycls', u'log')\n",
      "(u'ycls', u'log', u'\\u02dcycls')\n",
      "(u'log', u'\\u02dcycls', u'cid:88')\n",
      "(u'\\u02dcycls', u'cid:88', u'cid:88')\n",
      "(u'cid:88', u'cid:88', u'\\u02dcypst')\n",
      "(u'cid:88', u'\\u02dcypst', u'=||ypst')\n",
      "(u'\\u02dcypst', u'=||ypst', u'||2')\n",
      "(u'=||ypst', u'||2', u'|w\\u2217')\n",
      "(u'||2', u'|w\\u2217', u'|wcls')\n",
      "(u'|w\\u2217', u'|wcls', u'cid:17')\n",
      "(u'|wcls', u'cid:17', u'where')\n",
      "(u'cid:17', u'where', u'denotes')\n",
      "(u'where', u'denotes', u'the')\n",
      "(u'denotes', u'the', u'nth')\n",
      "(u'the', u'nth', u'sample')\n",
      "(u'nth', u'sample', u'\\u02dcycls')\n",
      "(u'sample', u'\\u02dcycls', u'are')\n",
      "(u'\\u02dcycls', u'are', u'computed')\n",
      "(u'are', u'computed', u'using')\n",
      "(u'computed', u'using', u'ycls')\n",
      "(u'using', u'ycls', u'and')\n",
      "(u'ycls', u'and', u'\\u02dcypst')\n",
      "(u'and', u'\\u02dcypst', u'the')\n",
      "(u'\\u02dcypst', u'the', u'ground')\n",
      "(u'the', u'ground', u'truth')\n",
      "(u'ground', u'truth', u'classi\\ufb01cation')\n",
      "(u'truth', u'classi\\ufb01cation', u'label')\n",
      "(u'classi\\ufb01cation', u'label', u'in-')\n",
      "(u'label', u'in-', u'dicating')\n",
      "(u'in-', u'dicating', u'whether')\n",
      "(u'dicating', u'whether', u'the')\n",
      "(u'whether', u'the', u'current')\n",
      "(u'the', u'current', u'body')\n",
      "(u'current', u'body', u'location')\n",
      "(u'body', u'location', u'estimation')\n",
      "(u'location', u'estimation', u'correct')\n",
      "(u'estimation', u'correct', u'not')\n",
      "(u'correct', u'not', u'Positive')\n",
      "(u'not', u'Positive', u'training')\n",
      "(u'Positive', u'training', u'samples')\n",
      "(u'training', u'samples', u'have')\n",
      "(u'samples', u'have', u'their')\n",
      "(u'have', u'their', u'part')\n",
      "(u'their', u'part', u'templates')\n",
      "(u'part', u'templates', u'placed')\n",
      "(u'templates', u'placed', u'around')\n",
      "(u'placed', u'around', u'annotated')\n",
      "(u'around', u'annotated', u'body')\n",
      "(u'annotated', u'body', u'locations')\n",
      "(u'body', u'locations', u'negative')\n",
      "(u'locations', u'negative', u'training')\n",
      "(u'negative', u'training', u'samples')\n",
      "(u'training', u'samples', u'have')\n",
      "(u'samples', u'have', u'their')\n",
      "(u'have', u'their', u'part')\n",
      "(u'their', u'part', u'templates')\n",
      "(u'part', u'templates', u'placed')\n",
      "(u'templates', u'placed', u'images')\n",
      "(u'placed', u'images', u'without')\n",
      "(u'images', u'without', u'human')\n",
      "(u'without', u'human', u'Therefore')\n",
      "(u'human', u'Therefore', u'ycls')\n",
      "(u'Therefore', u'ycls', u'can')\n",
      "(u'ycls', u'can', u'used')\n",
      "(u'can', u'used', u'for')\n",
      "(u'used', u'for', u'human')\n",
      "(u'for', u'human', u'detection')\n",
      "(u'human', u'detection', u'considering')\n",
      "(u'detection', u'considering', u'indi-')\n",
      "(u'considering', u'indi-', u'cator')\n",
      "(u'indi-', u'cator', u'whether')\n",
      "(u'cator', u'whether', u'the')\n",
      "(u'whether', u'the', u'rectangle')\n",
      "(u'the', u'rectangle', u'covering')\n",
      "(u'rectangle', u'covering', u'body')\n",
      "(u'covering', u'body', u'locations')\n",
      "(u'body', u'locations', u'contains')\n",
      "(u'locations', u'contains', u'human')\n",
      "(u'contains', u'human', u'ycls')\n",
      "(u'human', u'ycls', u'ypst')\n",
      "(u'ycls', u'ypst', u'ypst')\n",
      "(u'ypst', u'ypst', u'\\u02dcycls')\n",
      "(u'ypst', u'\\u02dcycls', u'the')\n",
      "(u'\\u02dcycls', u'the', u'cross-entropy')\n",
      "(u'the', u'cross-entropy', u'error')\n",
      "(u'cross-entropy', u'error', u'classi\\ufb01cation')\n",
      "(u'error', u'classi\\ufb01cation', u'contains')\n",
      "(u'classi\\ufb01cation', u'contains', u'the')\n",
      "(u'contains', u'the', u'ground')\n",
      "(u'the', u'ground', u'truth')\n",
      "(u'ground', u'truth', u'body')\n",
      "(u'truth', u'body', u'locations')\n",
      "(u'body', u'locations', u'\\u02dcypst')\n",
      "(u'locations', u'\\u02dcypst', u'the')\n",
      "(u'\\u02dcypst', u'the', u'sum')\n",
      "(u'the', u'sum', u'square')\n",
      "(u'sum', u'square', u'error')\n",
      "(u'square', u'error', u'body')\n",
      "(u'error', u'body', u'loca-')\n",
      "(u'body', u'loca-', u'tion')\n",
      "(u'loca-', u'tion', u'estimation')\n",
      "(u'tion', u'estimation', u'Since')\n",
      "(u'estimation', u'Since', u'negative')\n",
      "(u'Since', u'negative', u'background')\n",
      "(u'negative', u'background', u'samples')\n",
      "(u'background', u'samples', u'not')\n",
      "(u'samples', u'not', u'have')\n",
      "(u'not', u'have', u'ground')\n",
      "(u'have', u'ground', u'truth')\n",
      "(u'ground', u'truth', u'body')\n",
      "(u'truth', u'body', u'location')\n",
      "(u'body', u'location', u'the')\n",
      "(u'location', u'the', u'ycls')\n",
      "(u'the', u'ycls', u'multi-')\n",
      "(u'ycls', u'multi-', u'plied')\n",
      "(u'multi-', u'plied', u'ensure')\n",
      "(u'plied', u'ensure', u'that')\n",
      "(u'ensure', u'that', u'only')\n",
      "(u'that', u'only', u'positive')\n",
      "(u'only', u'positive', u'samples')\n",
      "(u'positive', u'samples', u'are')\n",
      "(u'samples', u'are', u'used')\n",
      "(u'are', u'used', u'learn')\n",
      "(u'used', u'learn', u'location')\n",
      "(u'learn', u'location', u'estimation')\n",
      "(u'location', u'estimation', u'wcls')\n",
      "(u'estimation', u'wcls', u'the')\n",
      "(u'wcls', u'the', u'norm')\n",
      "(u'the', u'norm', u'regularization')\n",
      "(u'norm', u'regularization', u'term')\n",
      "(u'regularization', u'term', u'the')\n",
      "(u'term', u'the', u'element')\n",
      "(u'the', u'element', u'and')\n",
      "(u'element', u'and', u'wcls')\n",
      "(u'and', u'wcls', u'the')\n",
      "(u'wcls', u'the', u'ith')\n",
      "(u'the', u'ith', u'and')\n",
      "(u'ith', u'and', u'element')\n",
      "(u'and', u'element', u'wcls')\n",
      "(u'element', u'wcls', u'The')\n",
      "(u'wcls', u'The', u'information')\n",
      "(u'The', u'information', u'sources')\n",
      "(u'information', u'sources', u'and')\n",
      "(u'sources', u'and', u'hidden')\n",
      "(u'and', u'hidden', u'nodes')\n",
      "(u'hidden', u'nodes', u'may')\n",
      "(u'nodes', u'may', u'have')\n",
      "(u'may', u'have', u'different')\n",
      "(u'have', u'different', u'purpose')\n",
      "(u'different', u'purpose', u'For')\n",
      "(u'purpose', u'For', u'example')\n",
      "(u'For', u'example', u'node')\n",
      "(u'example', u'node', u'may')\n",
      "(u'node', u'may', u'use')\n",
      "(u'may', u'use', u'the')\n",
      "(u'use', u'the', u'information')\n",
      "(u'the', u'information', u'source')\n",
      "(u'information', u'source', u'mixture')\n",
      "(u'source', u'mixture', u'type')\n",
      "(u'mixture', u'type', u'Hence')\n",
      "(u'type', u'Hence', u'wcls')\n",
      "(u'Hence', u'wcls', u'used')\n",
      "(u'wcls', u'used', u'encourage')\n",
      "(u'used', u'encourage', u'sparsity')\n",
      "(u'encourage', u'sparsity', u'the')\n",
      "(u'sparsity', u'the', u'weights')\n",
      "(u'the', u'weights', u'Body')\n",
      "(u'weights', u'Body', u'part')\n",
      "(u'Body', u'part', u'location')\n",
      "(u'part', u'location', u'estimation')\n",
      "(u'location', u'estimation', u'and')\n",
      "(u'estimation', u'and', u'human')\n",
      "(u'and', u'human', u'detection')\n",
      "(u'human', u'detection', u'are')\n",
      "(u'detection', u'are', u'both')\n",
      "(u'are', u'both', u'learned')\n",
      "(u'both', u'learned', u'through')\n",
      "(u'learned', u'through', u'shared')\n",
      "(u'through', u'shared', u'representation')\n",
      "(u'shared', u'representation', u'this')\n",
      "(u'representation', u'this', u'model')\n",
      "(u'this', u'model', u'They')\n",
      "(u'model', u'They', u'are')\n",
      "(u'They', u'are', u'jointly')\n",
      "(u'are', u'jointly', u'learned')\n",
      "(u'jointly', u'learned', u'because')\n",
      "(u'learned', u'because', u'they')\n",
      "(u'because', u'they', u'are')\n",
      "(u'they', u'are', u'dependent')\n",
      "(u'are', u'dependent', u'tasks')\n",
      "(u'dependent', u'tasks', u'4.3')\n",
      "(u'tasks', u'4.3', u'Analysis')\n",
      "(u'4.3', u'Analysis', u'The')\n",
      "(u'Analysis', u'The', u'mixture')\n",
      "(u'The', u'mixture', u'type')\n",
      "(u'mixture', u'type', u'used')\n",
      "(u'type', u'used', u'example')\n",
      "(u'used', u'example', u'for')\n",
      "(u'example', u'for', u'analysis')\n",
      "(u'for', u'analysis', u'the')\n",
      "(u'analysis', u'the', u'layer-wise')\n",
      "(u'the', u'layer-wise', u'pre-training')\n",
      "(u'layer-wise', u'pre-training', u'stage')\n",
      "(u'pre-training', u'stage', u'and')\n",
      "(u'stage', u'and', u'hidden')\n",
      "(u'and', u'hidden', u'vector')\n",
      "(u'hidden', u'vector', u'h1,3')\n",
      "(u'vector', u'h1,3', u'are')\n",
      "(u'h1,3', u'are', u'considered')\n",
      "(u'are', u'considered', u'restricted')\n",
      "(u'considered', u'restricted', u'Boltzmann')\n",
      "(u'restricted', u'Boltzmann', u'machine')\n",
      "(u'Boltzmann', u'machine', u'with')\n",
      "(u'machine', u'with', u'the')\n",
      "(u'with', u'the', u'following')\n",
      "(u'the', u'following', u'distribution')\n",
      "(u'following', u'distribution', u'h1,3')\n",
      "(u'distribution', u'h1,3', u'exp')\n",
      "(u'h1,3', u'exp', u'tTW1,3h1,3')\n",
      "(u'exp', u'tTW1,3h1,3', u'b1,3T')\n",
      "(u'tTW1,3h1,3', u'b1,3T', u'h1,3')\n",
      "(u'b1,3T', u'h1,3', u'cTt')\n",
      "(u'h1,3', u'cTt', u'Denote')\n",
      "(u'cTt', u'Denote', u'the')\n",
      "(u'Denote', u'the', u'jth')\n",
      "(u'the', u'jth', u'column')\n",
      "(u'jth', u'column', u'W1,3')\n",
      "(u'column', u'W1,3', u'w1,3\\u2217')\n",
      "(u'W1,3', u'w1,3\\u2217', u'Denote')\n",
      "(u'w1,3\\u2217', u'Denote', u'the')\n",
      "(u'Denote', u'the', u'jth')\n",
      "(u'the', u'jth', u'element')\n",
      "(u'jth', u'element', u'b1,3')\n",
      "(u'element', u'b1,3', u'The')\n",
      "(u'b1,3', u'The', u'marginal')\n",
      "(u'The', u'marginal', u'distribution')\n",
      "(u'marginal', u'distribution', u'can')\n",
      "(u'distribution', u'can', u'obtained')\n",
      "(u'can', u'obtained', u'follows')\n",
      "(u'obtained', u'follows', u'h1,3')\n",
      "(u'follows', u'h1,3', u'cid:88')\n",
      "(u'h1,3', u'cid:88', u'cid:88')\n",
      "(u'cid:88', u'cid:88', u'h1,3')\n",
      "(u'cid:88', u'h1,3', u'h1,3')\n",
      "(u'h1,3', u'h1,3', u'exp')\n",
      "(u'h1,3', u'exp', u'cTt')\n",
      "(u'exp', u'cTt', u'cid:89')\n",
      "(u'cTt', u'cid:89', u'cid:16')\n",
      "(u'cid:89', u'cid:16', u'exp')\n",
      "(u'cid:16', u'exp', u'tTW1,3h1,3')\n",
      "(u'exp', u'tTW1,3h1,3', u'b1,3T')\n",
      "(u'tTW1,3h1,3', u'b1,3T', u'h1,3')\n",
      "(u'b1,3T', u'h1,3', u'cTt')\n",
      "(u'h1,3', u'cTt', u'exp')\n",
      "(u'cTt', u'exp', u'tTw1,3\\u2217')\n",
      "(u'exp', u'tTw1,3\\u2217', u'cid:17')\n",
      "(u'tTw1,3\\u2217', u'cid:17', u'cid:89')\n",
      "(u'cid:17', u'cid:89', u'where')\n",
      "(u'cid:89', u'where', u'exp')\n",
      "(u'where', u'exp', u'tTw1,3\\u2217')\n",
      "(u'exp', u'tTw1,3\\u2217', u'and')\n",
      "(u'tTw1,3\\u2217', u'and', u'fully')\n",
      "(u'and', u'fully', u'connected')\n",
      "(u'fully', u'connected', u'graphical')\n",
      "(u'connected', u'graphical', u'model')\n",
      "(u'graphical', u'model', u'because')\n",
      "(u'model', u'because', u'can')\n",
      "(u'because', u'can', u'not')\n",
      "(u'can', u'not', u'factorized')\n",
      "(u'not', u'factorized', u'can')\n",
      "(u'factorized', u'can', u'considered')\n",
      "(u'can', u'considered', u'factor')\n",
      "(u'considered', u'factor', u'that')\n",
      "(u'factor', u'that', u'explains')\n",
      "(u'that', u'explains', u'fac-')\n",
      "(u'explains', u'fac-', u'tor')\n",
      "(u'fac-', u'tor', u'graph')\n",
      "(u'tor', u'graph', u'pose')\n",
      "(u'graph', u'pose', u'estimation')\n",
      "(u'pose', u'estimation', u'can')\n",
      "(u'estimation', u'can', u'con-')\n",
      "(u'can', u'con-', u'sidered')\n",
      "(u'con-', u'sidered', u'global')\n",
      "(u'sidered', u'global', u'pattern')\n",
      "(u'global', u'pattern', u'explaining')\n",
      "(u'pattern', u'explaining', u'the')\n",
      "(u'explaining', u'the', u'mixture')\n",
      "(u'the', u'mixture', u'type')\n",
      "(u'mixture', u'type', u'for')\n",
      "(u'type', u'for', u'all')\n",
      "(u'for', u'all', u'parts')\n",
      "(u'all', u'parts', u'both')\n",
      "(u'parts', u'both', u'training')\n",
      "(u'both', u'training', u'and')\n",
      "(u'training', u'and', u'inference')\n",
      "(u'and', u'inference', u'stages')\n",
      "(u'inference', u'stages', u'every')\n",
      "(u'stages', u'every', u'node')\n",
      "(u'every', u'node', u'h1,3')\n",
      "(u'node', u'h1,3', u'connected')\n",
      "(u'h1,3', u'connected', u'the')\n",
      "(u'connected', u'the', u'mixture')\n",
      "(u'the', u'mixture', u'types')\n",
      "(u'mixture', u'types', u'all')\n",
      "(u'types', u'all', u'parts')\n",
      "(u'all', u'parts', u'Therefore')\n",
      "(u'parts', u'Therefore', u'h1,3')\n",
      "(u'Therefore', u'h1,3', u'nonlinearly')\n",
      "(u'h1,3', u'nonlinearly', u'extracts')\n",
      "(u'nonlinearly', u'extracts', u'the')\n",
      "(u'extracts', u'the', u'global')\n",
      "(u'the', u'global', u'representa-')\n",
      "(u'global', u'representa-', u'tion')\n",
      "(u'representa-', u'tion', u'from')\n",
      "(u'tion', u'from', u't1,3')\n",
      "(u'from', u't1,3', u'Similarly')\n",
      "(u't1,3', u'Similarly', u'the')\n",
      "(u'Similarly', u'the', u'h2,3')\n",
      "(u'the', u'h2,3', u'extracts')\n",
      "(u'h2,3', u'extracts', u'higher-level')\n",
      "(u'extracts', u'higher-level', u'rep-')\n",
      "(u'higher-level', u'rep-', u'resentation')\n",
      "(u'rep-', u'resentation', u'from')\n",
      "(u'resentation', u'from', u'h1,3')\n",
      "(u'from', u'h1,3', u'Therefore')\n",
      "(u'h1,3', u'Therefore', u'the')\n",
      "(u'Therefore', u'the', u'stack')\n",
      "(u'the', u'stack', u'hidden')\n",
      "(u'stack', u'hidden', u'layers')\n",
      "(u'hidden', u'layers', u'extracts')\n",
      "(u'layers', u'extracts', u'global')\n",
      "(u'extracts', u'global', u'high-level')\n",
      "(u'global', u'high-level', u'representation')\n",
      "(u'high-level', u'representation', u'from')\n",
      "(u'representation', u'from', u'the')\n",
      "(u'from', u'the', u'informa-')\n",
      "(u'the', u'informa-', u'tion')\n",
      "(u'informa-', u'tion', u'source')\n",
      "(u'tion', u'source', u'The')\n",
      "(u'source', u'The', u'analysis')\n",
      "(u'The', u'analysis', u'mixture')\n",
      "(u'analysis', u'mixture', u'type')\n",
      "(u'mixture', u'type', u'applica-')\n",
      "(u'type', u'applica-', u'ble')\n",
      "(u'applica-', u'ble', u'deformation')\n",
      "(u'ble', u'deformation', u'and')\n",
      "(u'deformation', u'and', u'appearance')\n",
      "(u'and', u'appearance', u'score')\n",
      "(u'appearance', u'score', u'shown')\n",
      "(u'score', u'shown', u'Fig')\n",
      "(u'shown', u'Fig', u'h2,3')\n",
      "(u'Fig', u'h2,3', u'captures')\n",
      "(u'h2,3', u'captures', u'the')\n",
      "(u'captures', u'the', u'global')\n",
      "(u'the', u'global', u'articulation')\n",
      "(u'global', u'articulation', u'patterns')\n",
      "(u'articulation', u'patterns', u'human')\n",
      "(u'patterns', u'human', u'body')\n",
      "(u'human', u'body', u'One')\n",
      "(u'body', u'One', u'the')\n",
      "(u'One', u'the', u'nodes')\n",
      "(u'the', u'nodes', u'h2,3')\n",
      "(u'nodes', u'h2,3', u'has')\n",
      "(u'h2,3', u'has', u'high')\n",
      "(u'has', u'high', u'response')\n",
      "(u'high', u'response', u'people')\n",
      "(u'response', u'people', u'squat')\n",
      "(u'people', u'squat', u'Another')\n",
      "(u'squat', u'Another', u'node')\n",
      "(u'Another', u'node', u'has')\n",
      "(u'node', u'has', u'high')\n",
      "(u'has', u'high', u'response')\n",
      "(u'high', u'response', u'people')\n",
      "(u'response', u'people', u'standing')\n",
      "(u'people', u'standing', u'upright')\n",
      "(u'standing', u'upright', u'Yet')\n",
      "(u'upright', u'Yet', u'another')\n",
      "(u'Yet', u'another', u'node')\n",
      "(u'another', u'node', u'concisely')\n",
      "(u'node', u'concisely', u'captures')\n",
      "(u'concisely', u'captures', u'two')\n",
      "(u'captures', u'two', u'clusters')\n",
      "(u'two', u'clusters', u'pose')\n",
      "(u'clusters', u'pose', u'patterns')\n",
      "(u'pose', u'patterns', u'our')\n",
      "(u'patterns', u'our', u'deep')\n",
      "(u'our', u'deep', u'model')\n",
      "(u'deep', u'model', u'the')\n",
      "(u'model', u'the', u'\\ufb01rst')\n",
      "(u'the', u'\\ufb01rst', u'hidden')\n",
      "(u'\\ufb01rst', u'hidden', u'layer')\n",
      "(u'hidden', u'layer', u'has')\n",
      "(u'layer', u'has', u'hidden')\n",
      "(u'has', u'hidden', u'nodes')\n",
      "(u'hidden', u'nodes', u'the')\n",
      "(u'nodes', u'the', u'second')\n",
      "(u'the', u'second', u'layer')\n",
      "(u'second', u'layer', u'i.e')\n",
      "(u'layer', u'i.e', u'has')\n",
      "(u'i.e', u'has', u'hidden')\n",
      "(u'has', u'hidden', u'nodes')\n",
      "(u'hidden', u'nodes', u'and')\n",
      "(u'nodes', u'and', u'the')\n",
      "(u'and', u'the', u'third')\n",
      "(u'the', u'third', u'layer')\n",
      "(u'third', u'layer', u'i.e')\n",
      "(u'layer', u'i.e', u'has')\n",
      "(u'i.e', u'has', u'hidden')\n",
      "(u'has', u'hidden', u'nodes')\n",
      "(u'hidden', u'nodes', u'Since')\n",
      "(u'nodes', u'Since', u'the')\n",
      "(u'Since', u'the', u'dimensions')\n",
      "(u'the', u'dimensions', u'and')\n",
      "(u'dimensions', u'and', u'are')\n",
      "(u'and', u'are', u'small')\n",
      "(u'are', u'small', u'train-')\n",
      "(u'small', u'train-', u'ing')\n",
      "(u'train-', u'ing', u'the')\n",
      "(u'ing', u'the', u'deep')\n",
      "(u'the', u'deep', u'model')\n",
      "(u'deep', u'model', u'fast')\n",
      "(u'model', u'fast', u'Unlike')\n",
      "(u'fast', u'Unlike', u'loopy')\n",
      "(u'Unlike', u'loopy', u'graphical')\n",
      "(u'loopy', u'graphical', u'mod-')\n",
      "(u'graphical', u'mod-', u'els')\n",
      "(u'mod-', u'els', u'the')\n",
      "(u'els', u'the', u'deep')\n",
      "(u'the', u'deep', u'model')\n",
      "(u'deep', u'model', u'fast')\n",
      "(u'model', u'fast', u'the')\n",
      "(u'fast', u'the', u'inference')\n",
      "(u'the', u'inference', u'stage')\n",
      "(u'inference', u'stage', u'because')\n",
      "(u'stage', u'because', u'does')\n",
      "(u'because', u'does', u'not')\n",
      "(u'does', u'not', u'require')\n",
      "(u'not', u'require', u'loopy')\n",
      "(u'require', u'loopy', u'belief')\n",
      "(u'loopy', u'belief', u'propagation')\n",
      "(u'belief', u'propagation', u'sampling')\n",
      "(u'propagation', u'sampling', u'The')\n",
      "(u'sampling', u'The', u'extra')\n",
      "(u'The', u'extra', u'testing')\n",
      "(u'extra', u'testing', u'time')\n",
      "(u'testing', u'time', u'required')\n",
      "(u'time', u'required', u'our')\n",
      "(u'required', u'our', u'deep')\n",
      "(u'our', u'deep', u'model')\n",
      "(u'deep', u'model', u'less')\n",
      "(u'model', u'less', u'than')\n",
      "(u'less', u'than', u'percent')\n",
      "(u'than', u'percent', u'the')\n",
      "(u'percent', u'the', u'testing')\n",
      "(u'the', u'testing', u'time')\n",
      "(u'testing', u'time', u'required')\n",
      "(u'time', u'required', u'the')\n",
      "(u'required', u'the', u'approach')\n",
      "(u'the', u'approach', u'Experimental')\n",
      "(u'approach', u'Experimental', u'results')\n",
      "(u'Experimental', u'results', u'The')\n",
      "(u'results', u'The', u'proposed')\n",
      "(u'The', u'proposed', u'approach')\n",
      "(u'proposed', u'approach', u'evaluated')\n",
      "(u'approach', u'evaluated', u'three')\n",
      "(u'evaluated', u'three', u'datasets')\n",
      "(u'three', u'datasets', u'LSP')\n",
      "(u'datasets', u'LSP', u'PARSE')\n",
      "(u'LSP', u'PARSE', u'and')\n",
      "(u'PARSE', u'and', u'UIUC')\n",
      "(u'and', u'UIUC', u'people')\n",
      "(u'UIUC', u'people', u'The')\n",
      "(u'people', u'The', u'train-')\n",
      "(u'The', u'train-', u'ing')\n",
      "(u'train-', u'ing', u'procedure')\n",
      "(u'ing', u'procedure', u'and')\n",
      "(u'procedure', u'and', u'training')\n",
      "(u'and', u'training', u'set')\n",
      "(u'training', u'set', u'are')\n",
      "(u'set', u'are', u'the')\n",
      "(u'are', u'the', u'same')\n",
      "(u'the', u'same', u'Positive')\n",
      "(u'same', u'Positive', u'pose')\n",
      "(u'Positive', u'pose', u'estimation')\n",
      "(u'pose', u'estimation', u'for')\n",
      "(u'estimation', u'for', u'future')\n",
      "(u'for', u'future', u'applications')\n",
      "(u'future', u'applications', u'For')\n",
      "(u'applications', u'For', u'example')\n",
      "(u'For', u'example', u'character')\n",
      "(u'example', u'character', u'animation')\n",
      "(u'character', u'animation', u'the')\n",
      "(u'animation', u'the', u'rendering')\n",
      "(u'the', u'rendering', u'limb')\n",
      "(u'rendering', u'limb', u'possible')\n",
      "(u'limb', u'possible', u'only')\n",
      "(u'possible', u'only', u'when')\n",
      "(u'only', u'when', u'both')\n",
      "(u'when', u'both', u'end')\n",
      "(u'both', u'end', u'points')\n",
      "(u'end', u'points', u'the')\n",
      "(u'points', u'the', u'limb')\n",
      "(u'the', u'limb', u'are')\n",
      "(u'limb', u'are', u'correct')\n",
      "(u'are', u'correct', u'follow')\n",
      "(u'correct', u'follow', u'and')\n",
      "(u'follow', u'and', u'use')\n",
      "(u'and', u'use', u'the')\n",
      "(u'use', u'the', u'observer-centric')\n",
      "(u'the', u'observer-centric', u'anno-')\n",
      "(u'observer-centric', u'anno-', u'tations')\n",
      "(u'anno-', u'tations', u'for')\n",
      "(u'tations', u'for', u'all')\n",
      "(u'for', u'all', u'approaches')\n",
      "(u'all', u'approaches', u'when')\n",
      "(u'approaches', u'when', u'evaluate')\n",
      "(u'when', u'evaluate', u'the')\n",
      "(u'evaluate', u'the', u'LSP')\n",
      "(u'the', u'LSP', u'dataset')\n",
      "(u'LSP', u'dataset', u'5.2')\n",
      "(u'dataset', u'5.2', u'Overall')\n",
      "(u'5.2', u'Overall', u'experimental')\n",
      "(u'Overall', u'experimental', u'results')\n",
      "(u'experimental', u'results', u'Table')\n",
      "(u'results', u'Table', u'shows')\n",
      "(u'Table', u'shows', u'the')\n",
      "(u'shows', u'the', u'experimental')\n",
      "(u'the', u'experimental', u'results')\n",
      "(u'experimental', u'results', u'from')\n",
      "(u'results', u'from', u'the')\n",
      "(u'from', u'the', u'three')\n",
      "(u'the', u'three', u'datasets')\n",
      "(u'three', u'datasets', u'Pishchulin\\u2019s')\n",
      "(u'datasets', u'Pishchulin\\u2019s', u'approach')\n",
      "(u'Pishchulin\\u2019s', u'approach', u'used')\n",
      "(u'approach', u'used', u'the')\n",
      "(u'used', u'the', u'LSP+PARSE')\n",
      "(u'the', u'LSP+PARSE', u'training')\n",
      "(u'LSP+PARSE', u'training', u'set')\n",
      "(u'training', u'set', u'when')\n",
      "(u'set', u'when', u'evaluated')\n",
      "(u'when', u'evaluated', u'the')\n",
      "(u'evaluated', u'the', u'PARSE')\n",
      "(u'the', u'PARSE', u'dataset')\n",
      "(u'PARSE', u'dataset', u'and')\n",
      "(u'dataset', u'and', u'used')\n",
      "(u'and', u'used', u'the')\n",
      "(u'used', u'the', u'UIUC+LSP')\n",
      "(u'the', u'UIUC+LSP', u'training')\n",
      "(u'UIUC+LSP', u'training', u'set')\n",
      "(u'training', u'set', u'when')\n",
      "(u'set', u'when', u'evaluated')\n",
      "(u'when', u'evaluated', u'the')\n",
      "(u'evaluated', u'the', u'UIUC')\n",
      "(u'the', u'UIUC', u'dataset')\n",
      "(u'UIUC', u'dataset', u'evaluate')\n",
      "(u'dataset', u'evaluate', u'the')\n",
      "(u'evaluate', u'the', u'PARSE')\n",
      "(u'the', u'PARSE', u'dataset')\n",
      "(u'PARSE', u'dataset', u'Pishchulin\\u2019s')\n",
      "(u'dataset', u'Pishchulin\\u2019s', u'approach')\n",
      "(u'Pishchulin\\u2019s', u'approach', u'included')\n",
      "(u'approach', u'included', u'LSP+PARSE')\n",
      "(u'included', u'LSP+PARSE', u'and')\n",
      "(u'LSP+PARSE', u'and', u'ex-')\n",
      "(u'and', u'ex-', u'tra')\n",
      "(u'ex-', u'tra', u'animated')\n",
      "(u'tra', u'animated', u'samples')\n",
      "(u'animated', u'samples', u'for')\n",
      "(u'samples', u'for', u'training')\n",
      "(u'for', u'training', u'Johnson\\u2019s')\n",
      "(u'training', u'Johnson\\u2019s', u'approach')\n",
      "(u'Johnson\\u2019s', u'approach', u'included')\n",
      "(u'approach', u'included', u'10,000')\n",
      "(u'included', u'10,000', u'extra')\n",
      "(u'10,000', u'extra', u'training')\n",
      "(u'extra', u'training', u'samples')\n",
      "(u'training', u'samples', u'when')\n",
      "(u'samples', u'when', u'evaluated')\n",
      "(u'when', u'evaluated', u'the')\n",
      "(u'evaluated', u'the', u'PARSE')\n",
      "(u'the', u'PARSE', u'dataset')\n",
      "(u'PARSE', u'dataset', u'all')\n",
      "(u'dataset', u'all', u'experiments')\n",
      "(u'all', u'experiments', u'Andriluka\\u2019s')\n",
      "(u'experiments', u'Andriluka\\u2019s', u'ap-')\n",
      "(u'Andriluka\\u2019s', u'ap-', u'proach')\n",
      "(u'ap-', u'proach', u'Yang')\n",
      "(u'proach', u'Yang', u'and')\n",
      "(u'Yang', u'and', u'Ramanan\\u2019s')\n",
      "(u'and', u'Ramanan\\u2019s', u'approach')\n",
      "(u'Ramanan\\u2019s', u'approach', u'and')\n",
      "(u'approach', u'and', u'our')\n",
      "(u'and', u'our', u'approach')\n",
      "(u'our', u'approach', u'are')\n",
      "(u'approach', u'are', u'trained')\n",
      "(u'are', u'trained', u'the')\n",
      "(u'trained', u'the', u'training')\n",
      "(u'the', u'training', u'images')\n",
      "(u'training', u'images', u'the')\n",
      "(u'images', u'the', u'LSP')\n",
      "(u'the', u'LSP', u'dataset')\n",
      "(u'LSP', u'dataset', u'shown')\n",
      "(u'dataset', u'shown', u'Table')\n",
      "(u'shown', u'Table', u'our')\n",
      "(u'Table', u'our', u'deep')\n",
      "(u'our', u'deep', u'model')\n",
      "(u'deep', u'model', u'obviously')\n",
      "(u'model', u'obviously', u'improves')\n",
      "(u'obviously', u'improves', u'the')\n",
      "(u'improves', u'the', u'pose')\n",
      "(u'the', u'pose', u'estimation')\n",
      "(u'pose', u'estimation', u'accuracy')\n",
      "(u'estimation', u'accuracy', u'and')\n",
      "(u'accuracy', u'and', u'outperforms')\n",
      "(u'and', u'outperforms', u'all')\n",
      "(u'outperforms', u'all', u'the')\n",
      "(u'all', u'the', u'state-')\n",
      "(u'the', u'state-', u'of-the-art')\n",
      "(u'state-', u'of-the-art', u'these')\n",
      "(u'of-the-art', u'these', u'three')\n",
      "(u'these', u'three', u'datasets')\n",
      "(u'three', u'datasets', u'Speci\\ufb01cally')\n",
      "(u'datasets', u'Speci\\ufb01cally', u'our')\n",
      "(u'Speci\\ufb01cally', u'our', u'approach')\n",
      "(u'our', u'approach', u'better')\n",
      "(u'approach', u'better', u'detecting')\n",
      "(u'better', u'detecting', u'legs')\n",
      "(u'detecting', u'legs', u'arms')\n",
      "(u'legs', u'arms', u'and')\n",
      "(u'arms', u'and', u'head')\n",
      "(u'and', u'head', u'compared')\n",
      "(u'head', u'compared', u'with')\n",
      "(u'compared', u'with', u'ex-')\n",
      "(u'with', u'ex-', u'isting')\n",
      "(u'ex-', u'isting', u'approaches')\n",
      "(u'isting', u'approaches', u'The')\n",
      "(u'approaches', u'The', u'approach')\n",
      "(u'The', u'approach', u'Pishchulin')\n",
      "(u'approach', u'Pishchulin', u'better')\n",
      "(u'Pishchulin', u'better', u'than')\n",
      "(u'better', u'than', u'our')\n",
      "(u'than', u'our', u'approach')\n",
      "(u'our', u'approach', u'locating')\n",
      "(u'approach', u'locating', u'torso')\n",
      "(u'locating', u'torso', u'possibly')\n",
      "(u'torso', u'possibly', u'because')\n",
      "(u'possibly', u'because', u'the')\n",
      "(u'because', u'the', u'torso')\n",
      "(u'the', u'torso', u'region')\n",
      "(u'torso', u'region', u'included')\n",
      "(u'region', u'included', u'many')\n",
      "(u'included', u'many', u'poslets')\n",
      "(u'many', u'poslets', u'which')\n",
      "(u'poslets', u'which', u'helps')\n",
      "(u'which', u'helps', u'increase')\n",
      "(u'helps', u'increase', u'the')\n",
      "(u'increase', u'the', u'accuracy')\n",
      "(u'the', u'accuracy', u'their')\n",
      "(u'accuracy', u'their', u'approach')\n",
      "(u'their', u'approach', u'locating')\n",
      "(u'approach', u'locating', u'torso')\n",
      "(u'locating', u'torso', u'Our')\n",
      "(u'torso', u'Our', u'approach')\n",
      "(u'Our', u'approach', u'complementary')\n",
      "(u'approach', u'complementary', u'existing')\n",
      "(u'complementary', u'existing', u'approaches')\n",
      "(u'existing', u'approaches', u'because')\n",
      "(u'approaches', u'because', u'the')\n",
      "(u'because', u'the', u'information')\n",
      "(u'the', u'information', u'sources')\n",
      "(u'information', u'sources', u'provided')\n",
      "(u'sources', u'provided', u'these')\n",
      "(u'provided', u'these', u'ap-')\n",
      "(u'these', u'ap-', u'proaches')\n",
      "(u'ap-', u'proaches', u'can')\n",
      "(u'proaches', u'can', u'used')\n",
      "(u'can', u'used', u'our')\n",
      "(u'used', u'our', u'model')\n",
      "(u'our', u'model', u'improve')\n",
      "(u'model', u'improve', u'their')\n",
      "(u'improve', u'their', u're-')\n",
      "(u'their', u're-', u'sults')\n",
      "(u're-', u'sults', u'Currently')\n",
      "(u'sults', u'Currently', u'our')\n",
      "(u'Currently', u'our', u'model')\n",
      "(u'our', u'model', u'uses')\n",
      "(u'model', u'uses', u'the')\n",
      "(u'uses', u'the', u'approach')\n",
      "(u'the', u'approach', u'obtain')\n",
      "(u'approach', u'obtain', u'information')\n",
      "(u'obtain', u'information', u'sources')\n",
      "(u'information', u'sources', u'Compared')\n",
      "(u'sources', u'Compared', u'with')\n",
      "(u'Compared', u'with', u'the')\n",
      "(u'with', u'the', u'approach')\n",
      "(u'the', u'approach', u'our')\n",
      "(u'approach', u'our', u'approach')\n",
      "(u'our', u'approach', u'improves')\n",
      "(u'approach', u'improves', u'the')\n",
      "(u'improves', u'the', u'pose')\n",
      "(u'the', u'pose', u'estimation')\n",
      "(u'pose', u'estimation', u'accuracy')\n",
      "(u'estimation', u'accuracy', u'5.8')\n",
      "(u'accuracy', u'5.8', u'62.8')\n",
      "(u'5.8', u'62.8', u'vs.')\n",
      "(u'62.8', u'vs.', u'68.6')\n",
      "(u'vs.', u'68.6', u'PCP')\n",
      "(u'68.6', u'PCP', u'7.4')\n",
      "(u'PCP', u'7.4', u'63.6')\n",
      "(u'7.4', u'63.6', u'vs.')\n",
      "(u'63.6', u'vs.', u'71.0')\n",
      "(u'vs.', u'71.0', u'PCP')\n",
      "(u'71.0', u'PCP', u'and')\n",
      "(u'PCP', u'and', u'8.6')\n",
      "(u'and', u'8.6', u'57.0')\n",
      "(u'8.6', u'57.0', u'vs.')\n",
      "(u'57.0', u'vs.', u'65.6')\n",
      "(u'vs.', u'65.6', u'PCP')\n",
      "(u'65.6', u'PCP', u'respectively')\n",
      "(u'PCP', u'respectively', u'the')\n",
      "(u'respectively', u'the', u'LSP')\n",
      "(u'the', u'LSP', u'PARSE')\n",
      "(u'LSP', u'PARSE', u'and')\n",
      "(u'PARSE', u'and', u'UIUC')\n",
      "(u'and', u'UIUC', u'datasets')\n",
      "(u'UIUC', u'datasets', u'Fig')\n",
      "(u'datasets', u'Fig', u'shows')\n",
      "(u'Fig', u'shows', u'the')\n",
      "(u'shows', u'the', u'compar-')\n",
      "(u'the', u'compar-', u'ison')\n",
      "(u'compar-', u'ison', u'between')\n",
      "(u'ison', u'between', u'our')\n",
      "(u'between', u'our', u'approach')\n",
      "(u'our', u'approach', u'left')\n",
      "(u'approach', u'left', u'and')\n",
      "(u'left', u'and', u'the')\n",
      "(u'and', u'the', u'approach')\n",
      "(u'the', u'approach', u'right')\n",
      "(u'approach', u'right', u'5.3')\n",
      "(u'right', u'5.3', u'Results')\n",
      "(u'5.3', u'Results', u'different')\n",
      "(u'Results', u'different', u'designs')\n",
      "(u'different', u'designs', u'deep')\n",
      "(u'designs', u'deep', u'models')\n",
      "(u'deep', u'models', u'this')\n",
      "(u'models', u'this', u'section')\n",
      "(u'this', u'section', u'evaluate')\n",
      "(u'section', u'evaluate', u'different')\n",
      "(u'evaluate', u'different', u'designs')\n",
      "(u'different', u'designs', u'deep')\n",
      "(u'designs', u'deep', u'models')\n",
      "(u'deep', u'models', u'Yang')\n",
      "(u'models', u'Yang', u'and')\n",
      "(u'Yang', u'and', u'Ramanan\\u2019s')\n",
      "(u'and', u'Ramanan\\u2019s', u'approach')\n",
      "(u'Ramanan\\u2019s', u'approach', u'used')\n",
      "(u'approach', u'used', u'the')\n",
      "(u'used', u'the', u'baseline')\n",
      "(u'the', u'baseline', u'because')\n",
      "(u'baseline', u'because', u'this')\n",
      "(u'because', u'this', u'approach')\n",
      "(u'this', u'approach', u'used')\n",
      "(u'approach', u'used', u'our')\n",
      "(u'used', u'our', u'model')\n",
      "(u'our', u'model', u'for')\n",
      "(u'model', u'for', u'obtaining')\n",
      "(u'for', u'obtaining', u'information')\n",
      "(u'obtaining', u'information', u'sources')\n",
      "(u'information', u'sources', u'concise')\n",
      "(u'sources', u'concise', u'only')\n",
      "(u'concise', u'only', u'refer')\n",
      "(u'only', u'refer', u'the')\n",
      "(u'refer', u'the', u'PCP')\n",
      "(u'the', u'PCP', u'results')\n",
      "(u'PCP', u'results', u'the')\n",
      "(u'results', u'the', u'LSP')\n",
      "(u'the', u'LSP', u'dataset')\n",
      "(u'LSP', u'dataset', u'Depth')\n",
      "(u'dataset', u'Depth', u'model')\n",
      "(u'Depth', u'model', u'investigated')\n",
      "(u'model', u'investigated', u'Table')\n",
      "(u'investigated', u'Table', u'The')\n",
      "(u'Table', u'The', u'approach')\n",
      "(u'The', u'approach', u'uses')\n",
      "(u'approach', u'uses', u'linear-SVM')\n",
      "(u'uses', u'linear-SVM', u'for')\n",
      "(u'linear-SVM', u'for', u'combining')\n",
      "(u'for', u'combining', u'information')\n",
      "(u'combining', u'information', u'sources')\n",
      "(u'information', u'sources', u'also')\n",
      "(u'sources', u'also', u'trained')\n",
      "(u'also', u'trained', u'Kernel-SVM')\n",
      "(u'trained', u'Kernel-SVM', u'with')\n",
      "(u'Kernel-SVM', u'with', u'RBF')\n",
      "(u'with', u'RBF', u'kernel')\n",
      "(u'RBF', u'kernel', u'for')\n",
      "(u'kernel', u'for', u'learn-')\n",
      "(u'for', u'learn-', u'ing')\n",
      "(u'learn-', u'ing', u'non-linear')\n",
      "(u'ing', u'non-linear', u'model')\n",
      "(u'non-linear', u'model', u'using')\n",
      "(u'model', u'using', u'the')\n",
      "(u'using', u'the', u'off-the-shelf')\n",
      "(u'the', u'off-the-shelf', u'tool')\n",
      "(u'off-the-shelf', u'tool', u'Libsvm')\n",
      "(u'tool', u'Libsvm', u'The')\n",
      "(u'Libsvm', u'The', u'difference')\n",
      "(u'The', u'difference', u'PCP')\n",
      "(u'difference', u'PCP', u'between')\n",
      "(u'PCP', u'between', u'Linear')\n",
      "(u'between', u'Linear', u'SVM')\n",
      "(u'Linear', u'SVM', u'and')\n",
      "(u'SVM', u'and', u'kernel-')\n",
      "(u'and', u'kernel-', u'SVM')\n",
      "(u'kernel-', u'SVM', u'within')\n",
      "(u'SVM', u'within', u'62.8')\n",
      "(u'within', u'62.8', u'vs.')\n",
      "(u'62.8', u'vs.', u'64.2')\n",
      "(u'vs.', u'64.2', u'LSP')\n",
      "(u'64.2', u'LSP', u'Bengio')\n",
      "(u'LSP', u'Bengio', u'Figure')\n",
      "(u'Bengio', u'Figure', u'Visualization')\n",
      "(u'Figure', u'Visualization', u'mixture-type')\n",
      "(u'Visualization', u'mixture-type', u'patterns')\n",
      "(u'mixture-type', u'patterns', u'extracted')\n",
      "(u'patterns', u'extracted', u'hid-')\n",
      "(u'extracted', u'hid-', u'den')\n",
      "(u'hid-', u'den', u'nodes')\n",
      "(u'den', u'nodes', u'h2,3')\n",
      "(u'nodes', u'h2,3', u'use')\n",
      "(u'h2,3', u'use', u'the')\n",
      "(u'use', u'the', u'approach')\n",
      "(u'the', u'approach', u'and')\n",
      "(u'approach', u'and', u'visualize')\n",
      "(u'and', u'visualize', u'train-')\n",
      "(u'visualize', u'train-', u'ing')\n",
      "(u'train-', u'ing', u'samples')\n",
      "(u'ing', u'samples', u'with')\n",
      "(u'samples', u'with', u'the')\n",
      "(u'with', u'the', u'largest')\n",
      "(u'the', u'largest', u'responses')\n",
      "(u'largest', u'responses', u'each')\n",
      "(u'responses', u'each', u'hidden')\n",
      "(u'each', u'hidden', u'node')\n",
      "(u'hidden', u'node', u'Sam-')\n",
      "(u'node', u'Sam-', u'ples')\n",
      "(u'Sam-', u'ples', u'with')\n",
      "(u'ples', u'with', u'the')\n",
      "(u'with', u'the', u'highest')\n",
      "(u'the', u'highest', u'responses')\n",
      "(u'highest', u'responses', u'are')\n",
      "(u'responses', u'are', u'placed')\n",
      "(u'are', u'placed', u'the')\n",
      "(u'placed', u'the', u'upper-left')\n",
      "(u'the', u'upper-left', u'corner')\n",
      "(u'upper-left', u'corner', u'Hidden')\n",
      "(u'corner', u'Hidden', u'node')\n",
      "(u'Hidden', u'node', u'has')\n",
      "(u'node', u'has', u'high')\n",
      "(u'has', u'high', u'response')\n",
      "(u'high', u'response', u'people')\n",
      "(u'response', u'people', u'squat')\n",
      "(u'people', u'squat', u'Node')\n",
      "(u'squat', u'Node', u'has')\n",
      "(u'Node', u'has', u'high')\n",
      "(u'has', u'high', u'response')\n",
      "(u'high', u'response', u'standing')\n",
      "(u'response', u'standing', u'people')\n",
      "(u'standing', u'people', u'Node')\n",
      "(u'people', u'Node', u'has')\n",
      "(u'Node', u'has', u'high')\n",
      "(u'has', u'high', u'response')\n",
      "(u'high', u'response', u'two')\n",
      "(u'response', u'two', u'clusters')\n",
      "(u'two', u'clusters', u'pose')\n",
      "(u'clusters', u'pose', u'patterns')\n",
      "(u'pose', u'patterns', u'Best')\n",
      "(u'patterns', u'Best', u'viewed')\n",
      "(u'Best', u'viewed', u'color')\n",
      "(u'viewed', u'color', u'training')\n",
      "(u'color', u'training', u'samples')\n",
      "(u'training', u'samples', u'are')\n",
      "(u'samples', u'are', u'constrained')\n",
      "(u'are', u'constrained', u'have')\n",
      "(u'constrained', u'have', u'estimated')\n",
      "(u'have', u'estimated', u'part')\n",
      "(u'estimated', u'part', u'lo-')\n",
      "(u'part', u'lo-', u'cations')\n",
      "(u'lo-', u'cations', u'near')\n",
      "(u'cations', u'near', u'the')\n",
      "(u'near', u'the', u'ground')\n",
      "(u'the', u'ground', u'truth')\n",
      "(u'ground', u'truth', u'Part')\n",
      "(u'truth', u'Part', u'the')\n",
      "(u'Part', u'the', u'training')\n",
      "(u'the', u'training', u'data')\n",
      "(u'training', u'data', u'used')\n",
      "(u'data', u'used', u'for')\n",
      "(u'used', u'for', u'validation')\n",
      "(u'for', u'validation', u'5.1')\n",
      "(u'validation', u'5.1', u'Evaluation')\n",
      "(u'5.1', u'Evaluation', u'criteria')\n",
      "(u'Evaluation', u'criteria', u'all')\n",
      "(u'criteria', u'all', u'experiments')\n",
      "(u'all', u'experiments', u'use')\n",
      "(u'experiments', u'use', u'the')\n",
      "(u'use', u'the', u'most')\n",
      "(u'the', u'most', u'popular')\n",
      "(u'most', u'popular', u'criterion')\n",
      "(u'popular', u'criterion', u'which')\n",
      "(u'criterion', u'which', u'the')\n",
      "(u'which', u'the', u'percentage')\n",
      "(u'the', u'percentage', u'correctly')\n",
      "(u'percentage', u'correctly', u'localized')\n",
      "(u'correctly', u'localized', u'parts')\n",
      "(u'localized', u'parts', u'PCP')\n",
      "(u'parts', u'PCP', u'introduced')\n",
      "(u'PCP', u'introduced', u'stated')\n",
      "(u'introduced', u'stated', u'the')\n",
      "(u'stated', u'the', u'PCP')\n",
      "(u'the', u'PCP', u'scoring')\n",
      "(u'PCP', u'scoring', u'metric')\n",
      "(u'scoring', u'metric', u'has')\n",
      "(u'metric', u'has', u'been')\n",
      "(u'has', u'been', u'implemented')\n",
      "(u'been', u'implemented', u'different')\n",
      "(u'implemented', u'different', u'ways')\n",
      "(u'different', u'ways', u'different')\n",
      "(u'ways', u'different', u'papers')\n",
      "(u'different', u'papers', u'These')\n",
      "(u'papers', u'These', u'differences')\n",
      "(u'These', u'differences', u'have')\n",
      "(u'differences', u'have', u'two')\n",
      "(u'have', u'two', u'dimensions')\n",
      "(u'two', u'dimensions', u'There')\n",
      "(u'dimensions', u'There', u'are')\n",
      "(u'There', u'are', u'two')\n",
      "(u'are', u'two', u'ways')\n",
      "(u'two', u'ways', u'compute')\n",
      "(u'ways', u'compute', u'the')\n",
      "(u'compute', u'the', u'\\ufb01nal')\n",
      "(u'the', u'\\ufb01nal', u'PCP')\n",
      "(u'\\ufb01nal', u'PCP', u'score')\n",
      "(u'PCP', u'score', u'across')\n",
      "(u'score', u'across', u'the')\n",
      "(u'across', u'the', u'dataset')\n",
      "(u'the', u'dataset', u'the')\n",
      "(u'dataset', u'the', u'single')\n",
      "(u'the', u'single', u'way')\n",
      "(u'single', u'way', u'only')\n",
      "(u'way', u'only', u'single')\n",
      "(u'only', u'single', u'candi-')\n",
      "(u'single', u'candi-', u'date')\n",
      "(u'candi-', u'date', u'given')\n",
      "(u'date', u'given', u'the')\n",
      "(u'given', u'the', u'maximum')\n",
      "(u'the', u'maximum', u'scoring')\n",
      "(u'maximum', u'scoring', u'candidate')\n",
      "(u'scoring', u'candidate', u'al-')\n",
      "(u'candidate', u'al-', u'gorithm')\n",
      "(u'al-', u'gorithm', u'for')\n",
      "(u'gorithm', u'for', u'one')\n",
      "(u'for', u'one', u'image')\n",
      "(u'one', u'image', u'used')\n",
      "(u'image', u'used', u'The')\n",
      "(u'used', u'The', u'match')\n",
      "(u'The', u'match', u'way')\n",
      "(u'match', u'way', u'matches')\n",
      "(u'way', u'matches', u'multiple')\n",
      "(u'matches', u'multiple', u'candidates')\n",
      "(u'multiple', u'candidates', u'without')\n",
      "(u'candidates', u'without', u'penalizing')\n",
      "(u'without', u'penalizing', u'false')\n",
      "(u'penalizing', u'false', u'positives')\n",
      "(u'false', u'positives', u'There')\n",
      "(u'positives', u'There', u'are')\n",
      "(u'There', u'are', u'two')\n",
      "(u'are', u'two', u'de\\ufb01nitions')\n",
      "(u'two', u'de\\ufb01nitions', u'correct')\n",
      "(u'de\\ufb01nitions', u'correct', u'part')\n",
      "(u'correct', u'part', u'localization')\n",
      "(u'part', u'localization', u'For')\n",
      "(u'localization', u'For', u'the')\n",
      "(u'For', u'the', u'de\\ufb01nition')\n",
      "(u'the', u'de\\ufb01nition', u'both')\n",
      "(u'de\\ufb01nition', u'both', u'requires')\n",
      "(u'both', u'requires', u'both')\n",
      "(u'requires', u'both', u'end')\n",
      "(u'both', u'end', u'points')\n",
      "(u'end', u'points', u'part')\n",
      "(u'points', u'part', u'for')\n",
      "(u'part', u'for', u'example')\n",
      "(u'for', u'example', u'end')\n",
      "(u'example', u'end', u'points')\n",
      "(u'end', u'points', u'wrist')\n",
      "(u'points', u'wrist', u'and')\n",
      "(u'wrist', u'and', u'elbow')\n",
      "(u'and', u'elbow', u'for')\n",
      "(u'elbow', u'for', u'the')\n",
      "(u'for', u'the', u'part')\n",
      "(u'the', u'part', u'lower')\n",
      "(u'part', u'lower', u'arm')\n",
      "(u'lower', u'arm', u'correct')\n",
      "(u'arm', u'correct', u'For')\n",
      "(u'correct', u'For', u'the')\n",
      "(u'For', u'the', u'de\\ufb01nition')\n",
      "(u'the', u'de\\ufb01nition', u'avg')\n",
      "(u'de\\ufb01nition', u'avg', u'requires')\n",
      "(u'avg', u'requires', u'only')\n",
      "(u'requires', u'only', u'the')\n",
      "(u'only', u'the', u'average')\n",
      "(u'the', u'average', u'the')\n",
      "(u'average', u'the', u'endpoints')\n",
      "(u'the', u'endpoints', u'correct')\n",
      "(u'endpoints', u'correct', u'The')\n",
      "(u'correct', u'The', u'paper')\n",
      "(u'The', u'paper', u'used')\n",
      "(u'paper', u'used', u'\\u2018match+avg\\u2019')\n",
      "(u'used', u'\\u2018match+avg\\u2019', u'The')\n",
      "(u'\\u2018match+avg\\u2019', u'The', u'paper')\n",
      "(u'The', u'paper', u'used')\n",
      "(u'paper', u'used', u'\\u2018single+both\\u2019')\n",
      "(u'used', u'\\u2018single+both\\u2019', u'which')\n",
      "(u'\\u2018single+both\\u2019', u'which', u'the')\n",
      "(u'which', u'the', u'strictest')\n",
      "(u'the', u'strictest', u'case')\n",
      "(u'strictest', u'case', u'and')\n",
      "(u'case', u'and', u'generally')\n",
      "(u'and', u'generally', u'has')\n",
      "(u'generally', u'has', u'lower')\n",
      "(u'has', u'lower', u'PCP')\n",
      "(u'lower', u'PCP', u'value')\n",
      "(u'PCP', u'value', u'The')\n",
      "(u'value', u'The', u'paper')\n",
      "(u'The', u'paper', u'pro-')\n",
      "(u'paper', u'pro-', u'vides')\n",
      "(u'pro-', u'vides', u'results')\n",
      "(u'vides', u'results', u'for')\n",
      "(u'results', u'for', u'\\u2018match+both\\u2019')\n",
      "(u'for', u'\\u2018match+both\\u2019', u'and')\n",
      "(u'\\u2018match+both\\u2019', u'and', u'\\u2018match+avg\\u2019')\n",
      "(u'and', u'\\u2018match+avg\\u2019', u'follow')\n",
      "(u'\\u2018match+avg\\u2019', u'follow', u'and')\n",
      "(u'follow', u'and', u'evaluate')\n",
      "(u'and', u'evaluate', u'all')\n",
      "(u'evaluate', u'all', u'approaches')\n",
      "(u'all', u'approaches', u'using')\n",
      "(u'approaches', u'using', u'the')\n",
      "(u'using', u'the', u'strictest')\n",
      "(u'the', u'strictest', u'\\u2018sin-')\n",
      "(u'strictest', u'\\u2018sin-', u'gle+both\\u2019')\n",
      "(u'\\u2018sin-', u'gle+both\\u2019', u'criterion')\n",
      "(u'gle+both\\u2019', u'criterion', u'This')\n",
      "(u'criterion', u'This', u'used')\n",
      "(u'This', u'used', u'because')\n",
      "(u'used', u'because', u'the')\n",
      "(u'because', u'the', u'following')\n",
      "(u'the', u'following', u'reasons')\n",
      "(u'following', u'reasons', u'For')\n",
      "(u'reasons', u'For', u'\\u2018single\\u2019')\n",
      "(u'For', u'\\u2018single\\u2019', u'and')\n",
      "(u'\\u2018single\\u2019', u'and', u'\\u2018match\\u2019')\n",
      "(u'and', u'\\u2018match\\u2019', u'discussed')\n",
      "(u'\\u2018match\\u2019', u'discussed', u'the')\n",
      "(u'discussed', u'the', u'\\u2018match\\u2019')\n",
      "(u'the', u'\\u2018match\\u2019', u'way')\n",
      "(u'\\u2018match\\u2019', u'way', u'gives')\n",
      "(u'way', u'gives', u'unfair')\n",
      "(u'gives', u'unfair', u'advantage')\n",
      "(u'unfair', u'advantage', u'approaches')\n",
      "(u'advantage', u'approaches', u'that')\n",
      "(u'approaches', u'that', u'produce')\n",
      "(u'that', u'produce', u'large')\n",
      "(u'produce', u'large', u'number')\n",
      "(u'large', u'number', u'candidates')\n",
      "(u'number', u'candidates', u'because')\n",
      "(u'candidates', u'because', u'mis-')\n",
      "(u'because', u'mis-', u'matched')\n",
      "(u'mis-', u'matched', u'candidates')\n",
      "(u'matched', u'candidates', u'false')\n",
      "(u'candidates', u'false', u'positives')\n",
      "(u'false', u'positives', u'are')\n",
      "(u'positives', u'are', u'not')\n",
      "(u'are', u'not', u'penalized')\n",
      "(u'not', u'penalized', u'For')\n",
      "(u'penalized', u'For', u'\\u2018both\\u2019')\n",
      "(u'For', u'\\u2018both\\u2019', u'and')\n",
      "(u'\\u2018both\\u2019', u'and', u'\\u2018avg\\u2019')\n",
      "(u'and', u'\\u2018avg\\u2019', u'\\u2018both\\u2019')\n",
      "(u'\\u2018avg\\u2019', u'\\u2018both\\u2019', u'better')\n",
      "(u'\\u2018both\\u2019', u'better', u'describing')\n",
      "(u'better', u'describing', u'the')\n",
      "(u'describing', u'the', u'orientation')\n",
      "(u'the', u'orientation', u'body')\n",
      "(u'orientation', u'body', u'parts')\n",
      "(u'body', u'parts', u'and')\n",
      "(u'parts', u'and', u'will')\n",
      "(u'and', u'will', u'facilitate')\n",
      "(u'will', u'facilitate', u'the')\n",
      "(u'facilitate', u'the', u'use')\n",
      "(u'the', u'use', u'Table')\n",
      "(u'use', u'Table', u'Pose')\n",
      "(u'Table', u'Pose', u'estimation')\n",
      "(u'Pose', u'estimation', u'results')\n",
      "(u'estimation', u'results', u'PCP')\n",
      "(u'results', u'PCP', u'LSP')\n",
      "(u'PCP', u'LSP', u'UIUC')\n",
      "(u'LSP', u'UIUC', u'people')\n",
      "(u'UIUC', u'people', u'and')\n",
      "(u'people', u'and', u'PARSE')\n",
      "(u'and', u'PARSE', u'Method')\n",
      "(u'PARSE', u'Method', u'Torso')\n",
      "(u'Method', u'Torso', u'U.leg')\n",
      "(u'Torso', u'U.leg', u'L.leg')\n",
      "(u'U.leg', u'L.leg', u'U.arm')\n",
      "(u'L.leg', u'U.arm', u'L.arm')\n",
      "(u'U.arm', u'L.arm', u'head')\n",
      "(u'L.arm', u'head', u'Total')\n",
      "(u'head', u'Total', u'LSP')\n",
      "(u'Total', u'LSP', u'Andriluka')\n",
      "(u'LSP', u'Andriluka', u'80.9')\n",
      "(u'Andriluka', u'80.9', u'67.1')\n",
      "(u'80.9', u'67.1', u'60.7')\n",
      "(u'67.1', u'60.7', u'46.5')\n",
      "(u'60.7', u'46.5', u'Yang')\n",
      "(u'46.5', u'Yang', u'Ramanan')\n",
      "(u'Yang', u'Ramanan', u'81.0')\n",
      "(u'Ramanan', u'81.0', u'69.5')\n",
      "(u'81.0', u'69.5', u'65.9')\n",
      "(u'69.5', u'65.9', u'53.5')\n",
      "(u'65.9', u'53.5', u'Yang')\n",
      "(u'53.5', u'Yang', u'Ramanan')\n",
      "(u'Yang', u'Ramanan', u'82.9')\n",
      "(u'Ramanan', u'82.9', u'70.3')\n",
      "(u'82.9', u'70.3', u'67.0')\n",
      "(u'70.3', u'67.0', u'56.0')\n",
      "(u'67.0', u'56.0', u'Pishchulin')\n",
      "(u'56.0', u'Pishchulin', u'87.5')\n",
      "(u'Pishchulin', u'87.5', u'75.7')\n",
      "(u'87.5', u'75.7', u'68.0')\n",
      "(u'75.7', u'68.0', u'54.2')\n",
      "(u'68.0', u'54.2', u'26.4')\n",
      "(u'54.2', u'26.4', u'74.9')\n",
      "(u'26.4', u'74.9', u'55.7')\n",
      "(u'74.9', u'55.7', u'35.8')\n",
      "(u'55.7', u'35.8', u'76.8')\n",
      "(u'35.8', u'76.8', u'60.7')\n",
      "(u'76.8', u'60.7', u'39.8')\n",
      "(u'60.7', u'39.8', u'79.3')\n",
      "(u'39.8', u'79.3', u'62.8')\n",
      "(u'79.3', u'62.8', u'33.9')\n",
      "(u'62.8', u'33.9', u'78.1')\n",
      "(u'33.9', u'78.1', u'62.9')\n",
      "(u'78.1', u'62.9', u'Eichner')\n",
      "(u'62.9', u'Eichner', u'Ferrari')\n",
      "(u'Eichner', u'Ferrari', u'Ours')\n",
      "(u'Ferrari', u'Ours', u'86.2')\n",
      "(u'Ours', u'86.2', u'74.3')\n",
      "(u'86.2', u'74.3', u'69.3')\n",
      "(u'74.3', u'69.3', u'56.5')\n",
      "(u'69.3', u'56.5', u'85.8')\n",
      "(u'56.5', u'85.8', u'76.5')\n",
      "(u'85.8', u'76.5', u'72.2')\n",
      "(u'76.5', u'72.2', u'63.3')\n",
      "(u'72.2', u'63.3', u'37.4')\n",
      "(u'63.3', u'37.4', u'80.1')\n",
      "(u'37.4', u'80.1', u'64.3')\n",
      "(u'80.1', u'64.3', u'46.6')\n",
      "(u'64.3', u'46.6', u'83.1')\n",
      "(u'46.6', u'83.1', u'68.6')\n",
      "(u'83.1', u'68.6', u'PARSE')\n",
      "(u'68.6', u'PARSE', u'Andriluka')\n",
      "(u'PARSE', u'Andriluka', u'86.3')\n",
      "(u'Andriluka', u'86.3', u'66.3')\n",
      "(u'86.3', u'66.3', u'60.0')\n",
      "(u'66.3', u'60.0', u'54.6')\n",
      "(u'60.0', u'54.6', u'Yang')\n",
      "(u'54.6', u'Yang', u'Ramanan')\n",
      "(u'Yang', u'Ramanan', u'83.4')\n",
      "(u'Ramanan', u'83.4', u'68.8')\n",
      "(u'83.4', u'68.8', u'60.7')\n",
      "(u'68.8', u'60.7', u'59.8')\n",
      "(u'60.7', u'59.8', u'Yang')\n",
      "(u'59.8', u'Yang', u'Ramanan')\n",
      "(u'Yang', u'Ramanan', u'82.9')\n",
      "(u'Ramanan', u'82.9', u'68.8')\n",
      "(u'82.9', u'68.8', u'60.5')\n",
      "(u'68.8', u'60.5', u'63.4')\n",
      "(u'60.5', u'63.4', u'Pishchulin')\n",
      "(u'63.4', u'Pishchulin', u'88.8')\n",
      "(u'Pishchulin', u'88.8', u'77.3')\n",
      "(u'88.8', u'77.3', u'67.1')\n",
      "(u'77.3', u'67.1', u'53.7')\n",
      "(u'67.1', u'53.7', u'Pishchulin')\n",
      "(u'53.7', u'Pishchulin', u'92.2')\n",
      "(u'Pishchulin', u'92.2', u'74.6')\n",
      "(u'92.2', u'74.6', u'63.7')\n",
      "(u'74.6', u'63.7', u'54.9')\n",
      "(u'63.7', u'54.9', u'90.7')\n",
      "(u'54.9', u'90.7', u'80.0')\n",
      "(u'90.7', u'80.0', u'70.0')\n",
      "(u'80.0', u'70.0', u'59.3')\n",
      "(u'70.0', u'59.3', u'Johnson')\n",
      "(u'59.3', u'Johnson', u'35.6')\n",
      "(u'Johnson', u'35.6', u'72.7')\n",
      "(u'35.6', u'72.7', u'59.2')\n",
      "(u'72.7', u'59.2', u'40.7')\n",
      "(u'59.2', u'40.7', u'83.4')\n",
      "(u'40.7', u'83.4', u'62.7')\n",
      "(u'83.4', u'62.7', u'42.4')\n",
      "(u'62.7', u'42.4', u'82.4')\n",
      "(u'42.4', u'82.4', u'63.6')\n",
      "(u'82.4', u'63.6', u'36.1')\n",
      "(u'63.6', u'36.1', u'73.7')\n",
      "(u'36.1', u'73.7', u'63.1')\n",
      "(u'73.7', u'63.1', u'39.8')\n",
      "(u'63.1', u'39.8', u'70.7')\n",
      "(u'39.8', u'70.7', u'62.9')\n",
      "(u'70.7', u'62.9', u'37.1')\n",
      "(u'62.9', u'37.1', u'77.6')\n",
      "(u'37.1', u'77.6', u'66.1')\n",
      "(u'77.6', u'66.1', u'Everingham')\n",
      "(u'66.1', u'Everingham', u'Ours')\n",
      "(u'Everingham', u'Ours', u'87.6')\n",
      "(u'Ours', u'87.6', u'74.7')\n",
      "(u'87.6', u'74.7', u'67.1')\n",
      "(u'74.7', u'67.1', u'67.3')\n",
      "(u'67.1', u'67.3', u'89.3')\n",
      "(u'67.3', u'89.3', u'78.0')\n",
      "(u'89.3', u'78.0', u'72.0')\n",
      "(u'78.0', u'72.0', u'67.8')\n",
      "(u'72.0', u'67.8', u'45.8')\n",
      "(u'67.8', u'45.8', u'76.8')\n",
      "(u'45.8', u'76.8', u'67.4')\n",
      "(u'76.8', u'67.4', u'47.8')\n",
      "(u'67.4', u'47.8', u'89.3')\n",
      "(u'47.8', u'89.3', u'71.0')\n",
      "(u'89.3', u'71.0', u'UIUC')\n",
      "(u'71.0', u'UIUC', u'People')\n",
      "(u'UIUC', u'People', u'Andriluka')\n",
      "(u'People', u'Andriluka', u'88.3')\n",
      "(u'Andriluka', u'88.3', u'64.0')\n",
      "(u'88.3', u'64.0', u'50.6')\n",
      "(u'64.0', u'50.6', u'42.3')\n",
      "(u'50.6', u'42.3', u'Yang')\n",
      "(u'42.3', u'Yang', u'Ramanan')\n",
      "(u'Yang', u'Ramanan', u'78.1')\n",
      "(u'Ramanan', u'78.1', u'60.9')\n",
      "(u'78.1', u'60.9', u'53.2')\n",
      "(u'60.9', u'53.2', u'41.3')\n",
      "(u'53.2', u'41.3', u'Yang')\n",
      "(u'41.3', u'Yang', u'Ramanan')\n",
      "(u'Yang', u'Ramanan', u'81.8')\n",
      "(u'Ramanan', u'81.8', u'65.0')\n",
      "(u'81.8', u'65.0', u'55.1')\n",
      "(u'65.0', u'55.1', u'46.8')\n",
      "(u'55.1', u'46.8', u'Pishchulin')\n",
      "(u'46.8', u'Pishchulin', u'91.5')\n",
      "(u'Pishchulin', u'91.5', u'66.8')\n",
      "(u'91.5', u'66.8', u'54.7')\n",
      "(u'66.8', u'54.7', u'38.3')\n",
      "(u'54.7', u'38.3', u'86.8')\n",
      "(u'38.3', u'86.8', u'56.3')\n",
      "(u'86.8', u'56.3', u'50.2')\n",
      "(u'56.3', u'50.2', u'30.8')\n",
      "(u'50.2', u'30.8', u'89.1')\n",
      "(u'30.8', u'89.1', u'72.9')\n",
      "(u'89.1', u'72.9', u'62.4')\n",
      "(u'72.9', u'62.4', u'56.3')\n",
      "(u'62.4', u'56.3', u'Wang')\n",
      "(u'56.3', u'Wang', u'Ours')\n",
      "(u'Wang', u'Ours', u'21.3')\n",
      "(u'Ours', u'21.3', u'81.8')\n",
      "(u'21.3', u'81.8', u'52.6')\n",
      "(u'81.8', u'52.6', u'32.2')\n",
      "(u'52.6', u'32.2', u'76.1')\n",
      "(u'32.2', u'76.1', u'53.0')\n",
      "(u'76.1', u'53.0', u'37.7')\n",
      "(u'53.0', u'37.7', u'79.8')\n",
      "(u'37.7', u'79.8', u'57.0')\n",
      "(u'79.8', u'57.0', u'23.9')\n",
      "(u'57.0', u'23.9', u'85.0')\n",
      "(u'23.9', u'85.0', u'54.4')\n",
      "(u'85.0', u'54.4', u'20.3')\n",
      "(u'54.4', u'20.3', u'68.8')\n",
      "(u'20.3', u'68.8', u'47.0')\n",
      "(u'68.8', u'47.0', u'47.6')\n",
      "(u'47.0', u'47.6', u'89.1')\n",
      "(u'47.6', u'89.1', u'65.6')\n",
      "(u'89.1', u'65.6', u'Table')\n",
      "(u'65.6', u'Table', u'Results')\n",
      "(u'Table', u'Results', u'PCP')\n",
      "(u'Results', u'PCP', u'investigating')\n",
      "(u'PCP', u'investigating', u'model')\n",
      "(u'investigating', u'model', u'depth')\n",
      "(u'model', u'depth', u'Method')\n",
      "(u'depth', u'Method', u'Torso')\n",
      "(u'Method', u'Torso', u'U.leg')\n",
      "(u'Torso', u'U.leg', u'L.leg')\n",
      "(u'U.leg', u'L.leg', u'U.arm')\n",
      "(u'L.leg', u'U.arm', u'L.arm')\n",
      "(u'U.arm', u'L.arm', u'Head')\n",
      "(u'L.arm', u'Head', u'Total')\n",
      "(u'Head', u'Total', u'LSP')\n",
      "(u'Total', u'LSP', u'82.9')\n",
      "(u'LSP', u'82.9', u'70.3')\n",
      "(u'82.9', u'70.3', u'67.0')\n",
      "(u'70.3', u'67.0', u'Kernel')\n",
      "(u'67.0', u'Kernel', u'SVM')\n",
      "(u'Kernel', u'SVM', u'81.9')\n",
      "(u'SVM', u'81.9', u'72.2')\n",
      "(u'81.9', u'72.2', u'67.6')\n",
      "(u'72.2', u'67.6', u'hidden')\n",
      "(u'67.6', u'hidden', u'layer')\n",
      "(u'hidden', u'layer', u'84.9')\n",
      "(u'layer', u'84.9', u'73.9')\n",
      "(u'84.9', u'73.9', u'69.5')\n",
      "(u'73.9', u'69.5', u'hidden')\n",
      "(u'69.5', u'hidden', u'layers')\n",
      "(u'hidden', u'layers', u'85.0')\n",
      "(u'layers', u'85.0', u'74.6')\n",
      "(u'85.0', u'74.6', u'70.7')\n",
      "(u'74.6', u'70.7', u'Ours')\n",
      "(u'70.7', u'Ours', u'85.8')\n",
      "(u'Ours', u'85.8', u'76.5')\n",
      "(u'85.8', u'76.5', u'72.2')\n",
      "(u'76.5', u'72.2', u'PARSE')\n",
      "(u'72.2', u'PARSE', u'82.9')\n",
      "(u'PARSE', u'82.9', u'68.8')\n",
      "(u'82.9', u'68.8', u'60.5')\n",
      "(u'68.8', u'60.5', u'Kernel')\n",
      "(u'60.5', u'Kernel', u'SVM')\n",
      "(u'Kernel', u'SVM', u'81.0')\n",
      "(u'SVM', u'81.0', u'67.8')\n",
      "(u'81.0', u'67.8', u'61.2')\n",
      "(u'67.8', u'61.2', u'hidden')\n",
      "(u'61.2', u'hidden', u'layer')\n",
      "(u'hidden', u'layer', u'84.4')\n",
      "(u'layer', u'84.4', u'71.2')\n",
      "(u'84.4', u'71.2', u'63.2')\n",
      "(u'71.2', u'63.2', u'hidden')\n",
      "(u'63.2', u'hidden', u'layers')\n",
      "(u'hidden', u'layers', u'85.9')\n",
      "(u'layers', u'85.9', u'74.4')\n",
      "(u'85.9', u'74.4', u'68.3')\n",
      "(u'74.4', u'68.3', u'Ours')\n",
      "(u'68.3', u'Ours', u'89.3')\n",
      "(u'Ours', u'89.3', u'78.0')\n",
      "(u'89.3', u'78.0', u'72.0')\n",
      "(u'78.0', u'72.0', u'UIUC')\n",
      "(u'72.0', u'UIUC', u'81.8')\n",
      "(u'UIUC', u'81.8', u'65.0')\n",
      "(u'81.8', u'65.0', u'55.1')\n",
      "(u'65.0', u'55.1', u'Kernel')\n",
      "(u'55.1', u'Kernel', u'SVM')\n",
      "(u'Kernel', u'SVM', u'82.2')\n",
      "(u'SVM', u'82.2', u'65.0')\n",
      "(u'82.2', u'65.0', u'54.9')\n",
      "(u'65.0', u'54.9', u'hidden')\n",
      "(u'54.9', u'hidden', u'layer')\n",
      "(u'hidden', u'layer', u'83.0')\n",
      "(u'layer', u'83.0', u'65.6')\n",
      "(u'83.0', u'65.6', u'55.9')\n",
      "(u'65.6', u'55.9', u'hidden')\n",
      "(u'55.9', u'hidden', u'layers')\n",
      "(u'hidden', u'layers', u'84.2')\n",
      "(u'layers', u'84.2', u'68.4')\n",
      "(u'84.2', u'68.4', u'59.3')\n",
      "(u'68.4', u'59.3', u'Ours')\n",
      "(u'59.3', u'Ours', u'89.1')\n",
      "(u'Ours', u'89.1', u'72.9')\n",
      "(u'89.1', u'72.9', u'62.3')\n",
      "(u'72.9', u'62.3', u'58.8')\n",
      "(u'62.3', u'58.8', u'57.5')\n",
      "(u'58.8', u'57.5', u'61.2')\n",
      "(u'57.5', u'61.2', u'63.3')\n",
      "(u'61.2', u'63.3', u'63.4')\n",
      "(u'63.3', u'63.4', u'63.2')\n",
      "(u'63.4', u'63.2', u'62.4')\n",
      "(u'63.2', u'62.4', u'64.6')\n",
      "(u'62.4', u'64.6', u'67.8')\n",
      "(u'64.6', u'67.8', u'46.8')\n",
      "(u'67.8', u'46.8', u'50.2')\n",
      "(u'46.8', u'50.2', u'50.6')\n",
      "(u'50.2', u'50.6', u'53.0')\n",
      "(u'50.6', u'53.0', u'56.3')\n",
      "(u'53.0', u'56.3', u'39.8')\n",
      "(u'56.3', u'39.8', u'79.3')\n",
      "(u'39.8', u'79.3', u'62.8')\n",
      "(u'79.3', u'62.8', u'42.8')\n",
      "(u'62.8', u'42.8', u'77.5')\n",
      "(u'42.8', u'77.5', u'64.2')\n",
      "(u'77.5', u'64.2', u'42.9')\n",
      "(u'64.2', u'42.9', u'50.7')\n",
      "(u'42.9', u'50.7', u'62.3')\n",
      "(u'50.7', u'62.3', u'45.2')\n",
      "(u'62.3', u'45.2', u'82.2')\n",
      "(u'45.2', u'82.2', u'67.1')\n",
      "(u'82.2', u'67.1', u'46.6')\n",
      "(u'67.1', u'46.6', u'83.1')\n",
      "(u'46.6', u'83.1', u'68.6')\n",
      "(u'83.1', u'68.6', u'42.4')\n",
      "(u'68.6', u'42.4', u'82.4')\n",
      "(u'42.4', u'82.4', u'63.6')\n",
      "(u'82.4', u'63.6', u'44.1')\n",
      "(u'63.6', u'44.1', u'78.0')\n",
      "(u'44.1', u'78.0', u'63.2')\n",
      "(u'78.0', u'63.2', u'44.4')\n",
      "(u'63.2', u'44.4', u'70.2')\n",
      "(u'44.4', u'70.2', u'63.7')\n",
      "(u'70.2', u'63.7', u'46.3')\n",
      "(u'63.7', u'46.3', u'85.4')\n",
      "(u'46.3', u'85.4', u'67.9')\n",
      "(u'85.4', u'67.9', u'47.8')\n",
      "(u'67.9', u'47.8', u'89.3')\n",
      "(u'47.8', u'89.3', u'71.0')\n",
      "(u'89.3', u'71.0', u'37.7')\n",
      "(u'71.0', u'37.7', u'79.8')\n",
      "(u'37.7', u'79.8', u'57.0')\n",
      "(u'79.8', u'57.0', u'43.1')\n",
      "(u'57.0', u'43.1', u'80.6')\n",
      "(u'43.1', u'80.6', u'58.9')\n",
      "(u'80.6', u'58.9', u'42.3')\n",
      "(u'58.9', u'42.3', u'79.8')\n",
      "(u'42.3', u'79.8', u'59.2')\n",
      "(u'79.8', u'59.2', u'45.3')\n",
      "(u'59.2', u'45.3', u'83.4')\n",
      "(u'45.3', u'83.4', u'62.0')\n",
      "(u'83.4', u'62.0', u'47.6')\n",
      "(u'62.0', u'47.6', u'89.1')\n",
      "(u'47.6', u'89.1', u'65.6')\n",
      "(u'89.1', u'65.6', u'proved')\n",
      "(u'65.6', u'proved', u'that')\n",
      "(u'proved', u'that', u'linear-SVM')\n",
      "(u'that', u'linear-SVM', u'and')\n",
      "(u'linear-SVM', u'and', u'kernel-SVM')\n",
      "(u'and', u'kernel-SVM', u'are')\n",
      "(u'kernel-SVM', u'are', u'shallow')\n",
      "(u'are', u'shallow', u'mod-')\n",
      "(u'shallow', u'mod-', u'els')\n",
      "(u'mod-', u'els', u'With')\n",
      "(u'els', u'With', u'the')\n",
      "(u'With', u'the', u'deep')\n",
      "(u'the', u'deep', u'model')\n",
      "(u'deep', u'model', u'our')\n",
      "(u'model', u'our', u'approach')\n",
      "(u'our', u'approach', u'performs')\n",
      "(u'approach', u'performs', u'better')\n",
      "(u'performs', u'better', u'the')\n",
      "(u'better', u'the', u'number')\n",
      "(u'the', u'number', u'hidden')\n",
      "(u'number', u'hidden', u'layers')\n",
      "(u'hidden', u'layers', u'increases')\n",
      "(u'layers', u'increases', u'from')\n",
      "(u'increases', u'from', u'hidden')\n",
      "(u'from', u'hidden', u'layer')\n",
      "(u'hidden', u'layer', u'hidden')\n",
      "(u'layer', u'hidden', u'layers')\n",
      "(u'hidden', u'layers', u'the')\n",
      "(u'layers', u'the', u'estimation')\n",
      "(u'the', u'estimation', u'accuracy')\n",
      "(u'estimation', u'accuracy', u'increases')\n",
      "(u'accuracy', u'increases', u'from')\n",
      "(u'increases', u'from', u'62.3')\n",
      "(u'from', u'62.3', u'67.1')\n",
      "(u'62.3', u'67.1', u'With')\n",
      "(u'67.1', u'With', u'PCP')\n",
      "(u'With', u'PCP', u'68.6')\n",
      "(u'PCP', u'68.6', u'our')\n",
      "(u'68.6', u'our', u'\\ufb01nal')\n",
      "(u'our', u'\\ufb01nal', u'model')\n",
      "(u'\\ufb01nal', u'model', u'Fig')\n",
      "(u'model', u'Fig', u'uses')\n",
      "(u'Fig', u'uses', u'three')\n",
      "(u'uses', u'three', u'hidden')\n",
      "(u'three', u'hidden', u'layers')\n",
      "(u'hidden', u'layers', u'and')\n",
      "(u'layers', u'and', u'better')\n",
      "(u'and', u'better', u'than')\n",
      "(u'better', u'than', u'SVM')\n",
      "(u'than', u'SVM', u'and')\n",
      "(u'SVM', u'and', u'deep')\n",
      "(u'and', u'deep', u'models')\n",
      "(u'deep', u'models', u'with')\n",
      "(u'models', u'with', u'fewer')\n",
      "(u'with', u'fewer', u'layers')\n",
      "(u'fewer', u'layers', u'Table')\n",
      "(u'layers', u'Table', u'Results')\n",
      "(u'Table', u'Results', u'PCP')\n",
      "(u'Results', u'PCP', u'investigating')\n",
      "(u'PCP', u'investigating', u'deep')\n",
      "(u'investigating', u'deep', u'model')\n",
      "(u'deep', u'model', u'structures')\n",
      "(u'model', u'structures', u'Method')\n",
      "(u'structures', u'Method', u'Torso')\n",
      "(u'Method', u'Torso', u'U.leg')\n",
      "(u'Torso', u'U.leg', u'L.leg')\n",
      "(u'U.leg', u'L.leg', u'U.arm')\n",
      "(u'L.leg', u'U.arm', u'L.arm')\n",
      "(u'U.arm', u'L.arm', u'Head')\n",
      "(u'L.arm', u'Head', u'Total')\n",
      "(u'Head', u'Total', u'LSP')\n",
      "(u'Total', u'LSP', u'DBN')\n",
      "(u'LSP', u'DBN', u'Fig')\n",
      "(u'DBN', u'Fig', u'82.9')\n",
      "(u'Fig', u'82.9', u'73.2')\n",
      "(u'82.9', u'73.2', u'69.5')\n",
      "(u'73.2', u'69.5', u'Ours')\n",
      "(u'69.5', u'Ours', u'85.8')\n",
      "(u'Ours', u'85.8', u'76.5')\n",
      "(u'85.8', u'76.5', u'72.2')\n",
      "(u'76.5', u'72.2', u'PARSE')\n",
      "(u'72.2', u'PARSE', u'DBN')\n",
      "(u'PARSE', u'DBN', u'Fig')\n",
      "(u'DBN', u'Fig', u'82.0')\n",
      "(u'Fig', u'82.0', u'70.0')\n",
      "(u'82.0', u'70.0', u'64.6')\n",
      "(u'70.0', u'64.6', u'Ours')\n",
      "(u'64.6', u'Ours', u'89.3')\n",
      "(u'Ours', u'89.3', u'78.0')\n",
      "(u'89.3', u'78.0', u'72.0')\n",
      "(u'78.0', u'72.0', u'DBN')\n",
      "(u'72.0', u'DBN', u'Fig')\n",
      "(u'DBN', u'Fig', u'87.4')\n",
      "(u'Fig', u'87.4', u'68.4')\n",
      "(u'87.4', u'68.4', u'58.3')\n",
      "(u'68.4', u'58.3', u'Ours')\n",
      "(u'58.3', u'Ours', u'89.1')\n",
      "(u'Ours', u'89.1', u'72.9')\n",
      "(u'89.1', u'72.9', u'62.3')\n",
      "(u'72.9', u'62.3', u'UIUC')\n",
      "(u'62.3', u'UIUC', u'59.8')\n",
      "(u'UIUC', u'59.8', u'63.3')\n",
      "(u'59.8', u'63.3', u'62.9')\n",
      "(u'63.3', u'62.9', u'67.8')\n",
      "(u'62.9', u'67.8', u'52.2')\n",
      "(u'67.8', u'52.2', u'56.3')\n",
      "(u'52.2', u'56.3', u'43.8')\n",
      "(u'56.3', u'43.8', u'79.2')\n",
      "(u'43.8', u'79.2', u'65.5')\n",
      "(u'79.2', u'65.5', u'46.6')\n",
      "(u'65.5', u'46.6', u'83.1')\n",
      "(u'46.6', u'83.1', u'68.6')\n",
      "(u'83.1', u'68.6', u'46.3')\n",
      "(u'68.6', u'46.3', u'80.5')\n",
      "(u'46.3', u'80.5', u'65.0')\n",
      "(u'80.5', u'65.0', u'47.8')\n",
      "(u'65.0', u'47.8', u'89.3')\n",
      "(u'47.8', u'89.3', u'71.0')\n",
      "(u'89.3', u'71.0', u'44.3')\n",
      "(u'71.0', u'44.3', u'84.6')\n",
      "(u'44.3', u'84.6', u'61.8')\n",
      "(u'84.6', u'61.8', u'47.6')\n",
      "(u'61.8', u'47.6', u'89.1')\n",
      "(u'47.6', u'89.1', u'65.6')\n",
      "(u'89.1', u'65.6', u'Deep')\n",
      "(u'65.6', u'Deep', u'model')\n",
      "(u'Deep', u'model', u'structure')\n",
      "(u'model', u'structure', u'design')\n",
      "(u'structure', u'design', u'investigated')\n",
      "(u'design', u'investigated', u'Table')\n",
      "(u'investigated', u'Table', u'The')\n",
      "(u'Table', u'The', u'DBN')\n",
      "(u'The', u'DBN', u'Fig')\n",
      "(u'DBN', u'Fig', u'trains')\n",
      "(u'Fig', u'trains', u'three-layer')\n",
      "(u'trains', u'three-layer', u'deep')\n",
      "(u'three-layer', u'deep', u'model')\n",
      "(u'deep', u'model', u'over')\n",
      "(u'model', u'over', u'the')\n",
      "(u'over', u'the', u'concatenated')\n",
      "(u'the', u'concatenated', u'informations')\n",
      "(u'concatenated', u'informations', u'with')\n",
      "(u'informations', u'with', u'three')\n",
      "(u'with', u'three', u'hidden')\n",
      "(u'three', u'hidden', u'layers')\n",
      "(u'hidden', u'layers', u'The')\n",
      "(u'layers', u'The', u'model')\n",
      "(u'The', u'model', u'learns')\n",
      "(u'model', u'learns', u'high-order')\n",
      "(u'learns', u'high-order', u'representations')\n",
      "(u'high-order', u'representations', u'individu-')\n",
      "(u'representations', u'individu-', u'ally')\n",
      "(u'individu-', u'ally', u'The')\n",
      "(u'ally', u'The', u'model')\n",
      "(u'The', u'model', u'with')\n",
      "(u'model', u'with', u'PCP')\n",
      "(u'with', u'PCP', u'68.6')\n",
      "(u'PCP', u'68.6', u'better')\n",
      "(u'68.6', u'better', u'con-')\n",
      "(u'better', u'con-', u'structing')\n",
      "(u'con-', u'structing', u'the')\n",
      "(u'structing', u'the', u'high-order')\n",
      "(u'the', u'high-order', u'representations')\n",
      "(u'high-order', u'representations', u'and')\n",
      "(u'representations', u'and', u'therefore')\n",
      "(u'and', u'therefore', u'has')\n",
      "(u'therefore', u'has', u'higher')\n",
      "(u'has', u'higher', u'estimation')\n",
      "(u'higher', u'estimation', u'accuracy')\n",
      "(u'estimation', u'accuracy', u'compared')\n",
      "(u'accuracy', u'compared', u'with')\n",
      "(u'compared', u'with', u'the')\n",
      "(u'with', u'the', u'DBN')\n",
      "(u'the', u'DBN', u'Fig')\n",
      "(u'DBN', u'Fig', u'with')\n",
      "(u'Fig', u'with', u'PCP')\n",
      "(u'with', u'PCP', u'65.5')\n",
      "(u'PCP', u'65.5', u'Classi\\ufb01cation')\n",
      "(u'65.5', u'Classi\\ufb01cation', u'label')\n",
      "(u'Classi\\ufb01cation', u'label', u'and')\n",
      "(u'label', u'and', u'location')\n",
      "(u'and', u'location', u'learning')\n",
      "(u'location', u'learning', u'investigated')\n",
      "(u'learning', u'investigated', u'Table')\n",
      "(u'investigated', u'Table', u'There')\n",
      "(u'Table', u'There', u'are')\n",
      "(u'There', u'are', u'two')\n",
      "(u'are', u'two', u'sets')\n",
      "(u'two', u'sets', u'labels')\n",
      "(u'sets', u'labels', u'estimated')\n",
      "(u'labels', u'estimated', u'our')\n",
      "(u'estimated', u'our', u'deep')\n",
      "(u'our', u'deep', u'model')\n",
      "(u'deep', u'model', u'classi\\ufb01cation')\n",
      "(u'model', u'classi\\ufb01cation', u'label')\n",
      "(u'classi\\ufb01cation', u'label', u'ycls')\n",
      "(u'label', u'ycls', u'and')\n",
      "(u'ycls', u'and', u'part')\n",
      "(u'and', u'part', u'posi-')\n",
      "(u'part', u'posi-', u'tions')\n",
      "(u'posi-', u'tions', u'ypst')\n",
      "(u'tions', u'ypst', u'the')\n",
      "(u'ypst', u'the', u'experiments')\n",
      "(u'the', u'experiments', u'evaluate')\n",
      "(u'experiments', u'evaluate', u'different')\n",
      "(u'evaluate', u'different', u'ways')\n",
      "(u'different', u'ways', u'estimating')\n",
      "(u'ways', u'estimating', u'these')\n",
      "(u'estimating', u'these', u'labels')\n",
      "(u'these', u'labels', u'The')\n",
      "(u'labels', u'The', u'Only')\n",
      "(u'The', u'Only', u'ycls')\n",
      "(u'Only', u'ycls', u'Table')\n",
      "(u'ycls', u'Table', u'with')\n",
      "(u'Table', u'with', u'PCP')\n",
      "(u'with', u'PCP', u'63.7')\n",
      "(u'PCP', u'63.7', u'only')\n",
      "(u'63.7', u'only', u'estimates')\n",
      "(u'only', u'estimates', u'class')\n",
      "(u'estimates', u'class', u'label')\n",
      "(u'class', u'label', u'with')\n",
      "(u'label', u'with', u'part')\n",
      "(u'with', u'part', u'location')\n",
      "(u'part', u'location', u'directly')\n",
      "(u'location', u'directly', u'obtained')\n",
      "(u'directly', u'obtained', u'the')\n",
      "(u'obtained', u'the', u'approach')\n",
      "(u'the', u'approach', u'The')\n",
      "(u'approach', u'The', u'Only')\n",
      "(u'The', u'Only', u'ypst')\n",
      "(u'Only', u'ypst', u'with')\n",
      "(u'ypst', u'with', u'PCP')\n",
      "(u'with', u'PCP', u'64.1')\n",
      "(u'PCP', u'64.1', u'only')\n",
      "(u'64.1', u'only', u're\\ufb01nes')\n",
      "(u'only', u're\\ufb01nes', u'the')\n",
      "(u're\\ufb01nes', u'the', u'part')\n",
      "(u'the', u'part', u'location')\n",
      "(u'part', u'location', u'with')\n",
      "(u'location', u'with', u'class')\n",
      "(u'with', u'class', u'label')\n",
      "(u'class', u'label', u'directly')\n",
      "(u'label', u'directly', u'obtained')\n",
      "(u'directly', u'obtained', u'the')\n",
      "(u'obtained', u'the', u'approach')\n",
      "(u'the', u'approach', u'Separate')\n",
      "(u'approach', u'Separate', u'ycls+ypst')\n",
      "(u'Separate', u'ycls+ypst', u'with')\n",
      "(u'ycls+ypst', u'with', u'PCP')\n",
      "(u'with', u'PCP', u'64.7')\n",
      "(u'PCP', u'64.7', u'uses')\n",
      "(u'64.7', u'uses', u'two')\n",
      "(u'uses', u'two', u'deep')\n",
      "(u'two', u'deep', u'models')\n",
      "(u'deep', u'models', u'for')\n",
      "(u'models', u'for', u'esti-')\n",
      "(u'for', u'esti-', u'mating')\n",
      "(u'esti-', u'mating', u'ycls')\n",
      "(u'mating', u'ycls', u'and')\n",
      "(u'ycls', u'and', u'ypst')\n",
      "(u'and', u'ypst', u'separately')\n",
      "(u'ypst', u'separately', u'can')\n",
      "(u'separately', u'can', u'seen')\n",
      "(u'can', u'seen', u'that')\n",
      "(u'seen', u'that', u'both')\n",
      "(u'that', u'both', u'ycls')\n",
      "(u'both', u'ycls', u'and')\n",
      "(u'ycls', u'and', u'ypst')\n",
      "(u'and', u'ypst', u'are')\n",
      "(u'ypst', u'are', u'helpful')\n",
      "(u'are', u'helpful', u'for')\n",
      "(u'helpful', u'for', u'improving')\n",
      "(u'for', u'improving', u'accuracy')\n",
      "(u'improving', u'accuracy', u'Our')\n",
      "(u'accuracy', u'Our', u'model')\n",
      "(u'Our', u'model', u'uses')\n",
      "(u'model', u'uses', u'the')\n",
      "(u'uses', u'the', u'single')\n",
      "(u'the', u'single', u'deep')\n",
      "(u'single', u'deep', u'model')\n",
      "(u'deep', u'model', u'jointly')\n",
      "(u'model', u'jointly', u'learn')\n",
      "(u'jointly', u'learn', u'both')\n",
      "(u'learn', u'both', u'ycls')\n",
      "(u'both', u'ycls', u'and')\n",
      "(u'ycls', u'and', u'ypst')\n",
      "(u'and', u'ypst', u'PCP')\n",
      "(u'ypst', u'PCP', u'68.6')\n",
      "(u'PCP', u'68.6', u'and')\n",
      "(u'68.6', u'and', u'performs')\n",
      "(u'and', u'performs', u'better')\n",
      "(u'performs', u'better', u'than')\n",
      "(u'better', u'than', u'using')\n",
      "(u'than', u'using', u'two')\n",
      "(u'using', u'two', u'mod-')\n",
      "(u'two', u'mod-', u'els')\n",
      "(u'mod-', u'els', u'learn')\n",
      "(u'els', u'learn', u'them')\n",
      "(u'learn', u'them', u'separately')\n",
      "(u'them', u'separately', u'PCP')\n",
      "(u'separately', u'PCP', u'64.7')\n",
      "(u'PCP', u'64.7', u'because')\n",
      "(u'64.7', u'because', u'body')\n",
      "(u'because', u'body', u'lo-')\n",
      "(u'body', u'lo-', u'cation')\n",
      "(u'lo-', u'cation', u'and')\n",
      "(u'cation', u'and', u'the')\n",
      "(u'and', u'the', u'correctness')\n",
      "(u'the', u'correctness', u'candidate')\n",
      "(u'correctness', u'candidate', u'body')\n",
      "(u'candidate', u'body', u'location')\n",
      "(u'body', u'location', u'are')\n",
      "(u'location', u'are', u'dependent')\n",
      "(u'are', u'dependent', u'Analysis')\n",
      "(u'dependent', u'Analysis', u'Our')\n",
      "(u'Analysis', u'Our', u'model')\n",
      "(u'Our', u'model', u'extracts')\n",
      "(u'model', u'extracts', u'high-order')\n",
      "(u'extracts', u'high-order', u'representations')\n",
      "(u'high-order', u'representations', u'appearance')\n",
      "(u'representations', u'appearance', u'deformation')\n",
      "(u'appearance', u'deformation', u'and')\n",
      "(u'deformation', u'and', u'mixture')\n",
      "(u'and', u'mixture', u'types')\n",
      "(u'mixture', u'types', u'and')\n",
      "(u'types', u'and', u'better')\n",
      "(u'and', u'better', u'models')\n",
      "(u'better', u'models', u'their')\n",
      "(u'models', u'their', u'dependence')\n",
      "(u'their', u'dependence', u'the')\n",
      "(u'dependence', u'the', u'top')\n",
      "(u'the', u'top', u'layer')\n",
      "(u'top', u'layer', u'For')\n",
      "(u'layer', u'For', u'example')\n",
      "(u'For', u'example', u'the')\n",
      "(u'example', u'the', u'mixture')\n",
      "(u'the', u'mixture', u'types')\n",
      "(u'mixture', u'types', u'are')\n",
      "(u'types', u'are', u'upright')\n",
      "(u'are', u'upright', u'upper-')\n",
      "(u'upright', u'upper-', u'and')\n",
      "(u'upper-', u'and', u'lower-arms')\n",
      "(u'and', u'lower-arms', u'the')\n",
      "(u'lower-arms', u'the', u'weighted')\n",
      "(u'the', u'weighted', u'combination')\n",
      "(u'weighted', u'combination', u'the')\n",
      "(u'combination', u'the', u'locations')\n",
      "(u'the', u'locations', u'wrist')\n",
      "(u'locations', u'wrist', u'and')\n",
      "(u'wrist', u'and', u'shoul-')\n",
      "(u'and', u'shoul-', u'der')\n",
      "(u'shoul-', u'der', u'good')\n",
      "(u'der', u'good', u'estimation')\n",
      "(u'good', u'estimation', u'the')\n",
      "(u'estimation', u'the', u'location')\n",
      "(u'the', u'location', u'elbow')\n",
      "(u'location', u'elbow', u'the')\n",
      "(u'elbow', u'the', u'mixture')\n",
      "(u'the', u'mixture', u'types')\n",
      "(u'mixture', u'types', u'change')\n",
      "(u'types', u'change', u'such')\n",
      "(u'change', u'such', u'estimation')\n",
      "(u'such', u'estimation', u'should')\n",
      "(u'estimation', u'should', u'change')\n",
      "(u'should', u'change', u'cor-')\n",
      "(u'change', u'cor-', u'respondingly')\n",
      "(u'cor-', u'respondingly', u'Such')\n",
      "(u'respondingly', u'Such', u'complex')\n",
      "(u'Such', u'complex', u'dependence')\n",
      "(u'complex', u'dependence', u'can')\n",
      "(u'dependence', u'can', u'not')\n",
      "(u'can', u'not', u'mod-')\n",
      "(u'not', u'mod-', u'eled')\n",
      "(u'mod-', u'eled', u'linearly')\n",
      "(u'eled', u'linearly', u'and')\n",
      "(u'linearly', u'and', u'deep')\n",
      "(u'and', u'deep', u'model')\n",
      "(u'deep', u'model', u'better')\n",
      "(u'model', u'better', u'solution')\n",
      "(u'better', u'solution', u'When')\n",
      "(u'solution', u'When', u'different')\n",
      "(u'When', u'different', u'information')\n",
      "(u'different', u'information', u'sources')\n",
      "(u'information', u'sources', u'are')\n",
      "(u'sources', u'are', u'extracted')\n",
      "(u'are', u'extracted', u'separately')\n",
      "(u'extracted', u'separately', u'with')\n",
      "(u'separately', u'with', u'the')\n",
      "(u'with', u'the', u'\\ufb01rst')\n",
      "(u'the', u'\\ufb01rst', u'several')\n",
      "(u'\\ufb01rst', u'several', u'layers')\n",
      "(u'several', u'layers', u'the')\n",
      "(u'layers', u'the', u'connections')\n",
      "(u'the', u'connections', u'across')\n",
      "(u'connections', u'across', u'sources')\n",
      "(u'across', u'sources', u'are')\n",
      "(u'sources', u'are', u'removed')\n",
      "(u'are', u'removed', u'and')\n",
      "(u'removed', u'and', u'the')\n",
      "(u'and', u'the', u'number')\n",
      "(u'the', u'number', u'parameters')\n",
      "(u'number', u'parameters', u'reduced')\n",
      "(u'parameters', u'reduced', u'helps')\n",
      "(u'reduced', u'helps', u'regularize')\n",
      "(u'helps', u'regularize', u'optimization')\n",
      "(u'regularize', u'optimization', u'when')\n",
      "(u'optimization', u'when', u'training')\n",
      "(u'when', u'training', u'samples')\n",
      "(u'training', u'samples', u'are')\n",
      "(u'samples', u'are', u'lim-')\n",
      "(u'are', u'lim-', u'ited')\n",
      "(u'lim-', u'ited', u'Existing')\n",
      "(u'ited', u'Existing', u'methods')\n",
      "(u'Existing', u'methods', u'only')\n",
      "(u'methods', u'only', u'use')\n",
      "(u'only', u'use', u'ycls')\n",
      "(u'use', u'ycls', u'for')\n",
      "(u'ycls', u'for', u'supervision')\n",
      "(u'for', u'supervision', u'while')\n",
      "(u'supervision', u'while', u'use')\n",
      "(u'while', u'use', u'both')\n",
      "(u'use', u'both', u'ypts')\n",
      "(u'both', u'ypts', u'and')\n",
      "(u'ypts', u'and', u'ycls')\n",
      "(u'and', u'ycls', u'shown')\n",
      "(u'ycls', u'shown', u'Fig')\n",
      "(u'shown', u'Fig', u're\\ufb01ning')\n",
      "(u'Fig', u're\\ufb01ning', u'ypts')\n",
      "(u're\\ufb01ning', u'ypts', u'does')\n",
      "(u'ypts', u'does', u'help')\n",
      "(u'does', u'help', u'rectify')\n",
      "(u'help', u'rectify', u'incorrect')\n",
      "(u'rectify', u'incorrect', u'part')\n",
      "(u'incorrect', u'part', u'locations')\n",
      "(u'part', u'locations', u'based')\n",
      "(u'locations', u'based', u'the')\n",
      "(u'based', u'the', u'high')\n",
      "(u'the', u'high', u'order')\n",
      "(u'high', u'order', u'prior')\n",
      "(u'order', u'prior', u'model')\n",
      "(u'prior', u'model', u'body')\n",
      "(u'model', u'body', u'pose')\n",
      "(u'body', u'pose', u'Jointly')\n",
      "(u'pose', u'Jointly', u'learning')\n",
      "(u'Jointly', u'learning', u'Table')\n",
      "(u'learning', u'Table', u'PCP')\n",
      "(u'Table', u'PCP', u'results')\n",
      "(u'PCP', u'results', u'classi\\ufb01cation')\n",
      "(u'results', u'classi\\ufb01cation', u'label')\n",
      "(u'classi\\ufb01cation', u'label', u'and')\n",
      "(u'label', u'and', u'location')\n",
      "(u'and', u'location', u'learning')\n",
      "(u'location', u'learning', u'Method')\n",
      "(u'learning', u'Method', u'Torso')\n",
      "(u'Method', u'Torso', u'U.leg')\n",
      "(u'Torso', u'U.leg', u'L.leg')\n",
      "(u'U.leg', u'L.leg', u'U.arm')\n",
      "(u'L.leg', u'U.arm', u'L.arm')\n",
      "(u'U.arm', u'L.arm', u'Head')\n",
      "(u'L.arm', u'Head', u'Total')\n",
      "(u'Head', u'Total', u'LSP')\n",
      "(u'Total', u'LSP', u'82.9')\n",
      "(u'LSP', u'82.9', u'70.3')\n",
      "(u'82.9', u'70.3', u'67.0')\n",
      "(u'70.3', u'67.0', u'Only')\n",
      "(u'67.0', u'Only', u'ycls')\n",
      "(u'Only', u'ycls', u'82.0')\n",
      "(u'ycls', u'82.0', u'71.5')\n",
      "(u'82.0', u'71.5', u'68.0')\n",
      "(u'71.5', u'68.0', u'Only')\n",
      "(u'68.0', u'Only', u'ypst')\n",
      "(u'Only', u'ypst', u'80.4')\n",
      "(u'ypst', u'80.4', u'72.0')\n",
      "(u'80.4', u'72.0', u'68.0')\n",
      "(u'72.0', u'68.0', u'Separate')\n",
      "(u'68.0', u'Separate', u'ycls+ypst')\n",
      "(u'Separate', u'ycls+ypst', u'81.1')\n",
      "(u'ycls+ypst', u'81.1', u'72.8')\n",
      "(u'81.1', u'72.8', u'69.0')\n",
      "(u'72.8', u'69.0', u'Ours')\n",
      "(u'69.0', u'Ours', u'85.8')\n",
      "(u'Ours', u'85.8', u'76.5')\n",
      "(u'85.8', u'76.5', u'72.2')\n",
      "(u'76.5', u'72.2', u'PARSE')\n",
      "(u'72.2', u'PARSE', u'82.9')\n",
      "(u'PARSE', u'82.9', u'68.8')\n",
      "(u'82.9', u'68.8', u'60.5')\n",
      "(u'68.8', u'60.5', u'Only')\n",
      "(u'60.5', u'Only', u'ycls')\n",
      "(u'Only', u'ycls', u'81.0')\n",
      "(u'ycls', u'81.0', u'69.8')\n",
      "(u'81.0', u'69.8', u'66.1')\n",
      "(u'69.8', u'66.1', u'Only')\n",
      "(u'66.1', u'Only', u'ypst')\n",
      "(u'Only', u'ypst', u'80.5')\n",
      "(u'ypst', u'80.5', u'71.2')\n",
      "(u'80.5', u'71.2', u'65.4')\n",
      "(u'71.2', u'65.4', u'Separate')\n",
      "(u'65.4', u'Separate', u'ycls+ypst')\n",
      "(u'Separate', u'ycls+ypst', u'83.4')\n",
      "(u'ycls+ypst', u'83.4', u'73.7')\n",
      "(u'83.4', u'73.7', u'67.6')\n",
      "(u'73.7', u'67.6', u'89.3')\n",
      "(u'67.6', u'89.3', u'78.0')\n",
      "(u'89.3', u'78.0', u'72.0')\n",
      "(u'78.0', u'72.0', u'Ours')\n",
      "(u'72.0', u'Ours', u'UIUC')\n",
      "(u'Ours', u'UIUC', u'81.8')\n",
      "(u'UIUC', u'81.8', u'65.0')\n",
      "(u'81.8', u'65.0', u'55.1')\n",
      "(u'65.0', u'55.1', u'Only')\n",
      "(u'55.1', u'Only', u'ycls')\n",
      "(u'Only', u'ycls', u'85.4')\n",
      "(u'ycls', u'85.4', u'68.8')\n",
      "(u'85.4', u'68.8', u'59.3')\n",
      "(u'68.8', u'59.3', u'Only')\n",
      "(u'59.3', u'Only', u'ypst')\n",
      "(u'Only', u'ypst', u'82.6')\n",
      "(u'ypst', u'82.6', u'66.6')\n",
      "(u'82.6', u'66.6', u'58.3')\n",
      "(u'66.6', u'58.3', u'Separate')\n",
      "(u'58.3', u'Separate', u'ycls+ypst')\n",
      "(u'Separate', u'ycls+ypst', u'87.9')\n",
      "(u'ycls+ypst', u'87.9', u'69.6')\n",
      "(u'87.9', u'69.6', u'60.3')\n",
      "(u'69.6', u'60.3', u'Ours')\n",
      "(u'60.3', u'Ours', u'89.1')\n",
      "(u'Ours', u'89.1', u'72.9')\n",
      "(u'89.1', u'72.9', u'62.3')\n",
      "(u'72.9', u'62.3', u'56.0')\n",
      "(u'62.3', u'56.0', u'57.6')\n",
      "(u'56.0', u'57.6', u'59.2')\n",
      "(u'57.6', u'59.2', u'59.5')\n",
      "(u'59.2', u'59.5', u'63.3')\n",
      "(u'59.5', u'63.3', u'63.4')\n",
      "(u'63.3', u'63.4', u'60.5')\n",
      "(u'63.4', u'60.5', u'62.2')\n",
      "(u'60.5', u'62.2', u'64.4')\n",
      "(u'62.2', u'64.4', u'67.8')\n",
      "(u'64.4', u'67.8', u'46.8')\n",
      "(u'67.8', u'46.8', u'49.2')\n",
      "(u'46.8', u'49.2', u'52.2')\n",
      "(u'49.2', u'52.2', u'53.0')\n",
      "(u'52.2', u'53.0', u'56.3')\n",
      "(u'53.0', u'56.3', u'39.8')\n",
      "(u'56.3', u'39.8', u'79.3')\n",
      "(u'39.8', u'79.3', u'62.8')\n",
      "(u'79.3', u'62.8', u'42.0')\n",
      "(u'62.8', u'42.0', u'77.2')\n",
      "(u'42.0', u'77.2', u'63.7')\n",
      "(u'77.2', u'63.7', u'42.8')\n",
      "(u'63.7', u'42.8', u'76.8')\n",
      "(u'42.8', u'76.8', u'64.1')\n",
      "(u'76.8', u'64.1', u'43.0')\n",
      "(u'64.1', u'43.0', u'77.7')\n",
      "(u'43.0', u'77.7', u'64.7')\n",
      "(u'77.7', u'64.7', u'46.6')\n",
      "(u'64.7', u'46.6', u'83.1')\n",
      "(u'46.6', u'83.1', u'68.6')\n",
      "(u'83.1', u'68.6', u'42.4')\n",
      "(u'68.6', u'42.4', u'82.4')\n",
      "(u'42.4', u'82.4', u'63.6')\n",
      "(u'82.4', u'63.6', u'43.9')\n",
      "(u'63.6', u'43.9', u'76.1')\n",
      "(u'43.9', u'76.1', u'63.8')\n",
      "(u'76.1', u'63.8', u'44.4')\n",
      "(u'63.8', u'44.4', u'79.5')\n",
      "(u'44.4', u'79.5', u'64.6')\n",
      "(u'79.5', u'64.6', u'47.1')\n",
      "(u'64.6', u'47.1', u'82.0')\n",
      "(u'47.1', u'82.0', u'67.1')\n",
      "(u'82.0', u'67.1', u'47.8')\n",
      "(u'67.1', u'47.8', u'89.3')\n",
      "(u'47.8', u'89.3', u'71.0')\n",
      "(u'89.3', u'71.0', u'37.7')\n",
      "(u'71.0', u'37.7', u'79.8')\n",
      "(u'37.7', u'79.8', u'57.0')\n",
      "(u'79.8', u'57.0', u'40.5')\n",
      "(u'57.0', u'40.5', u'83.4')\n",
      "(u'40.5', u'83.4', u'60.4')\n",
      "(u'83.4', u'60.4', u'44.7')\n",
      "(u'60.4', u'44.7', u'81.8')\n",
      "(u'44.7', u'81.8', u'60.8')\n",
      "(u'81.8', u'60.8', u'44.3')\n",
      "(u'60.8', u'44.3', u'85.4')\n",
      "(u'44.3', u'85.4', u'62.8')\n",
      "(u'85.4', u'62.8', u'47.6')\n",
      "(u'62.8', u'47.6', u'89.1')\n",
      "(u'47.6', u'89.1', u'65.6')\n",
      "(u'89.1', u'65.6', u'ypst')\n",
      "(u'65.6', u'ypst', u'and')\n",
      "(u'ypst', u'and', u'ycls')\n",
      "(u'and', u'ycls', u'helps')\n",
      "(u'ycls', u'helps', u'\\ufb01nd')\n",
      "(u'helps', u'\\ufb01nd', u'their')\n",
      "(u'\\ufb01nd', u'their', u'shared')\n",
      "(u'their', u'shared', u'representation')\n",
      "(u'shared', u'representation', u'under')\n",
      "(u'representation', u'under', u'multi-task')\n",
      "(u'under', u'multi-task', u'learning')\n",
      "(u'multi-task', u'learning', u'framework')\n",
      "(u'learning', u'framework', u'for')\n",
      "(u'framework', u'for', u'which')\n",
      "(u'for', u'which', u'deep')\n",
      "(u'which', u'deep', u'model')\n",
      "(u'deep', u'model', u'ideal')\n",
      "(u'model', u'ideal', u'choice')\n",
      "(u'ideal', u'choice', u'Conclusion')\n",
      "(u'choice', u'Conclusion', u'This')\n",
      "(u'Conclusion', u'This', u'paper')\n",
      "(u'This', u'paper', u'has')\n",
      "(u'paper', u'has', u'proposed')\n",
      "(u'has', u'proposed', u'multi-source')\n",
      "(u'proposed', u'multi-source', u'deep')\n",
      "(u'multi-source', u'deep', u'model')\n",
      "(u'deep', u'model', u'for')\n",
      "(u'model', u'for', u'pose')\n",
      "(u'for', u'pose', u'estimation')\n",
      "(u'pose', u'estimation', u'non-linearly')\n",
      "(u'estimation', u'non-linearly', u'integrates')\n",
      "(u'non-linearly', u'integrates', u'three')\n",
      "(u'integrates', u'three', u'information')\n",
      "(u'three', u'information', u'sources')\n",
      "(u'information', u'sources', u'appearance')\n",
      "(u'sources', u'appearance', u'score')\n",
      "(u'appearance', u'score', u'deformation')\n",
      "(u'score', u'deformation', u'and')\n",
      "(u'deformation', u'and', u'appearance')\n",
      "(u'and', u'appearance', u'mixture')\n",
      "(u'appearance', u'mixture', u'type')\n",
      "(u'mixture', u'type', u'These')\n",
      "(u'type', u'These', u'information')\n",
      "(u'These', u'information', u'sources')\n",
      "(u'information', u'sources', u'are')\n",
      "(u'sources', u'are', u'used')\n",
      "(u'are', u'used', u'for')\n",
      "(u'used', u'for', u'de-')\n",
      "(u'for', u'de-', u'scribing')\n",
      "(u'de-', u'scribing', u'different')\n",
      "(u'scribing', u'different', u'aspects')\n",
      "(u'different', u'aspects', u'the')\n",
      "(u'aspects', u'the', u'single')\n",
      "(u'the', u'single', u'modality')\n",
      "(u'single', u'modality', u'data')\n",
      "(u'modality', u'data', u'which')\n",
      "(u'data', u'which', u'the')\n",
      "(u'which', u'the', u'image')\n",
      "(u'the', u'image', u'data')\n",
      "(u'image', u'data', u'our')\n",
      "(u'data', u'our', u'pose')\n",
      "(u'our', u'pose', u'estimation')\n",
      "(u'pose', u'estimation', u'approach')\n",
      "(u'estimation', u'approach', u'Exten-')\n",
      "(u'approach', u'Exten-', u'sive')\n",
      "(u'Exten-', u'sive', u'experimental')\n",
      "(u'sive', u'experimental', u'comparisons')\n",
      "(u'experimental', u'comparisons', u'three')\n",
      "(u'comparisons', u'three', u'public')\n",
      "(u'three', u'public', u'benchmark')\n",
      "(u'public', u'benchmark', u'datasets')\n",
      "(u'benchmark', u'datasets', u'show')\n",
      "(u'datasets', u'show', u'that')\n",
      "(u'show', u'that', u'the')\n",
      "(u'that', u'the', u'proposed')\n",
      "(u'the', u'proposed', u'model')\n",
      "(u'proposed', u'model', u'obviously')\n",
      "(u'model', u'obviously', u'improves')\n",
      "(u'obviously', u'improves', u'the')\n",
      "(u'improves', u'the', u'pose')\n",
      "(u'the', u'pose', u'estimation')\n",
      "(u'pose', u'estimation', u'accuracy')\n",
      "(u'estimation', u'accuracy', u'and')\n",
      "(u'accuracy', u'and', u'outperforms')\n",
      "(u'and', u'outperforms', u'the')\n",
      "(u'outperforms', u'the', u'state')\n",
      "(u'the', u'state', u'the')\n",
      "(u'state', u'the', u'art')\n",
      "(u'the', u'art', u'Since')\n",
      "(u'art', u'Since', u'this')\n",
      "(u'Since', u'this', u'model')\n",
      "(u'this', u'model', u'post-processing')\n",
      "(u'model', u'post-processing', u'informa-')\n",
      "(u'post-processing', u'informa-', u'tion')\n",
      "(u'informa-', u'tion', u'sources')\n",
      "(u'tion', u'sources', u'very')\n",
      "(u'sources', u'very', u'\\ufb02exible')\n",
      "(u'very', u'\\ufb02exible', u'terms')\n",
      "(u'\\ufb02exible', u'terms', u'integrating')\n",
      "(u'terms', u'integrating', u'with')\n",
      "(u'integrating', u'with', u'existing')\n",
      "(u'with', u'existing', u'approaches')\n",
      "(u'existing', u'approaches', u'that')\n",
      "(u'approaches', u'that', u'use')\n",
      "(u'that', u'use', u'different')\n",
      "(u'use', u'different', u'information')\n",
      "(u'different', u'information', u'sources')\n",
      "(u'information', u'sources', u'features')\n",
      "(u'sources', u'features', u'articulation')\n",
      "(u'features', u'articulation', u'models')\n",
      "(u'articulation', u'models', u'Learning')\n",
      "(u'models', u'Learning', u'deep')\n",
      "(u'Learning', u'deep', u'model')\n",
      "(u'deep', u'model', u'from')\n",
      "(u'model', u'from', u'pixels')\n",
      "(u'from', u'pixels', u'for')\n",
      "(u'pixels', u'for', u'pose')\n",
      "(u'for', u'pose', u'estimation')\n",
      "(u'pose', u'estimation', u'and')\n",
      "(u'estimation', u'and', u'analyzing')\n",
      "(u'and', u'analyzing', u'the')\n",
      "(u'analyzing', u'the', u'in\\ufb02uence')\n",
      "(u'the', u'in\\ufb02uence', u'training')\n",
      "(u'in\\ufb02uence', u'training', u'data')\n",
      "(u'training', u'data', u'number')\n",
      "(u'data', u'number', u'will')\n",
      "(u'number', u'will', u'the')\n",
      "(u'will', u'the', u'future')\n",
      "(u'the', u'future', u'work')\n",
      "(u'future', u'work', u'Acknowledgement')\n",
      "(u'work', u'Acknowledgement', u'This')\n",
      "(u'Acknowledgement', u'This', u'work')\n",
      "(u'This', u'work', u'supported')\n",
      "(u'work', u'supported', u'the')\n",
      "(u'supported', u'the', u'General')\n",
      "(u'the', u'General', u'Research')\n",
      "(u'General', u'Research', u'Fund')\n",
      "(u'Research', u'Fund', u'sponsored')\n",
      "(u'Fund', u'sponsored', u'the')\n",
      "(u'sponsored', u'the', u'Research')\n",
      "(u'the', u'Research', u'Grants')\n",
      "(u'Research', u'Grants', u'Council')\n",
      "(u'Grants', u'Council', u'Hong')\n",
      "(u'Council', u'Hong', u'Kong')\n",
      "(u'Hong', u'Kong', u'Project')\n",
      "(u'Kong', u'Project', u'CUHK')\n",
      "(u'Project', u'CUHK', u'CUHK')\n",
      "(u'CUHK', u'CUHK', u'CUHK')\n",
      "(u'CUHK', u'CUHK', u'National')\n",
      "(u'CUHK', u'National', u'Natural')\n",
      "(u'National', u'Natural', u'Science')\n",
      "(u'Natural', u'Science', u'Foun-')\n",
      "(u'Science', u'Foun-', u'dation')\n",
      "(u'Foun-', u'dation', u'China')\n",
      "(u'dation', u'China', u'Shenzhen')\n",
      "(u'China', u'Shenzhen', u'Basic')\n",
      "(u'Shenzhen', u'Basic', u'Research')\n",
      "(u'Basic', u'Research', u'Program')\n",
      "(u'Research', u'Program', u'JC201005270350A')\n",
      "(u'Program', u'JC201005270350A', u'JCYJ20120903092050890')\n",
      "(u'JC201005270350A', u'JCYJ20120903092050890', u'JCYJ20120617114614438')\n",
      "(u'JCYJ20120903092050890', u'JCYJ20120617114614438', u'and')\n",
      "(u'JCYJ20120617114614438', u'and', u'Guangdong')\n",
      "(u'and', u'Guangdong', u'Innovative')\n",
      "(u'Guangdong', u'Innovative', u'Research')\n",
      "(u'Innovative', u'Research', u'Team')\n",
      "(u'Research', u'Team', u'Program')\n",
      "(u'Team', u'Program', u'No.201001D0104648280')\n",
      "(u'Program', u'No.201001D0104648280', u'Figure')\n",
      "(u'No.201001D0104648280', u'Figure', u'Comparison')\n",
      "(u'Figure', u'Comparison', u'between')\n",
      "(u'Comparison', u'between', u'our')\n",
      "(u'between', u'our', u'method')\n",
      "(u'our', u'method', u'left')\n",
      "(u'method', u'left', u'and')\n",
      "(u'left', u'and', u'the')\n",
      "(u'and', u'the', u'approach')\n",
      "(u'the', u'approach', u'right')\n",
      "(u'approach', u'right', u'the')\n",
      "(u'right', u'the', u'LSP')\n",
      "(u'the', u'LSP', u'PARSE')\n",
      "(u'LSP', u'PARSE', u'and')\n",
      "(u'PARSE', u'and', u'UIUC')\n",
      "(u'and', u'UIUC', u'dataset')\n",
      "(u'UIUC', u'dataset', u'Our')\n",
      "(u'dataset', u'Our', u'ap-')\n",
      "(u'Our', u'ap-', u'proach')\n",
      "(u'ap-', u'proach', u'obtains')\n",
      "(u'proach', u'obtains', u'more')\n",
      "(u'obtains', u'more', u'reasonable')\n",
      "(u'more', u'reasonable', u'articulation')\n",
      "(u'reasonable', u'articulation', u'patterns')\n",
      "(u'articulation', u'patterns', u'and')\n",
      "(u'patterns', u'and', u'better')\n",
      "(u'and', u'better', u'solving')\n",
      "(u'better', u'solving', u'the')\n",
      "(u'solving', u'the', u'double')\n",
      "(u'the', u'double', u'counting')\n",
      "(u'double', u'counting', u'problem')\n",
      "(u'counting', u'problem', u'Best')\n",
      "(u'problem', u'Best', u'viewed')\n",
      "(u'Best', u'viewed', u'color')\n",
      "(u'viewed', u'color', u'References')\n",
      "(u'color', u'References', u'Andriluka')\n",
      "(u'References', u'Andriluka', u'Pishchulin')\n",
      "(u'Andriluka', u'Pishchulin', u'Gehler')\n",
      "(u'Pishchulin', u'Gehler', u'and')\n",
      "(u'Gehler', u'and', u'Schiele')\n",
      "(u'and', u'Schiele', u'human')\n",
      "(u'Schiele', u'human', u'pose')\n",
      "(u'human', u'pose', u'estimation')\n",
      "(u'pose', u'estimation', u'New')\n",
      "(u'estimation', u'New', u'benchmark')\n",
      "(u'New', u'benchmark', u'and')\n",
      "(u'benchmark', u'and', u'state')\n",
      "(u'and', u'state', u'the')\n",
      "(u'state', u'the', u'art')\n",
      "(u'the', u'art', u'analysis')\n",
      "(u'art', u'analysis', u'CVPR')\n",
      "(u'analysis', u'CVPR', u'Andriluka')\n",
      "(u'CVPR', u'Andriluka', u'Roth')\n",
      "(u'Andriluka', u'Roth', u'and')\n",
      "(u'Roth', u'and', u'Schiele')\n",
      "(u'and', u'Schiele', u'Pictorial')\n",
      "(u'Schiele', u'Pictorial', u'structures')\n",
      "(u'Pictorial', u'structures', u'revisited')\n",
      "(u'structures', u'revisited', u'people')\n",
      "(u'revisited', u'people', u'detection')\n",
      "(u'people', u'detection', u'and')\n",
      "(u'detection', u'and', u'articulated')\n",
      "(u'and', u'articulated', u'pose')\n",
      "(u'articulated', u'pose', u'estimation')\n",
      "(u'pose', u'estimation', u'CVPR')\n",
      "(u'estimation', u'CVPR', u'Bengio')\n",
      "(u'CVPR', u'Bengio', u'Learning')\n",
      "(u'Bengio', u'Learning', u'deep')\n",
      "(u'Learning', u'deep', u'architectures')\n",
      "(u'deep', u'architectures', u'for')\n",
      "(u'architectures', u'for', u'Foundations')\n",
      "(u'for', u'Foundations', u'and')\n",
      "(u'Foundations', u'and', u'Trends')\n",
      "(u'and', u'Trends', u'Machine')\n",
      "(u'Trends', u'Machine', u'Learning')\n",
      "(u'Machine', u'Learning', u':1\\u2013127')\n",
      "(u'Learning', u':1\\u2013127', u'Bengio')\n",
      "(u':1\\u2013127', u'Bengio', u'Courville')\n",
      "(u'Bengio', u'Courville', u'and')\n",
      "(u'Courville', u'and', u'Vincent')\n",
      "(u'and', u'Vincent', u'Representation')\n",
      "(u'Vincent', u'Representation', u'learning')\n",
      "(u'Representation', u'learning', u'review')\n",
      "(u'learning', u'review', u'and')\n",
      "(u'review', u'and', u'new')\n",
      "(u'and', u'new', u'perspectives')\n",
      "(u'new', u'perspectives', u'IEEE')\n",
      "(u'perspectives', u'IEEE', u'Trans')\n",
      "(u'IEEE', u'Trans', u'PAMI')\n",
      "(u'Trans', u'PAMI', u':1798\\u20131828')\n",
      "(u'PAMI', u':1798\\u20131828', u'Bishop')\n",
      "(u':1798\\u20131828', u'Bishop', u'and')\n",
      "(u'Bishop', u'and', u'Nasrabadi')\n",
      "(u'and', u'Nasrabadi', u'Pattern')\n",
      "(u'Nasrabadi', u'Pattern', u'recognition')\n",
      "(u'Pattern', u'recognition', u'and')\n",
      "(u'recognition', u'and', u'machine')\n",
      "(u'and', u'machine', u'learning')\n",
      "(u'machine', u'learning', u'springer')\n",
      "(u'learning', u'springer', u'Bourdev')\n",
      "(u'springer', u'Bourdev', u'and')\n",
      "(u'Bourdev', u'and', u'Malik')\n",
      "(u'and', u'Malik', u'Poselets')\n",
      "(u'Malik', u'Poselets', u'body')\n",
      "(u'Poselets', u'body', u'part')\n",
      "(u'body', u'part', u'detectors')\n",
      "(u'part', u'detectors', u'trained')\n",
      "(u'detectors', u'trained', u'using')\n",
      "(u'trained', u'using', u'human')\n",
      "(u'using', u'human', u'pose')\n",
      "(u'human', u'pose', u'annotations')\n",
      "(u'pose', u'annotations', u'ICCV')\n",
      "(u'annotations', u'ICCV', u'Dalal')\n",
      "(u'ICCV', u'Dalal', u'and')\n",
      "(u'Dalal', u'and', u'Triggs')\n",
      "(u'and', u'Triggs', u'Histograms')\n",
      "(u'Triggs', u'Histograms', u'oriented')\n",
      "(u'Histograms', u'oriented', u'gradients')\n",
      "(u'oriented', u'gradients', u'for')\n",
      "(u'gradients', u'for', u'human')\n",
      "(u'for', u'human', u'detection')\n",
      "(u'human', u'detection', u'CVPR')\n",
      "(u'detection', u'CVPR', u'Deng')\n",
      "(u'CVPR', u'Deng', u'Dong')\n",
      "(u'Deng', u'Dong', u'Socher')\n",
      "(u'Dong', u'Socher', u'L.-J')\n",
      "(u'Socher', u'L.-J', u'and')\n",
      "(u'L.-J', u'and', u'Fei-')\n",
      "(u'and', u'Fei-', u'Fei')\n",
      "(u'Fei-', u'Fei', u'Imagenet')\n",
      "(u'Fei', u'Imagenet', u'large-scale')\n",
      "(u'Imagenet', u'large-scale', u'hierarchical')\n",
      "(u'large-scale', u'hierarchical', u'image')\n",
      "(u'hierarchical', u'image', u'database')\n",
      "(u'image', u'database', u'CVPR')\n",
      "(u'database', u'CVPR', u'Desai')\n",
      "(u'CVPR', u'Desai', u'and')\n",
      "(u'Desai', u'and', u'Ramanan')\n",
      "(u'and', u'Ramanan', u'Detecting')\n",
      "(u'Ramanan', u'Detecting', u'actions')\n",
      "(u'Detecting', u'actions', u'poses')\n",
      "(u'actions', u'poses', u'and')\n",
      "(u'poses', u'and', u'objects')\n",
      "(u'and', u'objects', u'with')\n",
      "(u'objects', u'with', u'relational')\n",
      "(u'with', u'relational', u'phraselets')\n",
      "(u'relational', u'phraselets', u'ECCV')\n",
      "(u'phraselets', u'ECCV', u'Duan')\n",
      "(u'ECCV', u'Duan', u'Batra')\n",
      "(u'Duan', u'Batra', u'and')\n",
      "(u'Batra', u'and', u'Crandall')\n",
      "(u'and', u'Crandall', u'multi-layer')\n",
      "(u'Crandall', u'multi-layer', u'com-')\n",
      "(u'multi-layer', u'com-', u'posite')\n",
      "(u'com-', u'posite', u'model')\n",
      "(u'posite', u'model', u'for')\n",
      "(u'model', u'for', u'human')\n",
      "(u'for', u'human', u'pose')\n",
      "(u'human', u'pose', u'estimation')\n",
      "(u'pose', u'estimation', u'BMVC')\n",
      "(u'estimation', u'BMVC', u'Eichner')\n",
      "(u'BMVC', u'Eichner', u'and')\n",
      "(u'Eichner', u'and', u'Ferrari')\n",
      "(u'and', u'Ferrari', u'Appearance')\n",
      "(u'Ferrari', u'Appearance', u'sharing')\n",
      "(u'Appearance', u'sharing', u'for')\n",
      "(u'sharing', u'for', u'collective')\n",
      "(u'for', u'collective', u'human')\n",
      "(u'collective', u'human', u'pose')\n",
      "(u'human', u'pose', u'estimation')\n",
      "(u'pose', u'estimation', u'ACCV')\n",
      "(u'estimation', u'ACCV', u'Farabet')\n",
      "(u'ACCV', u'Farabet', u'Couprie')\n",
      "(u'Farabet', u'Couprie', u'Najman')\n",
      "(u'Couprie', u'Najman', u'and')\n",
      "(u'Najman', u'and', u'LeCun')\n",
      "(u'and', u'LeCun', u'Learning')\n",
      "(u'LeCun', u'Learning', u'hierarchical')\n",
      "(u'Learning', u'hierarchical', u'features')\n",
      "(u'hierarchical', u'features', u'for')\n",
      "(u'features', u'for', u'scene')\n",
      "(u'for', u'scene', u'labeling')\n",
      "(u'scene', u'labeling', u'IEEE')\n",
      "(u'labeling', u'IEEE', u'Trans')\n",
      "(u'IEEE', u'Trans', u'PAMI')\n",
      "(u'Trans', u'PAMI', u'30:1915\\u20131929')\n",
      "(u'PAMI', u'30:1915\\u20131929', u'Felzenszwalb')\n",
      "(u'30:1915\\u20131929', u'Felzenszwalb', u'and')\n",
      "(u'Felzenszwalb', u'and', u'Huttenlocher')\n",
      "(u'and', u'Huttenlocher', u'Pictorial')\n",
      "(u'Huttenlocher', u'Pictorial', u'struc-')\n",
      "(u'Pictorial', u'struc-', u'tures')\n",
      "(u'struc-', u'tures', u'for')\n",
      "(u'tures', u'for', u'object')\n",
      "(u'for', u'object', u'recognition')\n",
      "(u'object', u'recognition', u'IJCV')\n",
      "(u'recognition', u'IJCV', u'61:55\\u201379')\n",
      "(u'IJCV', u'61:55\\u201379', u'Ferrari')\n",
      "(u'61:55\\u201379', u'Ferrari', u'Marin-Jimenez')\n",
      "(u'Ferrari', u'Marin-Jimenez', u'and')\n",
      "(u'Marin-Jimenez', u'and', u'Zisserman')\n",
      "(u'and', u'Zisserman', u'Progressive')\n",
      "(u'Zisserman', u'Progressive', u'search')\n",
      "(u'Progressive', u'search', u'space')\n",
      "(u'search', u'space', u'reduction')\n",
      "(u'space', u'reduction', u'for')\n",
      "(u'reduction', u'for', u'human')\n",
      "(u'for', u'human', u'pose')\n",
      "(u'human', u'pose', u'estimation')\n",
      "(u'pose', u'estimation', u'CVPR')\n",
      "(u'estimation', u'CVPR', u'Left-OursRight-Yang')\n",
      "(u'CVPR', u'Left-OursRight-Yang', u'Ramanan')\n",
      "(u'Left-OursRight-Yang', u'Ramanan', u'Gkioxari')\n",
      "(u'Ramanan', u'Gkioxari', u'Arbel\\xb4aez')\n",
      "(u'Gkioxari', u'Arbel\\xb4aez', u'Bourdev')\n",
      "(u'Arbel\\xb4aez', u'Bourdev', u'and')\n",
      "(u'Bourdev', u'and', u'Malik')\n",
      "(u'and', u'Malik', u'Articu-')\n",
      "(u'Malik', u'Articu-', u'lated')\n",
      "(u'Articu-', u'lated', u'pose')\n",
      "(u'lated', u'pose', u'estimation')\n",
      "(u'pose', u'estimation', u'using')\n",
      "(u'estimation', u'using', u'discriminative')\n",
      "(u'using', u'discriminative', u'armlet')\n",
      "(u'discriminative', u'armlet', u'classi\\ufb01ers')\n",
      "(u'armlet', u'classi\\ufb01ers', u'CVPR')\n",
      "(u'classi\\ufb01ers', u'CVPR', u'Goodfellow')\n",
      "(u'CVPR', u'Goodfellow', u'Lee')\n",
      "(u'Goodfellow', u'Lee', u'Saxe')\n",
      "(u'Lee', u'Saxe', u'and')\n",
      "(u'Saxe', u'and', u'Measuring')\n",
      "(u'and', u'Measuring', u'invariances')\n",
      "(u'Measuring', u'invariances', u'deep')\n",
      "(u'invariances', u'deep', u'networks')\n",
      "(u'deep', u'networks', u'NIPS')\n",
      "(u'networks', u'NIPS', u'Guillaumin')\n",
      "(u'NIPS', u'Guillaumin', u'Verbeek')\n",
      "(u'Guillaumin', u'Verbeek', u'and')\n",
      "(u'Verbeek', u'and', u'Schmid')\n",
      "(u'and', u'Schmid', u'Multimodal')\n",
      "(u'Schmid', u'Multimodal', u'semi-supervised')\n",
      "(u'Multimodal', u'semi-supervised', u'learning')\n",
      "(u'semi-supervised', u'learning', u'for')\n",
      "(u'learning', u'for', u'image')\n",
      "(u'for', u'image', u'classi\\ufb01cation')\n",
      "(u'image', u'classi\\ufb01cation', u'CVPR')\n",
      "(u'classi\\ufb01cation', u'CVPR', u'Hinton')\n",
      "(u'CVPR', u'Hinton', u'Osindero')\n",
      "(u'Hinton', u'Osindero', u'and')\n",
      "(u'Osindero', u'and', u'Teh')\n",
      "(u'and', u'Teh', u'fast')\n",
      "(u'Teh', u'fast', u'learning')\n",
      "(u'fast', u'learning', u'al-')\n",
      "(u'learning', u'al-', u'gorithm')\n",
      "(u'al-', u'gorithm', u'for')\n",
      "(u'gorithm', u'for', u'deep')\n",
      "(u'for', u'deep', u'belief')\n",
      "(u'deep', u'belief', u'nets')\n",
      "(u'belief', u'nets', u'Neural')\n",
      "(u'nets', u'Neural', u'Computation')\n",
      "(u'Neural', u'Computation', u'18:1527\\u2013')\n",
      "(u'Computation', u'18:1527\\u2013', u'Hinton')\n",
      "(u'18:1527\\u2013', u'Hinton', u'and')\n",
      "(u'Hinton', u'and', u'Salakhutdinov')\n",
      "(u'and', u'Salakhutdinov', u'dimensionality')\n",
      "(u'Salakhutdinov', u'dimensionality', u'data')\n",
      "(u'dimensionality', u'data', u'with')\n",
      "(u'data', u'with', u'neural')\n",
      "(u'with', u'neural', u'networks')\n",
      "(u'neural', u'networks', u':504')\n",
      "(u'networks', u':504', u'July')\n",
      "(u':504', u'July', u'Reducing')\n",
      "(u'July', u'Reducing', u'the')\n",
      "(u'Reducing', u'the', u'Science')\n",
      "(u'the', u'Science', u'Jarrett')\n",
      "(u'Science', u'Jarrett', u'Kavukcuoglu')\n",
      "(u'Jarrett', u'Kavukcuoglu', u'Ranzato')\n",
      "(u'Kavukcuoglu', u'Ranzato', u'and')\n",
      "(u'Ranzato', u'and', u'LeCun')\n",
      "(u'and', u'LeCun', u'What')\n",
      "(u'LeCun', u'What', u'the')\n",
      "(u'What', u'the', u'best')\n",
      "(u'the', u'best', u'multi-stage')\n",
      "(u'best', u'multi-stage', u'architecture')\n",
      "(u'multi-stage', u'architecture', u'for')\n",
      "(u'architecture', u'for', u'object')\n",
      "(u'for', u'object', u'recog-')\n",
      "(u'object', u'recog-', u'nition')\n",
      "(u'recog-', u'nition', u'CVPR')\n",
      "(u'nition', u'CVPR', u'Johnson')\n",
      "(u'CVPR', u'Johnson', u'and')\n",
      "(u'Johnson', u'and', u'Everingham')\n",
      "(u'and', u'Everingham', u'Clustered')\n",
      "(u'Everingham', u'Clustered', u'pose')\n",
      "(u'Clustered', u'pose', u'and')\n",
      "(u'pose', u'and', u'nonlin-')\n",
      "(u'and', u'nonlin-', u'ear')\n",
      "(u'nonlin-', u'ear', u'appearance')\n",
      "(u'ear', u'appearance', u'models')\n",
      "(u'appearance', u'models', u'for')\n",
      "(u'models', u'for', u'human')\n",
      "(u'for', u'human', u'pose')\n",
      "(u'human', u'pose', u'estimation')\n",
      "(u'pose', u'estimation', u'BMVC')\n",
      "(u'estimation', u'BMVC', u'Johnson')\n",
      "(u'BMVC', u'Johnson', u'and')\n",
      "(u'Johnson', u'and', u'Everingham')\n",
      "(u'and', u'Everingham', u'Learning')\n",
      "(u'Everingham', u'Learning', u'effective')\n",
      "(u'Learning', u'effective', u'human')\n",
      "(u'effective', u'human', u'pose')\n",
      "(u'human', u'pose', u'estimation')\n",
      "(u'pose', u'estimation', u'from')\n",
      "(u'estimation', u'from', u'inaccurate')\n",
      "(u'from', u'inaccurate', u'annotation')\n",
      "(u'inaccurate', u'annotation', u'CVPR')\n",
      "(u'annotation', u'CVPR', u'Krizhevsky')\n",
      "(u'CVPR', u'Krizhevsky', u'Sutskever')\n",
      "(u'Krizhevsky', u'Sutskever', u'and')\n",
      "(u'Sutskever', u'and', u'Hinton')\n",
      "(u'and', u'Hinton', u'Imagenet')\n",
      "(u'Hinton', u'Imagenet', u'clas-')\n",
      "(u'Imagenet', u'clas-', u'si\\ufb01cation')\n",
      "(u'clas-', u'si\\ufb01cation', u'with')\n",
      "(u'si\\ufb01cation', u'with', u'deep')\n",
      "(u'with', u'deep', u'convolutional')\n",
      "(u'deep', u'convolutional', u'neural')\n",
      "(u'convolutional', u'neural', u'networks')\n",
      "(u'neural', u'networks', u'NIPS')\n",
      "(u'networks', u'NIPS', u'Kschischang')\n",
      "(u'NIPS', u'Kschischang', u'Frey')\n",
      "(u'Kschischang', u'Frey', u'and')\n",
      "(u'Frey', u'and', u'H.-A')\n",
      "(u'and', u'H.-A', u'Loeliger')\n",
      "(u'H.-A', u'Loeliger', u'Factor')\n",
      "(u'Loeliger', u'Factor', u'graphs')\n",
      "(u'Factor', u'graphs', u'and')\n",
      "(u'graphs', u'and', u'the')\n",
      "(u'and', u'the', u'sum-product')\n",
      "(u'the', u'sum-product', u'algorithm')\n",
      "(u'sum-product', u'algorithm', u'IEEE')\n",
      "(u'algorithm', u'IEEE', u'Trans')\n",
      "(u'IEEE', u'Trans', u'Inf')\n",
      "(u'Trans', u'Inf', u'The-')\n",
      "(u'Inf', u'The-', u'ory')\n",
      "(u'The-', u'ory', u':498\\u2013519')\n",
      "(u'ory', u':498\\u2013519', u'Larochelle')\n",
      "(u':498\\u2013519', u'Larochelle', u'Bengio')\n",
      "(u'Larochelle', u'Bengio', u'Louradour')\n",
      "(u'Bengio', u'Louradour', u'and')\n",
      "(u'Louradour', u'and', u'Lamblin')\n",
      "(u'and', u'Lamblin', u'Ex-')\n",
      "(u'Lamblin', u'Ex-', u'ploring')\n",
      "(u'Ex-', u'ploring', u'strategies')\n",
      "(u'ploring', u'strategies', u'for')\n",
      "(u'strategies', u'for', u'training')\n",
      "(u'for', u'training', u'deep')\n",
      "(u'training', u'deep', u'neural')\n",
      "(u'deep', u'neural', u'networks')\n",
      "(u'neural', u'networks', u'Ma-')\n",
      "(u'networks', u'Ma-', u'chine')\n",
      "(u'Ma-', u'chine', u'Learning')\n",
      "(u'chine', u'Learning', u'Research')\n",
      "(u'Learning', u'Research', u'10:1\\u201340')\n",
      "(u'Research', u'10:1\\u201340', u'Ranzato')\n",
      "(u'10:1\\u201340', u'Ranzato', u'Monga')\n",
      "(u'Ranzato', u'Monga', u'Devin')\n",
      "(u'Monga', u'Devin', u'Chen')\n",
      "(u'Devin', u'Chen', u'Corrado')\n",
      "(u'Chen', u'Corrado', u'Dean')\n",
      "(u'Corrado', u'Dean', u'and')\n",
      "(u'Dean', u'and', u'Building')\n",
      "(u'and', u'Building', u'high-level')\n",
      "(u'Building', u'high-level', u'features')\n",
      "(u'high-level', u'features', u'using')\n",
      "(u'features', u'using', u'large')\n",
      "(u'using', u'large', u'scale')\n",
      "(u'large', u'scale', u'unsupervised')\n",
      "(u'scale', u'unsupervised', u'learning')\n",
      "(u'unsupervised', u'learning', u'ICML')\n",
      "(u'learning', u'ICML', u'LeCun')\n",
      "(u'ICML', u'LeCun', u'Bottou')\n",
      "(u'LeCun', u'Bottou', u'Bengio')\n",
      "(u'Bottou', u'Bengio', u'and')\n",
      "(u'Bengio', u'and', u'Haffner')\n",
      "(u'and', u'Haffner', u'Gradient-')\n",
      "(u'Haffner', u'Gradient-', u'based')\n",
      "(u'Gradient-', u'based', u'learning')\n",
      "(u'based', u'learning', u'applied')\n",
      "(u'learning', u'applied', u'document')\n",
      "(u'applied', u'document', u'recognition')\n",
      "(u'document', u'recognition', u'Proceed-')\n",
      "(u'recognition', u'Proceed-', u'ings')\n",
      "(u'Proceed-', u'ings', u'the')\n",
      "(u'ings', u'the', u'IEEE')\n",
      "(u'the', u'IEEE', u':2278\\u20132324')\n",
      "(u'IEEE', u':2278\\u20132324', u'Zhao')\n",
      "(u':2278\\u20132324', u'Zhao', u'Xiao')\n",
      "(u'Zhao', u'Xiao', u'and')\n",
      "(u'Xiao', u'and', u'Wang')\n",
      "(u'and', u'Wang', u'Deepreid')\n",
      "(u'Wang', u'Deepreid', u'Deep')\n",
      "(u'Deepreid', u'Deep', u'\\ufb01lter')\n",
      "(u'Deep', u'\\ufb01lter', u'pairing')\n",
      "(u'\\ufb01lter', u'pairing', u'neural')\n",
      "(u'pairing', u'neural', u'network')\n",
      "(u'neural', u'network', u'for')\n",
      "(u'network', u'for', u'person')\n",
      "(u'for', u'person', u're-identi\\ufb01cation')\n",
      "(u'person', u're-identi\\ufb01cation', u'CVPR')\n",
      "(u're-identi\\ufb01cation', u'CVPR', u'Luo')\n",
      "(u'CVPR', u'Luo', u'Tian')\n",
      "(u'Luo', u'Tian', u'Wang')\n",
      "(u'Tian', u'Wang', u'and')\n",
      "(u'Wang', u'and', u'Tang')\n",
      "(u'and', u'Tang', u'Switchable')\n",
      "(u'Tang', u'Switchable', u'deep')\n",
      "(u'Switchable', u'deep', u'network')\n",
      "(u'deep', u'network', u'for')\n",
      "(u'network', u'for', u'pedestrian')\n",
      "(u'for', u'pedestrian', u'detection')\n",
      "(u'pedestrian', u'detection', u'CVPR')\n",
      "(u'detection', u'CVPR', u'Luo')\n",
      "(u'CVPR', u'Luo', u'Wang')\n",
      "(u'Luo', u'Wang', u'and')\n",
      "(u'Wang', u'and', u'Tang')\n",
      "(u'and', u'Tang', u'Hierarchical')\n",
      "(u'Tang', u'Hierarchical', u'face')\n",
      "(u'Hierarchical', u'face', u'parsing')\n",
      "(u'face', u'parsing', u'via')\n",
      "(u'parsing', u'via', u'deep')\n",
      "(u'via', u'deep', u'learning')\n",
      "(u'deep', u'learning', u'CVPR')\n",
      "(u'learning', u'CVPR', u'Luo')\n",
      "(u'CVPR', u'Luo', u'Wang')\n",
      "(u'Luo', u'Wang', u'and')\n",
      "(u'Wang', u'and', u'Tang')\n",
      "(u'and', u'Tang', u'deep')\n",
      "(u'Tang', u'deep', u'sum-product')\n",
      "(u'deep', u'sum-product', u'archi-')\n",
      "(u'sum-product', u'archi-', u'tecture')\n",
      "(u'archi-', u'tecture', u'for')\n",
      "(u'tecture', u'for', u'robust')\n",
      "(u'for', u'robust', u'facial')\n",
      "(u'robust', u'facial', u'attributes')\n",
      "(u'facial', u'attributes', u'analysis')\n",
      "(u'attributes', u'analysis', u'ICCV')\n",
      "(u'analysis', u'ICCV', u'Luo')\n",
      "(u'ICCV', u'Luo', u'Wang')\n",
      "(u'Luo', u'Wang', u'and')\n",
      "(u'Wang', u'and', u'Tang')\n",
      "(u'and', u'Tang', u'Pedestrian')\n",
      "(u'Tang', u'Pedestrian', u'parsing')\n",
      "(u'Pedestrian', u'parsing', u'via')\n",
      "(u'parsing', u'via', u'deep')\n",
      "(u'via', u'deep', u'decompositional')\n",
      "(u'deep', u'decompositional', u'neural')\n",
      "(u'decompositional', u'neural', u'network')\n",
      "(u'neural', u'network', u'ICCV')\n",
      "(u'network', u'ICCV', u'Mori')\n",
      "(u'ICCV', u'Mori', u'and')\n",
      "(u'Mori', u'and', u'Malik')\n",
      "(u'and', u'Malik', u'Estimating')\n",
      "(u'Malik', u'Estimating', u'human')\n",
      "(u'Estimating', u'human', u'body')\n",
      "(u'human', u'body', u'con\\ufb01gurations')\n",
      "(u'body', u'con\\ufb01gurations', u'using')\n",
      "(u'con\\ufb01gurations', u'using', u'shape')\n",
      "(u'using', u'shape', u'context')\n",
      "(u'shape', u'context', u'matching')\n",
      "(u'context', u'matching', u'ECCV')\n",
      "(u'matching', u'ECCV', u'Mori')\n",
      "(u'ECCV', u'Mori', u'and')\n",
      "(u'Mori', u'and', u'Malik')\n",
      "(u'and', u'Malik', u'Recovering')\n",
      "(u'Malik', u'Recovering', u'human')\n",
      "(u'Recovering', u'human', u'body')\n",
      "(u'human', u'body', u'con\\ufb01gura-')\n",
      "(u'body', u'con\\ufb01gura-', u'tions')\n",
      "(u'con\\ufb01gura-', u'tions', u'using')\n",
      "(u'tions', u'using', u'shape')\n",
      "(u'using', u'shape', u'contexts')\n",
      "(u'shape', u'contexts', u'IEEE')\n",
      "(u'contexts', u'IEEE', u'Trans')\n",
      "(u'IEEE', u'Trans', u'PAMI')\n",
      "(u'Trans', u'PAMI', u':1052\\u2013')\n",
      "(u'PAMI', u':1052\\u2013', u'Ngiam')\n",
      "(u':1052\\u2013', u'Ngiam', u'Khosla')\n",
      "(u'Ngiam', u'Khosla', u'Kim')\n",
      "(u'Khosla', u'Kim', u'Nam')\n",
      "(u'Kim', u'Nam', u'Lee')\n",
      "(u'Nam', u'Lee', u'and')\n",
      "(u'Lee', u'and', u'Multimodal')\n",
      "(u'and', u'Multimodal', u'deep')\n",
      "(u'Multimodal', u'deep', u'learning')\n",
      "(u'deep', u'learning', u'ICML')\n",
      "(u'learning', u'ICML', u'Norouzi')\n",
      "(u'ICML', u'Norouzi', u'Ranjbar')\n",
      "(u'Norouzi', u'Ranjbar', u'and')\n",
      "(u'Ranjbar', u'and', u'Mori')\n",
      "(u'and', u'Mori', u'Stacks')\n",
      "(u'Mori', u'Stacks', u'convolu-')\n",
      "(u'Stacks', u'convolu-', u'tional')\n",
      "(u'convolu-', u'tional', u'restricted')\n",
      "(u'tional', u'restricted', u'boltzmann')\n",
      "(u'restricted', u'boltzmann', u'machines')\n",
      "(u'boltzmann', u'machines', u'for')\n",
      "(u'machines', u'for', u'shift-invariant')\n",
      "(u'for', u'shift-invariant', u'fea-')\n",
      "(u'shift-invariant', u'fea-', u'ture')\n",
      "(u'fea-', u'ture', u'learning')\n",
      "(u'ture', u'learning', u'CVPR')\n",
      "(u'learning', u'CVPR', u'Ouyang')\n",
      "(u'CVPR', u'Ouyang', u'and')\n",
      "(u'Ouyang', u'and', u'Wang')\n",
      "(u'and', u'Wang', u'discriminative')\n",
      "(u'Wang', u'discriminative', u'deep')\n",
      "(u'discriminative', u'deep', u'model')\n",
      "(u'deep', u'model', u'for')\n",
      "(u'model', u'for', u'pedestrian')\n",
      "(u'for', u'pedestrian', u'detection')\n",
      "(u'pedestrian', u'detection', u'with')\n",
      "(u'detection', u'with', u'occlusion')\n",
      "(u'with', u'occlusion', u'handling')\n",
      "(u'occlusion', u'handling', u'CVPR')\n",
      "(u'handling', u'CVPR', u'Ouyang')\n",
      "(u'CVPR', u'Ouyang', u'and')\n",
      "(u'Ouyang', u'and', u'Wang')\n",
      "(u'and', u'Wang', u'Joint')\n",
      "(u'Wang', u'Joint', u'deep')\n",
      "(u'Joint', u'deep', u'learning')\n",
      "(u'deep', u'learning', u'for')\n",
      "(u'learning', u'for', u'pedestrian')\n",
      "(u'for', u'pedestrian', u'detection')\n",
      "(u'pedestrian', u'detection', u'ICCV')\n",
      "(u'detection', u'ICCV', u'Ouyang')\n",
      "(u'ICCV', u'Ouyang', u'Zeng')\n",
      "(u'Ouyang', u'Zeng', u'and')\n",
      "(u'Zeng', u'and', u'Wang')\n",
      "(u'and', u'Wang', u'Modeling')\n",
      "(u'Wang', u'Modeling', u'mutual')\n",
      "(u'Modeling', u'mutual', u'visi-')\n",
      "(u'mutual', u'visi-', u'bility')\n",
      "(u'visi-', u'bility', u'relationship')\n",
      "(u'bility', u'relationship', u'pedestrian')\n",
      "(u'relationship', u'pedestrian', u'detection')\n",
      "(u'pedestrian', u'detection', u'CVPR')\n",
      "(u'detection', u'CVPR', u'Pishchulin')\n",
      "(u'CVPR', u'Pishchulin', u'Andriluka')\n",
      "(u'Pishchulin', u'Andriluka', u'Gehler')\n",
      "(u'Andriluka', u'Gehler', u'and')\n",
      "(u'Gehler', u'and', u'Schiele')\n",
      "(u'and', u'Schiele', u'Pose-')\n",
      "(u'Schiele', u'Pose-', u'let')\n",
      "(u'Pose-', u'let', u'conditioned')\n",
      "(u'let', u'conditioned', u'pictorial')\n",
      "(u'conditioned', u'pictorial', u'structures')\n",
      "(u'pictorial', u'structures', u'CVPR')\n",
      "(u'structures', u'CVPR', u'Pishchulin')\n",
      "(u'CVPR', u'Pishchulin', u'Andriluka')\n",
      "(u'Pishchulin', u'Andriluka', u'Gehler')\n",
      "(u'Andriluka', u'Gehler', u'and')\n",
      "(u'Gehler', u'and', u'Schiele')\n",
      "(u'and', u'Schiele', u'Strong')\n",
      "(u'Schiele', u'Strong', u'appearance')\n",
      "(u'Strong', u'appearance', u'and')\n",
      "(u'appearance', u'and', u'expressive')\n",
      "(u'and', u'expressive', u'spatial')\n",
      "(u'expressive', u'spatial', u'models')\n",
      "(u'spatial', u'models', u'for')\n",
      "(u'models', u'for', u'human')\n",
      "(u'for', u'human', u'pose')\n",
      "(u'human', u'pose', u'estimation')\n",
      "(u'pose', u'estimation', u'ICCV')\n",
      "(u'estimation', u'ICCV', u'December')\n",
      "(u'ICCV', u'December', u'Pishchulin')\n",
      "(u'December', u'Pishchulin', u'Jain')\n",
      "(u'Pishchulin', u'Jain', u'Andriluka')\n",
      "(u'Jain', u'Andriluka', u'Thormahlen')\n",
      "(u'Andriluka', u'Thormahlen', u'and')\n",
      "(u'Thormahlen', u'and', u'Schiele')\n",
      "(u'and', u'Schiele', u'Articulated')\n",
      "(u'Schiele', u'Articulated', u'people')\n",
      "(u'Articulated', u'people', u'detection')\n",
      "(u'people', u'detection', u'and')\n",
      "(u'detection', u'and', u'pose')\n",
      "(u'and', u'pose', u'estimation')\n",
      "(u'pose', u'estimation', u'Reshaping')\n",
      "(u'estimation', u'Reshaping', u'the')\n",
      "(u'Reshaping', u'the', u'future')\n",
      "(u'the', u'future', u'CVPR')\n",
      "(u'future', u'CVPR', u'Poon')\n",
      "(u'CVPR', u'Poon', u'and')\n",
      "(u'Poon', u'and', u'Domingos')\n",
      "(u'and', u'Domingos', u'Sum-product')\n",
      "(u'Domingos', u'Sum-product', u'networks')\n",
      "(u'Sum-product', u'networks', u'new')\n",
      "(u'networks', u'new', u'deep')\n",
      "(u'new', u'deep', u'architecture')\n",
      "(u'deep', u'architecture', u'UAI')\n",
      "(u'architecture', u'UAI', u'Ramanan')\n",
      "(u'UAI', u'Ramanan', u'Learning')\n",
      "(u'Ramanan', u'Learning', u'parse')\n",
      "(u'Learning', u'parse', u'images')\n",
      "(u'parse', u'images', u'articulated')\n",
      "(u'images', u'articulated', u'bodies')\n",
      "(u'articulated', u'bodies', u'NIPS')\n",
      "(u'bodies', u'NIPS', u'Ranzato')\n",
      "(u'NIPS', u'Ranzato', u'Huang')\n",
      "(u'Ranzato', u'Huang', u'Y.-L.')\n",
      "(u'Huang', u'Y.-L.', u'Boureau')\n",
      "(u'Y.-L.', u'Boureau', u'and')\n",
      "(u'Boureau', u'and', u'Lecun')\n",
      "(u'and', u'Lecun', u'Un-')\n",
      "(u'Lecun', u'Un-', u'supervised')\n",
      "(u'Un-', u'supervised', u'learning')\n",
      "(u'supervised', u'learning', u'invariant')\n",
      "(u'learning', u'invariant', u'feature')\n",
      "(u'invariant', u'feature', u'hierarchies')\n",
      "(u'feature', u'hierarchies', u'with')\n",
      "(u'hierarchies', u'with', u'ap-')\n",
      "(u'with', u'ap-', u'plications')\n",
      "(u'ap-', u'plications', u'object')\n",
      "(u'plications', u'object', u'recognition')\n",
      "(u'object', u'recognition', u'CVPR')\n",
      "(u'recognition', u'CVPR', u'Sapp')\n",
      "(u'CVPR', u'Sapp', u'Toshev')\n",
      "(u'Sapp', u'Toshev', u'and')\n",
      "(u'Toshev', u'and', u'Taskar')\n",
      "(u'and', u'Taskar', u'Cascaded')\n",
      "(u'Taskar', u'Cascaded', u'models')\n",
      "(u'Cascaded', u'models', u'for')\n",
      "(u'models', u'for', u'articulated')\n",
      "(u'for', u'articulated', u'pose')\n",
      "(u'articulated', u'pose', u'estimation')\n",
      "(u'pose', u'estimation', u'ECCV')\n",
      "(u'estimation', u'ECCV', u'Srivastava')\n",
      "(u'ECCV', u'Srivastava', u'and')\n",
      "(u'Srivastava', u'and', u'Salakhutdinov')\n",
      "(u'and', u'Salakhutdinov', u'Multimodal')\n",
      "(u'Salakhutdinov', u'Multimodal', u'learning')\n",
      "(u'Multimodal', u'learning', u'with')\n",
      "(u'learning', u'with', u'deep')\n",
      "(u'with', u'deep', u'boltzmann')\n",
      "(u'deep', u'boltzmann', u'machines')\n",
      "(u'boltzmann', u'machines', u'NIPS')\n",
      "(u'machines', u'NIPS', u'Sun')\n",
      "(u'NIPS', u'Sun', u'and')\n",
      "(u'Sun', u'and', u'Savarese')\n",
      "(u'and', u'Savarese', u'Articulated')\n",
      "(u'Savarese', u'Articulated', u'part-based')\n",
      "(u'Articulated', u'part-based', u'model')\n",
      "(u'part-based', u'model', u'for')\n",
      "(u'model', u'for', u'joint')\n",
      "(u'for', u'joint', u'object')\n",
      "(u'joint', u'object', u'detection')\n",
      "(u'object', u'detection', u'and')\n",
      "(u'detection', u'and', u'pose')\n",
      "(u'and', u'pose', u'estimation')\n",
      "(u'pose', u'estimation', u'ICCV')\n",
      "(u'estimation', u'ICCV', u'Sun')\n",
      "(u'ICCV', u'Sun', u'Wang')\n",
      "(u'Sun', u'Wang', u'and')\n",
      "(u'Wang', u'and', u'Tang')\n",
      "(u'and', u'Tang', u'Deep')\n",
      "(u'Tang', u'Deep', u'convolutional')\n",
      "(u'Deep', u'convolutional', u'network')\n",
      "(u'convolutional', u'network', u'cascade')\n",
      "(u'network', u'cascade', u'for')\n",
      "(u'cascade', u'for', u'facial')\n",
      "(u'for', u'facial', u'point')\n",
      "(u'facial', u'point', u'detection')\n",
      "(u'point', u'detection', u'CVPR')\n",
      "(u'detection', u'CVPR', u'Sun')\n",
      "(u'CVPR', u'Sun', u'Wang')\n",
      "(u'Sun', u'Wang', u'and')\n",
      "(u'Wang', u'and', u'Tang')\n",
      "(u'and', u'Tang', u'Hybrid')\n",
      "(u'Tang', u'Hybrid', u'deep')\n",
      "(u'Hybrid', u'deep', u'learning')\n",
      "(u'deep', u'learning', u'for')\n",
      "(u'learning', u'for', u'computing')\n",
      "(u'for', u'computing', u'face')\n",
      "(u'computing', u'face', u'similarities')\n",
      "(u'face', u'similarities', u'ICCV')\n",
      "(u'similarities', u'ICCV', u'Sun')\n",
      "(u'ICCV', u'Sun', u'Wang')\n",
      "(u'Sun', u'Wang', u'and')\n",
      "(u'Wang', u'and', u'Tang')\n",
      "(u'and', u'Tang', u'Deep')\n",
      "(u'Tang', u'Deep', u'learning')\n",
      "(u'Deep', u'learning', u'face')\n",
      "(u'learning', u'face', u'represen-')\n",
      "(u'face', u'represen-', u'tation')\n",
      "(u'represen-', u'tation', u'from')\n",
      "(u'tation', u'from', u'predicting')\n",
      "(u'from', u'predicting', u'10,000')\n",
      "(u'predicting', u'10,000', u'classes')\n",
      "(u'10,000', u'classes', u'CVPR')\n",
      "(u'classes', u'CVPR', u'Tian')\n",
      "(u'CVPR', u'Tian', u'Zitnick')\n",
      "(u'Tian', u'Zitnick', u'and')\n",
      "(u'Zitnick', u'and', u'Narasimhan')\n",
      "(u'and', u'Narasimhan', u'Exploring')\n",
      "(u'Narasimhan', u'Exploring', u'the')\n",
      "(u'Exploring', u'the', u'spatial')\n",
      "(u'the', u'spatial', u'hierarchy')\n",
      "(u'spatial', u'hierarchy', u'mixture')\n",
      "(u'hierarchy', u'mixture', u'models')\n",
      "(u'mixture', u'models', u'for')\n",
      "(u'models', u'for', u'human')\n",
      "(u'for', u'human', u'pose')\n",
      "(u'human', u'pose', u'estima-')\n",
      "(u'pose', u'estima-', u'tion')\n",
      "(u'estima-', u'tion', u'ECCV')\n",
      "(u'tion', u'ECCV', u'Tran')\n",
      "(u'ECCV', u'Tran', u'and')\n",
      "(u'Tran', u'and', u'Forsyth')\n",
      "(u'and', u'Forsyth', u'Improved')\n",
      "(u'Forsyth', u'Improved', u'human')\n",
      "(u'Improved', u'human', u'parsing')\n",
      "(u'human', u'parsing', u'with')\n",
      "(u'parsing', u'with', u'full')\n",
      "(u'with', u'full', u'relational')\n",
      "(u'full', u'relational', u'model')\n",
      "(u'relational', u'model', u'ECCV')\n",
      "(u'model', u'ECCV', u'Wang')\n",
      "(u'ECCV', u'Wang', u'and')\n",
      "(u'Wang', u'and', u'Beyond')\n",
      "(u'and', u'Beyond', u'physical')\n",
      "(u'Beyond', u'physical', u'connections')\n",
      "(u'physical', u'connections', u'Tree')\n",
      "(u'connections', u'Tree', u'mod-')\n",
      "(u'Tree', u'mod-', u'els')\n",
      "(u'mod-', u'els', u'human')\n",
      "(u'els', u'human', u'pose')\n",
      "(u'human', u'pose', u'estimation')\n",
      "(u'pose', u'estimation', u'CVPR')\n",
      "(u'estimation', u'CVPR', u'Wang')\n",
      "(u'CVPR', u'Wang', u'and')\n",
      "(u'Wang', u'and', u'Mori')\n",
      "(u'and', u'Mori', u'Multiple')\n",
      "(u'Mori', u'Multiple', u'tree')\n",
      "(u'Multiple', u'tree', u'models')\n",
      "(u'tree', u'models', u'for')\n",
      "(u'models', u'for', u'occlusion')\n",
      "(u'for', u'occlusion', u'and')\n",
      "(u'occlusion', u'and', u'spatial')\n",
      "(u'and', u'spatial', u'constraints')\n",
      "(u'spatial', u'constraints', u'human')\n",
      "(u'constraints', u'human', u'pose')\n",
      "(u'human', u'pose', u'estimation')\n",
      "(u'pose', u'estimation', u'ECCV')\n",
      "(u'estimation', u'ECCV', u'Wang')\n",
      "(u'ECCV', u'Wang', u'Tran')\n",
      "(u'Wang', u'Tran', u'and')\n",
      "(u'Tran', u'and', u'Liao')\n",
      "(u'and', u'Liao', u'Learning')\n",
      "(u'Liao', u'Learning', u'hierarchical')\n",
      "(u'Learning', u'hierarchical', u'pose-')\n",
      "(u'hierarchical', u'pose-', u'lets')\n",
      "(u'pose-', u'lets', u'for')\n",
      "(u'lets', u'for', u'human')\n",
      "(u'for', u'human', u'parsing')\n",
      "(u'human', u'parsing', u'CVPR')\n",
      "(u'parsing', u'CVPR', u'Yang')\n",
      "(u'CVPR', u'Yang', u'and')\n",
      "(u'Yang', u'and', u'Ramanan')\n",
      "(u'and', u'Ramanan', u'Articulated')\n",
      "(u'Ramanan', u'Articulated', u'pose')\n",
      "(u'Articulated', u'pose', u'estimation')\n",
      "(u'pose', u'estimation', u'with')\n",
      "(u'estimation', u'with', u'\\ufb02exible')\n",
      "(u'with', u'\\ufb02exible', u'mixtures-of-parts')\n",
      "(u'\\ufb02exible', u'mixtures-of-parts', u'CVPR')\n",
      "(u'mixtures-of-parts', u'CVPR', u'Yang')\n",
      "(u'CVPR', u'Yang', u'and')\n",
      "(u'Yang', u'and', u'Ramanan')\n",
      "(u'and', u'Ramanan', u'Articulated')\n",
      "(u'Ramanan', u'Articulated', u'human')\n",
      "(u'Articulated', u'human', u'detection')\n",
      "(u'human', u'detection', u'with')\n",
      "(u'detection', u'with', u'\\ufb02exible')\n",
      "(u'with', u'\\ufb02exible', u'mixtures-of-parts')\n",
      "(u'\\ufb02exible', u'mixtures-of-parts', u'IEEE')\n",
      "(u'mixtures-of-parts', u'IEEE', u'Trans')\n",
      "(u'IEEE', u'Trans', u'PAMI')\n",
      "(u'Trans', u'PAMI', u'appear')\n",
      "(u'PAMI', u'appear', u'Zeiler')\n",
      "(u'appear', u'Zeiler', u'Taylor')\n",
      "(u'Zeiler', u'Taylor', u'and')\n",
      "(u'Taylor', u'and', u'Fergus')\n",
      "(u'and', u'Fergus', u'Adaptive')\n",
      "(u'Fergus', u'Adaptive', u'decon-')\n",
      "(u'Adaptive', u'decon-', u'volutional')\n",
      "(u'decon-', u'volutional', u'networks')\n",
      "(u'volutional', u'networks', u'for')\n",
      "(u'networks', u'for', u'mid')\n",
      "(u'for', u'mid', u'and')\n",
      "(u'mid', u'and', u'high')\n",
      "(u'and', u'high', u'level')\n",
      "(u'high', u'level', u'feature')\n",
      "(u'level', u'feature', u'learning')\n",
      "(u'feature', u'learning', u'ICCV')\n",
      "(u'learning', u'ICCV', u'Zeng')\n",
      "(u'ICCV', u'Zeng', u'Ouyang')\n",
      "(u'Zeng', u'Ouyang', u'and')\n",
      "(u'Ouyang', u'and', u'Wang')\n",
      "(u'and', u'Wang', u'Multi-stage')\n",
      "(u'Wang', u'Multi-stage', u'contextual')\n",
      "(u'Multi-stage', u'contextual', u'deep')\n",
      "(u'contextual', u'deep', u'learning')\n",
      "(u'deep', u'learning', u'for')\n",
      "(u'learning', u'for', u'pedestrian')\n",
      "(u'for', u'pedestrian', u'detection')\n",
      "(u'pedestrian', u'detection', u'ICCV')\n",
      "(u'detection', u'ICCV', u'Zhu')\n",
      "(u'ICCV', u'Zhu', u'Luo')\n",
      "(u'Zhu', u'Luo', u'Wang')\n",
      "(u'Luo', u'Wang', u'and')\n",
      "(u'Wang', u'and', u'Tang')\n",
      "(u'and', u'Tang', u'Deep')\n",
      "(u'Tang', u'Deep', u'learning')\n",
      "(u'Deep', u'learning', u'identity')\n",
      "(u'learning', u'identity', u'preserving')\n",
      "(u'identity', u'preserving', u'face')\n",
      "(u'preserving', u'face', u'space')\n",
      "(u'face', u'space', u'ICCV')\n"
     ]
    }
   ],
   "source": [
    "for grams in asd:\n",
    "    print grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Multi, ',', -19.579313278198242)\n",
      "(-, ',', -5.202415943145752)\n",
      "(source, ',', -8.812856674194336)\n",
      "(Deep, ',', -19.579313278198242)\n",
      "(Learning, ',', -19.579313278198242)\n",
      "(for, ',', -4.91396951675415)\n",
      "(Human, ',', -11.173282623291016)\n",
      "(Pose, ',', -19.579313278198242)\n",
      "(Estimation, ',', -19.579313278198242)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(Wanli, ',', -19.579313278198242)\n",
      "(Ouyang, ',', -19.579313278198242)\n",
      "(Xiao, ',', -19.579313278198242)\n",
      "(Chu, ',', -19.579313278198242)\n",
      "(Xiaogang, ',', -19.579313278198242)\n",
      "(Wang, ',', -19.579313278198242)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(Department, ',', -11.11874008178711)\n",
      "(of, ',', -4.1284637451171875)\n",
      "(Electronic, ',', -19.579313278198242)\n",
      "(Engineering, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(The, ',', -5.774222373962402)\n",
      "(Chinese, ',', -10.189560890197754)\n",
      "(University, ',', -10.51649284362793)\n",
      "(of, ',', -4.1284637451171875)\n",
      "(Hong, ',', -19.579313278198242)\n",
      "(Kong, ',', -19.579313278198242)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(wlouyang@ee.cuhk.edu.hk, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(xgwang@ee.cuhk.edu.hk, ',', -19.579313278198242)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(Abstract, ',', -19.579313278198242)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(Visual, ',', -11.759509086608887)\n",
      "(appearance, ',', -11.022025108337402)\n",
      "(score, ',', -10.567034721374512)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(appearance, ',', -11.022025108337402)\n",
      "(mixture, ',', -19.579313278198242)\n",
      "(type, ',', -8.814929008483887)\n",
      "(and, ',', -4.195279121398926)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(deformation, ',', -19.579313278198242)\n",
      "(are, ',', -5.160149097442627)\n",
      "(three, ',', -8.800739288330078)\n",
      "(important, ',', -8.519515037536621)\n",
      "(information, ',', -8.817930221557617)\n",
      "(sources, ',', -9.963014602661133)\n",
      "(for, ',', -4.91396951675415)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(human, ',', -8.437538146972656)\n",
      "(pose, ',', -11.87548542022705)\n",
      "(estimation, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(This, ',', -6.785318851470947)\n",
      "(paper, ',', -8.99613094329834)\n",
      "(proposes, ',', -19.579313278198242)\n",
      "(to, ',', -3.83851957321167)\n",
      "(build, ',', -9.439423561096191)\n",
      "(a, ',', -3.9830753803253174)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(multi, ',', -10.772948265075684)\n",
      "(-, ',', -5.202415943145752)\n",
      "(source, ',', -8.812856674194336)\n",
      "(deep, ',', -9.902402877807617)\n",
      "(model, ',', -9.600680351257324)\n",
      "(in, ',', -4.585874080657959)\n",
      "(order, ',', -8.68863582611084)\n",
      "(to, ',', -3.83851957321167)\n",
      "(extract, ',', -19.579313278198242)\n",
      "(non, ',', -8.436274528503418)\n",
      "(-, ',', -5.202415943145752)\n",
      "(linear, ',', -11.651655197143555)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(representation, ',', -10.828350067138672)\n",
      "(from, ',', -6.028810501098633)\n",
      "(these, ',', -7.158668518066406)\n",
      "(different, ',', -7.797479629516602)\n",
      "(aspects, ',', -10.912962913513184)\n",
      "(of, ',', -4.1284637451171875)\n",
      "(information, ',', -8.817930221557617)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(sources, ',', -9.963014602661133)\n",
      "(., ',', -3.0729479789733887)\n",
      "(With, ',', -9.147692680358887)\n",
      "(the, ',', -3.425445795059204)\n",
      "(deep, ',', -9.902402877807617)\n",
      "(model, ',', -9.600680351257324)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(the, ',', -3.425445795059204)\n",
      "(global, ',', -9.585124015808105)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(high, ',', -8.075313568115234)\n",
      "(-, ',', -5.202415943145752)\n",
      "(order, ',', -8.68863582611084)\n",
      "(hu-, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(man, ',', -7.677258014678955)\n",
      "(body, ',', -9.267072677612305)\n",
      "(articulation, ',', -19.579313278198242)\n",
      "(patterns, ',', -11.080256462097168)\n",
      "(in, ',', -4.585874080657959)\n",
      "(these, ',', -7.158668518066406)\n",
      "(information, ',', -8.817930221557617)\n",
      "(sources, ',', -9.963014602661133)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(are, ',', -5.160149097442627)\n",
      "(extracted, ',', -19.579313278198242)\n",
      "(for, ',', -4.91396951675415)\n",
      "(pose, ',', -11.87548542022705)\n",
      "(estimation, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(The, ',', -5.774222373962402)\n",
      "(task, ',', -10.82490348815918)\n",
      "(for, ',', -4.91396951675415)\n",
      "(estimat-, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(ing, ',', -19.579313278198242)\n",
      "(body, ',', -9.267072677612305)\n",
      "(locations, ',', -11.704129219055176)\n",
      "(and, ',', -4.195279121398926)\n",
      "(the, ',', -3.425445795059204)\n",
      "(task, ',', -10.82490348815918)\n",
      "(for, ',', -4.91396951675415)\n",
      "(human, ',', -8.437538146972656)\n",
      "(detection, ',', -19.579313278198242)\n",
      "(are, ',', -5.160149097442627)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(jointly, ',', -19.579313278198242)\n",
      "(learned, ',', -9.666524887084961)\n",
      "(using, ',', -7.910459041595459)\n",
      "(a, ',', -3.9830753803253174)\n",
      "(uniﬁed, ',', -19.579313278198242)\n",
      "(deep, ',', -9.902402877807617)\n",
      "(model, ',', -9.600680351257324)\n",
      "(., ',', -3.0729479789733887)\n",
      "(The, ',', -5.774222373962402)\n",
      "(proposed, ',', -10.828350067138672)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(approach, ',', -9.994755744934082)\n",
      "(can, ',', -5.9138712882995605)\n",
      "(be, ',', -5.210641384124756)\n",
      "(viewed, ',', -11.39624309539795)\n",
      "(as, ',', -5.507394313812256)\n",
      "(a, ',', -3.9830753803253174)\n",
      "(post, ',', -8.091059684753418)\n",
      "(-, ',', -5.202415943145752)\n",
      "(processing, ',', -11.223255157470703)\n",
      "(of, ',', -4.1284637451171875)\n",
      "(pose, ',', -11.87548542022705)\n",
      "(esti-, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(mation, ',', -19.579313278198242)\n",
      "(results, ',', -9.357637405395508)\n",
      "(and, ',', -4.195279121398926)\n",
      "(can, ',', -5.9138712882995605)\n",
      "(ﬂexibly, ',', -19.579313278198242)\n",
      "(integrate, ',', -19.579313278198242)\n",
      "(with, ',', -5.363764762878418)\n",
      "(existing, ',', -10.217281341552734)\n",
      "(meth-, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(ods, ',', -19.579313278198242)\n",
      "(by, ',', -6.114920139312744)\n",
      "(taking, ',', -8.66118335723877)\n",
      "(their, ',', -6.1832733154296875)\n",
      "(information, ',', -8.817930221557617)\n",
      "(sources, ',', -9.963014602661133)\n",
      "(as, ',', -5.507394313812256)\n",
      "(input, ',', -10.896196365356445)\n",
      "(., ',', -3.0729479789733887)\n",
      "(By, ',', -9.136184692382812)\n",
      "(extract-, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(ing, ',', -19.579313278198242)\n",
      "(the, ',', -3.425445795059204)\n",
      "(non, ',', -8.436274528503418)\n",
      "(-, ',', -5.202415943145752)\n",
      "(linear, ',', -11.651655197143555)\n",
      "(representation, ',', -10.828350067138672)\n",
      "(from, ',', -6.028810501098633)\n",
      "(multiple, ',', -9.72434139251709)\n",
      "(information, ',', -8.817930221557617)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(sources, ',', -9.963014602661133)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(the, ',', -3.425445795059204)\n",
      "(deep, ',', -9.902402877807617)\n",
      "(model, ',', -9.600680351257324)\n",
      "(outperforms, ',', -19.579313278198242)\n",
      "(state, ',', -7.88895320892334)\n",
      "(-, ',', -5.202415943145752)\n",
      "(of, ',', -4.1284637451171875)\n",
      "(-, ',', -5.202415943145752)\n",
      "(the, ',', -3.425445795059204)\n",
      "(-, ',', -5.202415943145752)\n",
      "(art, ',', -9.778430938720703)\n",
      "(by, ',', -6.114920139312744)\n",
      "(up, ',', -6.142737865447998)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(to, ',', -3.83851957321167)\n",
      "(8.6, ',', -19.579313278198242)\n",
      "(percent, ',', -10.030643463134766)\n",
      "(on, ',', -5.263686656951904)\n",
      "(three, ',', -8.800739288330078)\n",
      "(public, ',', -8.376832962036133)\n",
      "(benchmark, ',', -19.579313278198242)\n",
      "(datasets, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(1, ',', -7.901819705963135)\n",
      "(., ',', -3.0729479789733887)\n",
      "(Introduction, ',', -19.579313278198242)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(Human, ',', -11.173282623291016)\n",
      "(pose, ',', -11.87548542022705)\n",
      "(estimation, ',', -19.579313278198242)\n",
      "(is, ',', -4.3297648429870605)\n",
      "(the, ',', -3.425445795059204)\n",
      "(process, ',', -9.035293579101562)\n",
      "(of, ',', -4.1284637451171875)\n",
      "(determining, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(from, ',', -6.028810501098633)\n",
      "(an, ',', -5.953293800354004)\n",
      "(image, ',', -9.559075355529785)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(the, ',', -3.425445795059204)\n",
      "(positions, ',', -10.302310943603516)\n",
      "(of, ',', -4.1284637451171875)\n",
      "(human, ',', -8.437538146972656)\n",
      "(body, ',', -9.267072677612305)\n",
      "(parts, ',', -9.519133567810059)\n",
      "(such, ',', -7.619058609008789)\n",
      "(as, ',', -5.507394313812256)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(the, ',', -3.425445795059204)\n",
      "(head, ',', -8.585274696350098)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(shoulder, ',', -11.331380844116211)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(elbow, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(wrist, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(hip, ',', -11.446206092834473)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(knee, ',', -11.402353286743164)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(and, ',', -4.195279121398926)\n",
      "(ankle, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(It, ',', -5.963693141937256)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(is, ',', -4.3297648429870605)\n",
      "(a, ',', -3.9830753803253174)\n",
      "(fundamental, ',', -10.244829177856445)\n",
      "(problem, ',', -7.622410774230957)\n",
      "(in, ',', -4.585874080657959)\n",
      "(computer, ',', -8.867700576782227)\n",
      "(vision, ',', -11.104988098144531)\n",
      "(and, ',', -4.195279121398926)\n",
      "(has, ',', -6.2021942138671875)\n",
      "(abun-, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(dant, ',', -19.579313278198242)\n",
      "(important, ',', -8.519515037536621)\n",
      "(applications, ',', -10.240983963012695)\n",
      "(such, ',', -7.619058609008789)\n",
      "(as, ',', -5.507394313812256)\n",
      "(sports, ',', -10.700509071350098)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(action, ',', -9.37081241607666)\n",
      "(recogni-, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(tion, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(character, ',', -9.674116134643555)\n",
      "(animation, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(clinical, ',', -19.579313278198242)\n",
      "(analysis, ',', -10.233338356018066)\n",
      "(of, ',', -4.1284637451171875)\n",
      "(gait, ',', -19.579313278198242)\n",
      "(patholo-, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(gies, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(content, ',', -9.405153274536133)\n",
      "(-, ',', -5.202415943145752)\n",
      "(based, ',', -8.318479537963867)\n",
      "(video, ',', -8.32495403289795)\n",
      "(and, ',', -4.195279121398926)\n",
      "(image, ',', -9.559075355529785)\n",
      "(retrieval, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(and, ',', -4.195279121398926)\n",
      "(intelli-, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(gent, ',', -19.579313278198242)\n",
      "(video, ',', -8.32495403289795)\n",
      "(surveillance, ',', -11.74213981628418)\n",
      "(., ',', -3.0729479789733887)\n",
      "(Despite, ',', -11.286718368530273)\n",
      "(many, ',', -7.231371879577637)\n",
      "(years, ',', -7.1562957763671875)\n",
      "(of, ',', -4.1284637451171875)\n",
      "(research, ',', -9.126065254211426)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "([, ',', -5.266753196716309)\n",
      "(52, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(54, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(2, ',', -7.786293029785156)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(40, ',', -9.825430870056152)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(6, ',', -9.090521812438965)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(57, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(56, ',', -19.579313278198242)\n",
      "(], ',', -5.498423099517822)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(pose, ',', -11.87548542022705)\n",
      "(estimation, ',', -19.579313278198242)\n",
      "(remains, ',', -10.507684707641602)\n",
      "(a, ',', -3.9830753803253174)\n",
      "(difﬁ-, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(cult, ',', -11.24658489227295)\n",
      "(problem, ',', -7.622410774230957)\n",
      "(., ',', -3.0729479789733887)\n",
      "(One, ',', -8.563169479370117)\n",
      "(of, ',', -4.1284637451171875)\n",
      "(the, ',', -3.425445795059204)\n",
      "(most, ',', -7.022420406341553)\n",
      "(signiﬁcant, ',', -19.579313278198242)\n",
      "(challenges, ',', -11.772736549377441)\n",
      "(in, ',', -4.585874080657959)\n",
      "(pose, ',', -11.87548542022705)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(estimation, ',', -19.579313278198242)\n",
      "(is, ',', -4.3297648429870605)\n",
      "(how, ',', -6.555723190307617)\n",
      "(to, ',', -3.83851957321167)\n",
      "(model, ',', -9.600680351257324)\n",
      "(the, ',', -3.425445795059204)\n",
      "(complex, ',', -9.884106636047363)\n",
      "(human, ',', -8.437538146972656)\n",
      "(articulation, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(Many, ',', -9.751020431518555)\n",
      "(approaches, ',', -11.651655197143555)\n",
      "(have, ',', -5.235879421234131)\n",
      "(been, ',', -6.6935648918151855)\n",
      "(used, ',', -7.587561130523682)\n",
      "(to, ',', -3.83851957321167)\n",
      "(handle, ',', -9.948587417602539)\n",
      "(the, ',', -3.425445795059204)\n",
      "(com-, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(plex, ',', -19.579313278198242)\n",
      "(human, ',', -8.437538146972656)\n",
      "(articulation, ',', -19.579313278198242)\n",
      "(by, ',', -6.114920139312744)\n",
      "(using, ',', -7.910459041595459)\n",
      "(three, ',', -8.800739288330078)\n",
      "(information, ',', -8.817930221557617)\n",
      "(sources, ',', -9.963014602661133)\n",
      "(:, ',', -6.052439212799072)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(mixture, ',', -19.579313278198242)\n",
      "(type, ',', -8.814929008483887)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(appearance, ',', -11.022025108337402)\n",
      "(score, ',', -10.567034721374512)\n",
      "(and, ',', -4.195279121398926)\n",
      "(deformation, ',', -19.579313278198242)\n",
      "([, ',', -5.266753196716309)\n",
      "(57, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(52, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(54, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(11, ',', -10.191385269165039)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(58, ',', -19.579313278198242)\n",
      "(], ',', -5.498423099517822)\n",
      "(., ',', -3.0729479789733887)\n",
      "(Inﬂuenced, ',', -19.579313278198242)\n",
      "(by, ',', -6.114920139312744)\n",
      "(human, ',', -8.437538146972656)\n",
      "(body, ',', -9.267072677612305)\n",
      "(articulation, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(clothing, ',', -10.838761329650879)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(occlusion, ',', -19.579313278198242)\n",
      "(etc, ',', -8.06252384185791)\n",
      "(., ',', -3.0729479789733887)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(body, ',', -9.267072677612305)\n",
      "(part, ',', -7.779394626617432)\n",
      "(appearance, ',', -11.022025108337402)\n",
      "(varies, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(To, ',', -8.49110221862793)\n",
      "(handle, ',', -9.948587417602539)\n",
      "(this, ',', -5.417123317718506)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(variation, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(the, ',', -3.425445795059204)\n",
      "(appearance, ',', -11.022025108337402)\n",
      "(of, ',', -4.1284637451171875)\n",
      "(a, ',', -3.9830753803253174)\n",
      "(part, ',', -7.779394626617432)\n",
      "(is, ',', -4.3297648429870605)\n",
      "(clustered, ',', -19.579313278198242)\n",
      "(into, ',', -6.929868698120117)\n",
      "(multiple, ',', -9.72434139251709)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(mixture, ',', -19.579313278198242)\n",
      "(types, ',', -9.873420715332031)\n",
      "(as, ',', -5.507394313812256)\n",
      "(shown, ',', -10.03923225402832)\n",
      "(in, ',', -4.585874080657959)\n",
      "(Fig, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(1, ',', -7.901819705963135)\n",
      "(., ',', -3.0729479789733887)\n",
      "(For, ',', -8.115267753601074)\n",
      "(each, ',', -8.295592308044434)\n",
      "(mixture, ',', -19.579313278198242)\n",
      "(type, ',', -8.814929008483887)\n",
      "(of, ',', -4.1284637451171875)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(a, ',', -3.9830753803253174)\n",
      "(part, ',', -7.779394626617432)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(a, ',', -3.9830753803253174)\n",
      "(part, ',', -7.779394626617432)\n",
      "(template, ',', -19.579313278198242)\n",
      "(is, ',', -4.3297648429870605)\n",
      "(learned, ',', -9.666524887084961)\n",
      "(to, ',', -3.83851957321167)\n",
      "(capture, ',', -11.526168823242188)\n",
      "(its, ',', -7.355584621429443)\n",
      "(appearance, ',', -11.022025108337402)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(Then, ',', -8.409794807434082)\n",
      "(the, ',', -3.425445795059204)\n",
      "(appearance, ',', -11.022025108337402)\n",
      "(scores, ',', -11.433479309082031)\n",
      "((, ',', -5.70067024230957)\n",
      "(log, ',', -11.11874008178711)\n",
      "(-, ',', -5.202415943145752)\n",
      "(likelihoods, ',', -19.579313278198242)\n",
      "(), ',', -5.341753005981445)\n",
      "(of, ',', -4.1284637451171875)\n",
      "(body, ',', -9.267072677612305)\n",
      "(parts, ',', -9.519133567810059)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(Figure, ',', -19.579313278198242)\n",
      "(1, ',', -7.901819705963135)\n",
      "(., ',', -3.0729479789733887)\n",
      "(The, ',', -5.774222373962402)\n",
      "(motivation, ',', -10.872464179992676)\n",
      "(of, ',', -4.1284637451171875)\n",
      "(this, ',', -5.417123317718506)\n",
      "(paper, ',', -8.99613094329834)\n",
      "(in, ',', -4.585874080657959)\n",
      "(using, ',', -7.910459041595459)\n",
      "(multi, ',', -10.772948265075684)\n",
      "(-, ',', -5.202415943145752)\n",
      "(source, ',', -8.812856674194336)\n",
      "(deep, ',', -9.902402877807617)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(model, ',', -9.600680351257324)\n",
      "(for, ',', -4.91396951675415)\n",
      "(constructing, ',', -19.579313278198242)\n",
      "(the, ',', -3.425445795059204)\n",
      "(non, ',', -8.436274528503418)\n",
      "(-, ',', -5.202415943145752)\n",
      "(linear, ',', -11.651655197143555)\n",
      "(representation, ',', -10.828350067138672)\n",
      "(from, ',', -6.028810501098633)\n",
      "(three, ',', -8.800739288330078)\n",
      "(in-, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(formation, ',', -19.579313278198242)\n",
      "(sources, ',', -9.963014602661133)\n",
      "(:, ',', -6.052439212799072)\n",
      "(mixture, ',', -19.579313278198242)\n",
      "(type, ',', -8.814929008483887)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(appearance, ',', -11.022025108337402)\n",
      "(score, ',', -10.567034721374512)\n",
      "(and, ',', -4.195279121398926)\n",
      "(deforma-, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(tion, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(Best, ',', -10.38855266571045)\n",
      "(viewed, ',', -11.39624309539795)\n",
      "(in, ',', -4.585874080657959)\n",
      "(color, ',', -9.952893257141113)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(being, ',', -6.890480041503906)\n",
      "(at, ',', -5.8868231773376465)\n",
      "(different, ',', -7.797479629516602)\n",
      "(locations, ',', -11.704129219055176)\n",
      "(are, ',', -5.160149097442627)\n",
      "(obtained, ',', -19.579313278198242)\n",
      "(by, ',', -6.114920139312744)\n",
      "(convolving, ',', -19.579313278198242)\n",
      "(the, ',', -3.425445795059204)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(part, ',', -7.779394626617432)\n",
      "(templates, ',', -19.579313278198242)\n",
      "(with, ',', -5.363764762878418)\n",
      "(the, ',', -3.425445795059204)\n",
      "(visual, ',', -11.303234100341797)\n",
      "(features, ',', -10.029866218566895)\n",
      "(of, ',', -4.1284637451171875)\n",
      "(the, ',', -3.425445795059204)\n",
      "(input, ',', -10.896196365356445)\n",
      "(image, ',', -9.559075355529785)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(e.g., ',', -10.195960998535156)\n",
      "(HOG, ',', -19.579313278198242)\n",
      "([, ',', -5.266753196716309)\n",
      "(7, ',', -9.367201805114746)\n",
      "(], ',', -5.498423099517822)\n",
      "(., ',', -3.0729479789733887)\n",
      "(The, ',', -5.774222373962402)\n",
      "(appearance, ',', -11.022025108337402)\n",
      "(scores, ',', -11.433479309082031)\n",
      "(are, ',', -5.160149097442627)\n",
      "(inaccurate, ',', -11.275856971740723)\n",
      "(for, ',', -4.91396951675415)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(well, ',', -7.117936611175537)\n",
      "(-, ',', -5.202415943145752)\n",
      "(locating, ',', -19.579313278198242)\n",
      "(body, ',', -9.267072677612305)\n",
      "(parts, ',', -9.519133567810059)\n",
      "(because, ',', -6.412496566772461)\n",
      "(the, ',', -3.425445795059204)\n",
      "(part, ',', -7.779394626617432)\n",
      "(template, ',', -19.579313278198242)\n",
      "(is, ',', -4.3297648429870605)\n",
      "(imper-, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(fect, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(Therefore, ',', -10.595346450805664)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(the, ',', -3.425445795059204)\n",
      "(deformations, ',', -19.579313278198242)\n",
      "((, ',', -5.70067024230957)\n",
      "(relative, ',', -10.384114265441895)\n",
      "(locations, ',', -11.704129219055176)\n",
      "(), ',', -5.341753005981445)\n",
      "(among, ',', -9.610336303710938)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(body, ',', -9.267072677612305)\n",
      "(parts, ',', -9.519133567810059)\n",
      "(are, ',', -5.160149097442627)\n",
      "(used, ',', -7.587561130523682)\n",
      "(as, ',', -5.507394313812256)\n",
      "(for, ',', -4.91396951675415)\n",
      "(encoding, ',', -19.579313278198242)\n",
      "(likely, ',', -8.404275894165039)\n",
      "(pairwise, ',', -19.579313278198242)\n",
      "(poses, ',', -19.579313278198242)\n",
      "(;, ',', -6.282994747161865)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(for, ',', -4.91396951675415)\n",
      "(example, ',', -8.316376686096191)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(the, ',', -3.425445795059204)\n",
      "(head, ',', -8.585274696350098)\n",
      "(should, ',', -6.608358860015869)\n",
      "(not, ',', -5.21923828125)\n",
      "(be, ',', -5.210641384124756)\n",
      "(far, ',', -7.867900371551514)\n",
      "(from, ',', -6.028810501098633)\n",
      "(the, ',', -3.425445795059204)\n",
      "(neck, ',', -10.852813720703125)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(Existing, ',', -19.579313278198242)\n",
      "(approaches, ',', -11.651655197143555)\n",
      "(use, ',', -7.119458198547363)\n",
      "(log, ',', -11.11874008178711)\n",
      "(-, ',', -5.202415943145752)\n",
      "(linear, ',', -11.651655197143555)\n",
      "(models, ',', -10.62589168548584)\n",
      "(with, ',', -5.363764762878418)\n",
      "(pairwise, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(potentials, ',', -19.579313278198242)\n",
      "(of, ',', -4.1284637451171875)\n",
      "(these, ',', -7.158668518066406)\n",
      "(three, ',', -8.800739288330078)\n",
      "(information, ',', -8.817930221557617)\n",
      "(sources, ',', -9.963014602661133)\n",
      "([, ',', -5.266753196716309)\n",
      "(52, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(54, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(40, ',', -9.825430870056152)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(57, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(56, ',', -19.579313278198242)\n",
      "(], ',', -5.498423099517822)\n",
      "(to, ',', -3.83851957321167)\n",
      "(determine, ',', -10.438716888427734)\n",
      "(whether, ',', -8.656254768371582)\n",
      "(an, ',', -5.953293800354004)\n",
      "(estimated, ',', -11.822792053222656)\n",
      "(location, ',', -10.667635917663574)\n",
      "(is, ',', -4.3297648429870605)\n",
      "(correct, ',', -8.887826919555664)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(However, ',', -8.649198532104492)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(these, ',', -7.158668518066406)\n",
      "(information, ',', -8.817930221557617)\n",
      "(sources, ',', -9.963014602661133)\n",
      "(are, ',', -5.160149097442627)\n",
      "(not, ',', -5.21923828125)\n",
      "(log, ',', -11.11874008178711)\n",
      "(-, ',', -5.202415943145752)\n",
      "(linearly, ',', -19.579313278198242)\n",
      "(cor-, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(related, ',', -9.537954330444336)\n",
      "(when, ',', -6.408374786376953)\n",
      "(choosing, ',', -10.833541870117188)\n",
      "(the, ',', -3.425445795059204)\n",
      "(correct, ',', -8.887826919555664)\n",
      "(candidate, ',', -8.77760124206543)\n",
      "(., ',', -3.0729479789733887)\n",
      "(For, ',', -8.115267753601074)\n",
      "(the, ',', -3.425445795059204)\n",
      "(exam-, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(ple, ',', -19.579313278198242)\n",
      "(in, ',', -4.585874080657959)\n",
      "(Fig, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(1, ',', -7.901819705963135)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(linear, ',', -11.651655197143555)\n",
      "(models, ',', -10.62589168548584)\n",
      "(may, ',', -7.616482734680176)\n",
      "(ﬁnd, ',', -19.579313278198242)\n",
      "(that, ',', -4.348940372467041)\n",
      "(the, ',', -3.425445795059204)\n",
      "(estimated, ',', -11.822792053222656)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(result, ',', -9.138093948364258)\n",
      "(on, ',', -5.263686656951904)\n",
      "(the, ',', -3.425445795059204)\n",
      "(left, ',', -8.262680053710938)\n",
      "(and, ',', -4.195279121398926)\n",
      "(the, ',', -3.425445795059204)\n",
      "(result, ',', -9.138093948364258)\n",
      "(on, ',', -5.263686656951904)\n",
      "(the, ',', -3.425445795059204)\n",
      "(right, ',', -6.780907154083252)\n",
      "(have, ',', -5.235879421234131)\n",
      "(the, ',', -3.425445795059204)\n",
      "(same, ',', -7.054326057434082)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(deformation, ',', -19.579313278198242)\n",
      "(score, ',', -10.567034721374512)\n",
      "(because, ',', -6.412496566772461)\n",
      "(they, ',', -5.429816246032715)\n",
      "(simply, ',', -8.332181930541992)\n",
      "(linearly, ',', -19.579313278198242)\n",
      "(add, ',', -9.119792938232422)\n",
      "(local, ',', -9.037018775939941)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(deformation, ',', -19.579313278198242)\n",
      "(cost, ',', -8.777823448181152)\n",
      "(., ',', -3.0729479789733887)\n",
      "(While, ',', -9.248764991760254)\n",
      "(it, ',', -4.5064496994018555)\n",
      "(is, ',', -4.3297648429870605)\n",
      "(obvious, ',', -9.122611045837402)\n",
      "(for, ',', -4.91396951675415)\n",
      "(human, ',', -8.437538146972656)\n",
      "(to, ',', -3.83851957321167)\n",
      "(ﬁnd, ',', -19.579313278198242)\n",
      "(that, ',', -4.348940372467041)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(the, ',', -3.425445795059204)\n",
      "(result, ',', -9.138093948364258)\n",
      "(on, ',', -5.263686656951904)\n",
      "(the, ',', -3.425445795059204)\n",
      "(left, ',', -8.262680053710938)\n",
      "(is, ',', -4.3297648429870605)\n",
      "(not, ',', -5.21923828125)\n",
      "(reasonable, ',', -9.505705833435059)\n",
      "(., ',', -3.0729479789733887)\n",
      "(Similar, ',', -19.579313278198242)\n",
      "(situations, ',', -10.536919593811035)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(also, ',', -7.134331226348877)\n",
      "(occur, ',', -10.715816497802734)\n",
      "(for, ',', -4.91396951675415)\n",
      "(mixture, ',', -19.579313278198242)\n",
      "(type, ',', -8.814929008483887)\n",
      "(and, ',', -4.195279121398926)\n",
      "(appearance, ',', -11.022025108337402)\n",
      "(score, ',', -10.567034721374512)\n",
      "(., ',', -3.0729479789733887)\n",
      "(There-, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(fore, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(it, ',', -4.5064496994018555)\n",
      "(is, ',', -4.3297648429870605)\n",
      "(desirable, ',', -19.579313278198242)\n",
      "(to, ',', -3.83851957321167)\n",
      "(construct, ',', -11.540140151977539)\n",
      "(the, ',', -3.425445795059204)\n",
      "(non, ',', -8.436274528503418)\n",
      "(-, ',', -5.202415943145752)\n",
      "(linear, ',', -11.651655197143555)\n",
      "(representation, ',', -10.828350067138672)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(that, ',', -4.348940372467041)\n",
      "(identiﬁes, ',', -19.579313278198242)\n",
      "(reasonable, ',', -9.505705833435059)\n",
      "(conﬁgurations, ',', -19.579313278198242)\n",
      "(of, ',', -4.1284637451171875)\n",
      "(deformation, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(ap-, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(pearance, ',', -19.579313278198242)\n",
      "(score, ',', -10.567034721374512)\n",
      "(and, ',', -4.195279121398926)\n",
      "(mixture, ',', -19.579313278198242)\n",
      "(type, ',', -8.814929008483887)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(In, ',', -7.314908981323242)\n",
      "(order, ',', -8.68863582611084)\n",
      "(to, ',', -3.83851957321167)\n",
      "(construct, ',', -11.540140151977539)\n",
      "(useful, ',', -9.354865074157715)\n",
      "(representation, ',', -10.828350067138672)\n",
      "(from, ',', -6.028810501098633)\n",
      "(multi-, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(ple, ',', -19.579313278198242)\n",
      "(information, ',', -8.817930221557617)\n",
      "(sources, ',', -9.963014602661133)\n",
      "(for, ',', -4.91396951675415)\n",
      "(pose, ',', -11.87548542022705)\n",
      "(estimation, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(a, ',', -3.9830753803253174)\n",
      "(model, ',', -9.600680351257324)\n",
      "(should, ',', -6.608358860015869)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(1, ',', -7.901819705963135)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(Multi, ',', -19.579313278198242)\n",
      "(-, ',', -5.202415943145752)\n",
      "(sourcedeep, ',', -19.579313278198242)\n",
      "(modelEstimated, ',', -19.579313278198242)\n",
      "(resultType, ',', -19.579313278198242)\n",
      "(1Type, ',', -19.579313278198242)\n",
      "(2Non, ',', -19.579313278198242)\n",
      "(-, ',', -5.202415943145752)\n",
      "(Linearmodel?YesNoEstimated, ',', -19.579313278198242)\n",
      "(resultMixture, ',', -19.579313278198242)\n",
      "(typeHead, ',', -19.579313278198242)\n",
      "(topNeckDeformationAppearance, ',', -19.579313278198242)\n",
      "(scoreTemplateLinear, ',', -19.579313278198242)\n",
      "(model1389, ',', -19.579313278198242)\n",
      "(\f",
      ", ',', -19.579313278198242)\n",
      "(satisfy, ',', -11.890351295471191)\n",
      "(certain, ',', -8.85226821899414)\n",
      "(properties, ',', -11.175722122192383)\n",
      "(., ',', -3.0729479789733887)\n",
      "(First, ',', -9.444602012634277)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(the, ',', -3.425445795059204)\n",
      "(model, ',', -9.600680351257324)\n",
      "(should, ',', -6.608358860015869)\n",
      "(capture, ',', -11.526168823242188)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(the, ',', -3.425445795059204)\n",
      "(global, ',', -9.585124015808105)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(complex, ',', -9.884106636047363)\n",
      "(relationships, ',', -11.071413040161133)\n",
      "(among, ',', -9.610336303710938)\n",
      "(body, ',', -9.267072677612305)\n",
      "(parts, ',', -9.519133567810059)\n",
      "(., ',', -3.0729479789733887)\n",
      "(For, ',', -8.115267753601074)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(the, ',', -3.425445795059204)\n",
      "(example, ',', -8.316376686096191)\n",
      "(in, ',', -4.585874080657959)\n",
      "(Fig, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(1, ',', -7.901819705963135)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(the, ',', -3.425445795059204)\n",
      "(result, ',', -9.138093948364258)\n",
      "(on, ',', -5.263686656951904)\n",
      "(the, ',', -3.425445795059204)\n",
      "(left, ',', -8.262680053710938)\n",
      "(is, ',', -4.3297648429870605)\n",
      "(unreasonable, ',', -11.402353286743164)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(because, ',', -6.412496566772461)\n",
      "(of, ',', -4.1284637451171875)\n",
      "(its, ',', -7.355584621429443)\n",
      "(global, ',', -9.585124015808105)\n",
      "(conﬁguration, ',', -19.579313278198242)\n",
      "(in, ',', -4.585874080657959)\n",
      "(arm, ',', -10.667635917663574)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(torso, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(and, ',', -4.195279121398926)\n",
      "(leg, ',', -11.030441284179688)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(Second, ',', -10.421331405639648)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(since, ',', -7.805427551269531)\n",
      "(reasonable, ',', -9.505705833435059)\n",
      "(conﬁguration, ',', -19.579313278198242)\n",
      "(is, ',', -4.3297648429870605)\n",
      "(a, ',', -3.9830753803253174)\n",
      "(very, ',', -7.039885520935059)\n",
      "(abstract, ',', -11.207999229431152)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(concept, ',', -9.54990005493164)\n",
      "(while, ',', -7.631333827972412)\n",
      "(the, ',', -3.425445795059204)\n",
      "(information, ',', -8.817930221557617)\n",
      "(sources, ',', -9.963014602661133)\n",
      "(are, ',', -5.160149097442627)\n",
      "(less, ',', -7.764529705047607)\n",
      "(abstract, ',', -11.207999229431152)\n",
      "(con-, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(cepts, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(the, ',', -3.425445795059204)\n",
      "(model, ',', -9.600680351257324)\n",
      "(should, ',', -6.608358860015869)\n",
      "(construct, ',', -11.540140151977539)\n",
      "(more, ',', -6.0559234619140625)\n",
      "(abstract, ',', -11.207999229431152)\n",
      "(representa-, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(tion, ',', -19.579313278198242)\n",
      "(from, ',', -6.028810501098633)\n",
      "(the, ',', -3.425445795059204)\n",
      "(less, ',', -7.764529705047607)\n",
      "(abstract, ',', -11.207999229431152)\n",
      "(representation, ',', -10.828350067138672)\n",
      "(., ',', -3.0729479789733887)\n",
      "(Third, ',', -11.468875885009766)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(since, ',', -7.805427551269531)\n",
      "(dif-, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(ferent, ',', -19.579313278198242)\n",
      "(information, ',', -8.817930221557617)\n",
      "(sources, ',', -9.963014602661133)\n",
      "(describe, ',', -10.225749969482422)\n",
      "(different, ',', -7.797479629516602)\n",
      "(aspects, ',', -10.912962913513184)\n",
      "(of, ',', -4.1284637451171875)\n",
      "(hu-, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(man, ',', -7.677258014678955)\n",
      "(pose, ',', -11.87548542022705)\n",
      "(and, ',', -4.195279121398926)\n",
      "(have, ',', -5.235879421234131)\n",
      "(different, ',', -7.797479629516602)\n",
      "(statistical, ',', -11.639927864074707)\n",
      "(properties, ',', -11.175722122192383)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(the, ',', -3.425445795059204)\n",
      "(model, ',', -9.600680351257324)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(should, ',', -6.608358860015869)\n",
      "(learn, ',', -8.87330436706543)\n",
      "(useful, ',', -9.354865074157715)\n",
      "(representation, ',', -10.828350067138672)\n",
      "(from, ',', -6.028810501098633)\n",
      "(these, ',', -7.158668518066406)\n",
      "(sources, ',', -9.963014602661133)\n",
      "(and, ',', -4.195279121398926)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(fuse, ',', -19.579313278198242)\n",
      "(them, ',', -6.307651996612549)\n",
      "(into, ',', -6.929868698120117)\n",
      "(a, ',', -3.9830753803253174)\n",
      "(joint, ',', -11.446206092834473)\n",
      "(representation, ',', -10.828350067138672)\n",
      "(for, ',', -4.91396951675415)\n",
      "(pose, ',', -11.87548542022705)\n",
      "(estimation, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(The, ',', -5.774222373962402)\n",
      "(multi, ',', -10.772948265075684)\n",
      "(-, ',', -5.202415943145752)\n",
      "(source, ',', -8.812856674194336)\n",
      "(deep, ',', -9.902402877807617)\n",
      "(architecture, ',', -11.57960033416748)\n",
      "(we, ',', -6.075284481048584)\n",
      "(propose, ',', -11.24658489227295)\n",
      "(satisﬁes, ',', -19.579313278198242)\n",
      "(the, ',', -3.425445795059204)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(above, ',', -8.914969444274902)\n",
      "(requirement, ',', -11.026224136352539)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(There, ',', -7.277902603149414)\n",
      "(are, ',', -5.160149097442627)\n",
      "(three, ',', -8.800739288330078)\n",
      "(contributions, ',', -11.259783744812012)\n",
      "(of, ',', -4.1284637451171875)\n",
      "(this, ',', -5.417123317718506)\n",
      "(paper, ',', -8.99613094329834)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(1, ',', -7.901819705963135)\n",
      "(., ',', -3.0729479789733887)\n",
      "(We, ',', -7.376642227172852)\n",
      "(propose, ',', -11.24658489227295)\n",
      "(a, ',', -3.9830753803253174)\n",
      "(deep, ',', -9.902402877807617)\n",
      "(architecture, ',', -11.57960033416748)\n",
      "(to, ',', -3.83851957321167)\n",
      "(construct, ',', -11.540140151977539)\n",
      "(the, ',', -3.425445795059204)\n",
      "(non-, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(linear, ',', -11.651655197143555)\n",
      "(representation, ',', -10.828350067138672)\n",
      "(from, ',', -6.028810501098633)\n",
      "(different, ',', -7.797479629516602)\n",
      "(aspects, ',', -10.912962913513184)\n",
      "(of, ',', -4.1284637451171875)\n",
      "(informa-, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(tion, ',', -19.579313278198242)\n",
      "(sources, ',', -9.963014602661133)\n",
      "(., ',', -3.0729479789733887)\n",
      "(To, ',', -8.49110221862793)\n",
      "(the, ',', -3.425445795059204)\n",
      "(best, ',', -7.835007667541504)\n",
      "(of, ',', -4.1284637451171875)\n",
      "(our, ',', -7.124547481536865)\n",
      "(knowledge, ',', -9.320651054382324)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(this, ',', -5.417123317718506)\n",
      "(paper, ',', -8.99613094329834)\n",
      "(is, ',', -4.3297648429870605)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(the, ',', -3.425445795059204)\n",
      "(ﬁrst, ',', -19.579313278198242)\n",
      "(to, ',', -3.83851957321167)\n",
      "(use, ',', -7.119458198547363)\n",
      "(deep, ',', -9.902402877807617)\n",
      "(model, ',', -9.600680351257324)\n",
      "(for, ',', -4.91396951675415)\n",
      "(pose, ',', -11.87548542022705)\n",
      "(estimation, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(2, ',', -7.786293029785156)\n",
      "(., ',', -3.0729479789733887)\n",
      "(The, ',', -5.774222373962402)\n",
      "(body, ',', -9.267072677612305)\n",
      "(articulation, ',', -19.579313278198242)\n",
      "(patterns, ',', -11.080256462097168)\n",
      "((, ',', -5.70067024230957)\n",
      "(global, ',', -9.585124015808105)\n",
      "(and, ',', -4.195279121398926)\n",
      "(more, ',', -6.0559234619140625)\n",
      "(abstract, ',', -11.207999229431152)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(representations, ',', -19.579313278198242)\n",
      "(), ',', -5.341753005981445)\n",
      "(are, ',', -5.160149097442627)\n",
      "(captured, ',', -19.579313278198242)\n",
      "(by, ',', -6.114920139312744)\n",
      "(the, ',', -3.425445795059204)\n",
      "(deep, ',', -9.902402877807617)\n",
      "(model, ',', -9.600680351257324)\n",
      "(from, ',', -6.028810501098633)\n",
      "(the, ',', -3.425445795059204)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(information, ',', -8.817930221557617)\n",
      "(sources, ',', -9.963014602661133)\n",
      "((, ',', -5.70067024230957)\n",
      "(local, ',', -9.037018775939941)\n",
      "(and, ',', -4.195279121398926)\n",
      "(less, ',', -7.764529705047607)\n",
      "(abstract, ',', -11.207999229431152)\n",
      "(representa-, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(tions, ',', -19.579313278198242)\n",
      "(), ',', -5.341753005981445)\n",
      "(., ',', -3.0729479789733887)\n",
      "(For, ',', -8.115267753601074)\n",
      "(each, ',', -8.295592308044434)\n",
      "(information, ',', -8.817930221557617)\n",
      "(source, ',', -8.812856674194336)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(more, ',', -6.0559234619140625)\n",
      "(abstract, ',', -11.207999229431152)\n",
      "(repre-, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(sentation, ',', -19.579313278198242)\n",
      "(at, ',', -5.8868231773376465)\n",
      "(the, ',', -3.425445795059204)\n",
      "(higher, ',', -9.007245063781738)\n",
      "(layer, ',', -11.220696449279785)\n",
      "(is, ',', -4.3297648429870605)\n",
      "(composed, ',', -19.579313278198242)\n",
      "(by, ',', -6.114920139312744)\n",
      "(the, ',', -3.425445795059204)\n",
      "(less, ',', -7.764529705047607)\n",
      "(ab-, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(stract, ',', -19.579313278198242)\n",
      "(representation, ',', -10.828350067138672)\n",
      "(of, ',', -4.1284637451171875)\n",
      "(all, ',', -5.896712779998779)\n",
      "(body, ',', -9.267072677612305)\n",
      "(parts, ',', -9.519133567810059)\n",
      "(in, ',', -4.585874080657959)\n",
      "(the, ',', -3.425445795059204)\n",
      "(lower, ',', -9.25090217590332)\n",
      "(layer, ',', -11.220696449279785)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(Then, ',', -8.409794807434082)\n",
      "(representations, ',', -19.579313278198242)\n",
      "(of, ',', -4.1284637451171875)\n",
      "(all, ',', -5.896712779998779)\n",
      "(information, ',', -8.817930221557617)\n",
      "(sources, ',', -9.963014602661133)\n",
      "(in, ',', -4.585874080657959)\n",
      "(the, ',', -3.425445795059204)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(higher, ',', -9.007245063781738)\n",
      "(layer, ',', -11.220696449279785)\n",
      "(are, ',', -5.160149097442627)\n",
      "(fused, ',', -19.579313278198242)\n",
      "(for, ',', -4.91396951675415)\n",
      "(pose, ',', -11.87548542022705)\n",
      "(estimation, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(3, ',', -8.093857765197754)\n",
      "(., ',', -3.0729479789733887)\n",
      "(Both, ',', -9.805371284484863)\n",
      "(the, ',', -3.425445795059204)\n",
      "(task, ',', -10.82490348815918)\n",
      "(for, ',', -4.91396951675415)\n",
      "(detecting, ',', -19.579313278198242)\n",
      "(human, ',', -8.437538146972656)\n",
      "(and, ',', -4.195279121398926)\n",
      "(the, ',', -3.425445795059204)\n",
      "(task, ',', -10.82490348815918)\n",
      "(for, ',', -4.91396951675415)\n",
      "(esti-, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(mating, ',', -19.579313278198242)\n",
      "(body, ',', -9.267072677612305)\n",
      "(locations, ',', -11.704129219055176)\n",
      "(are, ',', -5.160149097442627)\n",
      "(jointly, ',', -19.579313278198242)\n",
      "(learned, ',', -9.666524887084961)\n",
      "(using, ',', -7.910459041595459)\n",
      "(a, ',', -3.9830753803253174)\n",
      "(single, ',', -8.629281997680664)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(deep, ',', -9.902402877807617)\n",
      "(model, ',', -9.600680351257324)\n",
      "(., ',', -3.0729479789733887)\n",
      "(Joint, ',', -19.579313278198242)\n",
      "(learning, ',', -9.830510139465332)\n",
      "(of, ',', -4.1284637451171875)\n",
      "(these, ',', -7.158668518066406)\n",
      "(tasks, ',', -11.48538875579834)\n",
      "(with, ',', -5.363764762878418)\n",
      "(a, ',', -3.9830753803253174)\n",
      "(shared, ',', -10.801104545593262)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(representation, ',', -10.828350067138672)\n",
      "(improves, ',', -19.579313278198242)\n",
      "(pose, ',', -11.87548542022705)\n",
      "(estimation, ',', -19.579313278198242)\n",
      "(accuracy, ',', -11.345757484436035)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(2, ',', -7.786293029785156)\n",
      "(., ',', -3.0729479789733887)\n",
      "(Related, ',', -19.579313278198242)\n",
      "(work, ',', -7.171552658081055)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(Human, ',', -11.173282623291016)\n",
      "(pose, ',', -11.87548542022705)\n",
      "(estimation, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(Pose, ',', -19.579313278198242)\n",
      "(estimation, ',', -19.579313278198242)\n",
      "(is, ',', -4.3297648429870605)\n",
      "(considered, ',', -9.297229766845703)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(as, ',', -5.507394313812256)\n",
      "(holistic, ',', -19.579313278198242)\n",
      "(recognition, ',', -11.375144958496094)\n",
      "(in, ',', -4.585874080657959)\n",
      "([, ',', -5.266753196716309)\n",
      "(15, ',', -9.510315895080566)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(33, ',', -11.822792053222656)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(34, ',', -11.746454238891602)\n",
      "(], ',', -5.498423099517822)\n",
      "(., ',', -3.0729479789733887)\n",
      "(On, ',', -8.899799346923828)\n",
      "(the, ',', -3.425445795059204)\n",
      "(other, ',', -6.661053657531738)\n",
      "(hand, ',', -8.514552116394043)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(many, ',', -7.231371879577637)\n",
      "(recent, ',', -9.695014953613281)\n",
      "(works, ',', -8.482634544372559)\n",
      "(use, ',', -7.119458198547363)\n",
      "(local, ',', -9.037018775939941)\n",
      "(body, ',', -9.267072677612305)\n",
      "(parts, ',', -9.519133567810059)\n",
      "([, ',', -5.266753196716309)\n",
      "(52, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(54, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(11, ',', -10.191385269165039)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(58, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(9, ',', -9.879417419433594)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(13, ',', -10.399734497070312)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(48, ',', -11.763898849487305)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(40, ',', -9.825430870056152)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(2, ',', -7.786293029785156)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(42, ',', -11.540140151977539)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(21, ',', -11.032556533813477)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(46, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(55, ',', -11.763898849487305)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(41, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(1, ',', -7.901819705963135)\n",
      "(], ',', -5.498423099517822)\n",
      "(in, ',', -4.585874080657959)\n",
      "(order, ',', -8.68863582611084)\n",
      "(to, ',', -3.83851957321167)\n",
      "(handle, ',', -9.948587417602539)\n",
      "(the, ',', -3.425445795059204)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(many, ',', -7.231371879577637)\n",
      "(degrees, ',', -10.885173797607422)\n",
      "(of, ',', -4.1284637451171875)\n",
      "(freedom, ',', -9.18703842163086)\n",
      "(in, ',', -4.585874080657959)\n",
      "(body, ',', -9.267072677612305)\n",
      "(part, ',', -7.779394626617432)\n",
      "(articulation, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(Since, ',', -9.441577911376953)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(the, ',', -3.425445795059204)\n",
      "(ﬁrst, ',', -19.579313278198242)\n",
      "(work, ',', -7.171552658081055)\n",
      "(in, ',', -4.585874080657959)\n",
      "([, ',', -5.266753196716309)\n",
      "(57, ',', -19.579313278198242)\n",
      "(], ',', -5.498423099517822)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(some, ',', -6.4027814865112305)\n",
      "(approaches, ',', -11.651655197143555)\n",
      "([, ',', -5.266753196716309)\n",
      "(52, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(54, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(11, ',', -10.191385269165039)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(58, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(9, ',', -9.879417419433594)\n",
      "(], ',', -5.498423099517822)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(have, ',', -5.235879421234131)\n",
      "(clustered, ',', -19.579313278198242)\n",
      "(part, ',', -7.779394626617432)\n",
      "(appearance, ',', -11.022025108337402)\n",
      "(into, ',', -6.929868698120117)\n",
      "(mixture, ',', -19.579313278198242)\n",
      "(types, ',', -9.873420715332031)\n",
      "(as, ',', -5.507394313812256)\n",
      "(shown, ',', -10.03923225402832)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(in, ',', -4.585874080657959)\n",
      "(Fig, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(1, ',', -7.901819705963135)\n",
      "(., ',', -3.0729479789733887)\n",
      "(There, ',', -7.277902603149414)\n",
      "(are, ',', -5.160149097442627)\n",
      "(also, ',', -7.134331226348877)\n",
      "(approaches, ',', -11.651655197143555)\n",
      "(that, ',', -4.348940372467041)\n",
      "(warp, ',', -19.579313278198242)\n",
      "(the, ',', -3.425445795059204)\n",
      "(part, ',', -7.779394626617432)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(template, ',', -19.579313278198242)\n",
      "(by, ',', -6.114920139312744)\n",
      "(ﬂexible, ',', -19.579313278198242)\n",
      "(sizes, ',', -19.579313278198242)\n",
      "(and, ',', -4.195279121398926)\n",
      "(orientations, ',', -19.579313278198242)\n",
      "([, ',', -5.266753196716309)\n",
      "(13, ',', -10.399734497070312)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(48, ',', -11.763898849487305)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(40, ',', -9.825430870056152)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(2, ',', -7.786293029785156)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(42, ',', -11.540140151977539)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(21, ',', -11.032556533813477)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(46, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(55, ',', -11.763898849487305)\n",
      "(], ',', -5.498423099517822)\n",
      "(., ',', -3.0729479789733887)\n",
      "(The, ',', -5.774222373962402)\n",
      "(appearance, ',', -11.022025108337402)\n",
      "(score, ',', -10.567034721374512)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(rotation, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(size, ',', -9.519598960876465)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(and, ',', -4.195279121398926)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(location, ',', -10.667635917663574)\n",
      "(used, ',', -7.587561130523682)\n",
      "(in, ',', -4.585874080657959)\n",
      "(these, ',', -7.158668518066406)\n",
      "(approaches, ',', -11.651655197143555)\n",
      "(can, ',', -5.9138712882995605)\n",
      "(be, ',', -5.210641384124756)\n",
      "(treated, ',', -10.202402114868164)\n",
      "(as, ',', -5.507394313812256)\n",
      "(multiple, ',', -9.72434139251709)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(information, ',', -8.817930221557617)\n",
      "(sources, ',', -9.963014602661133)\n",
      "(and, ',', -4.195279121398926)\n",
      "(used, ',', -7.587561130523682)\n",
      "(by, ',', -6.114920139312744)\n",
      "(our, ',', -7.124547481536865)\n",
      "(deep, ',', -9.902402877807617)\n",
      "(model, ',', -9.600680351257324)\n",
      "(for, ',', -4.91396951675415)\n",
      "(pose, ',', -11.87548542022705)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(estimation, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(In, ',', -7.314908981323242)\n",
      "(existing, ',', -10.217281341552734)\n",
      "(pose, ',', -11.87548542022705)\n",
      "(estimation, ',', -19.579313278198242)\n",
      "(approaches, ',', -11.651655197143555)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(the, ',', -3.425445795059204)\n",
      "(pair, ',', -10.901754379272461)\n",
      "(-, ',', -5.202415943145752)\n",
      "(wise, ',', -10.53305721282959)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(part, ',', -7.779394626617432)\n",
      "(deformation, ',', -19.579313278198242)\n",
      "(relationships, ',', -11.071413040161133)\n",
      "(are, ',', -5.160149097442627)\n",
      "(arranged, ',', -19.579313278198242)\n",
      "(in, ',', -4.585874080657959)\n",
      "(tree, ',', -10.449295997619629)\n",
      "(models, ',', -10.62589168548584)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "([, ',', -5.266753196716309)\n",
      "(52, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(54, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(2, ',', -7.786293029785156)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(40, ',', -9.825430870056152)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(57, ',', -19.579313278198242)\n",
      "(], ',', -5.498423099517822)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(multi, ',', -10.772948265075684)\n",
      "(-, ',', -5.202415943145752)\n",
      "(tree, ',', -10.449295997619629)\n",
      "(model, ',', -9.600680351257324)\n",
      "([, ',', -5.266753196716309)\n",
      "(55, ',', -11.763898849487305)\n",
      "(], ',', -5.498423099517822)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(or, ',', -5.715355396270752)\n",
      "(loopy, ',', -19.579313278198242)\n",
      "(mod-, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(els, ',', -19.579313278198242)\n",
      "([, ',', -5.266753196716309)\n",
      "(56, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(53, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(10, ',', -8.452024459838867)\n",
      "(], ',', -5.498423099517822)\n",
      "(., ',', -3.0729479789733887)\n",
      "(Tree, ',', -19.579313278198242)\n",
      "(models, ',', -10.62589168548584)\n",
      "(allow, ',', -9.234637260437012)\n",
      "(for, ',', -4.91396951675415)\n",
      "(efﬁcient, ',', -19.579313278198242)\n",
      "(and, ',', -4.195279121398926)\n",
      "(exact, ',', -9.701706886291504)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(inference, ',', -19.579313278198242)\n",
      "(but, ',', -5.525960445404053)\n",
      "(are, ',', -5.160149097442627)\n",
      "(insufﬁcient, ',', -19.579313278198242)\n",
      "(in, ',', -4.585874080657959)\n",
      "(modeling, ',', -19.579313278198242)\n",
      "(the, ',', -3.425445795059204)\n",
      "(complex, ',', -9.884106636047363)\n",
      "(re-, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(lationships, ',', -19.579313278198242)\n",
      "(among, ',', -9.610336303710938)\n",
      "(body, ',', -9.267072677612305)\n",
      "(parts, ',', -9.519133567810059)\n",
      "(., ',', -3.0729479789733887)\n",
      "(Hence, ',', -11.294941902160645)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(tree, ',', -10.449295997619629)\n",
      "(models, ',', -10.62589168548584)\n",
      "(often, ',', -8.611258506774902)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(suffer, ',', -10.446934700012207)\n",
      "(from, ',', -6.028810501098633)\n",
      "(double, ',', -9.830510139465332)\n",
      "(counting, ',', -10.761580467224121)\n",
      "(;, ',', -6.282994747161865)\n",
      "(for, ',', -4.91396951675415)\n",
      "(example, ',', -8.316376686096191)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(given, ',', -8.653700828552246)\n",
      "(the, ',', -3.425445795059204)\n",
      "(posi-, ',', -19.579313278198242)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(tion, ',', -19.579313278198242)\n",
      "(of, ',', -4.1284637451171875)\n",
      "(a, ',', -3.9830753803253174)\n",
      "(torso, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(the, ',', -3.425445795059204)\n",
      "(positions, ',', -10.302310943603516)\n",
      "(of, ',', -4.1284637451171875)\n",
      "(two, ',', -7.494669437408447)\n",
      "(legs, ',', -10.912962913513184)\n",
      "(are, ',', -5.160149097442627)\n",
      "(independent, ',', -10.099842071533203)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(and, ',', -4.195279121398926)\n",
      "(often, ',', -8.611258506774902)\n",
      "(respond, ',', -10.414461135864258)\n",
      "(to, ',', -3.83851957321167)\n",
      "(the, ',', -3.425445795059204)\n",
      "(same, ',', -7.054326057434082)\n",
      "(visual, ',', -11.303234100341797)\n",
      "(cue, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(Loopy, ',', -19.579313278198242)\n",
      "(models, ',', -10.62589168548584)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(allow, ',', -9.234637260437012)\n",
      "(more, ',', -6.0559234619140625)\n",
      "(complex, ',', -9.884106636047363)\n",
      "(relationships, ',', -11.071413040161133)\n",
      "(among, ',', -9.610336303710938)\n",
      "(parts, ',', -9.519133567810059)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(but, ',', -5.525960445404053)\n",
      "(require, ',', -9.662212371826172)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(approximate, ',', -19.579313278198242)\n",
      "(inference, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(Our, ',', -9.764631271362305)\n",
      "(deep, ',', -9.902402877807617)\n",
      "(architecture, ',', -11.57960033416748)\n",
      "(models, ',', -10.62589168548584)\n",
      "(the, ',', -3.425445795059204)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(complex, ',', -9.884106636047363)\n",
      "(relationships, ',', -11.071413040161133)\n",
      "(among, ',', -9.610336303710938)\n",
      "(parts, ',', -9.519133567810059)\n",
      "(and, ',', -4.195279121398926)\n",
      "(is, ',', -4.3297648429870605)\n",
      "(computationally, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(efﬁcient, ',', -19.579313278198242)\n",
      "(in, ',', -4.585874080657959)\n",
      "(both, ',', -7.8158979415893555)\n",
      "(training, ',', -10.32292366027832)\n",
      "(and, ',', -4.195279121398926)\n",
      "(testing, ',', -10.620267868041992)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(Deep, ',', -19.579313278198242)\n",
      "(learning, ',', -9.830510139465332)\n",
      "(., ',', -3.0729479789733887)\n",
      "(Since, ',', -9.441577911376953)\n",
      "(the, ',', -3.425445795059204)\n",
      "(breakthrough, ',', -19.579313278198242)\n",
      "(in, ',', -4.585874080657959)\n",
      "(deep, ',', -9.902402877807617)\n",
      "(learning, ',', -9.830510139465332)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(initiated, ',', -19.579313278198242)\n",
      "(by, ',', -6.114920139312744)\n",
      "(G., ',', -19.579313278198242)\n",
      "(Hinton, ',', -19.579313278198242)\n",
      "(in, ',', -4.585874080657959)\n",
      "([, ',', -5.266753196716309)\n",
      "(18, ',', -10.092371940612793)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(19, ',', -10.888833999633789)\n",
      "(], ',', -5.498423099517822)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(deep, ',', -9.902402877807617)\n",
      "(learning, ',', -9.830510139465332)\n",
      "(is, ',', -4.3297648429870605)\n",
      "(gain-, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(ing, ',', -19.579313278198242)\n",
      "(more, ',', -6.0559234619140625)\n",
      "(and, ',', -4.195279121398926)\n",
      "(more, ',', -6.0559234619140625)\n",
      "(attention, ',', -9.24485969543457)\n",
      "(., ',', -3.0729479789733887)\n",
      "(Bengio, ',', -19.579313278198242)\n",
      "([, ',', -5.266753196716309)\n",
      "(3, ',', -8.093857765197754)\n",
      "(], ',', -5.498423099517822)\n",
      "(proved, ',', -11.100445747375488)\n",
      "(that, ',', -4.348940372467041)\n",
      "(exist-, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(ing, ',', -19.579313278198242)\n",
      "(commonly, ',', -11.223255157470703)\n",
      "(used, ',', -7.587561130523682)\n",
      "(machine, ',', -9.18136978149414)\n",
      "(learning, ',', -9.830510139465332)\n",
      "(tools, ',', -10.08004379272461)\n",
      "(such, ',', -7.619058609008789)\n",
      "(as, ',', -5.507394313812256)\n",
      "(SVM, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(and, ',', -4.195279121398926)\n",
      "(Boosting, ',', -19.579313278198242)\n",
      "(are, ',', -5.160149097442627)\n",
      "(shallow, ',', -11.557884216308594)\n",
      "(models, ',', -10.62589168548584)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(and, ',', -4.195279121398926)\n",
      "(they, ',', -5.429816246032715)\n",
      "(may, ',', -7.616482734680176)\n",
      "(require, ',', -9.662212371826172)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(many, ',', -7.231371879577637)\n",
      "(more, ',', -6.0559234619140625)\n",
      "(computational, ',', -19.579313278198242)\n",
      "(elements, ',', -10.812932968139648)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(potentially, ',', -10.667635917663574)\n",
      "(exponen-, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(tially, ',', -19.579313278198242)\n",
      "(more, ',', -6.0559234619140625)\n",
      "((, ',', -5.70067024230957)\n",
      "(with, ',', -5.363764762878418)\n",
      "(respect, ',', -9.290532112121582)\n",
      "(to, ',', -3.83851957321167)\n",
      "(input, ',', -10.896196365356445)\n",
      "(size, ',', -9.519598960876465)\n",
      "(), ',', -5.341753005981445)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(than, ',', -6.372464179992676)\n",
      "(deep, ',', -9.902402877807617)\n",
      "(models, ',', -10.62589168548584)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(whose, ',', -9.930137634277344)\n",
      "(depth, ',', -10.966999053955078)\n",
      "(is, ',', -4.3297648429870605)\n",
      "(matched, ',', -19.579313278198242)\n",
      "(to, ',', -3.83851957321167)\n",
      "(the, ',', -3.425445795059204)\n",
      "(task, ',', -10.82490348815918)\n",
      "(., ',', -3.0729479789733887)\n",
      "(Deep, ',', -19.579313278198242)\n",
      "(architecture, ',', -11.57960033416748)\n",
      "(is, ',', -4.3297648429870605)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(found, ',', -8.413644790649414)\n",
      "(to, ',', -3.83851957321167)\n",
      "(yield, ',', -11.616876602172852)\n",
      "(better, ',', -7.226652145385742)\n",
      "(data, ',', -9.084769248962402)\n",
      "(representation, ',', -10.828350067138672)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(for, ',', -4.91396951675415)\n",
      "(example, ',', -8.316376686096191)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(in, ',', -4.585874080657959)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(terms, ',', -9.22973918914795)\n",
      "(of, ',', -4.1284637451171875)\n",
      "(classiﬁcation, ',', -19.579313278198242)\n",
      "(error, ',', -9.988028526306152)\n",
      "([, ',', -5.266753196716309)\n",
      "(25, ',', -10.009870529174805)\n",
      "(], ',', -5.498423099517822)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(invariance, ',', -19.579313278198242)\n",
      "(to, ',', -3.83851957321167)\n",
      "(input, ',', -10.896196365356445)\n",
      "(trans-, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(formations, ',', -19.579313278198242)\n",
      "([, ',', -5.266753196716309)\n",
      "(16, ',', -10.35681438446045)\n",
      "(], ',', -5.498423099517822)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(or, ',', -5.715355396270752)\n",
      "(modeling, ',', -19.579313278198242)\n",
      "(multi, ',', -10.772948265075684)\n",
      "(-, ',', -5.202415943145752)\n",
      "(modal, ',', -19.579313278198242)\n",
      "(data, ',', -9.084769248962402)\n",
      "([, ',', -5.266753196716309)\n",
      "(35, ',', -11.098182678222656)\n",
      "(], ',', -5.498423099517822)\n",
      "(., ',', -3.0729479789733887)\n",
      "(Deep, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(learning, ',', -9.830510139465332)\n",
      "(has, ',', -6.2021942138671875)\n",
      "(achieved, ',', -11.594344139099121)\n",
      "(spectacular, ',', -19.579313278198242)\n",
      "(progress, ',', -10.53049087524414)\n",
      "(in, ',', -4.585874080657959)\n",
      "(computer, ',', -8.867700576782227)\n",
      "(vi-, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(sion, ',', -19.579313278198242)\n",
      "([, ',', -5.266753196716309)\n",
      "(45, ',', -11.121050834655762)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(20, ',', -8.932963371276855)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(26, ',', -11.492071151733398)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(36, ',', -11.679567337036133)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(23, ',', -11.210525512695312)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(59, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(12, ',', -9.68836784362793)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(43, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(39, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(50, ',', -9.218977928161621)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(49, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(60, ',', -10.115797996520996)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(38, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(37, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(30, ',', -9.207648277282715)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(32, ',', -11.498798370361328)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(31, ',', -11.86569595336914)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(29, ',', -11.733567237854004)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(61, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(28, ',', -11.241353034973145)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(51, ',', -11.86569595336914)\n",
      "(], ',', -5.498423099517822)\n",
      "(., ',', -3.0729479789733887)\n",
      "(Recent, ',', -19.579313278198242)\n",
      "(progress, ',', -10.53049087524414)\n",
      "(on, ',', -5.263686656951904)\n",
      "(deep, ',', -9.902402877807617)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(learning, ',', -9.830510139465332)\n",
      "(is, ',', -4.3297648429870605)\n",
      "(reviewed, ',', -11.885371208190918)\n",
      "(in, ',', -4.585874080657959)\n",
      "([, ',', -5.266753196716309)\n",
      "(4, ',', -8.51557731628418)\n",
      "(], ',', -5.498423099517822)\n",
      "(., ',', -3.0729479789733887)\n",
      "(Krizhevsky, ',', -19.579313278198242)\n",
      "(et, ',', -11.39320182800293)\n",
      "(al, ',', -10.772948265075684)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "([, ',', -5.266753196716309)\n",
      "(23, ',', -11.210525512695312)\n",
      "(], ',', -5.498423099517822)\n",
      "(pro-, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(posed, ',', -19.579313278198242)\n",
      "(a, ',', -3.9830753803253174)\n",
      "(large, ',', -8.6246976852417)\n",
      "(-, ',', -5.202415943145752)\n",
      "(scale, ',', -9.841387748718262)\n",
      "(deep, ',', -9.902402877807617)\n",
      "(convolutional, ',', -19.579313278198242)\n",
      "(network, ',', -9.909953117370605)\n",
      "([, ',', -5.266753196716309)\n",
      "(27, ',', -11.449413299560547)\n",
      "(], ',', -5.498423099517822)\n",
      "(with, ',', -5.363764762878418)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(breakthrough, ',', -19.579313278198242)\n",
      "(on, ',', -5.263686656951904)\n",
      "(the, ',', -3.425445795059204)\n",
      "(large, ',', -8.6246976852417)\n",
      "(-, ',', -5.202415943145752)\n",
      "(scale, ',', -9.841387748718262)\n",
      "(ImageNet, ',', -19.579313278198242)\n",
      "(object, ',', -10.152877807617188)\n",
      "(recogni-, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(tion, ',', -19.579313278198242)\n",
      "(dataset, ',', -19.579313278198242)\n",
      "([, ',', -5.266753196716309)\n",
      "(8, ',', -8.982955932617188)\n",
      "(], ',', -5.498423099517822)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(attaining, ',', -19.579313278198242)\n",
      "(a, ',', -3.9830753803253174)\n",
      "(signiﬁcant, ',', -19.579313278198242)\n",
      "(gap, ',', -11.24658489227295)\n",
      "(compared, ',', -9.660063743591309)\n",
      "(with, ',', -5.363764762878418)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(existing, ',', -10.217281341552734)\n",
      "(approaches, ',', -11.651655197143555)\n",
      "(that, ',', -4.348940372467041)\n",
      "(use, ',', -7.119458198547363)\n",
      "(shallow, ',', -11.557884216308594)\n",
      "(models, ',', -10.62589168548584)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(and, ',', -4.195279121398926)\n",
      "(bringing, ',', -10.469587326049805)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(high, ',', -8.075313568115234)\n",
      "(impact, ',', -10.168817520141602)\n",
      "(to, ',', -3.83851957321167)\n",
      "(research, ',', -9.126065254211426)\n",
      "(in, ',', -4.585874080657959)\n",
      "(computer, ',', -8.867700576782227)\n",
      "(vision, ',', -11.104988098144531)\n",
      "(., ',', -3.0729479789733887)\n",
      "(Our, ',', -9.764631271362305)\n",
      "(approaches, ',', -11.651655197143555)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(in, ',', -4.585874080657959)\n",
      "([, ',', -5.266753196716309)\n",
      "(38, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(39, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(37, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(29, ',', -11.733567237854004)\n",
      "(], ',', -5.498423099517822)\n",
      "(learns, ',', -19.579313278198242)\n",
      "(feature, ',', -9.824798583984375)\n",
      "(learning, ',', -9.830510139465332)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(translational, ',', -19.579313278198242)\n",
      "(de-, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(formation, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(and, ',', -4.195279121398926)\n",
      "(occlusion, ',', -19.579313278198242)\n",
      "(relationship, ',', -9.987283706665039)\n",
      "(in, ',', -4.585874080657959)\n",
      "(pedestrian, ',', -19.579313278198242)\n",
      "(detec-, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(tion, ',', -19.579313278198242)\n",
      "(;, ',', -6.282994747161865)\n",
      "(the, ',', -3.425445795059204)\n",
      "(approach, ',', -9.994755744934082)\n",
      "(in, ',', -4.585874080657959)\n",
      "([, ',', -5.266753196716309)\n",
      "(50, ',', -9.218977928161621)\n",
      "(], ',', -5.498423099517822)\n",
      "(learns, ',', -19.579313278198242)\n",
      "(relational, ',', -19.579313278198242)\n",
      "(ﬁlter, ',', -19.579313278198242)\n",
      "(pairs, ',', -19.579313278198242)\n",
      "(in, ',', -4.585874080657959)\n",
      "(face, ',', -8.649394035339355)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(veriﬁcation, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(To, ',', -8.49110221862793)\n",
      "(the, ',', -3.425445795059204)\n",
      "(best, ',', -7.835007667541504)\n",
      "(of, ',', -4.1284637451171875)\n",
      "(our, ',', -7.124547481536865)\n",
      "(knowledge, ',', -9.320651054382324)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(however, ',', -8.975079536437988)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(deep, ',', -9.902402877807617)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(model, ',', -9.600680351257324)\n",
      "(for, ',', -4.91396951675415)\n",
      "(human, ',', -8.437538146972656)\n",
      "(pose, ',', -11.87548542022705)\n",
      "(estimation, ',', -19.579313278198242)\n",
      "(has, ',', -6.2021942138671875)\n",
      "(not, ',', -5.21923828125)\n",
      "(yet, ',', -8.25146484375)\n",
      "(been, ',', -6.6935648918151855)\n",
      "(explored, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(Our, ',', -9.764631271362305)\n",
      "(work, ',', -7.171552658081055)\n",
      "(is, ',', -4.3297648429870605)\n",
      "(inspired, ',', -11.320027351379395)\n",
      "(by, ',', -6.114920139312744)\n",
      "(multi, ',', -10.772948265075684)\n",
      "(-, ',', -5.202415943145752)\n",
      "(modality, ',', -19.579313278198242)\n",
      "(models, ',', -10.62589168548584)\n",
      "(that, ',', -4.348940372467041)\n",
      "(learn, ',', -8.87330436706543)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(from, ',', -6.028810501098633)\n",
      "(multiple, ',', -9.72434139251709)\n",
      "(modalities, ',', -19.579313278198242)\n",
      "(such, ',', -7.619058609008789)\n",
      "(as, ',', -5.507394313812256)\n",
      "(audio, ',', -10.80447006225586)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(visual, ',', -11.303234100341797)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(text, ',', -9.430851936340332)\n",
      "(data, ',', -9.084769248962402)\n",
      "([, ',', -5.266753196716309)\n",
      "(35, ',', -11.098182678222656)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(47, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(17, ',', -10.712736129760742)\n",
      "(], ',', -5.498423099517822)\n",
      "(., ',', -3.0729479789733887)\n",
      "(In, ',', -7.314908981323242)\n",
      "(contrast, ',', -11.093670845031738)\n",
      "(to, ',', -3.83851957321167)\n",
      "(these, ',', -7.158668518066406)\n",
      "(works, ',', -8.482634544372559)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(we, ',', -6.075284481048584)\n",
      "(investigate, ',', -11.695874214172363)\n",
      "(multi-, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(source, ',', -8.812856674194336)\n",
      "(learning, ',', -9.830510139465332)\n",
      "(from, ',', -6.028810501098633)\n",
      "(single, ',', -8.629281997680664)\n",
      "(modality, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(which, ',', -6.728941917419434)\n",
      "(is, ',', -4.3297648429870605)\n",
      "(image, ',', -9.559075355529785)\n",
      "(data, ',', -9.084769248962402)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(in, ',', -4.585874080657959)\n",
      "(pose, ',', -11.87548542022705)\n",
      "(estimation, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(3, ',', -8.093857765197754)\n",
      "(., ',', -3.0729479789733887)\n",
      "(Pictorial, ',', -19.579313278198242)\n",
      "(structure, ',', -10.26526165008545)\n",
      "(model, ',', -9.600680351257324)\n",
      "(for, ',', -4.91396951675415)\n",
      "(pose, ',', -11.87548542022705)\n",
      "(estimation, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(The, ',', -5.774222373962402)\n",
      "(model, ',', -9.600680351257324)\n",
      "(introduced, ',', -11.038928031921387)\n",
      "(in, ',', -4.585874080657959)\n",
      "(this, ',', -5.417123317718506)\n",
      "(section, ',', -10.175085067749023)\n",
      "(is, ',', -4.3297648429870605)\n",
      "(used, ',', -7.587561130523682)\n",
      "(to, ',', -3.83851957321167)\n",
      "(provide, ',', -9.187707901000977)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(our, ',', -7.124547481536865)\n",
      "(deep, ',', -9.902402877807617)\n",
      "(model, ',', -9.600680351257324)\n",
      "(with, ',', -5.363764762878418)\n",
      "(information, ',', -8.817930221557617)\n",
      "(sources, ',', -9.963014602661133)\n",
      "(., ',', -3.0729479789733887)\n",
      "(Pictorial, ',', -19.579313278198242)\n",
      "(struc-, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(ture, ',', -19.579313278198242)\n",
      "(model, ',', -9.600680351257324)\n",
      "(considers, ',', -11.885371208190918)\n",
      "(human, ',', -8.437538146972656)\n",
      "(body, ',', -9.267072677612305)\n",
      "(parts, ',', -9.519133567810059)\n",
      "(as, ',', -5.507394313812256)\n",
      "(nodes, ',', -19.579313278198242)\n",
      "(tied, ',', -10.708133697509766)\n",
      "(to-, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(gether, ',', -19.579313278198242)\n",
      "(in, ',', -4.585874080657959)\n",
      "(a, ',', -3.9830753803253174)\n",
      "(conditional, ',', -19.579313278198242)\n",
      "(random, ',', -9.599669456481934)\n",
      "(ﬁeld, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(Let, ',', -8.81032943725586)\n",
      "(lp, ',', -19.579313278198242)\n",
      "(for, ',', -4.91396951675415)\n",
      "(p, ',', -19.579313278198242)\n",
      "(=, ',', -8.43548583984375)\n",
      "(1, ',', -7.901819705963135)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(., ',', -3.0729479789733887)\n",
      "(., ',', -3.0729479789733887)\n",
      "(., ',', -3.0729479789733887)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(P, ',', -11.836889266967773)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(be, ',', -5.210641384124756)\n",
      "(the, ',', -3.425445795059204)\n",
      "(conﬁguration, ',', -19.579313278198242)\n",
      "(of, ',', -4.1284637451171875)\n",
      "(the, ',', -3.425445795059204)\n",
      "(pth, ',', -19.579313278198242)\n",
      "(part, ',', -7.779394626617432)\n",
      "(., ',', -3.0729479789733887)\n",
      "(The, ',', -5.774222373962402)\n",
      "(posterior, ',', -19.579313278198242)\n",
      "(of, ',', -4.1284637451171875)\n",
      "(a, ',', -3.9830753803253174)\n",
      "(con-, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(ﬁguration, ',', -19.579313278198242)\n",
      "(of, ',', -4.1284637451171875)\n",
      "(parts, ',', -9.519133567810059)\n",
      "(L, ',', -19.579313278198242)\n",
      "(=, ',', -8.43548583984375)\n",
      "({, ',', -10.464776039123535)\n",
      "(lp|p, ',', -19.579313278198242)\n",
      "(=, ',', -8.43548583984375)\n",
      "(1, ',', -7.901819705963135)\n",
      "(., ',', -3.0729479789733887)\n",
      "(., ',', -3.0729479789733887)\n",
      "(., ',', -3.0729479789733887)\n",
      "(P, ',', -11.836889266967773)\n",
      "(}, ',', -10.305375099182129)\n",
      "(given, ',', -8.653700828552246)\n",
      "(an, ',', -5.953293800354004)\n",
      "(image, ',', -9.559075355529785)\n",
      "(I, ',', -4.064180850982666)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(is, ',', -4.3297648429870605)\n",
      "(:, ',', -6.052439212799072)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(P, ',', -11.836889266967773)\n",
      "((, ',', -5.70067024230957)\n",
      "(L|I, ',', -19.579313278198242)\n",
      "(), ',', -5.341753005981445)\n",
      "(∝, ',', -19.579313278198242)\n",
      "(exp(cid:0, ',', -19.579313278198242)\n",
      "(), ',', -5.341753005981445)\n",
      "(P(cid:88, ',', -19.579313278198242)\n",
      "(), ',', -5.341753005981445)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "((, ',', -5.70067024230957)\n",
      "(cid:88, ',', -19.579313278198242)\n",
      "(), ',', -5.341753005981445)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(ψ(lp, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(lq)(cid:1, ',', -19.579313278198242)\n",
      "(), ',', -5.341753005981445)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "((, ',', -5.70067024230957)\n",
      "(1, ',', -7.901819705963135)\n",
      "(), ',', -5.341753005981445)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(φ(I|lp, ',', -19.579313278198242)\n",
      "(), ',', -5.341753005981445)\n",
      "(), ',', -5.341753005981445)\n",
      "(+, ',', -9.560532569885254)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(p=1, ',', -19.579313278198242)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "((, ',', -5.70067024230957)\n",
      "(p, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(q)∈E, ',', -19.579313278198242)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(ψ(lp, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(lq, ',', -19.579313278198242)\n",
      "(), ',', -5.341753005981445)\n",
      "(is, ',', -4.3297648429870605)\n",
      "(the, ',', -3.425445795059204)\n",
      "(pair, ',', -10.901754379272461)\n",
      "(-, ',', -5.202415943145752)\n",
      "(wise, ',', -10.53305721282959)\n",
      "(term, ',', -8.676319122314453)\n",
      "(that, ',', -4.348940372467041)\n",
      "(models, ',', -10.62589168548584)\n",
      "(the, ',', -3.425445795059204)\n",
      "(geometric, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(deformation, ',', -19.579313278198242)\n",
      "(constraint, ',', -19.579313278198242)\n",
      "(on, ',', -5.263686656951904)\n",
      "(the, ',', -3.425445795059204)\n",
      "(pth, ',', -19.579313278198242)\n",
      "(and, ',', -4.195279121398926)\n",
      "(qth, ',', -19.579313278198242)\n",
      "(parts, ',', -9.519133567810059)\n",
      "(;, ',', -6.282994747161865)\n",
      "(for, ',', -4.91396951675415)\n",
      "(exam-, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(ple, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(head, ',', -8.585274696350098)\n",
      "(shall, ',', -9.74282455444336)\n",
      "(not, ',', -5.21923828125)\n",
      "(be, ',', -5.210641384124756)\n",
      "(too, ',', -6.93158483505249)\n",
      "(far, ',', -7.867900371551514)\n",
      "(from, ',', -6.028810501098633)\n",
      "(torso, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(The, ',', -5.774222373962402)\n",
      "(edge, ',', -10.553829193115234)\n",
      "(set, ',', -8.572531700134277)\n",
      "(de-, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(noted, ',', -11.04319953918457)\n",
      "(by, ',', -6.114920139312744)\n",
      "(E, ',', -10.991082191467285)\n",
      "(is, ',', -4.3297648429870605)\n",
      "(arranged, ',', -19.579313278198242)\n",
      "(in, ',', -4.585874080657959)\n",
      "(tree, ',', -10.449295997619629)\n",
      "(models, ',', -10.62589168548584)\n",
      "([, ',', -5.266753196716309)\n",
      "(52, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(54, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(2, ',', -7.786293029785156)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(40, ',', -9.825430870056152)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(57, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(11, ',', -10.191385269165039)\n",
      "(], ',', -5.498423099517822)\n",
      "(\n",
      "\n",
      "\f",
      ", ',', -19.579313278198242)\n",
      "(or, ',', -5.715355396270752)\n",
      "(loopy, ',', -19.579313278198242)\n",
      "(models, ',', -10.62589168548584)\n",
      "([, ',', -5.266753196716309)\n",
      "(56, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(10, ',', -8.452024459838867)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(53, ',', -19.579313278198242)\n",
      "(], ',', -5.498423099517822)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(φ(I|lp, ',', -19.579313278198242)\n",
      "(), ',', -5.341753005981445)\n",
      "(is, ',', -4.3297648429870605)\n",
      "(the, ',', -3.425445795059204)\n",
      "(unary, ',', -19.579313278198242)\n",
      "(term, ',', -8.676319122314453)\n",
      "(that, ',', -4.348940372467041)\n",
      "(models, ',', -10.62589168548584)\n",
      "(the, ',', -3.425445795059204)\n",
      "(appearance, ',', -11.022025108337402)\n",
      "(of, ',', -4.1284637451171875)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(lp, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(The, ',', -5.774222373962402)\n",
      "(appearance, ',', -11.022025108337402)\n",
      "(varies, ',', -19.579313278198242)\n",
      "(as, ',', -5.507394313812256)\n",
      "(body, ',', -9.267072677612305)\n",
      "(articulates, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(To, ',', -8.49110221862793)\n",
      "(model, ',', -9.600680351257324)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(this, ',', -5.417123317718506)\n",
      "(variation, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(lp, ',', -19.579313278198242)\n",
      "(=, ',', -8.43548583984375)\n",
      "({, ',', -10.464776039123535)\n",
      "(sp, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(θp, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(zp, ',', -19.579313278198242)\n",
      "(}, ',', -10.305375099182129)\n",
      "(and, ',', -4.195279121398926)\n",
      "(φ(I|lp, ',', -19.579313278198242)\n",
      "(), ',', -5.341753005981445)\n",
      "(speciﬁes, ',', -19.579313278198242)\n",
      "(the, ',', -3.425445795059204)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(part, ',', -7.779394626617432)\n",
      "(appearance, ',', -11.022025108337402)\n",
      "(warped, ',', -19.579313278198242)\n",
      "(by, ',', -6.114920139312744)\n",
      "(size, ',', -9.519598960876465)\n",
      "(sp, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(orientation, ',', -19.579313278198242)\n",
      "(θp, ',', -19.579313278198242)\n",
      "(at, ',', -5.8868231773376465)\n",
      "(loca-, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(tion, ',', -19.579313278198242)\n",
      "(zp, ',', -19.579313278198242)\n",
      "(in, ',', -4.585874080657959)\n",
      "([, ',', -5.266753196716309)\n",
      "(2, ',', -7.786293029785156)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(40, ',', -9.825430870056152)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(13, ',', -10.399734497070312)\n",
      "(], ',', -5.498423099517822)\n",
      "(., ',', -3.0729479789733887)\n",
      "(Alternatively, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(Yang, ',', -19.579313278198242)\n",
      "(and, ',', -4.195279121398926)\n",
      "(Ramanan, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(propose, ',', -11.24658489227295)\n",
      "(to, ',', -3.83851957321167)\n",
      "(use, ',', -7.119458198547363)\n",
      "(appearance, ',', -11.022025108337402)\n",
      "(mixture, ',', -19.579313278198242)\n",
      "(type, ',', -8.814929008483887)\n",
      "(tp, ',', -19.579313278198242)\n",
      "(for, ',', -4.91396951675415)\n",
      "(approximat-, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(ing, ',', -19.579313278198242)\n",
      "(the, ',', -3.425445795059204)\n",
      "(variation, ',', -19.579313278198242)\n",
      "(in, ',', -4.585874080657959)\n",
      "(rotation, ',', -19.579313278198242)\n",
      "(θp, ',', -19.579313278198242)\n",
      "(and, ',', -4.195279121398926)\n",
      "(size, ',', -9.519598960876465)\n",
      "(sp, ',', -19.579313278198242)\n",
      "(in, ',', -4.585874080657959)\n",
      "([, ',', -5.266753196716309)\n",
      "(57, ',', -19.579313278198242)\n",
      "(], ',', -5.498423099517822)\n",
      "(., ',', -3.0729479789733887)\n",
      "(In, ',', -7.314908981323242)\n",
      "(this, ',', -5.417123317718506)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(model, ',', -9.600680351257324)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(lp, ',', -19.579313278198242)\n",
      "(=, ',', -8.43548583984375)\n",
      "({, ',', -10.464776039123535)\n",
      "(tp, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(zp, ',', -19.579313278198242)\n",
      "(}, ',', -10.305375099182129)\n",
      "(and, ',', -4.195279121398926)\n",
      "(φ(I|lp, ',', -19.579313278198242)\n",
      "(), ',', -5.341753005981445)\n",
      "(speciﬁes, ',', -19.579313278198242)\n",
      "(the, ',', -3.425445795059204)\n",
      "(part, ',', -7.779394626617432)\n",
      "(appear-, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(ance, ',', -19.579313278198242)\n",
      "(with, ',', -5.363764762878418)\n",
      "(mixture, ',', -19.579313278198242)\n",
      "(type, ',', -8.814929008483887)\n",
      "(tp, ',', -19.579313278198242)\n",
      "(at, ',', -5.8868231773376465)\n",
      "(location, ',', -10.667635917663574)\n",
      "(zp, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(The, ',', -5.774222373962402)\n",
      "(appearance, ',', -11.022025108337402)\n",
      "(of, ',', -4.1284637451171875)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(a, ',', -3.9830753803253174)\n",
      "(part, ',', -7.779394626617432)\n",
      "(is, ',', -4.3297648429870605)\n",
      "(clustered, ',', -19.579313278198242)\n",
      "(into, ',', -6.929868698120117)\n",
      "(multiple, ',', -9.72434139251709)\n",
      "(appearance, ',', -11.022025108337402)\n",
      "(mixture, ',', -19.579313278198242)\n",
      "(types, ',', -9.873420715332031)\n",
      "(as, ',', -5.507394313812256)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(shown, ',', -10.03923225402832)\n",
      "(in, ',', -4.585874080657959)\n",
      "(Fig, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(1, ',', -7.901819705963135)\n",
      "(., ',', -3.0729479789733887)\n",
      "(The, ',', -5.774222373962402)\n",
      "(overall, ',', -10.242904663085938)\n",
      "(model, ',', -9.600680351257324)\n",
      "(in, ',', -4.585874080657959)\n",
      "([, ',', -5.266753196716309)\n",
      "(57, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(58, ',', -19.579313278198242)\n",
      "(], ',', -5.498423099517822)\n",
      "(is, ',', -4.3297648429870605)\n",
      "(as, ',', -5.507394313812256)\n",
      "(follows, ',', -11.009532928466797)\n",
      "(:, ',', -6.052439212799072)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(P, ',', -11.836889266967773)\n",
      "((, ',', -5.70067024230957)\n",
      "(L|I, ',', -19.579313278198242)\n",
      "(), ',', -5.341753005981445)\n",
      "(∝, ',', -19.579313278198242)\n",
      "(exp(cid:0)S(I, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(t, ',', -10.763195991516113)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(z)(cid:1, ',', -19.579313278198242)\n",
      "(), ',', -5.341753005981445)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "((, ',', -5.70067024230957)\n",
      "(cid:88, ',', -19.579313278198242)\n",
      "(), ',', -5.341753005981445)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "((, ',', -5.70067024230957)\n",
      "(cid:88, ',', -19.579313278198242)\n",
      "(), ',', -5.341753005981445)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "((, ',', -5.70067024230957)\n",
      "(cid:88, ',', -19.579313278198242)\n",
      "(), ',', -5.341753005981445)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(p, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(q, ',', -19.579313278198242)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(Sc(t, ',', -19.579313278198242)\n",
      "(), ',', -5.341753005981445)\n",
      "(=, ',', -8.43548583984375)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(btp, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(p, ',', -19.579313278198242)\n",
      "(+, ',', -9.560532569885254)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(btp, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(tq, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(p, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(q, ',', -19.579313278198242)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(where, ',', -7.183883190155029)\n",
      "(S(I, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(t, ',', -10.763195991516113)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(z, ',', -19.579313278198242)\n",
      "(), ',', -5.341753005981445)\n",
      "(=, ',', -8.43548583984375)\n",
      "(Sc(t, ',', -19.579313278198242)\n",
      "(), ',', -5.341753005981445)\n",
      "(+, ',', -9.560532569885254)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(Sd(t, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(z, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(p, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(q, ',', -19.579313278198242)\n",
      "(), ',', -5.341753005981445)\n",
      "(+, ',', -9.560532569885254)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "((, ',', -5.70067024230957)\n",
      "(cid:88, ',', -19.579313278198242)\n",
      "(), ',', -5.341753005981445)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "((, ',', -5.70067024230957)\n",
      "(2, ',', -7.786293029785156)\n",
      "(), ',', -5.341753005981445)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(Sa(I, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(tp, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(zp, ',', -19.579313278198242)\n",
      "(), ',', -5.341753005981445)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "((, ',', -5.70067024230957)\n",
      "(3, ',', -8.093857765197754)\n",
      "(), ',', -5.341753005981445)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "((, ',', -5.70067024230957)\n",
      "(4, ',', -8.51557731628418)\n",
      "(), ',', -5.341753005981445)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(p, ',', -19.579313278198242)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(p, ',', -19.579313278198242)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(p, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(q, ',', -19.579313278198242)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(T, ',', -10.680953979492188)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(d(zp, ',', -19.579313278198242)\n",
      "(−, ',', -19.579313278198242)\n",
      "(zq, ',', -19.579313278198242)\n",
      "(), ',', -5.341753005981445)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(Sd(t, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(z, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(p, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(q, ',', -19.579313278198242)\n",
      "(), ',', -5.341753005981445)\n",
      "(=, ',', -8.43548583984375)\n",
      "(wtp, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(tq, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(p, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(q, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(T, ',', -10.680953979492188)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(Sa(I, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(tp, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(zp, ',', -19.579313278198242)\n",
      "(), ',', -5.341753005981445)\n",
      "(=, ',', -8.43548583984375)\n",
      "(wtp, ',', -19.579313278198242)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(p, ',', -19.579313278198242)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(f, ',', -11.259783744812012)\n",
      "((, ',', -5.70067024230957)\n",
      "(I, ',', -4.064180850982666)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(zp, ',', -19.579313278198242)\n",
      "(), ',', -5.341753005981445)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(compatibility, ',', -11.6917724609375)\n",
      "(/, ',', -9.364402770996094)\n",
      "(co, ',', -10.561731338500977)\n",
      "(-, ',', -5.202415943145752)\n",
      "(occurrence, ',', -19.579313278198242)\n",
      "(of, ',', -4.1284637451171875)\n",
      "(mixture, ',', -19.579313278198242)\n",
      "(types, ',', -9.873420715332031)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "((, ',', -5.70067024230957)\n",
      "(5, ',', -8.421856880187988)\n",
      "(), ',', -5.341753005981445)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(•, ',', -19.579313278198242)\n",
      "(Sc(t, ',', -19.579313278198242)\n",
      "(), ',', -5.341753005981445)\n",
      "(is, ',', -4.3297648429870605)\n",
      "(the, ',', -3.425445795059204)\n",
      "(pair, ',', -10.901754379272461)\n",
      "(-, ',', -5.202415943145752)\n",
      "(wise, ',', -10.53305721282959)\n",
      "(compatibility, ',', -11.6917724609375)\n",
      "(term, ',', -8.676319122314453)\n",
      "(that, ',', -4.348940372467041)\n",
      "(models, ',', -10.62589168548584)\n",
      "(the, ',', -3.425445795059204)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(•, ',', -19.579313278198242)\n",
      "(Sd(t, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(z, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(p, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(q, ',', -19.579313278198242)\n",
      "(), ',', -5.341753005981445)\n",
      "(is, ',', -4.3297648429870605)\n",
      "(the, ',', -3.425445795059204)\n",
      "(pair, ',', -10.901754379272461)\n",
      "(-, ',', -5.202415943145752)\n",
      "(wise, ',', -10.53305721282959)\n",
      "(deformation, ',', -19.579313278198242)\n",
      "(term, ',', -8.676319122314453)\n",
      "(that, ',', -4.348940372467041)\n",
      "(mod-, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(els, ',', -19.579313278198242)\n",
      "(the, ',', -3.425445795059204)\n",
      "(geometric, ',', -19.579313278198242)\n",
      "(deformation, ',', -19.579313278198242)\n",
      "(constraints, ',', -19.579313278198242)\n",
      "(on, ',', -5.263686656951904)\n",
      "(the, ',', -3.425445795059204)\n",
      "(pth, ',', -19.579313278198242)\n",
      "(and, ',', -4.195279121398926)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(qth, ',', -19.579313278198242)\n",
      "(parts, ',', -9.519133567810059)\n",
      "(., ',', -3.0729479789733887)\n",
      "(d(zp, ',', -19.579313278198242)\n",
      "(−, ',', -19.579313278198242)\n",
      "(zq, ',', -19.579313278198242)\n",
      "(), ',', -5.341753005981445)\n",
      "(=, ',', -8.43548583984375)\n",
      "([, ',', -5.266753196716309)\n",
      "(dx, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(dy, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(dx2dy2, ',', -19.579313278198242)\n",
      "(], ',', -5.498423099517822)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(T., ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(•, ',', -19.579313278198242)\n",
      "(Sa(I, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(tp, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(zp, ',', -19.579313278198242)\n",
      "(), ',', -5.341753005981445)\n",
      "(is, ',', -4.3297648429870605)\n",
      "(the, ',', -3.425445795059204)\n",
      "(unary, ',', -19.579313278198242)\n",
      "(appearance, ',', -11.022025108337402)\n",
      "(term, ',', -8.676319122314453)\n",
      "(that, ',', -4.348940372467041)\n",
      "(computes, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(p, ',', -19.579313278198242)\n",
      "(at, ',', -5.8868231773376465)\n",
      "(location, ',', -10.667635917663574)\n",
      "(zp, ',', -19.579313278198242)\n",
      "(of, ',', -4.1284637451171875)\n",
      "(the, ',', -3.425445795059204)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(the, ',', -3.425445795059204)\n",
      "(score, ',', -10.567034721374512)\n",
      "(of, ',', -4.1284637451171875)\n",
      "(placing, ',', -19.579313278198242)\n",
      "(a, ',', -3.9830753803253174)\n",
      "(template, ',', -19.579313278198242)\n",
      "(wtp, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(HOG, ',', -19.579313278198242)\n",
      "(feature, ',', -9.824798583984375)\n",
      "(map, ',', -10.397488594055176)\n",
      "(for, ',', -4.91396951675415)\n",
      "(image, ',', -9.559075355529785)\n",
      "(I, ',', -4.064180850982666)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(denoted, ',', -19.579313278198242)\n",
      "(by, ',', -6.114920139312744)\n",
      "(f, ',', -11.259783744812012)\n",
      "((, ',', -5.70067024230957)\n",
      "(I, ',', -4.064180850982666)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(zp, ',', -19.579313278198242)\n",
      "(), ',', -5.341753005981445)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(p, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(wtp, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(tq, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(Linear, ',', -19.579313278198242)\n",
      "(SVM, ',', -19.579313278198242)\n",
      "(is, ',', -4.3297648429870605)\n",
      "(used, ',', -7.587561130523682)\n",
      "(to, ',', -3.83851957321167)\n",
      "(learn, ',', -8.87330436706543)\n",
      "(the, ',', -3.425445795059204)\n",
      "(linear, ',', -11.651655197143555)\n",
      "(weights, ',', -19.579313278198242)\n",
      "(wtp, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(p, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(q, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(and, ',', -4.195279121398926)\n",
      "(compatibility, ',', -11.6917724609375)\n",
      "(biases, ',', -19.579313278198242)\n",
      "(btp, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(p, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(q, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(The, ',', -5.774222373962402)\n",
      "(model, ',', -9.600680351257324)\n",
      "(in, ',', -4.585874080657959)\n",
      "(Eq.(2)-(5, ',', -19.579313278198242)\n",
      "(), ',', -5.341753005981445)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(is, ',', -4.3297648429870605)\n",
      "(used, ',', -7.587561130523682)\n",
      "(in, ',', -4.585874080657959)\n",
      "(many, ',', -7.231371879577637)\n",
      "(approaches, ',', -11.651655197143555)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(with, ',', -5.363764762878418)\n",
      "(different, ',', -7.797479629516602)\n",
      "(implementations, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(on, ',', -5.263686656951904)\n",
      "(edge, ',', -10.553829193115234)\n",
      "(set, ',', -8.572531700134277)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(part, ',', -7.779394626617432)\n",
      "(size, ',', -9.519598960876465)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(and, ',', -4.195279121398926)\n",
      "(part, ',', -7.779394626617432)\n",
      "(locations, ',', -11.704129219055176)\n",
      "([, ',', -5.266753196716309)\n",
      "(52, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(54, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(57, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(10, ',', -8.452024459838867)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(11, ',', -10.191385269165039)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(58, ',', -19.579313278198242)\n",
      "(], ',', -5.498423099517822)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(p, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(btp, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(tq, ',', -19.579313278198242)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(4, ',', -8.51557731628418)\n",
      "(., ',', -3.0729479789733887)\n",
      "(The, ',', -5.774222373962402)\n",
      "(multi, ',', -10.772948265075684)\n",
      "(-, ',', -5.202415943145752)\n",
      "(source, ',', -8.812856674194336)\n",
      "(deep, ',', -9.902402877807617)\n",
      "(model, ',', -9.600680351257324)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(An, ',', -9.560046195983887)\n",
      "(overview, ',', -19.579313278198242)\n",
      "(of, ',', -4.1284637451171875)\n",
      "(our, ',', -7.124547481536865)\n",
      "(framework, ',', -11.075824737548828)\n",
      "(in, ',', -4.585874080657959)\n",
      "(the, ',', -3.425445795059204)\n",
      "(testing, ',', -10.620267868041992)\n",
      "(stage, ',', -10.263298034667969)\n",
      "(is, ',', -4.3297648429870605)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(shown, ',', -10.03923225402832)\n",
      "(in, ',', -4.585874080657959)\n",
      "(Fig, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(2, ',', -7.786293029785156)\n",
      "(., ',', -3.0729479789733887)\n",
      "(In, ',', -7.314908981323242)\n",
      "(this, ',', -5.417123317718506)\n",
      "(framework, ',', -11.075824737548828)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(an, ',', -5.953293800354004)\n",
      "(existing, ',', -10.217281341552734)\n",
      "(approach, ',', -9.994755744934082)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(is, ',', -4.3297648429870605)\n",
      "(used, ',', -7.587561130523682)\n",
      "(to, ',', -3.83851957321167)\n",
      "(generate, ',', -10.840506553649902)\n",
      "(candidate, ',', -8.77760124206543)\n",
      "(body, ',', -9.267072677612305)\n",
      "(locations, ',', -11.704129219055176)\n",
      "(with, ',', -5.363764762878418)\n",
      "(conserva-, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(tive, ',', -19.579313278198242)\n",
      "(thresholding, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(In, ',', -7.314908981323242)\n",
      "(the, ',', -3.425445795059204)\n",
      "(experiment, ',', -10.579070091247559)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(the, ',', -3.425445795059204)\n",
      "(existing, ',', -10.217281341552734)\n",
      "(approach, ',', -9.994755744934082)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(is, ',', -4.3297648429870605)\n",
      "(the, ',', -3.425445795059204)\n",
      "(off, ',', -7.239304065704346)\n",
      "(-, ',', -5.202415943145752)\n",
      "(the, ',', -3.425445795059204)\n",
      "(-, ',', -5.202415943145752)\n",
      "(shelf, ',', -19.579313278198242)\n",
      "(approach, ',', -9.994755744934082)\n",
      "(in, ',', -4.585874080657959)\n",
      "([, ',', -5.266753196716309)\n",
      "(58, ',', -19.579313278198242)\n",
      "(], ',', -5.498423099517822)\n",
      "(., ',', -3.0729479789733887)\n",
      "(A, ',', -7.381028175354004)\n",
      "(multi, ',', -10.772948265075684)\n",
      "(-, ',', -5.202415943145752)\n",
      "(source, ',', -8.812856674194336)\n",
      "(deep, ',', -9.902402877807617)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(model, ',', -9.600680351257324)\n",
      "(is, ',', -4.3297648429870605)\n",
      "(then, ',', -6.67340612411499)\n",
      "(applied, ',', -10.576382637023926)\n",
      "(to, ',', -3.83851957321167)\n",
      "(a, ',', -3.9830753803253174)\n",
      "(candidate, ',', -8.77760124206543)\n",
      "(of, ',', -4.1284637451171875)\n",
      "(all, ',', -5.896712779998779)\n",
      "(body, ',', -9.267072677612305)\n",
      "(locations, ',', -11.704129219055176)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(in, ',', -4.585874080657959)\n",
      "(order, ',', -8.68863582611084)\n",
      "(to, ',', -3.83851957321167)\n",
      "(determine, ',', -10.438716888427734)\n",
      "(whether, ',', -8.656254768371582)\n",
      "(its, ',', -7.355584621429443)\n",
      "(body, ',', -9.267072677612305)\n",
      "(locations, ',', -11.704129219055176)\n",
      "(are, ',', -5.160149097442627)\n",
      "(correct, ',', -8.887826919555664)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(Simultaneously, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(the, ',', -3.425445795059204)\n",
      "(body, ',', -9.267072677612305)\n",
      "(locations, ',', -11.704129219055176)\n",
      "(of, ',', -4.1284637451171875)\n",
      "(this, ',', -5.417123317718506)\n",
      "(candidate, ',', -8.77760124206543)\n",
      "(is, ',', -4.3297648429870605)\n",
      "(esti-, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(mated, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(One, ',', -8.563169479370117)\n",
      "(direct, ',', -9.74691390991211)\n",
      "(approach, ',', -9.994755744934082)\n",
      "(with, ',', -5.363764762878418)\n",
      "(which, ',', -6.728941917419434)\n",
      "(to, ',', -3.83851957321167)\n",
      "(train, ',', -10.436381340026855)\n",
      "(a, ',', -3.9830753803253174)\n",
      "(multi, ',', -10.772948265075684)\n",
      "(-, ',', -5.202415943145752)\n",
      "(source, ',', -8.812856674194336)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(model, ',', -9.600680351257324)\n",
      "(is, ',', -4.3297648429870605)\n",
      "(to, ',', -3.83851957321167)\n",
      "(train, ',', -10.436381340026855)\n",
      "(a, ',', -3.9830753803253174)\n",
      "(deep, ',', -9.902402877807617)\n",
      "(model, ',', -9.600680351257324)\n",
      "(over, ',', -7.045919418334961)\n",
      "(the, ',', -3.425445795059204)\n",
      "(concatenated, ',', -19.579313278198242)\n",
      "(infor-, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(mation, ',', -19.579313278198242)\n",
      "(sources, ',', -9.963014602661133)\n",
      "(as, ',', -5.507394313812256)\n",
      "(shown, ',', -10.03923225402832)\n",
      "(in, ',', -4.585874080657959)\n",
      "(Fig, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(3(a, ',', -19.579313278198242)\n",
      "(), ',', -5.341753005981445)\n",
      "(., ',', -3.0729479789733887)\n",
      "(This, ',', -6.785318851470947)\n",
      "(approach, ',', -9.994755744934082)\n",
      "(is, ',', -4.3297648429870605)\n",
      "(lim-, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(ited, ',', -19.579313278198242)\n",
      "(because, ',', -6.412496566772461)\n",
      "(information, ',', -8.817930221557617)\n",
      "(sources, ',', -9.963014602661133)\n",
      "(with, ',', -5.363764762878418)\n",
      "(different, ',', -7.797479629516602)\n",
      "(statistical, ',', -11.639927864074707)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(properties, ',', -11.175722122192383)\n",
      "(are, ',', -5.160149097442627)\n",
      "(mixed, ',', -10.821468353271484)\n",
      "(in, ',', -4.585874080657959)\n",
      "(the, ',', -3.425445795059204)\n",
      "(ﬁrst, ',', -19.579313278198242)\n",
      "(hidden, ',', -10.728233337402344)\n",
      "(layer, ',', -11.220696449279785)\n",
      "(., ',', -3.0729479789733887)\n",
      "(A, ',', -7.381028175354004)\n",
      "(better, ',', -7.226652145385742)\n",
      "(so-, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(lution, ',', -19.579313278198242)\n",
      "(is, ',', -4.3297648429870605)\n",
      "(to, ',', -3.83851957321167)\n",
      "(have, ',', -5.235879421234131)\n",
      "(their, ',', -6.1832733154296875)\n",
      "(high, ',', -8.075313568115234)\n",
      "(-, ',', -5.202415943145752)\n",
      "(level, ',', -8.576519012451172)\n",
      "(representations, ',', -19.579313278198242)\n",
      "(constructed, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(before, ',', -7.46220588684082)\n",
      "(they, ',', -5.429816246032715)\n",
      "(are, ',', -5.160149097442627)\n",
      "(mixed, ',', -10.821468353271484)\n",
      "(., ',', -3.0729479789733887)\n",
      "(Therefore, ',', -10.595346450805664)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(we, ',', -6.075284481048584)\n",
      "(use, ',', -7.119458198547363)\n",
      "(the, ',', -3.425445795059204)\n",
      "(architecture, ',', -11.57960033416748)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(as, ',', -5.507394313812256)\n",
      "(shown, ',', -10.03923225402832)\n",
      "(in, ',', -4.585874080657959)\n",
      "(Fig, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(3(b, ',', -19.579313278198242)\n",
      "(), ',', -5.341753005981445)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(in, ',', -4.585874080657959)\n",
      "(which, ',', -6.728941917419434)\n",
      "(each, ',', -8.295592308044434)\n",
      "(information, ',', -8.817930221557617)\n",
      "(source, ',', -8.812856674194336)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(Figure, ',', -19.579313278198242)\n",
      "(2, ',', -7.786293029785156)\n",
      "(., ',', -3.0729479789733887)\n",
      "(Framework, ',', -19.579313278198242)\n",
      "(in, ',', -4.585874080657959)\n",
      "(the, ',', -3.425445795059204)\n",
      "(testing, ',', -10.620267868041992)\n",
      "(stage, ',', -10.263298034667969)\n",
      "(., ',', -3.0729479789733887)\n",
      "(The, ',', -5.774222373962402)\n",
      "(existing, ',', -10.217281341552734)\n",
      "(approach, ',', -9.994755744934082)\n",
      "(is, ',', -4.3297648429870605)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(used, ',', -7.587561130523682)\n",
      "(to, ',', -3.83851957321167)\n",
      "(generate, ',', -10.840506553649902)\n",
      "(multiple, ',', -9.72434139251709)\n",
      "(candidate, ',', -8.77760124206543)\n",
      "(locations, ',', -11.704129219055176)\n",
      "(., ',', -3.0729479789733887)\n",
      "(A, ',', -7.381028175354004)\n",
      "(candidate, ',', -8.77760124206543)\n",
      "(is, ',', -4.3297648429870605)\n",
      "(used, ',', -7.587561130523682)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(as, ',', -5.507394313812256)\n",
      "(the, ',', -3.425445795059204)\n",
      "(input, ',', -10.896196365356445)\n",
      "(to, ',', -3.83851957321167)\n",
      "(a, ',', -3.9830753803253174)\n",
      "(deep, ',', -9.902402877807617)\n",
      "(model, ',', -9.600680351257324)\n",
      "(to, ',', -3.83851957321167)\n",
      "(determine, ',', -10.438716888427734)\n",
      "(whether, ',', -8.656254768371582)\n",
      "(the, ',', -3.425445795059204)\n",
      "(candidate, ',', -8.77760124206543)\n",
      "(is, ',', -4.3297648429870605)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(correct, ',', -8.887826919555664)\n",
      "(and, ',', -4.195279121398926)\n",
      "(estimate, ',', -11.583266258239746)\n",
      "(body, ',', -9.267072677612305)\n",
      "(locations, ',', -11.704129219055176)\n",
      "(., ',', -3.0729479789733887)\n",
      "(Best, ',', -10.38855266571045)\n",
      "(viewed, ',', -11.39624309539795)\n",
      "(in, ',', -4.585874080657959)\n",
      "(color, ',', -9.952893257141113)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(Figure, ',', -19.579313278198242)\n",
      "(3, ',', -8.093857765197754)\n",
      "(., ',', -3.0729479789733887)\n",
      "(Direct, ',', -19.579313278198242)\n",
      "(use, ',', -7.119458198547363)\n",
      "(of, ',', -4.1284637451171875)\n",
      "(deep, ',', -9.902402877807617)\n",
      "(model, ',', -9.600680351257324)\n",
      "((, ',', -5.70067024230957)\n",
      "(a, ',', -3.9830753803253174)\n",
      "(), ',', -5.341753005981445)\n",
      "(and, ',', -4.195279121398926)\n",
      "(the, ',', -3.425445795059204)\n",
      "(deep, ',', -9.902402877807617)\n",
      "(architecture, ',', -11.57960033416748)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(we, ',', -6.075284481048584)\n",
      "(propose, ',', -11.24658489227295)\n",
      "((, ',', -5.70067024230957)\n",
      "(b, ',', -10.387441635131836)\n",
      "(), ',', -5.341753005981445)\n",
      "(for, ',', -4.91396951675415)\n",
      "(part, ',', -7.779394626617432)\n",
      "(score, ',', -10.567034721374512)\n",
      "(s, ',', -8.883868217468262)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(deformation, ',', -19.579313278198242)\n",
      "(d, ',', -11.57960033416748)\n",
      "(and, ',', -4.195279121398926)\n",
      "(mixture, ',', -19.579313278198242)\n",
      "(type, ',', -8.814929008483887)\n",
      "(t., ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(Best, ',', -10.38855266571045)\n",
      "(viewed, ',', -11.39624309539795)\n",
      "(in, ',', -4.585874080657959)\n",
      "(color, ',', -9.952893257141113)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(is, ',', -4.3297648429870605)\n",
      "(connected, ',', -10.777859687805176)\n",
      "(to, ',', -3.83851957321167)\n",
      "(two, ',', -7.494669437408447)\n",
      "(layers, ',', -19.579313278198242)\n",
      "(for, ',', -4.91396951675415)\n",
      "(constructing, ',', -19.579313278198242)\n",
      "(high, ',', -8.075313568115234)\n",
      "(level, ',', -8.576519012451172)\n",
      "(rep-, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(resentation, ',', -19.579313278198242)\n",
      "(individually, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(High, ',', -10.776219367980957)\n",
      "(-, ',', -5.202415943145752)\n",
      "(level, ',', -8.576519012451172)\n",
      "(representations, ',', -19.579313278198242)\n",
      "(of, ',', -4.1284637451171875)\n",
      "(dif-, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(ferent, ',', -19.579313278198242)\n",
      "(information, ',', -8.817930221557617)\n",
      "(sources, ',', -9.963014602661133)\n",
      "(are, ',', -5.160149097442627)\n",
      "(then, ',', -6.67340612411499)\n",
      "(fused, ',', -19.579313278198242)\n",
      "(using, ',', -7.910459041595459)\n",
      "(other, ',', -6.661053657531738)\n",
      "(two, ',', -7.494669437408447)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(layers, ',', -19.579313278198242)\n",
      "(for, ',', -4.91396951675415)\n",
      "(pose, ',', -11.87548542022705)\n",
      "(estimation, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(4.1, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(Inference, ',', -19.579313278198242)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(The, ',', -5.774222373962402)\n",
      "(mixture, ',', -19.579313278198242)\n",
      "(type, ',', -8.814929008483887)\n",
      "(information, ',', -8.817930221557617)\n",
      "(t, ',', -10.763195991516113)\n",
      "(in, ',', -4.585874080657959)\n",
      "(Fig, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(3, ',', -8.093857765197754)\n",
      "(is, ',', -4.3297648429870605)\n",
      "(taken, ',', -8.92341136932373)\n",
      "(from, ',', -6.028810501098633)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(the, ',', -3.425445795059204)\n",
      "(t, ',', -10.763195991516113)\n",
      "(in, ',', -4.585874080657959)\n",
      "((, ',', -5.70067024230957)\n",
      "(3, ',', -8.093857765197754)\n",
      "(), ',', -5.341753005981445)\n",
      "(., ',', -3.0729479789733887)\n",
      "(The, ',', -5.774222373962402)\n",
      "(relative, ',', -10.384114265441895)\n",
      "(positions, ',', -10.302310943603516)\n",
      "(among, ',', -9.610336303710938)\n",
      "(parts, ',', -9.519133567810059)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(denoted, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(by, ',', -6.114920139312744)\n",
      "(d, ',', -11.57960033416748)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(comes, ',', -8.50893497467041)\n",
      "(from, ',', -6.028810501098633)\n",
      "(the, ',', -3.425445795059204)\n",
      "(deformation, ',', -19.579313278198242)\n",
      "(information, ',', -8.817930221557617)\n",
      "(d(zp, ',', -19.579313278198242)\n",
      "(−, ',', -19.579313278198242)\n",
      "(zq, ',', -19.579313278198242)\n",
      "(), ',', -5.341753005981445)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(in, ',', -4.585874080657959)\n",
      "((, ',', -5.70067024230957)\n",
      "(4, ',', -8.51557731628418)\n",
      "(), ',', -5.341753005981445)\n",
      "(., ',', -3.0729479789733887)\n",
      "(The, ',', -5.774222373962402)\n",
      "(appearance, ',', -11.022025108337402)\n",
      "(scores, ',', -11.433479309082031)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(denoted, ',', -19.579313278198242)\n",
      "(by, ',', -6.114920139312744)\n",
      "(s, ',', -8.883868217468262)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(is, ',', -4.3297648429870605)\n",
      "(obtained, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(from, ',', -6.028810501098633)\n",
      "(the, ',', -3.425445795059204)\n",
      "(unary, ',', -19.579313278198242)\n",
      "(appearance, ',', -11.022025108337402)\n",
      "(term, ',', -8.676319122314453)\n",
      "(in, ',', -4.585874080657959)\n",
      "((, ',', -5.70067024230957)\n",
      "(5, ',', -8.421856880187988)\n",
      "(), ',', -5.341753005981445)\n",
      "(., ',', -3.0729479789733887)\n",
      "(In, ',', -7.314908981323242)\n",
      "(our, ',', -7.124547481536865)\n",
      "(experiment, ',', -10.579070091247559)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(s, ',', -8.883868217468262)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(t, ',', -10.763195991516113)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(and, ',', -4.195279121398926)\n",
      "(d, ',', -11.57960033416748)\n",
      "(are, ',', -5.160149097442627)\n",
      "(obtained, ',', -19.579313278198242)\n",
      "(using, ',', -7.910459041595459)\n",
      "(the, ',', -3.425445795059204)\n",
      "(approach, ',', -9.994755744934082)\n",
      "(in, ',', -4.585874080657959)\n",
      "([, ',', -5.266753196716309)\n",
      "(58, ',', -19.579313278198242)\n",
      "(], ',', -5.498423099517822)\n",
      "(., ',', -3.0729479789733887)\n",
      "(At, ',', -8.632926940917969)\n",
      "(the, ',', -3.425445795059204)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(inference, ',', -19.579313278198242)\n",
      "(stage, ',', -10.263298034667969)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(the, ',', -3.425445795059204)\n",
      "(model, ',', -9.600680351257324)\n",
      "(is, ',', -4.3297648429870605)\n",
      "(as, ',', -5.507394313812256)\n",
      "(follows, ',', -11.009532928466797)\n",
      "(:, ',', -6.052439212799072)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(h1,1, ',', -19.579313278198242)\n",
      "(=, ',', -8.43548583984375)\n",
      "(a(sTW1,1, ',', -19.579313278198242)\n",
      "(+, ',', -9.560532569885254)\n",
      "(b1,1, ',', -19.579313278198242)\n",
      "(), ',', -5.341753005981445)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(h1,2, ',', -19.579313278198242)\n",
      "(=, ',', -8.43548583984375)\n",
      "(a(dTW1,2, ',', -19.579313278198242)\n",
      "(+, ',', -9.560532569885254)\n",
      "(b1,2, ',', -19.579313278198242)\n",
      "(), ',', -5.341753005981445)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(h1,3, ',', -19.579313278198242)\n",
      "(=, ',', -8.43548583984375)\n",
      "(a(tTW1,3, ',', -19.579313278198242)\n",
      "(+, ',', -9.560532569885254)\n",
      "(b1,3, ',', -19.579313278198242)\n",
      "(), ',', -5.341753005981445)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(h2,u, ',', -19.579313278198242)\n",
      "(=, ',', -8.43548583984375)\n",
      "(a(h1,uT, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(h2, ',', -19.579313278198242)\n",
      "(=, ',', -8.43548583984375)\n",
      "([, ',', -5.266753196716309)\n",
      "(h2,1, ',', -19.579313278198242)\n",
      "(T, ',', -10.680953979492188)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(h3, ',', -19.579313278198242)\n",
      "(=, ',', -8.43548583984375)\n",
      "(a(h2, ',', -19.579313278198242)\n",
      "(T, ',', -10.680953979492188)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(˜ycls, ',', -19.579313278198242)\n",
      "(=, ',', -8.43548583984375)\n",
      "(σ(h3, ',', -19.579313278198242)\n",
      "(T, ',', -10.680953979492188)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(˜ypst, ',', -19.579313278198242)\n",
      "(=, ',', -8.43548583984375)\n",
      "(h3, ',', -19.579313278198242)\n",
      "(T, ',', -10.680953979492188)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(Wpst, ',', -19.579313278198242)\n",
      "(+, ',', -9.560532569885254)\n",
      "(bpst, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(wcls, ',', -19.579313278198242)\n",
      "(+, ',', -9.560532569885254)\n",
      "(bcls, ',', -19.579313278198242)\n",
      "(), ',', -5.341753005981445)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(W2,u, ',', -19.579313278198242)\n",
      "(+, ',', -9.560532569885254)\n",
      "(b2,u, ',', -19.579313278198242)\n",
      "(), ',', -5.341753005981445)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(u, ',', -10.8943510055542)\n",
      "(=, ',', -8.43548583984375)\n",
      "(1, ',', -7.901819705963135)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(2, ',', -7.786293029785156)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(3, ',', -8.093857765197754)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(h2,2, ',', -19.579313278198242)\n",
      "(T, ',', -10.680953979492188)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(h2,3, ',', -19.579313278198242)\n",
      "(T, ',', -10.680953979492188)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(W3, ',', -19.579313278198242)\n",
      "(+, ',', -9.560532569885254)\n",
      "(b2, ',', -19.579313278198242)\n",
      "(), ',', -5.341753005981445)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(], ',', -5.498423099517822)\n",
      "(T, ',', -10.680953979492188)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "((, ',', -5.70067024230957)\n",
      "(6, ',', -9.090521812438965)\n",
      "(), ',', -5.341753005981445)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "((, ',', -5.70067024230957)\n",
      "(7, ',', -9.367201805114746)\n",
      "(), ',', -5.341753005981445)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "((, ',', -5.70067024230957)\n",
      "(8), ',', -19.579313278198242)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "((, ',', -5.70067024230957)\n",
      "(9, ',', -9.879417419433594)\n",
      "(), ',', -5.341753005981445)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "((, ',', -5.70067024230957)\n",
      "(10, ',', -8.452024459838867)\n",
      "(), ',', -5.341753005981445)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "((, ',', -5.70067024230957)\n",
      "(11, ',', -10.191385269165039)\n",
      "(), ',', -5.341753005981445)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "((, ',', -5.70067024230957)\n",
      "(12, ',', -9.68836784362793)\n",
      "(), ',', -5.341753005981445)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "((, ',', -5.70067024230957)\n",
      "(13, ',', -10.399734497070312)\n",
      "(), ',', -5.341753005981445)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(•, ',', -19.579313278198242)\n",
      "(σ(x, ',', -19.579313278198242)\n",
      "(), ',', -5.341753005981445)\n",
      "(=, ',', -8.43548583984375)\n",
      "((, ',', -5.70067024230957)\n",
      "(1, ',', -7.901819705963135)\n",
      "(+, ',', -9.560532569885254)\n",
      "(exp(−x))−1, ',', -19.579313278198242)\n",
      "(is, ',', -4.3297648429870605)\n",
      "(the, ',', -3.425445795059204)\n",
      "(sigmoid, ',', -19.579313278198242)\n",
      "(function, ',', -9.794262886047363)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(•, ',', -19.579313278198242)\n",
      "(a(∗, ',', -19.579313278198242)\n",
      "(), ',', -5.341753005981445)\n",
      "(is, ',', -4.3297648429870605)\n",
      "(the, ',', -3.425445795059204)\n",
      "(point, ',', -7.199707984924316)\n",
      "(-, ',', -5.202415943145752)\n",
      "(wise, ',', -10.53305721282959)\n",
      "(non, ',', -8.436274528503418)\n",
      "(-, ',', -5.202415943145752)\n",
      "(linear, ',', -11.651655197143555)\n",
      "(activation, ',', -19.579313278198242)\n",
      "(function, ',', -9.794262886047363)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(for, ',', -4.91396951675415)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(•, ',', -19.579313278198242)\n",
      "(W∗, ',', -19.579313278198242)\n",
      "(and, ',', -4.195279121398926)\n",
      "(wcls, ',', -19.579313278198242)\n",
      "(connect, ',', -11.192973136901855)\n",
      "(nodes, ',', -19.579313278198242)\n",
      "(between, ',', -8.03024673461914)\n",
      "(adjacent, ',', -19.579313278198242)\n",
      "(layers, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(•, ',', -19.579313278198242)\n",
      "(b∗, ',', -19.579313278198242)\n",
      "(and, ',', -4.195279121398926)\n",
      "(bcls, ',', -19.579313278198242)\n",
      "(are, ',', -5.160149097442627)\n",
      "(biases, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(which, ',', -6.728941917419434)\n",
      "(sigmoid, ',', -19.579313278198242)\n",
      "(function, ',', -9.794262886047363)\n",
      "(can, ',', -5.9138712882995605)\n",
      "(be, ',', -5.210641384124756)\n",
      "(used, ',', -7.587561130523682)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(Our, ',', -9.764631271362305)\n",
      "(deep, ',', -9.902402877807617)\n",
      "(modelExisting, ',', -19.579313278198242)\n",
      "(approachInputCandidatesResultsExisting, ',', -19.579313278198242)\n",
      "(approachDeep, ',', -19.579313278198242)\n",
      "(model, ',', -9.600680351257324)\n",
      "(........., ',', -19.579313278198242)\n",
      "((d), ',', -19.579313278198242)\n",
      "(..., ',', -5.788001537322998)\n",
      "(h3h1,3, ',', -19.579313278198242)\n",
      "(......, ',', -19.579313278198242)\n",
      "(sh2,3, ',', -19.579313278198242)\n",
      "(........................, ',', -19.579313278198242)\n",
      "(dt, ',', -19.579313278198242)\n",
      "(......, ',', -19.579313278198242)\n",
      "(ypstW1, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(1W1, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(2W1, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(3W2, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(1W2, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(2W2, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(3W3Wpstyclswclsh3h1, ',', -19.579313278198242)\n",
      "(..., ',', -5.788001537322998)\n",
      "(sh2, ',', -19.579313278198242)\n",
      "(............, ',', -19.579313278198242)\n",
      "(dt, ',', -19.579313278198242)\n",
      "(......, ',', -19.579313278198242)\n",
      "(ypstycls, ',', -19.579313278198242)\n",
      "(.................., ',', -19.579313278198242)\n",
      "((a)(b)h1,1h1,2h2,2h2,1, ',', -19.579313278198242)\n",
      "(\f",
      ", ',', -19.579313278198242)\n",
      "(ing, ',', -19.579313278198242)\n",
      "(non, ',', -8.436274528503418)\n",
      "(-, ',', -5.202415943145752)\n",
      "(linear, ',', -11.651655197143555)\n",
      "(representations, ',', -19.579313278198242)\n",
      "(from, ',', -6.028810501098633)\n",
      "(s, ',', -8.883868217468262)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(d, ',', -11.57960033416748)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(and, ',', -4.195279121398926)\n",
      "(t., ',', -19.579313278198242)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(•, ',', -19.579313278198242)\n",
      "(h∗, ',', -19.579313278198242)\n",
      "(are, ',', -5.160149097442627)\n",
      "(hidden, ',', -10.728233337402344)\n",
      "(nodes, ',', -19.579313278198242)\n",
      "(in, ',', -4.585874080657959)\n",
      "(different, ',', -7.797479629516602)\n",
      "(layers, ',', -19.579313278198242)\n",
      "(used, ',', -7.587561130523682)\n",
      "(for, ',', -4.91396951675415)\n",
      "(extract-, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(•, ',', -19.579313278198242)\n",
      "(˜ycls, ',', -19.579313278198242)\n",
      "(is, ',', -4.3297648429870605)\n",
      "(the, ',', -3.425445795059204)\n",
      "(estimated, ',', -11.822792053222656)\n",
      "(label, ',', -10.63723373413086)\n",
      "(indicating, ',', -19.579313278198242)\n",
      "(whether, ',', -8.656254768371582)\n",
      "(the, ',', -3.425445795059204)\n",
      "(candi-, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(date, ',', -9.923830032348633)\n",
      "(of, ',', -4.1284637451171875)\n",
      "(body, ',', -9.267072677612305)\n",
      "(locations, ',', -11.704129219055176)\n",
      "(is, ',', -4.3297648429870605)\n",
      "(correct, ',', -8.887826919555664)\n",
      "(., ',', -3.0729479789733887)\n",
      "(For, ',', -8.115267753601074)\n",
      "(pose, ',', -11.87548542022705)\n",
      "(estimatio, ',', -19.579313278198242)\n",
      "(of, ',', -4.1284637451171875)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(single, ',', -8.629281997680664)\n",
      "(human, ',', -8.437538146972656)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(the, ',', -3.425445795059204)\n",
      "(candidate, ',', -8.77760124206543)\n",
      "(with, ',', -5.363764762878418)\n",
      "(the, ',', -3.425445795059204)\n",
      "(largest, ',', -10.449295997619629)\n",
      "(˜ycls, ',', -19.579313278198242)\n",
      "(is, ',', -4.3297648429870605)\n",
      "(used, ',', -7.587561130523682)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(as, ',', -5.507394313812256)\n",
      "(the, ',', -3.425445795059204)\n",
      "(ﬁnal, ',', -19.579313278198242)\n",
      "(output, ',', -11.071413040161133)\n",
      "(in, ',', -4.585874080657959)\n",
      "(our, ',', -7.124547481536865)\n",
      "(experiments, ',', -11.601799011230469)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(•, ',', -19.579313278198242)\n",
      "(˜ypst, ',', -19.579313278198242)\n",
      "(contains, ',', -10.937688827514648)\n",
      "(the, ',', -3.425445795059204)\n",
      "(estimated, ',', -11.822792053222656)\n",
      "(part, ',', -7.779394626617432)\n",
      "(locations, ',', -11.704129219055176)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(Through, ',', -19.579313278198242)\n",
      "(the, ',', -3.425445795059204)\n",
      "(ﬁrst, ',', -19.579313278198242)\n",
      "(two, ',', -7.494669437408447)\n",
      "(separate, ',', -10.035319328308105)\n",
      "(layers, ',', -19.579313278198242)\n",
      "(in, ',', -4.585874080657959)\n",
      "(Eq, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "((, ',', -5.70067024230957)\n",
      "(6)-(9, ',', -19.579313278198242)\n",
      "(), ',', -5.341753005981445)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(each, ',', -8.295592308044434)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(information, ',', -8.817930221557617)\n",
      "(source, ',', -8.812856674194336)\n",
      "(has, ',', -6.2021942138671875)\n",
      "(its, ',', -7.355584621429443)\n",
      "(individual, ',', -9.336834907531738)\n",
      "(representation, ',', -10.828350067138672)\n",
      "(con-, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(structed, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(Then, ',', -8.409794807434082)\n",
      "(all, ',', -5.896712779998779)\n",
      "(high, ',', -8.075313568115234)\n",
      "(-, ',', -5.202415943145752)\n",
      "(order, ',', -8.68863582611084)\n",
      "(representations, ',', -19.579313278198242)\n",
      "(are, ',', -5.160149097442627)\n",
      "(combined, ',', -10.79107666015625)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(by, ',', -6.114920139312744)\n",
      "(two, ',', -7.494669437408447)\n",
      "(layers, ',', -19.579313278198242)\n",
      "(in, ',', -4.585874080657959)\n",
      "(Eq, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "((, ',', -5.70067024230957)\n",
      "(11)-(13, ',', -19.579313278198242)\n",
      "(), ',', -5.341753005981445)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(4.2, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(Training, ',', -19.579313278198242)\n",
      "(method, ',', -9.819748878479004)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(Denote, ',', -19.579313278198242)\n",
      "(the, ',', -3.425445795059204)\n",
      "(parameter, ',', -19.579313278198242)\n",
      "(set, ',', -8.572531700134277)\n",
      "(for, ',', -4.91396951675415)\n",
      "(the, ',', -3.425445795059204)\n",
      "(model, ',', -9.600680351257324)\n",
      "(in, ',', -4.585874080657959)\n",
      "(Eq, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "((, ',', -5.70067024230957)\n",
      "(6)-(13, ',', -19.579313278198242)\n",
      "(), ',', -5.341753005981445)\n",
      "(by, ',', -6.114920139312744)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(λ, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(λ, ',', -19.579313278198242)\n",
      "(=, ',', -8.43548583984375)\n",
      "({, ',', -10.464776039123535)\n",
      "(W∗, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(wcls, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(b∗, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(bcls}., ',', -19.579313278198242)\n",
      "(The, ',', -5.774222373962402)\n",
      "(objective, ',', -10.627302169799805)\n",
      "(function, ',', -9.794262886047363)\n",
      "(J(λ, ',', -19.579313278198242)\n",
      "(), ',', -5.341753005981445)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(for, ',', -4.91396951675415)\n",
      "(backpropagating, ',', -19.579313278198242)\n",
      "(error, ',', -9.988028526306152)\n",
      "(derivatives, ',', -11.11874008178711)\n",
      "(is, ',', -4.3297648429870605)\n",
      "(as, ',', -5.507394313812256)\n",
      "(follows, ',', -11.009532928466797)\n",
      "(:, ',', -6.052439212799072)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "((, ',', -5.70067024230957)\n",
      "(cid:88, ',', -19.579313278198242)\n",
      "(), ',', -5.341753005981445)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "((, ',', -5.70067024230957)\n",
      "(cid:16, ',', -19.579313278198242)\n",
      "(), ',', -5.341753005981445)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(J(λ, ',', -19.579313278198242)\n",
      "(), ',', -5.341753005981445)\n",
      "(=, ',', -8.43548583984375)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(J1(ycls, ',', -19.579313278198242)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(n, ',', -10.842255592346191)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(˜ycls, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(+, ',', -9.560532569885254)\n",
      "(J3(W∗, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(wcls, ',', -19.579313278198242)\n",
      "(), ',', -5.341753005981445)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(n, ',', -10.842255592346191)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(n, ',', -10.842255592346191)\n",
      "(), ',', -5.341753005981445)\n",
      "(+, ',', -9.560532569885254)\n",
      "(ycls, ',', -19.579313278198242)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(n, ',', -10.842255592346191)\n",
      "(J2(ypst, ',', -19.579313278198242)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(n, ',', -10.842255592346191)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(˜ypst, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(n, ',', -10.842255592346191)\n",
      "(), ',', -5.341753005981445)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "((, ',', -5.70067024230957)\n",
      "(14, ',', -10.376395225524902)\n",
      "(), ',', -5.341753005981445)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(n, ',', -10.842255592346191)\n",
      "(), ',', -5.341753005981445)\n",
      "(log(1, ',', -19.579313278198242)\n",
      "(−, ',', -19.579313278198242)\n",
      "(˜ycls, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(n, ',', -10.842255592346191)\n",
      "(), ',', -5.341753005981445)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(n, ',', -10.842255592346191)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(˜ycls, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(n, ',', -10.842255592346191)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(˜ypst, ',', -19.579313278198242)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(J1(ycls, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(J2(ypst, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(J3(W∗, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(wcls, ',', -19.579313278198242)\n",
      "(), ',', -5.341753005981445)\n",
      "(=, ',', -8.43548583984375)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(n, ',', -10.842255592346191)\n",
      "(), ',', -5.341753005981445)\n",
      "(=, ',', -8.43548583984375)\n",
      "(−, ',', -19.579313278198242)\n",
      "(ycls, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(n, ',', -10.842255592346191)\n",
      "(), ',', -5.341753005981445)\n",
      "(−, ',', -19.579313278198242)\n",
      "((, ',', -5.70067024230957)\n",
      "(1, ',', -7.901819705963135)\n",
      "(−, ',', -19.579313278198242)\n",
      "(ycls, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(n, ',', -10.842255592346191)\n",
      "(log(˜ycls, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "((, ',', -5.70067024230957)\n",
      "(cid:88, ',', -19.579313278198242)\n",
      "(), ',', -5.341753005981445)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "((, ',', -5.70067024230957)\n",
      "(cid:88, ',', -19.579313278198242)\n",
      "(), ',', -5.341753005981445)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(n, ',', -10.842255592346191)\n",
      "(−, ',', -19.579313278198242)\n",
      "(˜ypst, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(n, ',', -10.842255592346191)\n",
      "(), ',', -5.341753005981445)\n",
      "(=, ',', -8.43548583984375)\n",
      "(||ypst, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(n, ',', -10.842255592346191)\n",
      "(||2, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(|w∗, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(i, ',', -6.991203784942627)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(j|, ',', -19.579313278198242)\n",
      "(+, ',', -9.560532569885254)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(|wcls, ',', -19.579313278198242)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(|, ',', -9.67520523071289)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(i, ',', -6.991203784942627)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "((, ',', -5.70067024230957)\n",
      "(cid:17, ',', -19.579313278198242)\n",
      "(), ',', -5.341753005981445)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "((, ',', -5.70067024230957)\n",
      "(15, ',', -9.510315895080566)\n",
      "(), ',', -5.341753005981445)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(i, ',', -6.991203784942627)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(j, ',', -19.579313278198242)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(i, ',', -6.991203784942627)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(where, ',', -7.183883190155029)\n",
      "(∗n, ',', -19.579313278198242)\n",
      "(denotes, ',', -19.579313278198242)\n",
      "(the, ',', -3.425445795059204)\n",
      "(nth, ',', -19.579313278198242)\n",
      "(sample, ',', -11.082479476928711)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(n, ',', -10.842255592346191)\n",
      "(=, ',', -8.43548583984375)\n",
      "(1, ',', -7.901819705963135)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(2, ',', -7.786293029785156)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(., ',', -3.0729479789733887)\n",
      "(., ',', -3.0729479789733887)\n",
      "(., ',', -3.0729479789733887)\n",
      "(N., ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(•, ',', -19.579313278198242)\n",
      "(˜ycls, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(n, ',', -10.842255592346191)\n",
      "(are, ',', -5.160149097442627)\n",
      "(computed, ',', -19.579313278198242)\n",
      "(using, ',', -7.910459041595459)\n",
      "(Eq, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "((, ',', -5.70067024230957)\n",
      "(6)-(13, ',', -19.579313278198242)\n",
      "(), ',', -5.341753005981445)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(•, ',', -19.579313278198242)\n",
      "(ycls, ',', -19.579313278198242)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(n, ',', -10.842255592346191)\n",
      "(and, ',', -4.195279121398926)\n",
      "(˜ypst, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(n, ',', -10.842255592346191)\n",
      "(∈, ',', -19.579313278198242)\n",
      "({, ',', -10.464776039123535)\n",
      "(0, ',', -9.884106636047363)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(1, ',', -7.901819705963135)\n",
      "(}, ',', -10.305375099182129)\n",
      "(is, ',', -4.3297648429870605)\n",
      "(the, ',', -3.425445795059204)\n",
      "(ground, ',', -9.634105682373047)\n",
      "(truth, ',', -8.972647666931152)\n",
      "(classiﬁcation, ',', -19.579313278198242)\n",
      "(label, ',', -10.63723373413086)\n",
      "(in-, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(dicating, ',', -19.579313278198242)\n",
      "(whether, ',', -8.656254768371582)\n",
      "(the, ',', -3.425445795059204)\n",
      "(current, ',', -8.65842056274414)\n",
      "(body, ',', -9.267072677612305)\n",
      "(location, ',', -10.667635917663574)\n",
      "(estimation, ',', -19.579313278198242)\n",
      "(is, ',', -4.3297648429870605)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(correct, ',', -8.887826919555664)\n",
      "(or, ',', -5.715355396270752)\n",
      "(not, ',', -5.21923828125)\n",
      "(., ',', -3.0729479789733887)\n",
      "(Positive, ',', -19.579313278198242)\n",
      "(training, ',', -10.32292366027832)\n",
      "(samples, ',', -19.579313278198242)\n",
      "(have, ',', -5.235879421234131)\n",
      "(their, ',', -6.1832733154296875)\n",
      "(part, ',', -7.779394626617432)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(templates, ',', -19.579313278198242)\n",
      "(placed, ',', -10.85635757446289)\n",
      "(around, ',', -7.538910388946533)\n",
      "(annotated, ',', -19.579313278198242)\n",
      "(body, ',', -9.267072677612305)\n",
      "(locations, ',', -11.704129219055176)\n",
      "(., ',', -3.0729479789733887)\n",
      "(As, ',', -7.891604900360107)\n",
      "(in, ',', -4.585874080657959)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "([, ',', -5.266753196716309)\n",
      "(58, ',', -19.579313278198242)\n",
      "(], ',', -5.498423099517822)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(negative, ',', -9.487476348876953)\n",
      "(training, ',', -10.32292366027832)\n",
      "(samples, ',', -19.579313278198242)\n",
      "(have, ',', -5.235879421234131)\n",
      "(their, ',', -6.1832733154296875)\n",
      "(part, ',', -7.779394626617432)\n",
      "(templates, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(placed, ',', -10.85635757446289)\n",
      "(on, ',', -5.263686656951904)\n",
      "(images, ',', -10.350371360778809)\n",
      "(without, ',', -7.6655049324035645)\n",
      "(human, ',', -8.437538146972656)\n",
      "(., ',', -3.0729479789733887)\n",
      "(Therefore, ',', -10.595346450805664)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(ycls, ',', -19.579313278198242)\n",
      "(can, ',', -5.9138712882995605)\n",
      "(be, ',', -5.210641384124756)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(used, ',', -7.587561130523682)\n",
      "(for, ',', -4.91396951675415)\n",
      "(human, ',', -8.437538146972656)\n",
      "(detection, ',', -19.579313278198242)\n",
      "(by, ',', -6.114920139312744)\n",
      "(considering, ',', -9.819748878479004)\n",
      "(it, ',', -4.5064496994018555)\n",
      "(as, ',', -5.507394313812256)\n",
      "(an, ',', -5.953293800354004)\n",
      "(indi-, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(cator, ',', -19.579313278198242)\n",
      "(on, ',', -5.263686656951904)\n",
      "(whether, ',', -8.656254768371582)\n",
      "(the, ',', -3.425445795059204)\n",
      "(rectangle, ',', -19.579313278198242)\n",
      "(covering, ',', -11.251843452453613)\n",
      "(body, ',', -9.267072677612305)\n",
      "(locations, ',', -11.704129219055176)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(contains, ',', -10.937688827514648)\n",
      "(a, ',', -3.9830753803253174)\n",
      "(human, ',', -8.437538146972656)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(•, ',', -19.579313278198242)\n",
      "(J1(ycls, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(•, ',', -19.579313278198242)\n",
      "(ypst, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(•, ',', -19.579313278198242)\n",
      "(J2(ypst, ',', -19.579313278198242)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(n, ',', -10.842255592346191)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(˜ycls, ',', -19.579313278198242)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(n, ',', -10.842255592346191)\n",
      "(), ',', -5.341753005981445)\n",
      "(is, ',', -4.3297648429870605)\n",
      "(the, ',', -3.425445795059204)\n",
      "(cross, ',', -10.30639934539795)\n",
      "(-, ',', -5.202415943145752)\n",
      "(entropy, ',', -19.579313278198242)\n",
      "(error, ',', -9.988028526306152)\n",
      "(on, ',', -5.263686656951904)\n",
      "(classiﬁcation, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(n, ',', -10.842255592346191)\n",
      "(contains, ',', -10.937688827514648)\n",
      "(the, ',', -3.425445795059204)\n",
      "(ground, ',', -9.634105682373047)\n",
      "(truth, ',', -8.972647666931152)\n",
      "(body, ',', -9.267072677612305)\n",
      "(locations, ',', -11.704129219055176)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(n, ',', -10.842255592346191)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(˜ypst, ',', -19.579313278198242)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(n, ',', -10.842255592346191)\n",
      "(), ',', -5.341753005981445)\n",
      "(is, ',', -4.3297648429870605)\n",
      "(the, ',', -3.425445795059204)\n",
      "(sum, ',', -10.786099433898926)\n",
      "(of, ',', -4.1284637451171875)\n",
      "(square, ',', -10.890669822692871)\n",
      "(error, ',', -9.988028526306152)\n",
      "(on, ',', -5.263686656951904)\n",
      "(body, ',', -9.267072677612305)\n",
      "(loca-, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(tion, ',', -19.579313278198242)\n",
      "(estimation, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(Since, ',', -9.441577911376953)\n",
      "(negative, ',', -9.487476348876953)\n",
      "(background, ',', -10.037664413452148)\n",
      "(samples, ',', -19.579313278198242)\n",
      "(do, ',', -5.300070285797119)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(not, ',', -5.21923828125)\n",
      "(have, ',', -5.235879421234131)\n",
      "(ground, ',', -9.634105682373047)\n",
      "(truth, ',', -8.972647666931152)\n",
      "(body, ',', -9.267072677612305)\n",
      "(location, ',', -10.667635917663574)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(the, ',', -3.425445795059204)\n",
      "(ycls, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(n, ',', -10.842255592346191)\n",
      "(is, ',', -4.3297648429870605)\n",
      "(multi-, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(plied, ',', -19.579313278198242)\n",
      "(by, ',', -6.114920139312744)\n",
      "(J2, ',', -19.579313278198242)\n",
      "(in, ',', -4.585874080657959)\n",
      "((, ',', -5.70067024230957)\n",
      "(14, ',', -10.376395225524902)\n",
      "(), ',', -5.341753005981445)\n",
      "(to, ',', -3.83851957321167)\n",
      "(ensure, ',', -10.492765426635742)\n",
      "(that, ',', -4.348940372467041)\n",
      "(only, ',', -6.547285556793213)\n",
      "(positive, ',', -9.71806526184082)\n",
      "(samples, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(are, ',', -5.160149097442627)\n",
      "(used, ',', -7.587561130523682)\n",
      "(to, ',', -3.83851957321167)\n",
      "(learn, ',', -8.87330436706543)\n",
      "(location, ',', -10.667635917663574)\n",
      "(estimation, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(•, ',', -19.579313278198242)\n",
      "(J3(W∗, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(wcls, ',', -19.579313278198242)\n",
      "(), ',', -5.341753005981445)\n",
      "(is, ',', -4.3297648429870605)\n",
      "(the, ',', -3.425445795059204)\n",
      "(L1, ',', -19.579313278198242)\n",
      "(norm, ',', -11.529644012451172)\n",
      "(regularization, ',', -19.579313278198242)\n",
      "(term, ',', -8.676319122314453)\n",
      "(., ',', -3.0729479789733887)\n",
      "(w∗, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(i, ',', -6.991203784942627)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(j, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(is, ',', -4.3297648429870605)\n",
      "(the, ',', -3.425445795059204)\n",
      "((, ',', -5.70067024230957)\n",
      "(i, ',', -6.991203784942627)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(j)th, ',', -19.579313278198242)\n",
      "(element, ',', -11.038928031921387)\n",
      "(in, ',', -4.585874080657959)\n",
      "(W∗, ',', -19.579313278198242)\n",
      "(and, ',', -4.195279121398926)\n",
      "(wcls, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(is, ',', -4.3297648429870605)\n",
      "(the, ',', -3.425445795059204)\n",
      "(ith, ',', -19.579313278198242)\n",
      "(and, ',', -4.195279121398926)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(element, ',', -11.038928031921387)\n",
      "(in, ',', -4.585874080657959)\n",
      "(wcls, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(The, ',', -5.774222373962402)\n",
      "(information, ',', -8.817930221557617)\n",
      "(sources, ',', -9.963014602661133)\n",
      "(and, ',', -4.195279121398926)\n",
      "(hidden, ',', -10.728233337402344)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(nodes, ',', -19.579313278198242)\n",
      "(may, ',', -7.616482734680176)\n",
      "(have, ',', -5.235879421234131)\n",
      "(different, ',', -7.797479629516602)\n",
      "(purpose, ',', -9.62111759185791)\n",
      "(., ',', -3.0729479789733887)\n",
      "(For, ',', -8.115267753601074)\n",
      "(example, ',', -8.316376686096191)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(a, ',', -3.9830753803253174)\n",
      "(node, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(in, ',', -4.585874080657959)\n",
      "(h3, ',', -19.579313278198242)\n",
      "(may, ',', -7.616482734680176)\n",
      "(use, ',', -7.119458198547363)\n",
      "(the, ',', -3.425445795059204)\n",
      "(information, ',', -8.817930221557617)\n",
      "(source, ',', -8.812856674194336)\n",
      "(mixture, ',', -19.579313278198242)\n",
      "(type, ',', -8.814929008483887)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(Hence, ',', -11.294941902160645)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(J3(W∗, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(wcls, ',', -19.579313278198242)\n",
      "(), ',', -5.341753005981445)\n",
      "(is, ',', -4.3297648429870605)\n",
      "(used, ',', -7.587561130523682)\n",
      "(to, ',', -3.83851957321167)\n",
      "(encourage, ',', -10.54729175567627)\n",
      "(sparsity, ',', -19.579313278198242)\n",
      "(in, ',', -4.585874080657959)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(the, ',', -3.425445795059204)\n",
      "(weights, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(i, ',', -6.991203784942627)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(Body, ',', -19.579313278198242)\n",
      "(part, ',', -7.779394626617432)\n",
      "(location, ',', -10.667635917663574)\n",
      "(estimation, ',', -19.579313278198242)\n",
      "(and, ',', -4.195279121398926)\n",
      "(human, ',', -8.437538146972656)\n",
      "(detection, ',', -19.579313278198242)\n",
      "(are, ',', -5.160149097442627)\n",
      "(both, ',', -7.8158979415893555)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(learned, ',', -9.666524887084961)\n",
      "(through, ',', -7.777187347412109)\n",
      "(shared, ',', -10.801104545593262)\n",
      "(representation, ',', -10.828350067138672)\n",
      "(in, ',', -4.585874080657959)\n",
      "(this, ',', -5.417123317718506)\n",
      "(model, ',', -9.600680351257324)\n",
      "(., ',', -3.0729479789733887)\n",
      "(They, ',', -6.996388912200928)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(are, ',', -5.160149097442627)\n",
      "(jointly, ',', -19.579313278198242)\n",
      "(learned, ',', -9.666524887084961)\n",
      "(because, ',', -6.412496566772461)\n",
      "(they, ',', -5.429816246032715)\n",
      "(are, ',', -5.160149097442627)\n",
      "(dependent, ',', -11.062646865844727)\n",
      "(tasks, ',', -11.48538875579834)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(4.3, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(Analysis, ',', -19.579313278198242)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(The, ',', -5.774222373962402)\n",
      "(mixture, ',', -19.579313278198242)\n",
      "(type, ',', -8.814929008483887)\n",
      "(t, ',', -10.763195991516113)\n",
      "(is, ',', -4.3297648429870605)\n",
      "(used, ',', -7.587561130523682)\n",
      "(as, ',', -5.507394313812256)\n",
      "(an, ',', -5.953293800354004)\n",
      "(example, ',', -8.316376686096191)\n",
      "(for, ',', -4.91396951675415)\n",
      "(analysis, ',', -10.233338356018066)\n",
      "(., ',', -3.0729479789733887)\n",
      "(In, ',', -7.314908981323242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(the, ',', -3.425445795059204)\n",
      "(layer, ',', -11.220696449279785)\n",
      "(-, ',', -5.202415943145752)\n",
      "(wise, ',', -10.53305721282959)\n",
      "(pre, ',', -9.939319610595703)\n",
      "(-, ',', -5.202415943145752)\n",
      "(training, ',', -10.32292366027832)\n",
      "(stage, ',', -10.263298034667969)\n",
      "([, ',', -5.266753196716309)\n",
      "(18, ',', -10.092371940612793)\n",
      "(], ',', -5.498423099517822)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(t, ',', -10.763195991516113)\n",
      "(and, ',', -4.195279121398926)\n",
      "(hidden, ',', -10.728233337402344)\n",
      "(vector, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(h1,3, ',', -19.579313278198242)\n",
      "(are, ',', -5.160149097442627)\n",
      "(considered, ',', -9.297229766845703)\n",
      "(as, ',', -5.507394313812256)\n",
      "(a, ',', -3.9830753803253174)\n",
      "(restricted, ',', -11.86569595336914)\n",
      "(Boltzmann, ',', -19.579313278198242)\n",
      "(machine, ',', -9.18136978149414)\n",
      "(with, ',', -5.363764762878418)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(the, ',', -3.425445795059204)\n",
      "(following, ',', -9.569315910339355)\n",
      "(distribution, ',', -10.511449813842773)\n",
      "(:, ',', -6.052439212799072)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(p(t, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(h1,3, ',', -19.579313278198242)\n",
      "(), ',', -5.341753005981445)\n",
      "(∝, ',', -19.579313278198242)\n",
      "(exp(tTW1,3h1,3, ',', -19.579313278198242)\n",
      "(+, ',', -9.560532569885254)\n",
      "(b1,3, ',', -19.579313278198242)\n",
      "(T, ',', -10.680953979492188)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(h1,3, ',', -19.579313278198242)\n",
      "(+, ',', -9.560532569885254)\n",
      "(cTt, ',', -19.579313278198242)\n",
      "(), ',', -5.341753005981445)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "((, ',', -5.70067024230957)\n",
      "(16, ',', -10.35681438446045)\n",
      "(), ',', -5.341753005981445)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(Denote, ',', -19.579313278198242)\n",
      "(the, ',', -3.425445795059204)\n",
      "(jth, ',', -19.579313278198242)\n",
      "(column, ',', -11.420912742614746)\n",
      "(of, ',', -4.1284637451171875)\n",
      "(W1,3, ',', -19.579313278198242)\n",
      "(by, ',', -6.114920139312744)\n",
      "(w1,3∗,j, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(Denote, ',', -19.579313278198242)\n",
      "(the, ',', -3.425445795059204)\n",
      "(jth, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(element, ',', -11.038928031921387)\n",
      "(of, ',', -4.1284637451171875)\n",
      "(b1,3, ',', -19.579313278198242)\n",
      "(by, ',', -6.114920139312744)\n",
      "(bj, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(The, ',', -5.774222373962402)\n",
      "(marginal, ',', -11.725067138671875)\n",
      "(distribution, ',', -10.511449813842773)\n",
      "(p(t, ',', -19.579313278198242)\n",
      "(), ',', -5.341753005981445)\n",
      "(can, ',', -5.9138712882995605)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(be, ',', -5.210641384124756)\n",
      "(obtained, ',', -19.579313278198242)\n",
      "(as, ',', -5.507394313812256)\n",
      "(follows, ',', -11.009532928466797)\n",
      "(:, ',', -6.052439212799072)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(p(t, ',', -19.579313278198242)\n",
      "(), ',', -5.341753005981445)\n",
      "(=, ',', -8.43548583984375)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(p(t, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(h1,3, ',', -19.579313278198242)\n",
      "(), ',', -5.341753005981445)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "((, ',', -5.70067024230957)\n",
      "(cid:88, ',', -19.579313278198242)\n",
      "(), ',', -5.341753005981445)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(∝(cid:88, ',', -19.579313278198242)\n",
      "(), ',', -5.341753005981445)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(h1,3, ',', -19.579313278198242)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(h1,3, ',', -19.579313278198242)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(∝, ',', -19.579313278198242)\n",
      "(exp(cTt, ',', -19.579313278198242)\n",
      "(), ',', -5.341753005981445)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "((, ',', -5.70067024230957)\n",
      "(cid:89, ',', -19.579313278198242)\n",
      "(), ',', -5.341753005981445)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "((, ',', -5.70067024230957)\n",
      "(cid:16, ',', -19.579313278198242)\n",
      "(), ',', -5.341753005981445)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(j, ',', -19.579313278198242)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(exp(tTW1,3h1,3, ',', -19.579313278198242)\n",
      "(+, ',', -9.560532569885254)\n",
      "(b1,3, ',', -19.579313278198242)\n",
      "(T, ',', -10.680953979492188)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(h1,3, ',', -19.579313278198242)\n",
      "(+, ',', -9.560532569885254)\n",
      "(cTt, ',', -19.579313278198242)\n",
      "(), ',', -5.341753005981445)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(1, ',', -7.901819705963135)\n",
      "(+, ',', -9.560532569885254)\n",
      "(exp(tTw1,3∗,j, ',', -19.579313278198242)\n",
      "(+, ',', -9.560532569885254)\n",
      "(bj, ',', -19.579313278198242)\n",
      "(), ',', -5.341753005981445)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(=, ',', -8.43548583984375)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "((, ',', -5.70067024230957)\n",
      "(cid:17, ',', -19.579313278198242)\n",
      "(), ',', -5.341753005981445)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "((, ',', -5.70067024230957)\n",
      "(17, ',', -10.712736129760742)\n",
      "(), ',', -5.341753005981445)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(φj(t, ',', -19.579313278198242)\n",
      "(), ',', -5.341753005981445)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "((, ',', -5.70067024230957)\n",
      "(cid:89, ',', -19.579313278198242)\n",
      "(), ',', -5.341753005981445)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(j, ',', -19.579313278198242)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(where, ',', -7.183883190155029)\n",
      "(φj(t, ',', -19.579313278198242)\n",
      "(), ',', -5.341753005981445)\n",
      "(=, ',', -8.43548583984375)\n",
      "(1, ',', -7.901819705963135)\n",
      "(+, ',', -9.560532569885254)\n",
      "(exp(tTw1,3∗,j, ',', -19.579313278198242)\n",
      "(+, ',', -9.560532569885254)\n",
      "(bj, ',', -19.579313278198242)\n",
      "(), ',', -5.341753005981445)\n",
      "(and, ',', -4.195279121398926)\n",
      "(φj(t, ',', -19.579313278198242)\n",
      "(), ',', -5.341753005981445)\n",
      "(is, ',', -4.3297648429870605)\n",
      "(a, ',', -3.9830753803253174)\n",
      "(fully, ',', -9.811596870422363)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(connected, ',', -10.777859687805176)\n",
      "(graphical, ',', -19.579313278198242)\n",
      "(model, ',', -9.600680351257324)\n",
      "(because, ',', -6.412496566772461)\n",
      "(it, ',', -4.5064496994018555)\n",
      "(can, ',', -5.9138712882995605)\n",
      "(not, ',', -5.21923828125)\n",
      "(be, ',', -5.210641384124756)\n",
      "(factorized, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(φj(t, ',', -19.579313278198242)\n",
      "(), ',', -5.341753005981445)\n",
      "(can, ',', -5.9138712882995605)\n",
      "(be, ',', -5.210641384124756)\n",
      "(considered, ',', -9.297229766845703)\n",
      "(as, ',', -5.507394313812256)\n",
      "(a, ',', -3.9830753803253174)\n",
      "(factor, ',', -10.192298889160156)\n",
      "(that, ',', -4.348940372467041)\n",
      "(explains, ',', -10.669107437133789)\n",
      "(t, ',', -10.763195991516113)\n",
      "(in, ',', -4.585874080657959)\n",
      "(fac-, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(tor, ',', -19.579313278198242)\n",
      "(graph, ',', -11.322853088378906)\n",
      "([, ',', -5.266753196716309)\n",
      "(5, ',', -8.421856880187988)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(24, ',', -10.54468822479248)\n",
      "(], ',', -5.498423099517822)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(In, ',', -7.314908981323242)\n",
      "(pose, ',', -11.87548542022705)\n",
      "(estimation, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(φj(t, ',', -19.579313278198242)\n",
      "(), ',', -5.341753005981445)\n",
      "(can, ',', -5.9138712882995605)\n",
      "(be, ',', -5.210641384124756)\n",
      "(con-, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(sidered, ',', -19.579313278198242)\n",
      "(as, ',', -5.507394313812256)\n",
      "(a, ',', -3.9830753803253174)\n",
      "(global, ',', -9.585124015808105)\n",
      "(pattern, ',', -10.640089988708496)\n",
      "(explaining, ',', -10.801104545593262)\n",
      "(the, ',', -3.425445795059204)\n",
      "(mixture, ',', -19.579313278198242)\n",
      "(type, ',', -8.814929008483887)\n",
      "(t, ',', -10.763195991516113)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(for, ',', -4.91396951675415)\n",
      "(all, ',', -5.896712779998779)\n",
      "(parts, ',', -9.519133567810059)\n",
      "(., ',', -3.0729479789733887)\n",
      "(In, ',', -7.314908981323242)\n",
      "(both, ',', -7.8158979415893555)\n",
      "(training, ',', -10.32292366027832)\n",
      "(and, ',', -4.195279121398926)\n",
      "(inference, ',', -19.579313278198242)\n",
      "(stages, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(every, ',', -7.491902828216553)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(node, ',', -19.579313278198242)\n",
      "(in, ',', -4.585874080657959)\n",
      "(h1,3, ',', -19.579313278198242)\n",
      "(is, ',', -4.3297648429870605)\n",
      "(connected, ',', -10.777859687805176)\n",
      "(to, ',', -3.83851957321167)\n",
      "(the, ',', -3.425445795059204)\n",
      "(mixture, ',', -19.579313278198242)\n",
      "(types, ',', -9.873420715332031)\n",
      "(of, ',', -4.1284637451171875)\n",
      "(all, ',', -5.896712779998779)\n",
      "(parts, ',', -9.519133567810059)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(Therefore, ',', -10.595346450805664)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(h1,3, ',', -19.579313278198242)\n",
      "(nonlinearly, ',', -19.579313278198242)\n",
      "(extracts, ',', -19.579313278198242)\n",
      "(the, ',', -3.425445795059204)\n",
      "(global, ',', -9.585124015808105)\n",
      "(representa-, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(tion, ',', -19.579313278198242)\n",
      "(from, ',', -6.028810501098633)\n",
      "(t1,3, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(Similarly, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(the, ',', -3.425445795059204)\n",
      "(h2,3, ',', -19.579313278198242)\n",
      "(extracts, ',', -19.579313278198242)\n",
      "(higher, ',', -9.007245063781738)\n",
      "(-, ',', -5.202415943145752)\n",
      "(level, ',', -8.576519012451172)\n",
      "(rep-, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(resentation, ',', -19.579313278198242)\n",
      "(from, ',', -6.028810501098633)\n",
      "(h1,3, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(Therefore, ',', -10.595346450805664)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(the, ',', -3.425445795059204)\n",
      "(stack, ',', -11.267788887023926)\n",
      "(of, ',', -4.1284637451171875)\n",
      "(hidden, ',', -10.728233337402344)\n",
      "(layers, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(extracts, ',', -19.579313278198242)\n",
      "(global, ',', -9.585124015808105)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(high, ',', -8.075313568115234)\n",
      "(-, ',', -5.202415943145752)\n",
      "(level, ',', -8.576519012451172)\n",
      "(representation, ',', -10.828350067138672)\n",
      "(from, ',', -6.028810501098633)\n",
      "(the, ',', -3.425445795059204)\n",
      "(informa-, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(tion, ',', -19.579313278198242)\n",
      "(source, ',', -8.812856674194336)\n",
      "(t., ',', -19.579313278198242)\n",
      "(The, ',', -5.774222373962402)\n",
      "(analysis, ',', -10.233338356018066)\n",
      "(to, ',', -3.83851957321167)\n",
      "(mixture, ',', -19.579313278198242)\n",
      "(type, ',', -8.814929008483887)\n",
      "(t, ',', -10.763195991516113)\n",
      "(is, ',', -4.3297648429870605)\n",
      "(applica-, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(ble, ',', -19.579313278198242)\n",
      "(to, ',', -3.83851957321167)\n",
      "(deformation, ',', -19.579313278198242)\n",
      "(and, ',', -4.195279121398926)\n",
      "(appearance, ',', -11.022025108337402)\n",
      "(score, ',', -10.567034721374512)\n",
      "(., ',', -3.0729479789733887)\n",
      "(As, ',', -7.891604900360107)\n",
      "(shown, ',', -10.03923225402832)\n",
      "(in, ',', -4.585874080657959)\n",
      "(Fig, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(4, ',', -8.51557731628418)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(h2,3, ',', -19.579313278198242)\n",
      "(captures, ',', -19.579313278198242)\n",
      "(the, ',', -3.425445795059204)\n",
      "(global, ',', -9.585124015808105)\n",
      "(articulation, ',', -19.579313278198242)\n",
      "(patterns, ',', -11.080256462097168)\n",
      "(of, ',', -4.1284637451171875)\n",
      "(human, ',', -8.437538146972656)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(body, ',', -9.267072677612305)\n",
      "(., ',', -3.0729479789733887)\n",
      "(One, ',', -8.563169479370117)\n",
      "(of, ',', -4.1284637451171875)\n",
      "(the, ',', -3.425445795059204)\n",
      "(nodes, ',', -19.579313278198242)\n",
      "(in, ',', -4.585874080657959)\n",
      "(h2,3, ',', -19.579313278198242)\n",
      "(has, ',', -6.2021942138671875)\n",
      "(high, ',', -8.075313568115234)\n",
      "(response, ',', -9.270339012145996)\n",
      "(to, ',', -3.83851957321167)\n",
      "(people, ',', -5.853616714477539)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(squat, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(Another, ',', -9.868120193481445)\n",
      "(node, ',', -19.579313278198242)\n",
      "(has, ',', -6.2021942138671875)\n",
      "(high, ',', -8.075313568115234)\n",
      "(response, ',', -9.270339012145996)\n",
      "(to, ',', -3.83851957321167)\n",
      "(people, ',', -5.853616714477539)\n",
      "(standing, ',', -10.07271957397461)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(upright, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(Yet, ',', -10.381902694702148)\n",
      "(another, ',', -7.790835380554199)\n",
      "(node, ',', -19.579313278198242)\n",
      "(concisely, ',', -19.579313278198242)\n",
      "(captures, ',', -19.579313278198242)\n",
      "(two, ',', -7.494669437408447)\n",
      "(clusters, ',', -19.579313278198242)\n",
      "(of, ',', -4.1284637451171875)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(pose, ',', -11.87548542022705)\n",
      "(patterns, ',', -11.080256462097168)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(In, ',', -7.314908981323242)\n",
      "(our, ',', -7.124547481536865)\n",
      "(deep, ',', -9.902402877807617)\n",
      "(model, ',', -9.600680351257324)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(the, ',', -3.425445795059204)\n",
      "(ﬁrst, ',', -19.579313278198242)\n",
      "(hidden, ',', -10.728233337402344)\n",
      "(layer, ',', -11.220696449279785)\n",
      "(has, ',', -6.2021942138671875)\n",
      "(200, ',', -10.376395225524902)\n",
      "(hidden, ',', -10.728233337402344)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(nodes, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(the, ',', -3.425445795059204)\n",
      "(second, ',', -8.6246976852417)\n",
      "(layer, ',', -11.220696449279785)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(i.e., ',', -9.802892684936523)\n",
      "(h2, ',', -19.579313278198242)\n",
      "(in, ',', -4.585874080657959)\n",
      "(Eq, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "((, ',', -5.70067024230957)\n",
      "(10, ',', -8.452024459838867)\n",
      "(), ',', -5.341753005981445)\n",
      "(has, ',', -6.2021942138671875)\n",
      "(150, ',', -11.107267379760742)\n",
      "(hidden, ',', -10.728233337402344)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(nodes, ',', -19.579313278198242)\n",
      "(and, ',', -4.195279121398926)\n",
      "(the, ',', -3.425445795059204)\n",
      "(third, ',', -9.136503219604492)\n",
      "(layer, ',', -11.220696449279785)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(i.e., ',', -9.802892684936523)\n",
      "(h3, ',', -19.579313278198242)\n",
      "(in, ',', -4.585874080657959)\n",
      "(Eq, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "((, ',', -5.70067024230957)\n",
      "(11, ',', -10.191385269165039)\n",
      "(), ',', -5.341753005981445)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(has, ',', -6.2021942138671875)\n",
      "(100, ',', -8.705434799194336)\n",
      "(hidden, ',', -10.728233337402344)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(nodes, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(Since, ',', -9.441577911376953)\n",
      "(the, ',', -3.425445795059204)\n",
      "(dimensions, ',', -19.579313278198242)\n",
      "(of, ',', -4.1284637451171875)\n",
      "(s, ',', -8.883868217468262)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(d, ',', -11.57960033416748)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(and, ',', -4.195279121398926)\n",
      "(t, ',', -10.763195991516113)\n",
      "(are, ',', -5.160149097442627)\n",
      "(small, ',', -8.337030410766602)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(train-, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(ing, ',', -19.579313278198242)\n",
      "(of, ',', -4.1284637451171875)\n",
      "(the, ',', -3.425445795059204)\n",
      "(deep, ',', -9.902402877807617)\n",
      "(model, ',', -9.600680351257324)\n",
      "(is, ',', -4.3297648429870605)\n",
      "(fast, ',', -9.421085357666016)\n",
      "(., ',', -3.0729479789733887)\n",
      "(Unlike, ',', -11.598064422607422)\n",
      "(loopy, ',', -19.579313278198242)\n",
      "(graphical, ',', -19.579313278198242)\n",
      "(mod-, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(els, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(the, ',', -3.425445795059204)\n",
      "(deep, ',', -9.902402877807617)\n",
      "(model, ',', -9.600680351257324)\n",
      "(is, ',', -4.3297648429870605)\n",
      "(fast, ',', -9.421085357666016)\n",
      "(in, ',', -4.585874080657959)\n",
      "(the, ',', -3.425445795059204)\n",
      "(inference, ',', -19.579313278198242)\n",
      "(stage, ',', -10.263298034667969)\n",
      "(because, ',', -6.412496566772461)\n",
      "(it, ',', -4.5064496994018555)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(does, ',', -6.390727519989014)\n",
      "(not, ',', -5.21923828125)\n",
      "(require, ',', -9.662212371826172)\n",
      "(loopy, ',', -19.579313278198242)\n",
      "(belief, ',', -9.547979354858398)\n",
      "(propagation, ',', -19.579313278198242)\n",
      "(or, ',', -5.715355396270752)\n",
      "(sampling, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(The, ',', -5.774222373962402)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(extra, ',', -9.46030044555664)\n",
      "(testing, ',', -10.620267868041992)\n",
      "(time, ',', -6.55948543548584)\n",
      "(required, ',', -9.576201438903809)\n",
      "(by, ',', -6.114920139312744)\n",
      "(our, ',', -7.124547481536865)\n",
      "(deep, ',', -9.902402877807617)\n",
      "(model, ',', -9.600680351257324)\n",
      "(is, ',', -4.3297648429870605)\n",
      "(less, ',', -7.764529705047607)\n",
      "(than, ',', -6.372464179992676)\n",
      "(10, ',', -8.452024459838867)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(percent, ',', -10.030643463134766)\n",
      "(of, ',', -4.1284637451171875)\n",
      "(the, ',', -3.425445795059204)\n",
      "(testing, ',', -10.620267868041992)\n",
      "(time, ',', -6.55948543548584)\n",
      "(required, ',', -9.576201438903809)\n",
      "(by, ',', -6.114920139312744)\n",
      "(the, ',', -3.425445795059204)\n",
      "(approach, ',', -9.994755744934082)\n",
      "(in, ',', -4.585874080657959)\n",
      "([, ',', -5.266753196716309)\n",
      "(58, ',', -19.579313278198242)\n",
      "(], ',', -5.498423099517822)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(5, ',', -8.421856880187988)\n",
      "(., ',', -3.0729479789733887)\n",
      "(Experimental, ',', -19.579313278198242)\n",
      "(results, ',', -9.357637405395508)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(The, ',', -5.774222373962402)\n",
      "(proposed, ',', -10.828350067138672)\n",
      "(approach, ',', -9.994755744934082)\n",
      "(is, ',', -4.3297648429870605)\n",
      "(evaluated, ',', -19.579313278198242)\n",
      "(on, ',', -5.263686656951904)\n",
      "(three, ',', -8.800739288330078)\n",
      "(datasets, ',', -19.579313278198242)\n",
      "(:, ',', -6.052439212799072)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(LSP, ',', -19.579313278198242)\n",
      "([, ',', -5.266753196716309)\n",
      "(21, ',', -11.032556533813477)\n",
      "(], ',', -5.498423099517822)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(PARSE, ',', -19.579313278198242)\n",
      "([, ',', -5.266753196716309)\n",
      "(44, ',', -19.579313278198242)\n",
      "(], ',', -5.498423099517822)\n",
      "(and, ',', -4.195279121398926)\n",
      "(UIUC, ',', -19.579313278198242)\n",
      "(people, ',', -5.853616714477539)\n",
      "([, ',', -5.266753196716309)\n",
      "(53, ',', -19.579313278198242)\n",
      "(], ',', -5.498423099517822)\n",
      "(., ',', -3.0729479789733887)\n",
      "(The, ',', -5.774222373962402)\n",
      "(train-, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(ing, ',', -19.579313278198242)\n",
      "(procedure, ',', -11.306013107299805)\n",
      "(and, ',', -4.195279121398926)\n",
      "(training, ',', -10.32292366027832)\n",
      "(set, ',', -8.572531700134277)\n",
      "(are, ',', -5.160149097442627)\n",
      "(the, ',', -3.425445795059204)\n",
      "(same, ',', -7.054326057434082)\n",
      "(as, ',', -5.507394313812256)\n",
      "([, ',', -5.266753196716309)\n",
      "(58, ',', -19.579313278198242)\n",
      "(], ',', -5.498423099517822)\n",
      "(., ',', -3.0729479789733887)\n",
      "(Positive, ',', -19.579313278198242)\n",
      "(\n",
      "\n",
      "\f",
      ", ',', -19.579313278198242)\n",
      "(pose, ',', -11.87548542022705)\n",
      "(estimation, ',', -19.579313278198242)\n",
      "(for, ',', -4.91396951675415)\n",
      "(future, ',', -8.836128234863281)\n",
      "(applications, ',', -10.240983963012695)\n",
      "(., ',', -3.0729479789733887)\n",
      "(For, ',', -8.115267753601074)\n",
      "(example, ',', -8.316376686096191)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(in, ',', -4.585874080657959)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(character, ',', -9.674116134643555)\n",
      "(animation, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(the, ',', -3.425445795059204)\n",
      "(rendering, ',', -11.683619499206543)\n",
      "(of, ',', -4.1284637451171875)\n",
      "(a, ',', -3.9830753803253174)\n",
      "(limb, ',', -19.579313278198242)\n",
      "(is, ',', -4.3297648429870605)\n",
      "(possible, ',', -8.464611053466797)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(only, ',', -6.547285556793213)\n",
      "(when, ',', -6.408374786376953)\n",
      "(both, ',', -7.8158979415893555)\n",
      "(end, ',', -7.972523212432861)\n",
      "(points, ',', -8.80964183807373)\n",
      "(of, ',', -4.1284637451171875)\n",
      "(the, ',', -3.425445795059204)\n",
      "(limb, ',', -19.579313278198242)\n",
      "(are, ',', -5.160149097442627)\n",
      "(correct, ',', -8.887826919555664)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(We, ',', -7.376642227172852)\n",
      "(follow, ',', -9.378072738647461)\n",
      "([, ',', -5.266753196716309)\n",
      "(11, ',', -10.191385269165039)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(40, ',', -9.825430870056152)\n",
      "(], ',', -5.498423099517822)\n",
      "(and, ',', -4.195279121398926)\n",
      "(use, ',', -7.119458198547363)\n",
      "(the, ',', -3.425445795059204)\n",
      "(observer, ',', -19.579313278198242)\n",
      "(-, ',', -5.202415943145752)\n",
      "(centric, ',', -19.579313278198242)\n",
      "(anno-, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(tations, ',', -19.579313278198242)\n",
      "(for, ',', -4.91396951675415)\n",
      "(all, ',', -5.896712779998779)\n",
      "(approaches, ',', -11.651655197143555)\n",
      "(when, ',', -6.408374786376953)\n",
      "(we, ',', -6.075284481048584)\n",
      "(evaluate, ',', -11.832168579101562)\n",
      "(on, ',', -5.263686656951904)\n",
      "(the, ',', -3.425445795059204)\n",
      "(LSP, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(dataset, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(5.2, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(Overall, ',', -19.579313278198242)\n",
      "(experimental, ',', -19.579313278198242)\n",
      "(results, ',', -9.357637405395508)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(Table, ',', -19.579313278198242)\n",
      "(1, ',', -7.901819705963135)\n",
      "(shows, ',', -9.112006187438965)\n",
      "(the, ',', -3.425445795059204)\n",
      "(experimental, ',', -19.579313278198242)\n",
      "(results, ',', -9.357637405395508)\n",
      "(from, ',', -6.028810501098633)\n",
      "(the, ',', -3.425445795059204)\n",
      "(three, ',', -8.800739288330078)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(datasets, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(Pishchulin, ',', -19.579313278198242)\n",
      "(’s, ',', -8.840847969055176)\n",
      "(approach, ',', -9.994755744934082)\n",
      "(in, ',', -4.585874080657959)\n",
      "([, ',', -5.266753196716309)\n",
      "(40, ',', -9.825430870056152)\n",
      "(], ',', -5.498423099517822)\n",
      "(used, ',', -7.587561130523682)\n",
      "(the, ',', -3.425445795059204)\n",
      "(LSP+PARSE, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(training, ',', -10.32292366027832)\n",
      "(set, ',', -8.572531700134277)\n",
      "(when, ',', -6.408374786376953)\n",
      "(evaluated, ',', -19.579313278198242)\n",
      "(on, ',', -5.263686656951904)\n",
      "(the, ',', -3.425445795059204)\n",
      "(PARSE, ',', -19.579313278198242)\n",
      "(dataset, ',', -19.579313278198242)\n",
      "(and, ',', -4.195279121398926)\n",
      "(used, ',', -7.587561130523682)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(the, ',', -3.425445795059204)\n",
      "(UIUC+LSP, ',', -19.579313278198242)\n",
      "(training, ',', -10.32292366027832)\n",
      "(set, ',', -8.572531700134277)\n",
      "(when, ',', -6.408374786376953)\n",
      "(evaluated, ',', -19.579313278198242)\n",
      "(on, ',', -5.263686656951904)\n",
      "(the, ',', -3.425445795059204)\n",
      "(UIUC, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(dataset, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(To, ',', -8.49110221862793)\n",
      "(evaluate, ',', -11.832168579101562)\n",
      "(on, ',', -5.263686656951904)\n",
      "(the, ',', -3.425445795059204)\n",
      "(PARSE, ',', -19.579313278198242)\n",
      "(dataset, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(Pishchulin, ',', -19.579313278198242)\n",
      "(’s, ',', -8.840847969055176)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(approach, ',', -9.994755744934082)\n",
      "([, ',', -5.266753196716309)\n",
      "(40, ',', -9.825430870056152)\n",
      "(], ',', -5.498423099517822)\n",
      "(+, ',', -9.560532569885254)\n",
      "([, ',', -5.266753196716309)\n",
      "(42, ',', -11.540140151977539)\n",
      "(], ',', -5.498423099517822)\n",
      "(included, ',', -10.173290252685547)\n",
      "(LSP+PARSE, ',', -19.579313278198242)\n",
      "(and, ',', -4.195279121398926)\n",
      "(2744, ',', -19.579313278198242)\n",
      "(ex-, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(tra, ',', -19.579313278198242)\n",
      "(animated, ',', -19.579313278198242)\n",
      "(samples, ',', -19.579313278198242)\n",
      "(for, ',', -4.91396951675415)\n",
      "(training, ',', -10.32292366027832)\n",
      "(., ',', -3.0729479789733887)\n",
      "(Johnson, ',', -11.605546951293945)\n",
      "(’s, ',', -8.840847969055176)\n",
      "(approach, ',', -9.994755744934082)\n",
      "(in, ',', -4.585874080657959)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "([, ',', -5.266753196716309)\n",
      "(22, ',', -11.036799430847168)\n",
      "(], ',', -5.498423099517822)\n",
      "(included, ',', -10.173290252685547)\n",
      "(10,000, ',', -11.007466316223145)\n",
      "(extra, ',', -9.46030044555664)\n",
      "(training, ',', -10.32292366027832)\n",
      "(samples, ',', -19.579313278198242)\n",
      "(when, ',', -6.408374786376953)\n",
      "(evaluated, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(on, ',', -5.263686656951904)\n",
      "(the, ',', -3.425445795059204)\n",
      "(PARSE, ',', -19.579313278198242)\n",
      "(dataset, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(In, ',', -7.314908981323242)\n",
      "(all, ',', -5.896712779998779)\n",
      "(experiments, ',', -11.601799011230469)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(Andriluka, ',', -19.579313278198242)\n",
      "(’s, ',', -8.840847969055176)\n",
      "(ap-, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(proach, ',', -19.579313278198242)\n",
      "(in, ',', -4.585874080657959)\n",
      "([, ',', -5.266753196716309)\n",
      "(2, ',', -7.786293029785156)\n",
      "(], ',', -5.498423099517822)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(Yang, ',', -19.579313278198242)\n",
      "(and, ',', -4.195279121398926)\n",
      "(Ramanan, ',', -19.579313278198242)\n",
      "(’s, ',', -8.840847969055176)\n",
      "(approach, ',', -9.994755744934082)\n",
      "(in, ',', -4.585874080657959)\n",
      "([, ',', -5.266753196716309)\n",
      "(57, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(58, ',', -19.579313278198242)\n",
      "(], ',', -5.498423099517822)\n",
      "(and, ',', -4.195279121398926)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(our, ',', -7.124547481536865)\n",
      "(approach, ',', -9.994755744934082)\n",
      "(are, ',', -5.160149097442627)\n",
      "(trained, ',', -10.797750473022461)\n",
      "(on, ',', -5.263686656951904)\n",
      "(the, ',', -3.425445795059204)\n",
      "(1000, ',', -10.40650463104248)\n",
      "(training, ',', -10.32292366027832)\n",
      "(images, ',', -10.350371360778809)\n",
      "(of, ',', -4.1284637451171875)\n",
      "(the, ',', -3.425445795059204)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(LSP, ',', -19.579313278198242)\n",
      "(dataset, ',', -19.579313278198242)\n",
      "([, ',', -5.266753196716309)\n",
      "(21, ',', -11.032556533813477)\n",
      "(], ',', -5.498423099517822)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(As, ',', -7.891604900360107)\n",
      "(shown, ',', -10.03923225402832)\n",
      "(in, ',', -4.585874080657959)\n",
      "(Table, ',', -19.579313278198242)\n",
      "(1, ',', -7.901819705963135)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(our, ',', -7.124547481536865)\n",
      "(deep, ',', -9.902402877807617)\n",
      "(model, ',', -9.600680351257324)\n",
      "(obviously, ',', -9.016507148742676)\n",
      "(improves, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(the, ',', -3.425445795059204)\n",
      "(pose, ',', -11.87548542022705)\n",
      "(estimation, ',', -19.579313278198242)\n",
      "(accuracy, ',', -11.345757484436035)\n",
      "(and, ',', -4.195279121398926)\n",
      "(outperforms, ',', -19.579313278198242)\n",
      "(all, ',', -5.896712779998779)\n",
      "(the, ',', -3.425445795059204)\n",
      "(state-, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(of, ',', -4.1284637451171875)\n",
      "(-, ',', -5.202415943145752)\n",
      "(the, ',', -3.425445795059204)\n",
      "(-, ',', -5.202415943145752)\n",
      "(art, ',', -9.778430938720703)\n",
      "(on, ',', -5.263686656951904)\n",
      "(these, ',', -7.158668518066406)\n",
      "(three, ',', -8.800739288330078)\n",
      "(datasets, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(Speciﬁcally, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(our, ',', -7.124547481536865)\n",
      "(approach, ',', -9.994755744934082)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(is, ',', -4.3297648429870605)\n",
      "(better, ',', -7.226652145385742)\n",
      "(in, ',', -4.585874080657959)\n",
      "(detecting, ',', -19.579313278198242)\n",
      "(legs, ',', -10.912962913513184)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(arms, ',', -10.243865966796875)\n",
      "(and, ',', -4.195279121398926)\n",
      "(head, ',', -8.585274696350098)\n",
      "(compared, ',', -9.660063743591309)\n",
      "(with, ',', -5.363764762878418)\n",
      "(ex-, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(isting, ',', -19.579313278198242)\n",
      "(approaches, ',', -11.651655197143555)\n",
      "(., ',', -3.0729479789733887)\n",
      "(The, ',', -5.774222373962402)\n",
      "(approach, ',', -9.994755744934082)\n",
      "(of, ',', -4.1284637451171875)\n",
      "(Pishchulin, ',', -19.579313278198242)\n",
      "(et, ',', -11.39320182800293)\n",
      "(al, ',', -10.772948265075684)\n",
      "(., ',', -3.0729479789733887)\n",
      "([, ',', -5.266753196716309)\n",
      "(42, ',', -11.540140151977539)\n",
      "(], ',', -5.498423099517822)\n",
      "(is, ',', -4.3297648429870605)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(better, ',', -7.226652145385742)\n",
      "(than, ',', -6.372464179992676)\n",
      "(our, ',', -7.124547481536865)\n",
      "(approach, ',', -9.994755744934082)\n",
      "(in, ',', -4.585874080657959)\n",
      "(locating, ',', -19.579313278198242)\n",
      "(torso, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(possibly, ',', -9.415185928344727)\n",
      "(because, ',', -6.412496566772461)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(the, ',', -3.425445795059204)\n",
      "(torso, ',', -19.579313278198242)\n",
      "(region, ',', -10.685432434082031)\n",
      "(is, ',', -4.3297648429870605)\n",
      "(included, ',', -10.173290252685547)\n",
      "(in, ',', -4.585874080657959)\n",
      "(many, ',', -7.231371879577637)\n",
      "(poslets, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(which, ',', -6.728941917419434)\n",
      "(helps, ',', -10.029866218566895)\n",
      "(to, ',', -3.83851957321167)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(increase, ',', -9.376858711242676)\n",
      "(the, ',', -3.425445795059204)\n",
      "(accuracy, ',', -11.345757484436035)\n",
      "(of, ',', -4.1284637451171875)\n",
      "(their, ',', -6.1832733154296875)\n",
      "(approach, ',', -9.994755744934082)\n",
      "(in, ',', -4.585874080657959)\n",
      "(locating, ',', -19.579313278198242)\n",
      "(torso, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(Our, ',', -9.764631271362305)\n",
      "(approach, ',', -9.994755744934082)\n",
      "(is, ',', -4.3297648429870605)\n",
      "(complementary, ',', -19.579313278198242)\n",
      "(to, ',', -3.83851957321167)\n",
      "(existing, ',', -10.217281341552734)\n",
      "(approaches, ',', -11.651655197143555)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(because, ',', -6.412496566772461)\n",
      "(the, ',', -3.425445795059204)\n",
      "(information, ',', -8.817930221557617)\n",
      "(sources, ',', -9.963014602661133)\n",
      "(provided, ',', -10.061431884765625)\n",
      "(by, ',', -6.114920139312744)\n",
      "(these, ',', -7.158668518066406)\n",
      "(ap-, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(proaches, ',', -19.579313278198242)\n",
      "(can, ',', -5.9138712882995605)\n",
      "(be, ',', -5.210641384124756)\n",
      "(used, ',', -7.587561130523682)\n",
      "(by, ',', -6.114920139312744)\n",
      "(our, ',', -7.124547481536865)\n",
      "(model, ',', -9.600680351257324)\n",
      "(to, ',', -3.83851957321167)\n",
      "(improve, ',', -10.438716888427734)\n",
      "(their, ',', -6.1832733154296875)\n",
      "(re-, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(sults, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(Currently, ',', -11.87057876586914)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(our, ',', -7.124547481536865)\n",
      "(model, ',', -9.600680351257324)\n",
      "(uses, ',', -9.424895286560059)\n",
      "(the, ',', -3.425445795059204)\n",
      "(approach, ',', -9.994755744934082)\n",
      "(in, ',', -4.585874080657959)\n",
      "([, ',', -5.266753196716309)\n",
      "(58, ',', -19.579313278198242)\n",
      "(], ',', -5.498423099517822)\n",
      "(to, ',', -3.83851957321167)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(obtain, ',', -11.505571365356445)\n",
      "(information, ',', -8.817930221557617)\n",
      "(sources, ',', -9.963014602661133)\n",
      "(., ',', -3.0729479789733887)\n",
      "(Compared, ',', -19.579313278198242)\n",
      "(with, ',', -5.363764762878418)\n",
      "(the, ',', -3.425445795059204)\n",
      "(approach, ',', -9.994755744934082)\n",
      "(in, ',', -4.585874080657959)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "([, ',', -5.266753196716309)\n",
      "(58, ',', -19.579313278198242)\n",
      "(], ',', -5.498423099517822)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(our, ',', -7.124547481536865)\n",
      "(approach, ',', -9.994755744934082)\n",
      "(improves, ',', -19.579313278198242)\n",
      "(the, ',', -3.425445795059204)\n",
      "(pose, ',', -11.87548542022705)\n",
      "(estimation, ',', -19.579313278198242)\n",
      "(accuracy, ',', -11.345757484436035)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(by, ',', -6.114920139312744)\n",
      "(5.8%, ',', -19.579313278198242)\n",
      "((, ',', -5.70067024230957)\n",
      "(62.8%, ',', -19.579313278198242)\n",
      "(vs., ',', -10.26821517944336)\n",
      "(68.6%, ',', -19.579313278198242)\n",
      "(PCP, ',', -19.579313278198242)\n",
      "(), ',', -5.341753005981445)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(7.4%, ',', -19.579313278198242)\n",
      "((, ',', -5.70067024230957)\n",
      "(63.6%, ',', -19.579313278198242)\n",
      "(vs., ',', -10.26821517944336)\n",
      "(71.0%, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(PCP, ',', -19.579313278198242)\n",
      "(), ',', -5.341753005981445)\n",
      "(and, ',', -4.195279121398926)\n",
      "(8.6%, ',', -19.579313278198242)\n",
      "((, ',', -5.70067024230957)\n",
      "(57.0%, ',', -19.579313278198242)\n",
      "(vs., ',', -10.26821517944336)\n",
      "(65.6%, ',', -19.579313278198242)\n",
      "(PCP, ',', -19.579313278198242)\n",
      "(), ',', -5.341753005981445)\n",
      "(respectively, ',', -19.579313278198242)\n",
      "(on, ',', -5.263686656951904)\n",
      "(the, ',', -3.425445795059204)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(LSP, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(PARSE, ',', -19.579313278198242)\n",
      "(and, ',', -4.195279121398926)\n",
      "(UIUC, ',', -19.579313278198242)\n",
      "(datasets, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(Fig, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(5, ',', -8.421856880187988)\n",
      "(shows, ',', -9.112006187438965)\n",
      "(the, ',', -3.425445795059204)\n",
      "(compar-, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(ison, ',', -19.579313278198242)\n",
      "(between, ',', -8.03024673461914)\n",
      "(our, ',', -7.124547481536865)\n",
      "(approach, ',', -9.994755744934082)\n",
      "((, ',', -5.70067024230957)\n",
      "(left, ',', -8.262680053710938)\n",
      "(), ',', -5.341753005981445)\n",
      "(and, ',', -4.195279121398926)\n",
      "(the, ',', -3.425445795059204)\n",
      "(approach, ',', -9.994755744934082)\n",
      "(in, ',', -4.585874080657959)\n",
      "([, ',', -5.266753196716309)\n",
      "(58, ',', -19.579313278198242)\n",
      "(], ',', -5.498423099517822)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "((, ',', -5.70067024230957)\n",
      "(right, ',', -6.780907154083252)\n",
      "(), ',', -5.341753005981445)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(5.3, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(Results, ',', -19.579313278198242)\n",
      "(on, ',', -5.263686656951904)\n",
      "(different, ',', -7.797479629516602)\n",
      "(designs, ',', -11.643821716308594)\n",
      "(of, ',', -4.1284637451171875)\n",
      "(deep, ',', -9.902402877807617)\n",
      "(models, ',', -10.62589168548584)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(In, ',', -7.314908981323242)\n",
      "(this, ',', -5.417123317718506)\n",
      "(section, ',', -10.175085067749023)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(we, ',', -6.075284481048584)\n",
      "(evaluate, ',', -11.832168579101562)\n",
      "(different, ',', -7.797479629516602)\n",
      "(designs, ',', -11.643821716308594)\n",
      "(of, ',', -4.1284637451171875)\n",
      "(deep, ',', -9.902402877807617)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(models, ',', -10.62589168548584)\n",
      "(., ',', -3.0729479789733887)\n",
      "(Yang, ',', -19.579313278198242)\n",
      "(and, ',', -4.195279121398926)\n",
      "(Ramanan, ',', -19.579313278198242)\n",
      "(’s, ',', -8.840847969055176)\n",
      "(approach, ',', -9.994755744934082)\n",
      "(in, ',', -4.585874080657959)\n",
      "([, ',', -5.266753196716309)\n",
      "(58, ',', -19.579313278198242)\n",
      "(], ',', -5.498423099517822)\n",
      "(is, ',', -4.3297648429870605)\n",
      "(used, ',', -7.587561130523682)\n",
      "(as, ',', -5.507394313812256)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(the, ',', -3.425445795059204)\n",
      "(baseline, ',', -19.579313278198242)\n",
      "(because, ',', -6.412496566772461)\n",
      "(this, ',', -5.417123317718506)\n",
      "(approach, ',', -9.994755744934082)\n",
      "(is, ',', -4.3297648429870605)\n",
      "(used, ',', -7.587561130523682)\n",
      "(by, ',', -6.114920139312744)\n",
      "(our, ',', -7.124547481536865)\n",
      "(model, ',', -9.600680351257324)\n",
      "(for, ',', -4.91396951675415)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(obtaining, ',', -19.579313278198242)\n",
      "(information, ',', -8.817930221557617)\n",
      "(sources, ',', -9.963014602661133)\n",
      "(., ',', -3.0729479789733887)\n",
      "(To, ',', -8.49110221862793)\n",
      "(be, ',', -5.210641384124756)\n",
      "(concise, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(we, ',', -6.075284481048584)\n",
      "(only, ',', -6.547285556793213)\n",
      "(refer, ',', -10.454032897949219)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(to, ',', -3.83851957321167)\n",
      "(the, ',', -3.425445795059204)\n",
      "(PCP, ',', -19.579313278198242)\n",
      "(results, ',', -9.357637405395508)\n",
      "(on, ',', -5.263686656951904)\n",
      "(the, ',', -3.425445795059204)\n",
      "(LSP, ',', -19.579313278198242)\n",
      "(dataset, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(Depth, ',', -19.579313278198242)\n",
      "(of, ',', -4.1284637451171875)\n",
      "(model, ',', -9.600680351257324)\n",
      "(is, ',', -4.3297648429870605)\n",
      "(investigated, ',', -11.86569595336914)\n",
      "(in, ',', -4.585874080657959)\n",
      "(Table, ',', -19.579313278198242)\n",
      "(2, ',', -7.786293029785156)\n",
      "(., ',', -3.0729479789733887)\n",
      "(The, ',', -5.774222373962402)\n",
      "(approach, ',', -9.994755744934082)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(in, ',', -4.585874080657959)\n",
      "([, ',', -5.266753196716309)\n",
      "(58, ',', -19.579313278198242)\n",
      "(], ',', -5.498423099517822)\n",
      "(uses, ',', -9.424895286560059)\n",
      "(linear, ',', -11.651655197143555)\n",
      "(-, ',', -5.202415943145752)\n",
      "(SVM, ',', -19.579313278198242)\n",
      "(for, ',', -4.91396951675415)\n",
      "(combining, ',', -19.579313278198242)\n",
      "(information, ',', -8.817930221557617)\n",
      "(sources, ',', -9.963014602661133)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(We, ',', -7.376642227172852)\n",
      "(also, ',', -7.134331226348877)\n",
      "(trained, ',', -10.797750473022461)\n",
      "(a, ',', -3.9830753803253174)\n",
      "(Kernel, ',', -19.579313278198242)\n",
      "(-, ',', -5.202415943145752)\n",
      "(SVM, ',', -19.579313278198242)\n",
      "(with, ',', -5.363764762878418)\n",
      "(RBF, ',', -19.579313278198242)\n",
      "(kernel, ',', -10.995153427124023)\n",
      "(for, ',', -4.91396951675415)\n",
      "(learn-, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(ing, ',', -19.579313278198242)\n",
      "(a, ',', -3.9830753803253174)\n",
      "(non, ',', -8.436274528503418)\n",
      "(-, ',', -5.202415943145752)\n",
      "(linear, ',', -11.651655197143555)\n",
      "(model, ',', -9.600680351257324)\n",
      "(using, ',', -7.910459041595459)\n",
      "(the, ',', -3.425445795059204)\n",
      "(off, ',', -7.239304065704346)\n",
      "(-, ',', -5.202415943145752)\n",
      "(the, ',', -3.425445795059204)\n",
      "(-, ',', -5.202415943145752)\n",
      "(shelf, ',', -19.579313278198242)\n",
      "(tool, ',', -9.873420715332031)\n",
      "(Libsvm, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(The, ',', -5.774222373962402)\n",
      "(difference, ',', -8.556026458740234)\n",
      "(in, ',', -4.585874080657959)\n",
      "(PCP, ',', -19.579313278198242)\n",
      "(between, ',', -8.03024673461914)\n",
      "(Linear, ',', -19.579313278198242)\n",
      "(SVM, ',', -19.579313278198242)\n",
      "(and, ',', -4.195279121398926)\n",
      "(kernel-, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(SVM, ',', -19.579313278198242)\n",
      "(is, ',', -4.3297648429870605)\n",
      "(within, ',', -8.974809646606445)\n",
      "(2%, ',', -19.579313278198242)\n",
      "((, ',', -5.70067024230957)\n",
      "(62.8%, ',', -19.579313278198242)\n",
      "(vs., ',', -10.26821517944336)\n",
      "(64.2%, ',', -19.579313278198242)\n",
      "(on, ',', -5.263686656951904)\n",
      "(LSP, ',', -19.579313278198242)\n",
      "(), ',', -5.341753005981445)\n",
      "(., ',', -3.0729479789733887)\n",
      "(Bengio, ',', -19.579313278198242)\n",
      "([, ',', -5.266753196716309)\n",
      "(3, ',', -8.093857765197754)\n",
      "(], ',', -5.498423099517822)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(Figure, ',', -19.579313278198242)\n",
      "(4, ',', -8.51557731628418)\n",
      "(., ',', -3.0729479789733887)\n",
      "(Visualization, ',', -19.579313278198242)\n",
      "(of, ',', -4.1284637451171875)\n",
      "(mixture, ',', -19.579313278198242)\n",
      "(-, ',', -5.202415943145752)\n",
      "(type, ',', -8.814929008483887)\n",
      "(patterns, ',', -11.080256462097168)\n",
      "(extracted, ',', -19.579313278198242)\n",
      "(by, ',', -6.114920139312744)\n",
      "(hid-, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(den, ',', -19.579313278198242)\n",
      "(nodes, ',', -19.579313278198242)\n",
      "(in, ',', -4.585874080657959)\n",
      "(h2,3, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(We, ',', -7.376642227172852)\n",
      "(use, ',', -7.119458198547363)\n",
      "(the, ',', -3.425445795059204)\n",
      "(approach, ',', -9.994755744934082)\n",
      "(in, ',', -4.585874080657959)\n",
      "([, ',', -5.266753196716309)\n",
      "(26, ',', -11.492071151733398)\n",
      "(], ',', -5.498423099517822)\n",
      "(and, ',', -4.195279121398926)\n",
      "(visualize, ',', -19.579313278198242)\n",
      "(train-, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(ing, ',', -19.579313278198242)\n",
      "(samples, ',', -19.579313278198242)\n",
      "(with, ',', -5.363764762878418)\n",
      "(the, ',', -3.425445795059204)\n",
      "(largest, ',', -10.449295997619629)\n",
      "(responses, ',', -11.11184024810791)\n",
      "(on, ',', -5.263686656951904)\n",
      "(each, ',', -8.295592308044434)\n",
      "(hidden, ',', -10.728233337402344)\n",
      "(node, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(Sam-, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(ples, ',', -19.579313278198242)\n",
      "(with, ',', -5.363764762878418)\n",
      "(the, ',', -3.425445795059204)\n",
      "(highest, ',', -10.414461135864258)\n",
      "(responses, ',', -11.11184024810791)\n",
      "(are, ',', -5.160149097442627)\n",
      "(placed, ',', -10.85635757446289)\n",
      "(at, ',', -5.8868231773376465)\n",
      "(the, ',', -3.425445795059204)\n",
      "(upper, ',', -10.651596069335938)\n",
      "(-, ',', -5.202415943145752)\n",
      "(left, ',', -8.262680053710938)\n",
      "(corner, ',', -10.489069938659668)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(Hidden, ',', -19.579313278198242)\n",
      "(node, ',', -19.579313278198242)\n",
      "(1, ',', -7.901819705963135)\n",
      "(has, ',', -6.2021942138671875)\n",
      "(high, ',', -8.075313568115234)\n",
      "(response, ',', -9.270339012145996)\n",
      "(to, ',', -3.83851957321167)\n",
      "(people, ',', -5.853616714477539)\n",
      "(squat, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(Node, ',', -19.579313278198242)\n",
      "(2, ',', -7.786293029785156)\n",
      "(has, ',', -6.2021942138671875)\n",
      "(high, ',', -8.075313568115234)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(response, ',', -9.270339012145996)\n",
      "(to, ',', -3.83851957321167)\n",
      "(standing, ',', -10.07271957397461)\n",
      "(people, ',', -5.853616714477539)\n",
      "(., ',', -3.0729479789733887)\n",
      "(Node, ',', -19.579313278198242)\n",
      "(3, ',', -8.093857765197754)\n",
      "(has, ',', -6.2021942138671875)\n",
      "(high, ',', -8.075313568115234)\n",
      "(response, ',', -9.270339012145996)\n",
      "(to, ',', -3.83851957321167)\n",
      "(two, ',', -7.494669437408447)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(clusters, ',', -19.579313278198242)\n",
      "(of, ',', -4.1284637451171875)\n",
      "(pose, ',', -11.87548542022705)\n",
      "(patterns, ',', -11.080256462097168)\n",
      "(., ',', -3.0729479789733887)\n",
      "(Best, ',', -10.38855266571045)\n",
      "(viewed, ',', -11.39624309539795)\n",
      "(in, ',', -4.585874080657959)\n",
      "(color, ',', -9.952893257141113)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(training, ',', -10.32292366027832)\n",
      "(samples, ',', -19.579313278198242)\n",
      "(are, ',', -5.160149097442627)\n",
      "(constrained, ',', -19.579313278198242)\n",
      "(to, ',', -3.83851957321167)\n",
      "(have, ',', -5.235879421234131)\n",
      "(estimated, ',', -11.822792053222656)\n",
      "(part, ',', -7.779394626617432)\n",
      "(lo-, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(cations, ',', -19.579313278198242)\n",
      "(near, ',', -9.217597961425781)\n",
      "(the, ',', -3.425445795059204)\n",
      "(ground, ',', -9.634105682373047)\n",
      "(truth, ',', -8.972647666931152)\n",
      "(., ',', -3.0729479789733887)\n",
      "(Part, ',', -10.951260566711426)\n",
      "(of, ',', -4.1284637451171875)\n",
      "(the, ',', -3.425445795059204)\n",
      "(training, ',', -10.32292366027832)\n",
      "(data, ',', -9.084769248962402)\n",
      "(is, ',', -4.3297648429870605)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(used, ',', -7.587561130523682)\n",
      "(for, ',', -4.91396951675415)\n",
      "(validation, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(5.1, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(Evaluation, ',', -19.579313278198242)\n",
      "(criteria, ',', -11.267788887023926)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(In, ',', -7.314908981323242)\n",
      "(all, ',', -5.896712779998779)\n",
      "(experiments, ',', -11.601799011230469)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(we, ',', -6.075284481048584)\n",
      "(use, ',', -7.119458198547363)\n",
      "(the, ',', -3.425445795059204)\n",
      "(most, ',', -7.022420406341553)\n",
      "(popular, ',', -9.392342567443848)\n",
      "(criterion, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(which, ',', -6.728941917419434)\n",
      "(is, ',', -4.3297648429870605)\n",
      "(the, ',', -3.425445795059204)\n",
      "(percentage, ',', -10.06948184967041)\n",
      "(of, ',', -4.1284637451171875)\n",
      "(correctly, ',', -10.082497596740723)\n",
      "(localized, ',', -19.579313278198242)\n",
      "(parts, ',', -9.519133567810059)\n",
      "((, ',', -5.70067024230957)\n",
      "(PCP, ',', -19.579313278198242)\n",
      "(), ',', -5.341753005981445)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(introduced, ',', -11.038928031921387)\n",
      "(in, ',', -4.585874080657959)\n",
      "([, ',', -5.266753196716309)\n",
      "(14, ',', -10.376395225524902)\n",
      "(], ',', -5.498423099517822)\n",
      "(., ',', -3.0729479789733887)\n",
      "(As, ',', -7.891604900360107)\n",
      "(stated, ',', -10.064643859863281)\n",
      "(in, ',', -4.585874080657959)\n",
      "([, ',', -5.266753196716309)\n",
      "(42, ',', -11.540140151977539)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(58, ',', -19.579313278198242)\n",
      "(], ',', -5.498423099517822)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(the, ',', -3.425445795059204)\n",
      "(PCP, ',', -19.579313278198242)\n",
      "(scoring, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(metric, ',', -11.663522720336914)\n",
      "(has, ',', -6.2021942138671875)\n",
      "(been, ',', -6.6935648918151855)\n",
      "(implemented, ',', -10.740806579589844)\n",
      "(in, ',', -4.585874080657959)\n",
      "(different, ',', -7.797479629516602)\n",
      "(ways, ',', -9.124493598937988)\n",
      "(in, ',', -4.585874080657959)\n",
      "(different, ',', -7.797479629516602)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(papers, ',', -10.725114822387695)\n",
      "(., ',', -3.0729479789733887)\n",
      "(These, ',', -8.924954414367676)\n",
      "(differences, ',', -10.269201278686523)\n",
      "(have, ',', -5.235879421234131)\n",
      "(two, ',', -7.494669437408447)\n",
      "(dimensions, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(1, ',', -7.901819705963135)\n",
      "(., ',', -3.0729479789733887)\n",
      "(There, ',', -7.277902603149414)\n",
      "(are, ',', -5.160149097442627)\n",
      "(two, ',', -7.494669437408447)\n",
      "(ways, ',', -9.124493598937988)\n",
      "(to, ',', -3.83851957321167)\n",
      "(compute, ',', -19.579313278198242)\n",
      "(the, ',', -3.425445795059204)\n",
      "(ﬁnal, ',', -19.579313278198242)\n",
      "(PCP, ',', -19.579313278198242)\n",
      "(score, ',', -10.567034721374512)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(across, ',', -9.273614883422852)\n",
      "(the, ',', -3.425445795059204)\n",
      "(dataset, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(In, ',', -7.314908981323242)\n",
      "(the, ',', -3.425445795059204)\n",
      "(single, ',', -8.629281997680664)\n",
      "(way, ',', -6.743997097015381)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(only, ',', -6.547285556793213)\n",
      "(a, ',', -3.9830753803253174)\n",
      "(single, ',', -8.629281997680664)\n",
      "(candi-, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(date, ',', -9.923830032348633)\n",
      "((, ',', -5.70067024230957)\n",
      "(given, ',', -8.653700828552246)\n",
      "(by, ',', -6.114920139312744)\n",
      "(the, ',', -3.425445795059204)\n",
      "(maximum, ',', -11.178168296813965)\n",
      "(scoring, ',', -19.579313278198242)\n",
      "(candidate, ',', -8.77760124206543)\n",
      "(of, ',', -4.1284637451171875)\n",
      "(an, ',', -5.953293800354004)\n",
      "(al-, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(gorithm, ',', -19.579313278198242)\n",
      "(), ',', -5.341753005981445)\n",
      "(for, ',', -4.91396951675415)\n",
      "(one, ',', -6.081182956695557)\n",
      "(image, ',', -9.559075355529785)\n",
      "(is, ',', -4.3297648429870605)\n",
      "(used, ',', -7.587561130523682)\n",
      "(., ',', -3.0729479789733887)\n",
      "(The, ',', -5.774222373962402)\n",
      "(match, ',', -10.385222434997559)\n",
      "(way, ',', -6.743997097015381)\n",
      "(matches, ',', -11.704129219055176)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(multiple, ',', -9.72434139251709)\n",
      "(candidates, ',', -9.305854797363281)\n",
      "(without, ',', -7.6655049324035645)\n",
      "(penalizing, ',', -19.579313278198242)\n",
      "(false, ',', -9.506165504455566)\n",
      "(positives, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(2, ',', -7.786293029785156)\n",
      "(., ',', -3.0729479789733887)\n",
      "(There, ',', -7.277902603149414)\n",
      "(are, ',', -5.160149097442627)\n",
      "(two, ',', -7.494669437408447)\n",
      "(deﬁnitions, ',', -19.579313278198242)\n",
      "(of, ',', -4.1284637451171875)\n",
      "(a, ',', -3.9830753803253174)\n",
      "(correct, ',', -8.887826919555664)\n",
      "(part, ',', -7.779394626617432)\n",
      "(localization, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(For, ',', -8.115267753601074)\n",
      "(the, ',', -3.425445795059204)\n",
      "(deﬁnition, ',', -19.579313278198242)\n",
      "(both, ',', -7.8158979415893555)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(it, ',', -4.5064496994018555)\n",
      "(requires, ',', -9.901719093322754)\n",
      "(both, ',', -7.8158979415893555)\n",
      "(end, ',', -7.972523212432861)\n",
      "(points, ',', -8.80964183807373)\n",
      "(of, ',', -4.1284637451171875)\n",
      "(a, ',', -3.9830753803253174)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(part, ',', -7.779394626617432)\n",
      "((, ',', -5.70067024230957)\n",
      "(for, ',', -4.91396951675415)\n",
      "(example, ',', -8.316376686096191)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(end, ',', -7.972523212432861)\n",
      "(points, ',', -8.80964183807373)\n",
      "(wrist, ',', -19.579313278198242)\n",
      "(and, ',', -4.195279121398926)\n",
      "(elbow, ',', -19.579313278198242)\n",
      "(for, ',', -4.91396951675415)\n",
      "(the, ',', -3.425445795059204)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(part, ',', -7.779394626617432)\n",
      "(lower, ',', -9.25090217590332)\n",
      "(arm, ',', -10.667635917663574)\n",
      "(), ',', -5.341753005981445)\n",
      "(to, ',', -3.83851957321167)\n",
      "(be, ',', -5.210641384124756)\n",
      "(correct, ',', -8.887826919555664)\n",
      "(., ',', -3.0729479789733887)\n",
      "(For, ',', -8.115267753601074)\n",
      "(the, ',', -3.425445795059204)\n",
      "(deﬁnition, ',', -19.579313278198242)\n",
      "(avg, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(it, ',', -4.5064496994018555)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(requires, ',', -9.901719093322754)\n",
      "(only, ',', -6.547285556793213)\n",
      "(the, ',', -3.425445795059204)\n",
      "(average, ',', -9.035868644714355)\n",
      "(of, ',', -4.1284637451171875)\n",
      "(the, ',', -3.425445795059204)\n",
      "(endpoints, ',', -19.579313278198242)\n",
      "(to, ',', -3.83851957321167)\n",
      "(be, ',', -5.210641384124756)\n",
      "(correct, ',', -8.887826919555664)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(The, ',', -5.774222373962402)\n",
      "(paper, ',', -8.99613094329834)\n",
      "(in, ',', -4.585874080657959)\n",
      "([, ',', -5.266753196716309)\n",
      "(54, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(57, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(52, ',', -19.579313278198242)\n",
      "(], ',', -5.498423099517822)\n",
      "(used, ',', -7.587561130523682)\n",
      "(‘, ',', -10.899898529052734)\n",
      "(match+avg’., ',', -19.579313278198242)\n",
      "(The, ',', -5.774222373962402)\n",
      "(paper, ',', -8.99613094329834)\n",
      "(in, ',', -4.585874080657959)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "([, ',', -5.266753196716309)\n",
      "(40, ',', -9.825430870056152)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(2, ',', -7.786293029785156)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(42, ',', -11.540140151977539)\n",
      "(], ',', -5.498423099517822)\n",
      "(used, ',', -7.587561130523682)\n",
      "(‘, ',', -10.899898529052734)\n",
      "(single+both’, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(which, ',', -6.728941917419434)\n",
      "(is, ',', -4.3297648429870605)\n",
      "(the, ',', -3.425445795059204)\n",
      "(strictest, ',', -19.579313278198242)\n",
      "(case, ',', -7.9823994636535645)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(and, ',', -4.195279121398926)\n",
      "(generally, ',', -9.2901611328125)\n",
      "(has, ',', -6.2021942138671875)\n",
      "(lower, ',', -9.25090217590332)\n",
      "(PCP, ',', -19.579313278198242)\n",
      "(value, ',', -8.531943321228027)\n",
      "(., ',', -3.0729479789733887)\n",
      "(The, ',', -5.774222373962402)\n",
      "(paper, ',', -8.99613094329834)\n",
      "(in, ',', -4.585874080657959)\n",
      "([, ',', -5.266753196716309)\n",
      "(10, ',', -8.452024459838867)\n",
      "(], ',', -5.498423099517822)\n",
      "(pro-, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(vides, ',', -19.579313278198242)\n",
      "(results, ',', -9.357637405395508)\n",
      "(for, ',', -4.91396951675415)\n",
      "(‘, ',', -10.899898529052734)\n",
      "(match+both’, ',', -19.579313278198242)\n",
      "(and, ',', -4.195279121398926)\n",
      "(‘, ',', -10.899898529052734)\n",
      "(match+avg’., ',', -19.579313278198242)\n",
      "(We, ',', -7.376642227172852)\n",
      "(follow, ',', -9.378072738647461)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "([, ',', -5.266753196716309)\n",
      "(42, ',', -11.540140151977539)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(40, ',', -9.825430870056152)\n",
      "(], ',', -5.498423099517822)\n",
      "(and, ',', -4.195279121398926)\n",
      "(evaluate, ',', -11.832168579101562)\n",
      "(all, ',', -5.896712779998779)\n",
      "(approaches, ',', -11.651655197143555)\n",
      "(using, ',', -7.910459041595459)\n",
      "(the, ',', -3.425445795059204)\n",
      "(strictest, ',', -19.579313278198242)\n",
      "(‘, ',', -10.899898529052734)\n",
      "(sin-, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(gle+both’, ',', -19.579313278198242)\n",
      "(criterion, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(This, ',', -6.785318851470947)\n",
      "(is, ',', -4.3297648429870605)\n",
      "(used, ',', -7.587561130523682)\n",
      "(because, ',', -6.412496566772461)\n",
      "(of, ',', -4.1284637451171875)\n",
      "(the, ',', -3.425445795059204)\n",
      "(following, ',', -9.569315910339355)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(reasons, ',', -9.083260536193848)\n",
      "(:, ',', -6.052439212799072)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(1, ',', -7.901819705963135)\n",
      "(., ',', -3.0729479789733887)\n",
      "(For, ',', -8.115267753601074)\n",
      "(‘, ',', -10.899898529052734)\n",
      "(single’, ',', -19.579313278198242)\n",
      "(and, ',', -4.195279121398926)\n",
      "(‘, ',', -10.899898529052734)\n",
      "(match’, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(as, ',', -5.507394313812256)\n",
      "(discussed, ',', -11.15158462524414)\n",
      "(in, ',', -4.585874080657959)\n",
      "([, ',', -5.266753196716309)\n",
      "(58, ',', -19.579313278198242)\n",
      "(], ',', -5.498423099517822)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(the, ',', -3.425445795059204)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(‘, ',', -10.899898529052734)\n",
      "(match’, ',', -19.579313278198242)\n",
      "(way, ',', -6.743997097015381)\n",
      "(gives, ',', -9.235690116882324)\n",
      "(unfair, ',', -10.777859687805176)\n",
      "(advantage, ',', -9.968113899230957)\n",
      "(to, ',', -3.83851957321167)\n",
      "(approaches, ',', -11.651655197143555)\n",
      "(that, ',', -4.348940372467041)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(produce, ',', -9.855643272399902)\n",
      "(a, ',', -3.9830753803253174)\n",
      "(large, ',', -8.6246976852417)\n",
      "(number, ',', -8.322697639465332)\n",
      "(of, ',', -4.1284637451171875)\n",
      "(candidates, ',', -9.305854797363281)\n",
      "(because, ',', -6.412496566772461)\n",
      "(mis-, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(matched, ',', -19.579313278198242)\n",
      "(candidates, ',', -9.305854797363281)\n",
      "((, ',', -5.70067024230957)\n",
      "(false, ',', -9.506165504455566)\n",
      "(positives, ',', -19.579313278198242)\n",
      "(), ',', -5.341753005981445)\n",
      "(are, ',', -5.160149097442627)\n",
      "(not, ',', -5.21923828125)\n",
      "(penalized, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(2, ',', -7.786293029785156)\n",
      "(., ',', -3.0729479789733887)\n",
      "(For, ',', -8.115267753601074)\n",
      "(‘, ',', -10.899898529052734)\n",
      "(both’, ',', -19.579313278198242)\n",
      "(and, ',', -4.195279121398926)\n",
      "(‘, ',', -10.899898529052734)\n",
      "(avg’, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(‘, ',', -10.899898529052734)\n",
      "(both’, ',', -19.579313278198242)\n",
      "(is, ',', -4.3297648429870605)\n",
      "(better, ',', -7.226652145385742)\n",
      "(at, ',', -5.8868231773376465)\n",
      "(describing, ',', -11.233556747436523)\n",
      "(the, ',', -3.425445795059204)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(orientation, ',', -19.579313278198242)\n",
      "(of, ',', -4.1284637451171875)\n",
      "(body, ',', -9.267072677612305)\n",
      "(parts, ',', -9.519133567810059)\n",
      "(and, ',', -4.195279121398926)\n",
      "(will, ',', -6.10568380355835)\n",
      "(facilitate, ',', -19.579313278198242)\n",
      "(the, ',', -3.425445795059204)\n",
      "(use, ',', -7.119458198547363)\n",
      "(of, ',', -4.1284637451171875)\n",
      "(\n",
      "\n",
      "\f",
      ", ',', -19.579313278198242)\n",
      "(Table, ',', -19.579313278198242)\n",
      "(1, ',', -7.901819705963135)\n",
      "(., ',', -3.0729479789733887)\n",
      "(Pose, ',', -19.579313278198242)\n",
      "(estimation, ',', -19.579313278198242)\n",
      "(results, ',', -9.357637405395508)\n",
      "((, ',', -5.70067024230957)\n",
      "(PCP, ',', -19.579313278198242)\n",
      "(), ',', -5.341753005981445)\n",
      "(on, ',', -5.263686656951904)\n",
      "(LSP, ',', -19.579313278198242)\n",
      "([, ',', -5.266753196716309)\n",
      "(21, ',', -11.032556533813477)\n",
      "(], ',', -5.498423099517822)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(UIUC, ',', -19.579313278198242)\n",
      "(people, ',', -5.853616714477539)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "([, ',', -5.266753196716309)\n",
      "(53, ',', -19.579313278198242)\n",
      "(], ',', -5.498423099517822)\n",
      "(and, ',', -4.195279121398926)\n",
      "(PARSE, ',', -19.579313278198242)\n",
      "([, ',', -5.266753196716309)\n",
      "(44, ',', -19.579313278198242)\n",
      "(], ',', -5.498423099517822)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(Method, ',', -19.579313278198242)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(Torso, ',', -19.579313278198242)\n",
      "(U.leg, ',', -19.579313278198242)\n",
      "(L.leg, ',', -19.579313278198242)\n",
      "(U.arm, ',', -19.579313278198242)\n",
      "(L.arm, ',', -19.579313278198242)\n",
      "(head, ',', -8.585274696350098)\n",
      "(Total, ',', -11.708281517028809)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(LSP, ',', -19.579313278198242)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(Andriluka, ',', -19.579313278198242)\n",
      "(et, ',', -11.39320182800293)\n",
      "(al, ',', -10.772948265075684)\n",
      "(., ',', -3.0729479789733887)\n",
      "([, ',', -5.266753196716309)\n",
      "(2, ',', -7.786293029785156)\n",
      "(], ',', -5.498423099517822)\n",
      "(80.9, ',', -19.579313278198242)\n",
      "(67.1, ',', -19.579313278198242)\n",
      "(60.7, ',', -19.579313278198242)\n",
      "(46.5, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(Yang&Ramanan, ',', -19.579313278198242)\n",
      "([, ',', -5.266753196716309)\n",
      "(57, ',', -19.579313278198242)\n",
      "(], ',', -5.498423099517822)\n",
      "(81.0, ',', -19.579313278198242)\n",
      "(69.5, ',', -19.579313278198242)\n",
      "(65.9, ',', -19.579313278198242)\n",
      "(53.5, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(Yang&Ramanan, ',', -19.579313278198242)\n",
      "([, ',', -5.266753196716309)\n",
      "(58, ',', -19.579313278198242)\n",
      "(], ',', -5.498423099517822)\n",
      "(82.9, ',', -19.579313278198242)\n",
      "(70.3, ',', -19.579313278198242)\n",
      "(67.0, ',', -19.579313278198242)\n",
      "(56.0, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(Pishchulin, ',', -19.579313278198242)\n",
      "(et, ',', -11.39320182800293)\n",
      "(al, ',', -10.772948265075684)\n",
      "(., ',', -3.0729479789733887)\n",
      "([, ',', -5.266753196716309)\n",
      "(40, ',', -9.825430870056152)\n",
      "(], ',', -5.498423099517822)\n",
      "(87.5, ',', -19.579313278198242)\n",
      "(75.7, ',', -19.579313278198242)\n",
      "(68.0, ',', -19.579313278198242)\n",
      "(54.2, ',', -19.579313278198242)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(26.4, ',', -19.579313278198242)\n",
      "(74.9, ',', -19.579313278198242)\n",
      "(55.7, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(35.8, ',', -19.579313278198242)\n",
      "(76.8, ',', -19.579313278198242)\n",
      "(60.7, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(39.8, ',', -19.579313278198242)\n",
      "(79.3, ',', -19.579313278198242)\n",
      "(62.8, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(33.9, ',', -19.579313278198242)\n",
      "(78.1, ',', -19.579313278198242)\n",
      "(62.9, ',', -19.579313278198242)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(Eichner, ',', -19.579313278198242)\n",
      "(&, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(Ferrari, ',', -19.579313278198242)\n",
      "([, ',', -5.266753196716309)\n",
      "(11, ',', -10.191385269165039)\n",
      "(], ',', -5.498423099517822)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(Ours, ',', -19.579313278198242)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(86.2, ',', -19.579313278198242)\n",
      "(74.3, ',', -19.579313278198242)\n",
      "(69.3, ',', -19.579313278198242)\n",
      "(56.5, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(85.8, ',', -19.579313278198242)\n",
      "(76.5, ',', -19.579313278198242)\n",
      "(72.2, ',', -19.579313278198242)\n",
      "(63.3, ',', -19.579313278198242)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(37.4, ',', -19.579313278198242)\n",
      "(80.1, ',', -19.579313278198242)\n",
      "(64.3, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(46.6, ',', -19.579313278198242)\n",
      "(83.1, ',', -19.579313278198242)\n",
      "(68.6, ',', -19.579313278198242)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(PARSE, ',', -19.579313278198242)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(Andriluka, ',', -19.579313278198242)\n",
      "(et, ',', -11.39320182800293)\n",
      "(al, ',', -10.772948265075684)\n",
      "(., ',', -3.0729479789733887)\n",
      "([, ',', -5.266753196716309)\n",
      "(2, ',', -7.786293029785156)\n",
      "(], ',', -5.498423099517822)\n",
      "(86.3, ',', -19.579313278198242)\n",
      "(66.3, ',', -19.579313278198242)\n",
      "(60.0, ',', -19.579313278198242)\n",
      "(54.6, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(Yang&Ramanan, ',', -19.579313278198242)\n",
      "([, ',', -5.266753196716309)\n",
      "(57, ',', -19.579313278198242)\n",
      "(], ',', -5.498423099517822)\n",
      "(83.4, ',', -19.579313278198242)\n",
      "(68.8, ',', -19.579313278198242)\n",
      "(60.7, ',', -19.579313278198242)\n",
      "(59.8, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(Yang&Ramanan, ',', -19.579313278198242)\n",
      "([, ',', -5.266753196716309)\n",
      "(58, ',', -19.579313278198242)\n",
      "(], ',', -5.498423099517822)\n",
      "(82.9, ',', -19.579313278198242)\n",
      "(68.8, ',', -19.579313278198242)\n",
      "(60.5, ',', -19.579313278198242)\n",
      "(63.4, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(Pishchulin, ',', -19.579313278198242)\n",
      "(et, ',', -11.39320182800293)\n",
      "(al, ',', -10.772948265075684)\n",
      "(., ',', -3.0729479789733887)\n",
      "([, ',', -5.266753196716309)\n",
      "(42, ',', -11.540140151977539)\n",
      "(], ',', -5.498423099517822)\n",
      "(88.8, ',', -19.579313278198242)\n",
      "(77.3, ',', -19.579313278198242)\n",
      "(67.1, ',', -19.579313278198242)\n",
      "(53.7, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(Pishchulin, ',', -19.579313278198242)\n",
      "(et, ',', -11.39320182800293)\n",
      "(al, ',', -10.772948265075684)\n",
      "(., ',', -3.0729479789733887)\n",
      "([, ',', -5.266753196716309)\n",
      "(40, ',', -9.825430870056152)\n",
      "(], ',', -5.498423099517822)\n",
      "(92.2, ',', -19.579313278198242)\n",
      "(74.6, ',', -19.579313278198242)\n",
      "(63.7, ',', -19.579313278198242)\n",
      "(54.9, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(90.7, ',', -19.579313278198242)\n",
      "(80.0, ',', -19.579313278198242)\n",
      "(70.0, ',', -19.579313278198242)\n",
      "(59.3, ',', -19.579313278198242)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "([, ',', -5.266753196716309)\n",
      "(40]+[42, ',', -19.579313278198242)\n",
      "(], ',', -5.498423099517822)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(Johnson, ',', -11.605546951293945)\n",
      "(&, ',', -19.579313278198242)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(35.6, ',', -19.579313278198242)\n",
      "(72.7, ',', -19.579313278198242)\n",
      "(59.2, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(40.7, ',', -19.579313278198242)\n",
      "(83.4, ',', -19.579313278198242)\n",
      "(62.7, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(42.4, ',', -19.579313278198242)\n",
      "(82.4, ',', -19.579313278198242)\n",
      "(63.6, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(36.1, ',', -19.579313278198242)\n",
      "(73.7, ',', -19.579313278198242)\n",
      "(63.1, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(39.8, ',', -19.579313278198242)\n",
      "(70.7, ',', -19.579313278198242)\n",
      "(62.9, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(37.1, ',', -19.579313278198242)\n",
      "(77.6, ',', -19.579313278198242)\n",
      "(66.1, ',', -19.579313278198242)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(Everingham, ',', -19.579313278198242)\n",
      "([, ',', -5.266753196716309)\n",
      "(22, ',', -11.036799430847168)\n",
      "(], ',', -5.498423099517822)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(Ours, ',', -19.579313278198242)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(87.6, ',', -19.579313278198242)\n",
      "(74.7, ',', -19.579313278198242)\n",
      "(67.1, ',', -19.579313278198242)\n",
      "(67.3, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(89.3, ',', -19.579313278198242)\n",
      "(78.0, ',', -19.579313278198242)\n",
      "(72.0, ',', -19.579313278198242)\n",
      "(67.8, ',', -19.579313278198242)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(45.8, ',', -19.579313278198242)\n",
      "(76.8, ',', -19.579313278198242)\n",
      "(67.4, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(47.8, ',', -19.579313278198242)\n",
      "(89.3, ',', -19.579313278198242)\n",
      "(71.0, ',', -19.579313278198242)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(UIUC, ',', -19.579313278198242)\n",
      "(People, ',', -8.621843338012695)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(Andriluka, ',', -19.579313278198242)\n",
      "(et, ',', -11.39320182800293)\n",
      "(al, ',', -10.772948265075684)\n",
      "(., ',', -3.0729479789733887)\n",
      "([, ',', -5.266753196716309)\n",
      "(2, ',', -7.786293029785156)\n",
      "(], ',', -5.498423099517822)\n",
      "(88.3, ',', -19.579313278198242)\n",
      "(64.0, ',', -19.579313278198242)\n",
      "(50.6, ',', -19.579313278198242)\n",
      "(42.3, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(Yang&Ramanan, ',', -19.579313278198242)\n",
      "([, ',', -5.266753196716309)\n",
      "(57, ',', -19.579313278198242)\n",
      "(], ',', -5.498423099517822)\n",
      "(78.1, ',', -19.579313278198242)\n",
      "(60.9, ',', -19.579313278198242)\n",
      "(53.2, ',', -19.579313278198242)\n",
      "(41.3, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(Yang&Ramanan, ',', -19.579313278198242)\n",
      "([, ',', -5.266753196716309)\n",
      "(58, ',', -19.579313278198242)\n",
      "(], ',', -5.498423099517822)\n",
      "(81.8, ',', -19.579313278198242)\n",
      "(65.0, ',', -19.579313278198242)\n",
      "(55.1, ',', -19.579313278198242)\n",
      "(46.8, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(Pishchulin, ',', -19.579313278198242)\n",
      "(et, ',', -11.39320182800293)\n",
      "(al, ',', -10.772948265075684)\n",
      "(., ',', -3.0729479789733887)\n",
      "([, ',', -5.266753196716309)\n",
      "(40, ',', -9.825430870056152)\n",
      "(], ',', -5.498423099517822)\n",
      "(91.5, ',', -19.579313278198242)\n",
      "(66.8, ',', -19.579313278198242)\n",
      "(54.7, ',', -19.579313278198242)\n",
      "(38.3, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(86.8, ',', -19.579313278198242)\n",
      "(56.3, ',', -19.579313278198242)\n",
      "(50.2, ',', -19.579313278198242)\n",
      "(30.8, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(89.1, ',', -19.579313278198242)\n",
      "(72.9, ',', -19.579313278198242)\n",
      "(62.4, ',', -19.579313278198242)\n",
      "(56.3, ',', -19.579313278198242)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(Wang, ',', -19.579313278198242)\n",
      "(et, ',', -11.39320182800293)\n",
      "(al, ',', -10.772948265075684)\n",
      "(., ',', -3.0729479789733887)\n",
      "([, ',', -5.266753196716309)\n",
      "(56, ',', -19.579313278198242)\n",
      "(], ',', -5.498423099517822)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(Ours, ',', -19.579313278198242)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(21.3, ',', -19.579313278198242)\n",
      "(81.8, ',', -19.579313278198242)\n",
      "(52.6, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(32.2, ',', -19.579313278198242)\n",
      "(76.1, ',', -19.579313278198242)\n",
      "(53.0, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(37.7, ',', -19.579313278198242)\n",
      "(79.8, ',', -19.579313278198242)\n",
      "(57.0, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(23.9, ',', -19.579313278198242)\n",
      "(85.0, ',', -19.579313278198242)\n",
      "(54.4, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(20.3, ',', -19.579313278198242)\n",
      "(68.8, ',', -19.579313278198242)\n",
      "(47.0, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(47.6, ',', -19.579313278198242)\n",
      "(89.1, ',', -19.579313278198242)\n",
      "(65.6, ',', -19.579313278198242)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(Table, ',', -19.579313278198242)\n",
      "(2, ',', -7.786293029785156)\n",
      "(., ',', -3.0729479789733887)\n",
      "(Results, ',', -19.579313278198242)\n",
      "((, ',', -5.70067024230957)\n",
      "(PCP, ',', -19.579313278198242)\n",
      "(), ',', -5.341753005981445)\n",
      "(on, ',', -5.263686656951904)\n",
      "(investigating, ',', -19.579313278198242)\n",
      "(model, ',', -9.600680351257324)\n",
      "(depth, ',', -10.966999053955078)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(Method, ',', -19.579313278198242)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(Torso, ',', -19.579313278198242)\n",
      "(U.leg, ',', -19.579313278198242)\n",
      "(L.leg, ',', -19.579313278198242)\n",
      "(U.arm, ',', -19.579313278198242)\n",
      "(L.arm, ',', -19.579313278198242)\n",
      "(Head, ',', -19.579313278198242)\n",
      "(Total, ',', -11.708281517028809)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(LSP, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "([, ',', -5.266753196716309)\n",
      "(58, ',', -19.579313278198242)\n",
      "(], ',', -5.498423099517822)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(82.9, ',', -19.579313278198242)\n",
      "(70.3, ',', -19.579313278198242)\n",
      "(67.0, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(Kernel, ',', -19.579313278198242)\n",
      "(SVM, ',', -19.579313278198242)\n",
      "(81.9, ',', -19.579313278198242)\n",
      "(72.2, ',', -19.579313278198242)\n",
      "(67.6, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(1, ',', -7.901819705963135)\n",
      "(hidden, ',', -10.728233337402344)\n",
      "(layer, ',', -11.220696449279785)\n",
      "(84.9, ',', -19.579313278198242)\n",
      "(73.9, ',', -19.579313278198242)\n",
      "(69.5, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(2, ',', -7.786293029785156)\n",
      "(hidden, ',', -10.728233337402344)\n",
      "(layers, ',', -19.579313278198242)\n",
      "(85.0, ',', -19.579313278198242)\n",
      "(74.6, ',', -19.579313278198242)\n",
      "(70.7, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(Ours, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(85.8, ',', -19.579313278198242)\n",
      "(76.5, ',', -19.579313278198242)\n",
      "(72.2, ',', -19.579313278198242)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(PARSE, ',', -19.579313278198242)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "([, ',', -5.266753196716309)\n",
      "(58, ',', -19.579313278198242)\n",
      "(], ',', -5.498423099517822)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(82.9, ',', -19.579313278198242)\n",
      "(68.8, ',', -19.579313278198242)\n",
      "(60.5, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(Kernel, ',', -19.579313278198242)\n",
      "(SVM, ',', -19.579313278198242)\n",
      "(81.0, ',', -19.579313278198242)\n",
      "(67.8, ',', -19.579313278198242)\n",
      "(61.2, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(1, ',', -7.901819705963135)\n",
      "(hidden, ',', -10.728233337402344)\n",
      "(layer, ',', -11.220696449279785)\n",
      "(84.4, ',', -19.579313278198242)\n",
      "(71.2, ',', -19.579313278198242)\n",
      "(63.2, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(2, ',', -7.786293029785156)\n",
      "(hidden, ',', -10.728233337402344)\n",
      "(layers, ',', -19.579313278198242)\n",
      "(85.9, ',', -19.579313278198242)\n",
      "(74.4, ',', -19.579313278198242)\n",
      "(68.3, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(Ours, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(89.3, ',', -19.579313278198242)\n",
      "(78.0, ',', -19.579313278198242)\n",
      "(72.0, ',', -19.579313278198242)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(UIUC, ',', -19.579313278198242)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "([, ',', -5.266753196716309)\n",
      "(58, ',', -19.579313278198242)\n",
      "(], ',', -5.498423099517822)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(81.8, ',', -19.579313278198242)\n",
      "(65.0, ',', -19.579313278198242)\n",
      "(55.1, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(Kernel, ',', -19.579313278198242)\n",
      "(SVM, ',', -19.579313278198242)\n",
      "(82.2, ',', -19.579313278198242)\n",
      "(65.0, ',', -19.579313278198242)\n",
      "(54.9, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(1, ',', -7.901819705963135)\n",
      "(hidden, ',', -10.728233337402344)\n",
      "(layer, ',', -11.220696449279785)\n",
      "(83.0, ',', -19.579313278198242)\n",
      "(65.6, ',', -19.579313278198242)\n",
      "(55.9, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(2, ',', -7.786293029785156)\n",
      "(hidden, ',', -10.728233337402344)\n",
      "(layers, ',', -19.579313278198242)\n",
      "(84.2, ',', -19.579313278198242)\n",
      "(68.4, ',', -19.579313278198242)\n",
      "(59.3, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(Ours, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(89.1, ',', -19.579313278198242)\n",
      "(72.9, ',', -19.579313278198242)\n",
      "(62.3, ',', -19.579313278198242)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(56, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(58.8, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(57.5, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(61.2, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(63.3, ',', -19.579313278198242)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(63.4, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(63.2, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(62.4, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(64.6, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(67.8, ',', -19.579313278198242)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(46.8, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(50.2, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(50.6, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(53.0, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(56.3, ',', -19.579313278198242)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(39.8, ',', -19.579313278198242)\n",
      "(79.3, ',', -19.579313278198242)\n",
      "(62.8, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(42.8, ',', -19.579313278198242)\n",
      "(77.5, ',', -19.579313278198242)\n",
      "(64.2, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(42.9, ',', -19.579313278198242)\n",
      "(50.7, ',', -19.579313278198242)\n",
      "(62.3, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(45.2, ',', -19.579313278198242)\n",
      "(82.2, ',', -19.579313278198242)\n",
      "(67.1, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(46.6, ',', -19.579313278198242)\n",
      "(83.1, ',', -19.579313278198242)\n",
      "(68.6, ',', -19.579313278198242)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(42.4, ',', -19.579313278198242)\n",
      "(82.4, ',', -19.579313278198242)\n",
      "(63.6, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(44.1, ',', -19.579313278198242)\n",
      "(78.0, ',', -19.579313278198242)\n",
      "(63.2, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(44.4, ',', -19.579313278198242)\n",
      "(70.2, ',', -19.579313278198242)\n",
      "(63.7, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(46.3, ',', -19.579313278198242)\n",
      "(85.4, ',', -19.579313278198242)\n",
      "(67.9, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(47.8, ',', -19.579313278198242)\n",
      "(89.3, ',', -19.579313278198242)\n",
      "(71.0, ',', -19.579313278198242)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(37.7, ',', -19.579313278198242)\n",
      "(79.8, ',', -19.579313278198242)\n",
      "(57.0, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(43.1, ',', -19.579313278198242)\n",
      "(80.6, ',', -19.579313278198242)\n",
      "(58.9, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(42.3, ',', -19.579313278198242)\n",
      "(79.8, ',', -19.579313278198242)\n",
      "(59.2, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(45.3, ',', -19.579313278198242)\n",
      "(83.4, ',', -19.579313278198242)\n",
      "(62.0, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(47.6, ',', -19.579313278198242)\n",
      "(89.1, ',', -19.579313278198242)\n",
      "(65.6, ',', -19.579313278198242)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(proved, ',', -11.100445747375488)\n",
      "(that, ',', -4.348940372467041)\n",
      "(linear, ',', -11.651655197143555)\n",
      "(-, ',', -5.202415943145752)\n",
      "(SVM, ',', -19.579313278198242)\n",
      "(and, ',', -4.195279121398926)\n",
      "(kernel, ',', -10.995153427124023)\n",
      "(-, ',', -5.202415943145752)\n",
      "(SVM, ',', -19.579313278198242)\n",
      "(are, ',', -5.160149097442627)\n",
      "(shallow, ',', -11.557884216308594)\n",
      "(mod-, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(els, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(With, ',', -9.147692680358887)\n",
      "(the, ',', -3.425445795059204)\n",
      "(deep, ',', -9.902402877807617)\n",
      "(model, ',', -9.600680351257324)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(our, ',', -7.124547481536865)\n",
      "(approach, ',', -9.994755744934082)\n",
      "(performs, ',', -19.579313278198242)\n",
      "(better, ',', -7.226652145385742)\n",
      "(., ',', -3.0729479789733887)\n",
      "(As, ',', -7.891604900360107)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(the, ',', -3.425445795059204)\n",
      "(number, ',', -8.322697639465332)\n",
      "(of, ',', -4.1284637451171875)\n",
      "(hidden, ',', -10.728233337402344)\n",
      "(layers, ',', -19.579313278198242)\n",
      "(increases, ',', -10.54729175567627)\n",
      "(from, ',', -6.028810501098633)\n",
      "(1, ',', -7.901819705963135)\n",
      "(hidden, ',', -10.728233337402344)\n",
      "(layer, ',', -11.220696449279785)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(to, ',', -3.83851957321167)\n",
      "(2, ',', -7.786293029785156)\n",
      "(hidden, ',', -10.728233337402344)\n",
      "(layers, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(the, ',', -3.425445795059204)\n",
      "(estimation, ',', -19.579313278198242)\n",
      "(accuracy, ',', -11.345757484436035)\n",
      "(increases, ',', -10.54729175567627)\n",
      "(from, ',', -6.028810501098633)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(62.3%, ',', -19.579313278198242)\n",
      "(to, ',', -3.83851957321167)\n",
      "(67.1%, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(With, ',', -9.147692680358887)\n",
      "(PCP, ',', -19.579313278198242)\n",
      "(68.6%, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(our, ',', -7.124547481536865)\n",
      "(ﬁnal, ',', -19.579313278198242)\n",
      "(model, ',', -9.600680351257324)\n",
      "(in, ',', -4.585874080657959)\n",
      "(Fig, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(3(b, ',', -19.579313278198242)\n",
      "(), ',', -5.341753005981445)\n",
      "(uses, ',', -9.424895286560059)\n",
      "(three, ',', -8.800739288330078)\n",
      "(hidden, ',', -10.728233337402344)\n",
      "(layers, ',', -19.579313278198242)\n",
      "(and, ',', -4.195279121398926)\n",
      "(is, ',', -4.3297648429870605)\n",
      "(better, ',', -7.226652145385742)\n",
      "(than, ',', -6.372464179992676)\n",
      "(SVM, ',', -19.579313278198242)\n",
      "(and, ',', -4.195279121398926)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(deep, ',', -9.902402877807617)\n",
      "(models, ',', -10.62589168548584)\n",
      "(with, ',', -5.363764762878418)\n",
      "(fewer, ',', -10.669107437133789)\n",
      "(layers, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(Table, ',', -19.579313278198242)\n",
      "(3, ',', -8.093857765197754)\n",
      "(., ',', -3.0729479789733887)\n",
      "(Results, ',', -19.579313278198242)\n",
      "((, ',', -5.70067024230957)\n",
      "(PCP, ',', -19.579313278198242)\n",
      "(), ',', -5.341753005981445)\n",
      "(on, ',', -5.263686656951904)\n",
      "(investigating, ',', -19.579313278198242)\n",
      "(deep, ',', -9.902402877807617)\n",
      "(model, ',', -9.600680351257324)\n",
      "(structures, ',', -11.369197845458984)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(Method, ',', -19.579313278198242)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(Torso, ',', -19.579313278198242)\n",
      "(U.leg, ',', -19.579313278198242)\n",
      "(L.leg, ',', -19.579313278198242)\n",
      "(U.arm, ',', -19.579313278198242)\n",
      "(L.arm, ',', -19.579313278198242)\n",
      "(Head, ',', -19.579313278198242)\n",
      "(Total, ',', -11.708281517028809)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(LSP, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(DBN, ',', -19.579313278198242)\n",
      "(in, ',', -4.585874080657959)\n",
      "(Fig, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(3(a, ',', -19.579313278198242)\n",
      "(), ',', -5.341753005981445)\n",
      "(82.9, ',', -19.579313278198242)\n",
      "(73.2, ',', -19.579313278198242)\n",
      "(69.5, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(Ours, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(85.8, ',', -19.579313278198242)\n",
      "(76.5, ',', -19.579313278198242)\n",
      "(72.2, ',', -19.579313278198242)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(PARSE, ',', -19.579313278198242)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(DBN, ',', -19.579313278198242)\n",
      "(in, ',', -4.585874080657959)\n",
      "(Fig, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(3(a, ',', -19.579313278198242)\n",
      "(), ',', -5.341753005981445)\n",
      "(82.0, ',', -19.579313278198242)\n",
      "(70.0, ',', -19.579313278198242)\n",
      "(64.6, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(Ours, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(89.3, ',', -19.579313278198242)\n",
      "(78.0, ',', -19.579313278198242)\n",
      "(72.0, ',', -19.579313278198242)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(DBN, ',', -19.579313278198242)\n",
      "(in, ',', -4.585874080657959)\n",
      "(Fig, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(3(a, ',', -19.579313278198242)\n",
      "(), ',', -5.341753005981445)\n",
      "(87.4, ',', -19.579313278198242)\n",
      "(68.4, ',', -19.579313278198242)\n",
      "(58.3, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(Ours, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(89.1, ',', -19.579313278198242)\n",
      "(72.9, ',', -19.579313278198242)\n",
      "(62.3, ',', -19.579313278198242)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(UIUC, ',', -19.579313278198242)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(59.8, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(63.3, ',', -19.579313278198242)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(62.9, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(67.8, ',', -19.579313278198242)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(52.2, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(56.3, ',', -19.579313278198242)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(43.8, ',', -19.579313278198242)\n",
      "(79.2, ',', -19.579313278198242)\n",
      "(65.5, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(46.6, ',', -19.579313278198242)\n",
      "(83.1, ',', -19.579313278198242)\n",
      "(68.6, ',', -19.579313278198242)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(46.3, ',', -19.579313278198242)\n",
      "(80.5, ',', -19.579313278198242)\n",
      "(65.0, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(47.8, ',', -19.579313278198242)\n",
      "(89.3, ',', -19.579313278198242)\n",
      "(71.0, ',', -19.579313278198242)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(44.3, ',', -19.579313278198242)\n",
      "(84.6, ',', -19.579313278198242)\n",
      "(61.8, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(47.6, ',', -19.579313278198242)\n",
      "(89.1, ',', -19.579313278198242)\n",
      "(65.6, ',', -19.579313278198242)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(Deep, ',', -19.579313278198242)\n",
      "(model, ',', -9.600680351257324)\n",
      "(structure, ',', -10.26526165008545)\n",
      "(design, ',', -9.488831520080566)\n",
      "(is, ',', -4.3297648429870605)\n",
      "(investigated, ',', -11.86569595336914)\n",
      "(in, ',', -4.585874080657959)\n",
      "(Table, ',', -19.579313278198242)\n",
      "(3, ',', -8.093857765197754)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(The, ',', -5.774222373962402)\n",
      "(DBN, ',', -19.579313278198242)\n",
      "(in, ',', -4.585874080657959)\n",
      "(Fig, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(3(a, ',', -19.579313278198242)\n",
      "(), ',', -5.341753005981445)\n",
      "(trains, ',', -11.87548542022705)\n",
      "(a, ',', -3.9830753803253174)\n",
      "(three, ',', -8.800739288330078)\n",
      "(-, ',', -5.202415943145752)\n",
      "(layer, ',', -11.220696449279785)\n",
      "(deep, ',', -9.902402877807617)\n",
      "(model, ',', -9.600680351257324)\n",
      "(over, ',', -7.045919418334961)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(the, ',', -3.425445795059204)\n",
      "(concatenated, ',', -19.579313278198242)\n",
      "(informations, ',', -19.579313278198242)\n",
      "(with, ',', -5.363764762878418)\n",
      "(three, ',', -8.800739288330078)\n",
      "(hidden, ',', -10.728233337402344)\n",
      "(layers, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(The, ',', -5.774222373962402)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(model, ',', -9.600680351257324)\n",
      "(in, ',', -4.585874080657959)\n",
      "(3(b, ',', -19.579313278198242)\n",
      "(), ',', -5.341753005981445)\n",
      "(learns, ',', -19.579313278198242)\n",
      "(high, ',', -8.075313568115234)\n",
      "(-, ',', -5.202415943145752)\n",
      "(order, ',', -8.68863582611084)\n",
      "(representations, ',', -19.579313278198242)\n",
      "(individu-, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(ally, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(The, ',', -5.774222373962402)\n",
      "(model, ',', -9.600680351257324)\n",
      "(in, ',', -4.585874080657959)\n",
      "(3(b, ',', -19.579313278198242)\n",
      "(), ',', -5.341753005981445)\n",
      "(with, ',', -5.363764762878418)\n",
      "(PCP, ',', -19.579313278198242)\n",
      "(68.6%, ',', -19.579313278198242)\n",
      "(is, ',', -4.3297648429870605)\n",
      "(better, ',', -7.226652145385742)\n",
      "(in, ',', -4.585874080657959)\n",
      "(con-, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(structing, ',', -19.579313278198242)\n",
      "(the, ',', -3.425445795059204)\n",
      "(high, ',', -8.075313568115234)\n",
      "(-, ',', -5.202415943145752)\n",
      "(order, ',', -8.68863582611084)\n",
      "(representations, ',', -19.579313278198242)\n",
      "(and, ',', -4.195279121398926)\n",
      "(therefore, ',', -9.624737739562988)\n",
      "(has, ',', -6.2021942138671875)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(higher, ',', -9.007245063781738)\n",
      "(estimation, ',', -19.579313278198242)\n",
      "(accuracy, ',', -11.345757484436035)\n",
      "(compared, ',', -9.660063743591309)\n",
      "(with, ',', -5.363764762878418)\n",
      "(the, ',', -3.425445795059204)\n",
      "(DBN, ',', -19.579313278198242)\n",
      "(in, ',', -4.585874080657959)\n",
      "(Fig, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(3(a, ',', -19.579313278198242)\n",
      "(), ',', -5.341753005981445)\n",
      "(with, ',', -5.363764762878418)\n",
      "(PCP, ',', -19.579313278198242)\n",
      "(65.5%, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(Classiﬁcation, ',', -19.579313278198242)\n",
      "(label, ',', -10.63723373413086)\n",
      "(and, ',', -4.195279121398926)\n",
      "(location, ',', -10.667635917663574)\n",
      "(learning, ',', -9.830510139465332)\n",
      "(is, ',', -4.3297648429870605)\n",
      "(investigated, ',', -11.86569595336914)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(in, ',', -4.585874080657959)\n",
      "(Table, ',', -19.579313278198242)\n",
      "(4, ',', -8.51557731628418)\n",
      "(., ',', -3.0729479789733887)\n",
      "(There, ',', -7.277902603149414)\n",
      "(are, ',', -5.160149097442627)\n",
      "(two, ',', -7.494669437408447)\n",
      "(sets, ',', -10.755142211914062)\n",
      "(of, ',', -4.1284637451171875)\n",
      "(labels, ',', -11.671512603759766)\n",
      "(to, ',', -3.83851957321167)\n",
      "(be, ',', -5.210641384124756)\n",
      "(estimated, ',', -11.822792053222656)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(in, ',', -4.585874080657959)\n",
      "(our, ',', -7.124547481536865)\n",
      "(deep, ',', -9.902402877807617)\n",
      "(model, ',', -9.600680351257324)\n",
      "(:, ',', -6.052439212799072)\n",
      "(classiﬁcation, ',', -19.579313278198242)\n",
      "(label, ',', -10.63723373413086)\n",
      "(ycls, ',', -19.579313278198242)\n",
      "(and, ',', -4.195279121398926)\n",
      "(part, ',', -7.779394626617432)\n",
      "(posi-, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(tions, ',', -19.579313278198242)\n",
      "(ypst, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(In, ',', -7.314908981323242)\n",
      "(the, ',', -3.425445795059204)\n",
      "(experiments, ',', -11.601799011230469)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(we, ',', -6.075284481048584)\n",
      "(evaluate, ',', -11.832168579101562)\n",
      "(different, ',', -7.797479629516602)\n",
      "(ways, ',', -9.124493598937988)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(of, ',', -4.1284637451171875)\n",
      "(estimating, ',', -19.579313278198242)\n",
      "(these, ',', -7.158668518066406)\n",
      "(labels, ',', -11.671512603759766)\n",
      "(., ',', -3.0729479789733887)\n",
      "(The, ',', -5.774222373962402)\n",
      "(Only, ',', -9.382535934448242)\n",
      "(ycls, ',', -19.579313278198242)\n",
      "(in, ',', -4.585874080657959)\n",
      "(Table, ',', -19.579313278198242)\n",
      "(4, ',', -8.51557731628418)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(with, ',', -5.363764762878418)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(PCP, ',', -19.579313278198242)\n",
      "(63.7%, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(only, ',', -6.547285556793213)\n",
      "(estimates, ',', -19.579313278198242)\n",
      "(class, ',', -8.61464786529541)\n",
      "(label, ',', -10.63723373413086)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(with, ',', -5.363764762878418)\n",
      "(part, ',', -7.779394626617432)\n",
      "(location, ',', -10.667635917663574)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(directly, ',', -9.405570030212402)\n",
      "(obtained, ',', -19.579313278198242)\n",
      "(by, ',', -6.114920139312744)\n",
      "(the, ',', -3.425445795059204)\n",
      "(approach, ',', -9.994755744934082)\n",
      "(in, ',', -4.585874080657959)\n",
      "([, ',', -5.266753196716309)\n",
      "(58, ',', -19.579313278198242)\n",
      "(], ',', -5.498423099517822)\n",
      "(., ',', -3.0729479789733887)\n",
      "(The, ',', -5.774222373962402)\n",
      "(Only, ',', -9.382535934448242)\n",
      "(ypst, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(with, ',', -5.363764762878418)\n",
      "(PCP, ',', -19.579313278198242)\n",
      "(64.1%, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(only, ',', -6.547285556793213)\n",
      "(reﬁnes, ',', -19.579313278198242)\n",
      "(the, ',', -3.425445795059204)\n",
      "(part, ',', -7.779394626617432)\n",
      "(location, ',', -10.667635917663574)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(with, ',', -5.363764762878418)\n",
      "(class, ',', -8.61464786529541)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(label, ',', -10.63723373413086)\n",
      "(directly, ',', -9.405570030212402)\n",
      "(obtained, ',', -19.579313278198242)\n",
      "(by, ',', -6.114920139312744)\n",
      "(the, ',', -3.425445795059204)\n",
      "(approach, ',', -9.994755744934082)\n",
      "(in, ',', -4.585874080657959)\n",
      "([, ',', -5.266753196716309)\n",
      "(58, ',', -19.579313278198242)\n",
      "(], ',', -5.498423099517822)\n",
      "(., ',', -3.0729479789733887)\n",
      "(Separate, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(ycls+ypst, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(with, ',', -5.363764762878418)\n",
      "(PCP, ',', -19.579313278198242)\n",
      "(64.7%, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(uses, ',', -9.424895286560059)\n",
      "(two, ',', -7.494669437408447)\n",
      "(deep, ',', -9.902402877807617)\n",
      "(models, ',', -10.62589168548584)\n",
      "(for, ',', -4.91396951675415)\n",
      "(esti-, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(mating, ',', -19.579313278198242)\n",
      "(ycls, ',', -19.579313278198242)\n",
      "(and, ',', -4.195279121398926)\n",
      "(ypst, ',', -19.579313278198242)\n",
      "(separately, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(It, ',', -5.963693141937256)\n",
      "(can, ',', -5.9138712882995605)\n",
      "(be, ',', -5.210641384124756)\n",
      "(seen, ',', -8.123892784118652)\n",
      "(that, ',', -4.348940372467041)\n",
      "(both, ',', -7.8158979415893555)\n",
      "(ycls, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(and, ',', -4.195279121398926)\n",
      "(ypst, ',', -19.579313278198242)\n",
      "(are, ',', -5.160149097442627)\n",
      "(helpful, ',', -10.679465293884277)\n",
      "(for, ',', -4.91396951675415)\n",
      "(improving, ',', -11.554309844970703)\n",
      "(accuracy, ',', -11.345757484436035)\n",
      "(., ',', -3.0729479789733887)\n",
      "(Our, ',', -9.764631271362305)\n",
      "(model, ',', -9.600680351257324)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(uses, ',', -9.424895286560059)\n",
      "(the, ',', -3.425445795059204)\n",
      "(single, ',', -8.629281997680664)\n",
      "(deep, ',', -9.902402877807617)\n",
      "(model, ',', -9.600680351257324)\n",
      "(to, ',', -3.83851957321167)\n",
      "(jointly, ',', -19.579313278198242)\n",
      "(learn, ',', -8.87330436706543)\n",
      "(both, ',', -7.8158979415893555)\n",
      "(ycls, ',', -19.579313278198242)\n",
      "(and, ',', -4.195279121398926)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(ypst, ',', -19.579313278198242)\n",
      "((, ',', -5.70067024230957)\n",
      "(PCP, ',', -19.579313278198242)\n",
      "(68.6%, ',', -19.579313278198242)\n",
      "(), ',', -5.341753005981445)\n",
      "(and, ',', -4.195279121398926)\n",
      "(performs, ',', -19.579313278198242)\n",
      "(better, ',', -7.226652145385742)\n",
      "(than, ',', -6.372464179992676)\n",
      "(using, ',', -7.910459041595459)\n",
      "(two, ',', -7.494669437408447)\n",
      "(mod-, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(els, ',', -19.579313278198242)\n",
      "(to, ',', -3.83851957321167)\n",
      "(learn, ',', -8.87330436706543)\n",
      "(them, ',', -6.307651996612549)\n",
      "(separately, ',', -19.579313278198242)\n",
      "((, ',', -5.70067024230957)\n",
      "(PCP, ',', -19.579313278198242)\n",
      "(64.7%, ',', -19.579313278198242)\n",
      "(), ',', -5.341753005981445)\n",
      "(because, ',', -6.412496566772461)\n",
      "(body, ',', -9.267072677612305)\n",
      "(lo-, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(cation, ',', -19.579313278198242)\n",
      "(and, ',', -4.195279121398926)\n",
      "(the, ',', -3.425445795059204)\n",
      "(correctness, ',', -19.579313278198242)\n",
      "(of, ',', -4.1284637451171875)\n",
      "(candidate, ',', -8.77760124206543)\n",
      "(body, ',', -9.267072677612305)\n",
      "(location, ',', -10.667635917663574)\n",
      "(are, ',', -5.160149097442627)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(dependent, ',', -11.062646865844727)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(Analysis, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(Our, ',', -9.764631271362305)\n",
      "(model, ',', -9.600680351257324)\n",
      "(extracts, ',', -19.579313278198242)\n",
      "(high, ',', -8.075313568115234)\n",
      "(-, ',', -5.202415943145752)\n",
      "(order, ',', -8.68863582611084)\n",
      "(representations, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(of, ',', -4.1284637451171875)\n",
      "(appearance, ',', -11.022025108337402)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(deformation, ',', -19.579313278198242)\n",
      "(and, ',', -4.195279121398926)\n",
      "(mixture, ',', -19.579313278198242)\n",
      "(types, ',', -9.873420715332031)\n",
      "(and, ',', -4.195279121398926)\n",
      "(better, ',', -7.226652145385742)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(models, ',', -10.62589168548584)\n",
      "(their, ',', -6.1832733154296875)\n",
      "(dependence, ',', -19.579313278198242)\n",
      "(at, ',', -5.8868231773376465)\n",
      "(the, ',', -3.425445795059204)\n",
      "(top, ',', -8.581252098083496)\n",
      "(layer, ',', -11.220696449279785)\n",
      "(., ',', -3.0729479789733887)\n",
      "(For, ',', -8.115267753601074)\n",
      "(example, ',', -8.316376686096191)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(if, ',', -5.885849475860596)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(the, ',', -3.425445795059204)\n",
      "(mixture, ',', -19.579313278198242)\n",
      "(types, ',', -9.873420715332031)\n",
      "(are, ',', -5.160149097442627)\n",
      "(upright, ',', -19.579313278198242)\n",
      "(upper-, ',', -19.579313278198242)\n",
      "(and, ',', -4.195279121398926)\n",
      "(lower, ',', -9.25090217590332)\n",
      "(-, ',', -5.202415943145752)\n",
      "(arms, ',', -10.243865966796875)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(the, ',', -3.425445795059204)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(weighted, ',', -19.579313278198242)\n",
      "(combination, ',', -10.814634323120117)\n",
      "(of, ',', -4.1284637451171875)\n",
      "(the, ',', -3.425445795059204)\n",
      "(locations, ',', -11.704129219055176)\n",
      "(of, ',', -4.1284637451171875)\n",
      "(wrist, ',', -19.579313278198242)\n",
      "(and, ',', -4.195279121398926)\n",
      "(shoul-, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(der, ',', -19.579313278198242)\n",
      "(is, ',', -4.3297648429870605)\n",
      "(a, ',', -3.9830753803253174)\n",
      "(good, ',', -6.718321323394775)\n",
      "(estimation, ',', -19.579313278198242)\n",
      "(on, ',', -5.263686656951904)\n",
      "(the, ',', -3.425445795059204)\n",
      "(location, ',', -10.667635917663574)\n",
      "(of, ',', -4.1284637451171875)\n",
      "(elbow, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(If, ',', -6.505216598510742)\n",
      "(the, ',', -3.425445795059204)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(mixture, ',', -19.579313278198242)\n",
      "(types, ',', -9.873420715332031)\n",
      "(change, ',', -8.022180557250977)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(such, ',', -7.619058609008789)\n",
      "(estimation, ',', -19.579313278198242)\n",
      "(should, ',', -6.608358860015869)\n",
      "(change, ',', -8.022180557250977)\n",
      "(cor-, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(respondingly, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(Such, ',', -10.892508506774902)\n",
      "(complex, ',', -9.884106636047363)\n",
      "(dependence, ',', -19.579313278198242)\n",
      "(can, ',', -5.9138712882995605)\n",
      "(not, ',', -5.21923828125)\n",
      "(be, ',', -5.210641384124756)\n",
      "(mod-, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(eled, ',', -19.579313278198242)\n",
      "(linearly, ',', -19.579313278198242)\n",
      "(and, ',', -4.195279121398926)\n",
      "(deep, ',', -9.902402877807617)\n",
      "(model, ',', -9.600680351257324)\n",
      "(is, ',', -4.3297648429870605)\n",
      "(a, ',', -3.9830753803253174)\n",
      "(better, ',', -7.226652145385742)\n",
      "(solution, ',', -9.291645050048828)\n",
      "(., ',', -3.0729479789733887)\n",
      "(When, ',', -8.025211334228516)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(different, ',', -7.797479629516602)\n",
      "(information, ',', -8.817930221557617)\n",
      "(sources, ',', -9.963014602661133)\n",
      "(are, ',', -5.160149097442627)\n",
      "(extracted, ',', -19.579313278198242)\n",
      "(separately, ',', -19.579313278198242)\n",
      "(with, ',', -5.363764762878418)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(the, ',', -3.425445795059204)\n",
      "(ﬁrst, ',', -19.579313278198242)\n",
      "(several, ',', -8.988698959350586)\n",
      "(layers, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(the, ',', -3.425445795059204)\n",
      "(connections, ',', -11.144454956054688)\n",
      "(across, ',', -9.273614883422852)\n",
      "(sources, ',', -9.963014602661133)\n",
      "(are, ',', -5.160149097442627)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(removed, ',', -10.024443626403809)\n",
      "(and, ',', -4.195279121398926)\n",
      "(the, ',', -3.425445795059204)\n",
      "(number, ',', -8.322697639465332)\n",
      "(of, ',', -4.1284637451171875)\n",
      "(parameters, ',', -19.579313278198242)\n",
      "(is, ',', -4.3297648429870605)\n",
      "(reduced, ',', -10.933844566345215)\n",
      "(., ',', -3.0729479789733887)\n",
      "(It, ',', -5.963693141937256)\n",
      "(helps, ',', -10.029866218566895)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(to, ',', -3.83851957321167)\n",
      "(regularize, ',', -19.579313278198242)\n",
      "(optimization, ',', -19.579313278198242)\n",
      "(when, ',', -6.408374786376953)\n",
      "(training, ',', -10.32292366027832)\n",
      "(samples, ',', -19.579313278198242)\n",
      "(are, ',', -5.160149097442627)\n",
      "(lim-, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(ited, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(Existing, ',', -19.579313278198242)\n",
      "(methods, ',', -10.367645263671875)\n",
      "(only, ',', -6.547285556793213)\n",
      "(use, ',', -7.119458198547363)\n",
      "(ycls, ',', -19.579313278198242)\n",
      "(for, ',', -4.91396951675415)\n",
      "(supervision, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(while, ',', -7.631333827972412)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(we, ',', -6.075284481048584)\n",
      "(use, ',', -7.119458198547363)\n",
      "(both, ',', -7.8158979415893555)\n",
      "(ypts, ',', -19.579313278198242)\n",
      "(and, ',', -4.195279121398926)\n",
      "(ycls, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(As, ',', -7.891604900360107)\n",
      "(shown, ',', -10.03923225402832)\n",
      "(in, ',', -4.585874080657959)\n",
      "(Fig, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(4, ',', -8.51557731628418)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(reﬁning, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(ypts, ',', -19.579313278198242)\n",
      "(does, ',', -6.390727519989014)\n",
      "(help, ',', -8.049466133117676)\n",
      "(to, ',', -3.83851957321167)\n",
      "(rectify, ',', -19.579313278198242)\n",
      "(incorrect, ',', -10.551209449768066)\n",
      "(part, ',', -7.779394626617432)\n",
      "(locations, ',', -11.704129219055176)\n",
      "(based, ',', -8.318479537963867)\n",
      "(on, ',', -5.263686656951904)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(the, ',', -3.425445795059204)\n",
      "(high, ',', -8.075313568115234)\n",
      "(order, ',', -8.68863582611084)\n",
      "(prior, ',', -10.569696426391602)\n",
      "(model, ',', -9.600680351257324)\n",
      "(of, ',', -4.1284637451171875)\n",
      "(body, ',', -9.267072677612305)\n",
      "(pose, ',', -11.87548542022705)\n",
      "(., ',', -3.0729479789733887)\n",
      "(Jointly, ',', -19.579313278198242)\n",
      "(learning, ',', -9.830510139465332)\n",
      "(\n",
      "\n",
      "\f",
      ", ',', -19.579313278198242)\n",
      "(Table, ',', -19.579313278198242)\n",
      "(4, ',', -8.51557731628418)\n",
      "(., ',', -3.0729479789733887)\n",
      "(PCP, ',', -19.579313278198242)\n",
      "(results, ',', -9.357637405395508)\n",
      "(on, ',', -5.263686656951904)\n",
      "(classiﬁcation, ',', -19.579313278198242)\n",
      "(label, ',', -10.63723373413086)\n",
      "(and, ',', -4.195279121398926)\n",
      "(location, ',', -10.667635917663574)\n",
      "(learning, ',', -9.830510139465332)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(Method, ',', -19.579313278198242)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(Torso, ',', -19.579313278198242)\n",
      "(U.leg, ',', -19.579313278198242)\n",
      "(L.leg, ',', -19.579313278198242)\n",
      "(U.arm, ',', -19.579313278198242)\n",
      "(L.arm, ',', -19.579313278198242)\n",
      "(Head, ',', -19.579313278198242)\n",
      "(Total, ',', -11.708281517028809)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(LSP, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "([, ',', -5.266753196716309)\n",
      "(58, ',', -19.579313278198242)\n",
      "(], ',', -5.498423099517822)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(82.9, ',', -19.579313278198242)\n",
      "(70.3, ',', -19.579313278198242)\n",
      "(67.0, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(Only, ',', -9.382535934448242)\n",
      "(ycls, ',', -19.579313278198242)\n",
      "(82.0, ',', -19.579313278198242)\n",
      "(71.5, ',', -19.579313278198242)\n",
      "(68.0, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(Only, ',', -9.382535934448242)\n",
      "(ypst, ',', -19.579313278198242)\n",
      "(80.4, ',', -19.579313278198242)\n",
      "(72.0, ',', -19.579313278198242)\n",
      "(68.0, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(Separate, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(ycls+ypst, ',', -19.579313278198242)\n",
      "(81.1, ',', -19.579313278198242)\n",
      "(72.8, ',', -19.579313278198242)\n",
      "(69.0, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(Ours, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(85.8, ',', -19.579313278198242)\n",
      "(76.5, ',', -19.579313278198242)\n",
      "(72.2, ',', -19.579313278198242)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(PARSE, ',', -19.579313278198242)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "([, ',', -5.266753196716309)\n",
      "(58, ',', -19.579313278198242)\n",
      "(], ',', -5.498423099517822)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(82.9, ',', -19.579313278198242)\n",
      "(68.8, ',', -19.579313278198242)\n",
      "(60.5, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(Only, ',', -9.382535934448242)\n",
      "(ycls, ',', -19.579313278198242)\n",
      "(81.0, ',', -19.579313278198242)\n",
      "(69.8, ',', -19.579313278198242)\n",
      "(66.1, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(Only, ',', -9.382535934448242)\n",
      "(ypst, ',', -19.579313278198242)\n",
      "(80.5, ',', -19.579313278198242)\n",
      "(71.2, ',', -19.579313278198242)\n",
      "(65.4, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(Separate, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(ycls+ypst, ',', -19.579313278198242)\n",
      "(83.4, ',', -19.579313278198242)\n",
      "(73.7, ',', -19.579313278198242)\n",
      "(67.6, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(89.3, ',', -19.579313278198242)\n",
      "(78.0, ',', -19.579313278198242)\n",
      "(72.0, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(Ours, ',', -19.579313278198242)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(UIUC, ',', -19.579313278198242)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "([, ',', -5.266753196716309)\n",
      "(58, ',', -19.579313278198242)\n",
      "(], ',', -5.498423099517822)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(81.8, ',', -19.579313278198242)\n",
      "(65.0, ',', -19.579313278198242)\n",
      "(55.1, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(Only, ',', -9.382535934448242)\n",
      "(ycls, ',', -19.579313278198242)\n",
      "(85.4, ',', -19.579313278198242)\n",
      "(68.8, ',', -19.579313278198242)\n",
      "(59.3, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(Only, ',', -9.382535934448242)\n",
      "(ypst, ',', -19.579313278198242)\n",
      "(82.6, ',', -19.579313278198242)\n",
      "(66.6, ',', -19.579313278198242)\n",
      "(58.3, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(Separate, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(ycls+ypst, ',', -19.579313278198242)\n",
      "(87.9, ',', -19.579313278198242)\n",
      "(69.6, ',', -19.579313278198242)\n",
      "(60.3, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(Ours, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(89.1, ',', -19.579313278198242)\n",
      "(72.9, ',', -19.579313278198242)\n",
      "(62.3, ',', -19.579313278198242)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(56.0, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(57.6, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(59.2, ',', -19.579313278198242)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(59.5, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(63.3, ',', -19.579313278198242)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(63.4, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(60.5, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(62.2, ',', -19.579313278198242)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(64.4, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(67.8, ',', -19.579313278198242)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(46.8, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(49.2, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(52.2, ',', -19.579313278198242)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(53.0, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(56.3, ',', -19.579313278198242)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(39.8, ',', -19.579313278198242)\n",
      "(79.3, ',', -19.579313278198242)\n",
      "(62.8, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(42.0, ',', -19.579313278198242)\n",
      "(77.2, ',', -19.579313278198242)\n",
      "(63.7, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(42.8, ',', -19.579313278198242)\n",
      "(76.8, ',', -19.579313278198242)\n",
      "(64.1, ',', -19.579313278198242)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(43.0, ',', -19.579313278198242)\n",
      "(77.7, ',', -19.579313278198242)\n",
      "(64.7, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(46.6, ',', -19.579313278198242)\n",
      "(83.1, ',', -19.579313278198242)\n",
      "(68.6, ',', -19.579313278198242)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(42.4, ',', -19.579313278198242)\n",
      "(82.4, ',', -19.579313278198242)\n",
      "(63.6, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(43.9, ',', -19.579313278198242)\n",
      "(76.1, ',', -19.579313278198242)\n",
      "(63.8, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(44.4, ',', -19.579313278198242)\n",
      "(79.5, ',', -19.579313278198242)\n",
      "(64.6, ',', -19.579313278198242)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(47.1, ',', -19.579313278198242)\n",
      "(82.0, ',', -19.579313278198242)\n",
      "(67.1, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(47.8, ',', -19.579313278198242)\n",
      "(89.3, ',', -19.579313278198242)\n",
      "(71.0, ',', -19.579313278198242)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(37.7, ',', -19.579313278198242)\n",
      "(79.8, ',', -19.579313278198242)\n",
      "(57.0, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(40.5, ',', -19.579313278198242)\n",
      "(83.4, ',', -19.579313278198242)\n",
      "(60.4, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(44.7, ',', -19.579313278198242)\n",
      "(81.8, ',', -19.579313278198242)\n",
      "(60.8, ',', -19.579313278198242)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(44.3, ',', -19.579313278198242)\n",
      "(85.4, ',', -19.579313278198242)\n",
      "(62.8, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(47.6, ',', -19.579313278198242)\n",
      "(89.1, ',', -19.579313278198242)\n",
      "(65.6, ',', -19.579313278198242)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(ypst, ',', -19.579313278198242)\n",
      "(and, ',', -4.195279121398926)\n",
      "(ycls, ',', -19.579313278198242)\n",
      "(helps, ',', -10.029866218566895)\n",
      "(to, ',', -3.83851957321167)\n",
      "(ﬁnd, ',', -19.579313278198242)\n",
      "(their, ',', -6.1832733154296875)\n",
      "(shared, ',', -10.801104545593262)\n",
      "(representation, ',', -10.828350067138672)\n",
      "(under, ',', -8.145137786865234)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(a, ',', -3.9830753803253174)\n",
      "(multi, ',', -10.772948265075684)\n",
      "(-, ',', -5.202415943145752)\n",
      "(task, ',', -10.82490348815918)\n",
      "(learning, ',', -9.830510139465332)\n",
      "(framework, ',', -11.075824737548828)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(for, ',', -4.91396951675415)\n",
      "(which, ',', -6.728941917419434)\n",
      "(deep, ',', -9.902402877807617)\n",
      "(model, ',', -9.600680351257324)\n",
      "(is, ',', -4.3297648429870605)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(an, ',', -5.953293800354004)\n",
      "(ideal, ',', -10.64582633972168)\n",
      "(choice, ',', -8.749574661254883)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(6, ',', -9.090521812438965)\n",
      "(., ',', -3.0729479789733887)\n",
      "(Conclusion, ',', -19.579313278198242)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(This, ',', -6.785318851470947)\n",
      "(paper, ',', -8.99613094329834)\n",
      "(has, ',', -6.2021942138671875)\n",
      "(proposed, ',', -10.828350067138672)\n",
      "(a, ',', -3.9830753803253174)\n",
      "(multi, ',', -10.772948265075684)\n",
      "(-, ',', -5.202415943145752)\n",
      "(source, ',', -8.812856674194336)\n",
      "(deep, ',', -9.902402877807617)\n",
      "(model, ',', -9.600680351257324)\n",
      "(for, ',', -4.91396951675415)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(pose, ',', -11.87548542022705)\n",
      "(estimation, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(It, ',', -5.963693141937256)\n",
      "(non, ',', -8.436274528503418)\n",
      "(-, ',', -5.202415943145752)\n",
      "(linearly, ',', -19.579313278198242)\n",
      "(integrates, ',', -19.579313278198242)\n",
      "(three, ',', -8.800739288330078)\n",
      "(information, ',', -8.817930221557617)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(sources, ',', -9.963014602661133)\n",
      "(:, ',', -6.052439212799072)\n",
      "(appearance, ',', -11.022025108337402)\n",
      "(score, ',', -10.567034721374512)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(deformation, ',', -19.579313278198242)\n",
      "(and, ',', -4.195279121398926)\n",
      "(appearance, ',', -11.022025108337402)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(mixture, ',', -19.579313278198242)\n",
      "(type, ',', -8.814929008483887)\n",
      "(., ',', -3.0729479789733887)\n",
      "(These, ',', -8.924954414367676)\n",
      "(information, ',', -8.817930221557617)\n",
      "(sources, ',', -9.963014602661133)\n",
      "(are, ',', -5.160149097442627)\n",
      "(used, ',', -7.587561130523682)\n",
      "(for, ',', -4.91396951675415)\n",
      "(de-, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(scribing, ',', -19.579313278198242)\n",
      "(different, ',', -7.797479629516602)\n",
      "(aspects, ',', -10.912962913513184)\n",
      "(of, ',', -4.1284637451171875)\n",
      "(the, ',', -3.425445795059204)\n",
      "(single, ',', -8.629281997680664)\n",
      "(modality, ',', -19.579313278198242)\n",
      "(data, ',', -9.084769248962402)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(which, ',', -6.728941917419434)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(is, ',', -4.3297648429870605)\n",
      "(the, ',', -3.425445795059204)\n",
      "(image, ',', -9.559075355529785)\n",
      "(data, ',', -9.084769248962402)\n",
      "(in, ',', -4.585874080657959)\n",
      "(our, ',', -7.124547481536865)\n",
      "(pose, ',', -11.87548542022705)\n",
      "(estimation, ',', -19.579313278198242)\n",
      "(approach, ',', -9.994755744934082)\n",
      "(., ',', -3.0729479789733887)\n",
      "(Exten-, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(sive, ',', -19.579313278198242)\n",
      "(experimental, ',', -19.579313278198242)\n",
      "(comparisons, ',', -19.579313278198242)\n",
      "(on, ',', -5.263686656951904)\n",
      "(three, ',', -8.800739288330078)\n",
      "(public, ',', -8.376832962036133)\n",
      "(benchmark, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(datasets, ',', -19.579313278198242)\n",
      "(show, ',', -8.208195686340332)\n",
      "(that, ',', -4.348940372467041)\n",
      "(the, ',', -3.425445795059204)\n",
      "(proposed, ',', -10.828350067138672)\n",
      "(model, ',', -9.600680351257324)\n",
      "(obviously, ',', -9.016507148742676)\n",
      "(improves, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(the, ',', -3.425445795059204)\n",
      "(pose, ',', -11.87548542022705)\n",
      "(estimation, ',', -19.579313278198242)\n",
      "(accuracy, ',', -11.345757484436035)\n",
      "(and, ',', -4.195279121398926)\n",
      "(outperforms, ',', -19.579313278198242)\n",
      "(the, ',', -3.425445795059204)\n",
      "(state, ',', -7.88895320892334)\n",
      "(of, ',', -4.1284637451171875)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(the, ',', -3.425445795059204)\n",
      "(art, ',', -9.778430938720703)\n",
      "(., ',', -3.0729479789733887)\n",
      "(Since, ',', -9.441577911376953)\n",
      "(this, ',', -5.417123317718506)\n",
      "(model, ',', -9.600680351257324)\n",
      "(is, ',', -4.3297648429870605)\n",
      "(a, ',', -3.9830753803253174)\n",
      "(post, ',', -8.091059684753418)\n",
      "(-, ',', -5.202415943145752)\n",
      "(processing, ',', -11.223255157470703)\n",
      "(of, ',', -4.1284637451171875)\n",
      "(informa-, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(tion, ',', -19.579313278198242)\n",
      "(sources, ',', -9.963014602661133)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(it, ',', -4.5064496994018555)\n",
      "(is, ',', -4.3297648429870605)\n",
      "(very, ',', -7.039885520935059)\n",
      "(ﬂexible, ',', -19.579313278198242)\n",
      "(in, ',', -4.585874080657959)\n",
      "(terms, ',', -9.22973918914795)\n",
      "(of, ',', -4.1284637451171875)\n",
      "(integrating, ',', -19.579313278198242)\n",
      "(with, ',', -5.363764762878418)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(existing, ',', -10.217281341552734)\n",
      "(approaches, ',', -11.651655197143555)\n",
      "(that, ',', -4.348940372467041)\n",
      "(use, ',', -7.119458198547363)\n",
      "(different, ',', -7.797479629516602)\n",
      "(information, ',', -8.817930221557617)\n",
      "(sources, ',', -9.963014602661133)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(features, ',', -10.029866218566895)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(or, ',', -5.715355396270752)\n",
      "(articulation, ',', -19.579313278198242)\n",
      "(models, ',', -10.62589168548584)\n",
      "(., ',', -3.0729479789733887)\n",
      "(Learning, ',', -19.579313278198242)\n",
      "(deep, ',', -9.902402877807617)\n",
      "(model, ',', -9.600680351257324)\n",
      "(from, ',', -6.028810501098633)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(pixels, ',', -11.687687873840332)\n",
      "(for, ',', -4.91396951675415)\n",
      "(pose, ',', -11.87548542022705)\n",
      "(estimation, ',', -19.579313278198242)\n",
      "(and, ',', -4.195279121398926)\n",
      "(analyzing, ',', -19.579313278198242)\n",
      "(the, ',', -3.425445795059204)\n",
      "(inﬂuence, ',', -19.579313278198242)\n",
      "(of, ',', -4.1284637451171875)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(training, ',', -10.32292366027832)\n",
      "(data, ',', -9.084769248962402)\n",
      "(number, ',', -8.322697639465332)\n",
      "(will, ',', -6.10568380355835)\n",
      "(be, ',', -5.210641384124756)\n",
      "(the, ',', -3.425445795059204)\n",
      "(future, ',', -8.836128234863281)\n",
      "(work, ',', -7.171552658081055)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(7, ',', -9.367201805114746)\n",
      "(., ',', -3.0729479789733887)\n",
      "(Acknowledgement, ',', -19.579313278198242)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(This, ',', -6.785318851470947)\n",
      "(work, ',', -7.171552658081055)\n",
      "(is, ',', -4.3297648429870605)\n",
      "(supported, ',', -9.985053062438965)\n",
      "(by, ',', -6.114920139312744)\n",
      "(the, ',', -3.425445795059204)\n",
      "(General, ',', -11.15158462524414)\n",
      "(Research, ',', -11.832168579101562)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(Fund, ',', -19.579313278198242)\n",
      "(sponsored, ',', -11.598064422607422)\n",
      "(by, ',', -6.114920139312744)\n",
      "(the, ',', -3.425445795059204)\n",
      "(Research, ',', -11.832168579101562)\n",
      "(Grants, ',', -19.579313278198242)\n",
      "(Council, ',', -11.77718448638916)\n",
      "(of, ',', -4.1284637451171875)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(Hong, ',', -19.579313278198242)\n",
      "(Kong, ',', -19.579313278198242)\n",
      "((, ',', -5.70067024230957)\n",
      "(Project, ',', -11.836889266967773)\n",
      "(No, ',', -7.341436386108398)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(CUHK, ',', -19.579313278198242)\n",
      "(417110, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(CUHK, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(417011, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(CUHK, ',', -19.579313278198242)\n",
      "(429412, ',', -19.579313278198242)\n",
      "(), ',', -5.341753005981445)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(National, ',', -10.254507064819336)\n",
      "(Natural, ',', -19.579313278198242)\n",
      "(Science, ',', -10.600831031799316)\n",
      "(Foun-, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(dation, ',', -19.579313278198242)\n",
      "(of, ',', -4.1284637451171875)\n",
      "(China, ',', -9.557137489318848)\n",
      "((, ',', -5.70067024230957)\n",
      "(91320101, ',', -19.579313278198242)\n",
      "(), ',', -5.341753005981445)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(Shenzhen, ',', -19.579313278198242)\n",
      "(Basic, ',', -19.579313278198242)\n",
      "(Research, ',', -11.832168579101562)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(Program, ',', -19.579313278198242)\n",
      "((, ',', -5.70067024230957)\n",
      "(JC201005270350A, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(JCYJ20120903092050890, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(JCYJ20120617114614438, ',', -19.579313278198242)\n",
      "(), ',', -5.341753005981445)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(and, ',', -4.195279121398926)\n",
      "(Guangdong, ',', -19.579313278198242)\n",
      "(Innovative, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(Research, ',', -11.832168579101562)\n",
      "(Team, ',', -19.579313278198242)\n",
      "(Program, ',', -19.579313278198242)\n",
      "((, ',', -5.70067024230957)\n",
      "(No.201001D0104648280, ',', -19.579313278198242)\n",
      "(), ',', -5.341753005981445)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(Figure, ',', -19.579313278198242)\n",
      "(5, ',', -8.421856880187988)\n",
      "(., ',', -3.0729479789733887)\n",
      "(Comparison, ',', -19.579313278198242)\n",
      "(between, ',', -8.03024673461914)\n",
      "(our, ',', -7.124547481536865)\n",
      "(method, ',', -9.819748878479004)\n",
      "((, ',', -5.70067024230957)\n",
      "(left, ',', -8.262680053710938)\n",
      "(), ',', -5.341753005981445)\n",
      "(and, ',', -4.195279121398926)\n",
      "(the, ',', -3.425445795059204)\n",
      "(approach, ',', -9.994755744934082)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(in, ',', -4.585874080657959)\n",
      "([, ',', -5.266753196716309)\n",
      "(58, ',', -19.579313278198242)\n",
      "(], ',', -5.498423099517822)\n",
      "((, ',', -5.70067024230957)\n",
      "(right, ',', -6.780907154083252)\n",
      "(), ',', -5.341753005981445)\n",
      "(on, ',', -5.263686656951904)\n",
      "(the, ',', -3.425445795059204)\n",
      "(LSP, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(PARSE, ',', -19.579313278198242)\n",
      "(and, ',', -4.195279121398926)\n",
      "(UIUC, ',', -19.579313278198242)\n",
      "(dataset, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(Our, ',', -9.764631271362305)\n",
      "(ap-, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(proach, ',', -19.579313278198242)\n",
      "(obtains, ',', -19.579313278198242)\n",
      "(more, ',', -6.0559234619140625)\n",
      "(reasonable, ',', -9.505705833435059)\n",
      "(articulation, ',', -19.579313278198242)\n",
      "(patterns, ',', -11.080256462097168)\n",
      "(and, ',', -4.195279121398926)\n",
      "(is, ',', -4.3297648429870605)\n",
      "(better, ',', -7.226652145385742)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(in, ',', -4.585874080657959)\n",
      "(solving, ',', -11.540140151977539)\n",
      "(the, ',', -3.425445795059204)\n",
      "(double, ',', -9.830510139465332)\n",
      "(counting, ',', -10.761580467224121)\n",
      "(problem, ',', -7.622410774230957)\n",
      "(., ',', -3.0729479789733887)\n",
      "(Best, ',', -10.38855266571045)\n",
      "(viewed, ',', -11.39624309539795)\n",
      "(in, ',', -4.585874080657959)\n",
      "(color, ',', -9.952893257141113)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(References, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "([, ',', -5.266753196716309)\n",
      "(1, ',', -7.901819705963135)\n",
      "(], ',', -5.498423099517822)\n",
      "(M., ',', -19.579313278198242)\n",
      "(Andriluka, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(L., ',', -19.579313278198242)\n",
      "(Pishchulin, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(P., ',', -19.579313278198242)\n",
      "(Gehler, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(and, ',', -4.195279121398926)\n",
      "(B., ',', -11.687687873840332)\n",
      "(Schiele, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(2d, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(human, ',', -8.437538146972656)\n",
      "(pose, ',', -11.87548542022705)\n",
      "(estimation, ',', -19.579313278198242)\n",
      "(:, ',', -6.052439212799072)\n",
      "(New, ',', -9.029277801513672)\n",
      "(benchmark, ',', -19.579313278198242)\n",
      "(and, ',', -4.195279121398926)\n",
      "(state, ',', -7.88895320892334)\n",
      "(of, ',', -4.1284637451171875)\n",
      "(the, ',', -3.425445795059204)\n",
      "(art, ',', -9.778430938720703)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(analysis, ',', -10.233338356018066)\n",
      "(., ',', -3.0729479789733887)\n",
      "(In, ',', -7.314908981323242)\n",
      "(CVPR, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(2014, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "([, ',', -5.266753196716309)\n",
      "(2, ',', -7.786293029785156)\n",
      "(], ',', -5.498423099517822)\n",
      "(M., ',', -19.579313278198242)\n",
      "(Andriluka, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(S., ',', -19.579313278198242)\n",
      "(Roth, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(and, ',', -4.195279121398926)\n",
      "(B., ',', -11.687687873840332)\n",
      "(Schiele, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(Pictorial, ',', -19.579313278198242)\n",
      "(structures, ',', -11.369197845458984)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(revisited, ',', -19.579313278198242)\n",
      "(:, ',', -6.052439212799072)\n",
      "(people, ',', -5.853616714477539)\n",
      "(detection, ',', -19.579313278198242)\n",
      "(and, ',', -4.195279121398926)\n",
      "(articulated, ',', -19.579313278198242)\n",
      "(pose, ',', -11.87548542022705)\n",
      "(estimation, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(In, ',', -7.314908981323242)\n",
      "(CVPR, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(2009, ',', -11.436646461486816)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "([, ',', -5.266753196716309)\n",
      "(3, ',', -8.093857765197754)\n",
      "(], ',', -5.498423099517822)\n",
      "(Y., ',', -19.579313278198242)\n",
      "(Bengio, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(Learning, ',', -19.579313278198242)\n",
      "(deep, ',', -9.902402877807617)\n",
      "(architectures, ',', -19.579313278198242)\n",
      "(for, ',', -4.91396951675415)\n",
      "(AI, ',', -11.594344139099121)\n",
      "(., ',', -3.0729479789733887)\n",
      "(Foundations, ',', -19.579313278198242)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(and, ',', -4.195279121398926)\n",
      "(Trends, ',', -19.579313278198242)\n",
      "(in, ',', -4.585874080657959)\n",
      "(Machine, ',', -19.579313278198242)\n",
      "(Learning, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(2(1):1–127, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(2009, ',', -11.436646461486816)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "([, ',', -5.266753196716309)\n",
      "(4, ',', -8.51557731628418)\n",
      "(], ',', -5.498423099517822)\n",
      "(Y., ',', -19.579313278198242)\n",
      "(Bengio, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(A., ',', -11.529644012451172)\n",
      "(Courville, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(and, ',', -4.195279121398926)\n",
      "(P., ',', -19.579313278198242)\n",
      "(Vincent, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(Representation, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(learning, ',', -9.830510139465332)\n",
      "(:, ',', -6.052439212799072)\n",
      "(A, ',', -7.381028175354004)\n",
      "(review, ',', -10.490300178527832)\n",
      "(and, ',', -4.195279121398926)\n",
      "(new, ',', -7.5666375160217285)\n",
      "(perspectives, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(IEEE, ',', -19.579313278198242)\n",
      "(Trans, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(PAMI, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(35(8):1798–1828, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(2013, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "([, ',', -5.266753196716309)\n",
      "(5, ',', -8.421856880187988)\n",
      "(], ',', -5.498423099517822)\n",
      "(C., ',', -11.6917724609375)\n",
      "(M., ',', -19.579313278198242)\n",
      "(Bishop, ',', -19.579313278198242)\n",
      "(and, ',', -4.195279121398926)\n",
      "(N., ',', -19.579313278198242)\n",
      "(M., ',', -19.579313278198242)\n",
      "(Nasrabadi, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(Pattern, ',', -19.579313278198242)\n",
      "(recognition, ',', -11.375144958496094)\n",
      "(and, ',', -4.195279121398926)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(machine, ',', -9.18136978149414)\n",
      "(learning, ',', -9.830510139465332)\n",
      "(., ',', -3.0729479789733887)\n",
      "(springer, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(2006, ',', -10.604964256286621)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "([, ',', -5.266753196716309)\n",
      "(6, ',', -9.090521812438965)\n",
      "(], ',', -5.498423099517822)\n",
      "(L., ',', -19.579313278198242)\n",
      "(Bourdev, ',', -19.579313278198242)\n",
      "(and, ',', -4.195279121398926)\n",
      "(J., ',', -19.579313278198242)\n",
      "(Malik, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(Poselets, ',', -19.579313278198242)\n",
      "(:, ',', -6.052439212799072)\n",
      "(body, ',', -9.267072677612305)\n",
      "(part, ',', -7.779394626617432)\n",
      "(detectors, ',', -19.579313278198242)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(trained, ',', -10.797750473022461)\n",
      "(using, ',', -7.910459041595459)\n",
      "(3D, ',', -11.643821716308594)\n",
      "(human, ',', -8.437538146972656)\n",
      "(pose, ',', -11.87548542022705)\n",
      "(annotations, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(In, ',', -7.314908981323242)\n",
      "(ICCV, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(2009, ',', -11.436646461486816)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "([, ',', -5.266753196716309)\n",
      "(7, ',', -9.367201805114746)\n",
      "(], ',', -5.498423099517822)\n",
      "(N., ',', -19.579313278198242)\n",
      "(Dalal, ',', -19.579313278198242)\n",
      "(and, ',', -4.195279121398926)\n",
      "(B., ',', -11.687687873840332)\n",
      "(Triggs, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(Histograms, ',', -19.579313278198242)\n",
      "(of, ',', -4.1284637451171875)\n",
      "(oriented, ',', -11.687687873840332)\n",
      "(gradients, ',', -19.579313278198242)\n",
      "(for, ',', -4.91396951675415)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(human, ',', -8.437538146972656)\n",
      "(detection, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(In, ',', -7.314908981323242)\n",
      "(CVPR, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(2005, ',', -10.968984603881836)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "([, ',', -5.266753196716309)\n",
      "(8, ',', -8.982955932617188)\n",
      "(], ',', -5.498423099517822)\n",
      "(J., ',', -19.579313278198242)\n",
      "(Deng, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(W., ',', -10.755142211914062)\n",
      "(Dong, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(R., ',', -19.579313278198242)\n",
      "(Socher, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(L, ',', -19.579313278198242)\n",
      "(.-, ',', -19.579313278198242)\n",
      "(J., ',', -19.579313278198242)\n",
      "(Li, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(K., ',', -19.579313278198242)\n",
      "(Li, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(and, ',', -4.195279121398926)\n",
      "(L., ',', -19.579313278198242)\n",
      "(Fei-, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(Fei, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(Imagenet, ',', -19.579313278198242)\n",
      "(:, ',', -6.052439212799072)\n",
      "(a, ',', -3.9830753803253174)\n",
      "(large, ',', -8.6246976852417)\n",
      "(-, ',', -5.202415943145752)\n",
      "(scale, ',', -9.841387748718262)\n",
      "(hierarchical, ',', -19.579313278198242)\n",
      "(image, ',', -9.559075355529785)\n",
      "(database, ',', -10.464776039123535)\n",
      "(., ',', -3.0729479789733887)\n",
      "(In, ',', -7.314908981323242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(CVPR, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(2009, ',', -11.436646461486816)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "([, ',', -5.266753196716309)\n",
      "(9, ',', -9.879417419433594)\n",
      "(], ',', -5.498423099517822)\n",
      "(C., ',', -11.6917724609375)\n",
      "(Desai, ',', -19.579313278198242)\n",
      "(and, ',', -4.195279121398926)\n",
      "(D., ',', -19.579313278198242)\n",
      "(Ramanan, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(Detecting, ',', -19.579313278198242)\n",
      "(actions, ',', -9.416446685791016)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(poses, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(and, ',', -4.195279121398926)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(objects, ',', -10.769686698913574)\n",
      "(with, ',', -5.363764762878418)\n",
      "(relational, ',', -19.579313278198242)\n",
      "(phraselets, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(In, ',', -7.314908981323242)\n",
      "(ECCV, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(2012, ',', -11.137375831604004)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "([, ',', -5.266753196716309)\n",
      "(10, ',', -8.452024459838867)\n",
      "(], ',', -5.498423099517822)\n",
      "(K., ',', -19.579313278198242)\n",
      "(Duan, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(D., ',', -19.579313278198242)\n",
      "(Batra, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(and, ',', -4.195279121398926)\n",
      "(D., ',', -19.579313278198242)\n",
      "(J., ',', -19.579313278198242)\n",
      "(Crandall, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(A, ',', -7.381028175354004)\n",
      "(multi, ',', -10.772948265075684)\n",
      "(-, ',', -5.202415943145752)\n",
      "(layer, ',', -11.220696449279785)\n",
      "(com-, ',', -19.579313278198242)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(posite, ',', -19.579313278198242)\n",
      "(model, ',', -9.600680351257324)\n",
      "(for, ',', -4.91396951675415)\n",
      "(human, ',', -8.437538146972656)\n",
      "(pose, ',', -11.87548542022705)\n",
      "(estimation, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(In, ',', -7.314908981323242)\n",
      "(BMVC, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(2012, ',', -11.137375831604004)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "([, ',', -5.266753196716309)\n",
      "(11, ',', -10.191385269165039)\n",
      "(], ',', -5.498423099517822)\n",
      "(M., ',', -19.579313278198242)\n",
      "(Eichner, ',', -19.579313278198242)\n",
      "(and, ',', -4.195279121398926)\n",
      "(V., ',', -19.579313278198242)\n",
      "(Ferrari, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(Appearance, ',', -19.579313278198242)\n",
      "(sharing, ',', -10.739226341247559)\n",
      "(for, ',', -4.91396951675415)\n",
      "(collective, ',', -10.838761329650879)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(human, ',', -8.437538146972656)\n",
      "(pose, ',', -11.87548542022705)\n",
      "(estimation, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(In, ',', -7.314908981323242)\n",
      "(ACCV, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(2012, ',', -11.137375831604004)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "([, ',', -5.266753196716309)\n",
      "(12, ',', -9.68836784362793)\n",
      "(], ',', -5.498423099517822)\n",
      "(C., ',', -11.6917724609375)\n",
      "(Farabet, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(C., ',', -11.6917724609375)\n",
      "(Couprie, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(L., ',', -19.579313278198242)\n",
      "(Najman, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(and, ',', -4.195279121398926)\n",
      "(Y., ',', -19.579313278198242)\n",
      "(LeCun, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(Learning, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(hierarchical, ',', -19.579313278198242)\n",
      "(features, ',', -10.029866218566895)\n",
      "(for, ',', -4.91396951675415)\n",
      "(scene, ',', -10.45284652709961)\n",
      "(labeling, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(IEEE, ',', -19.579313278198242)\n",
      "(Trans, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(PAMI, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(30:1915–1929, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(2013, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "([, ',', -5.266753196716309)\n",
      "(13, ',', -10.399734497070312)\n",
      "(], ',', -5.498423099517822)\n",
      "(P., ',', -19.579313278198242)\n",
      "(F., ',', -19.579313278198242)\n",
      "(Felzenszwalb, ',', -19.579313278198242)\n",
      "(and, ',', -4.195279121398926)\n",
      "(D., ',', -19.579313278198242)\n",
      "(P., ',', -19.579313278198242)\n",
      "(Huttenlocher, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(Pictorial, ',', -19.579313278198242)\n",
      "(struc-, ',', -19.579313278198242)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(tures, ',', -19.579313278198242)\n",
      "(for, ',', -4.91396951675415)\n",
      "(object, ',', -10.152877807617188)\n",
      "(recognition, ',', -11.375144958496094)\n",
      "(., ',', -3.0729479789733887)\n",
      "(IJCV, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(61:55–79, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(2005, ',', -10.968984603881836)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "([, ',', -5.266753196716309)\n",
      "(14, ',', -10.376395225524902)\n",
      "(], ',', -5.498423099517822)\n",
      "(V., ',', -19.579313278198242)\n",
      "(Ferrari, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(M., ',', -19.579313278198242)\n",
      "(Marin, ',', -19.579313278198242)\n",
      "(-, ',', -5.202415943145752)\n",
      "(Jimenez, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(and, ',', -4.195279121398926)\n",
      "(A., ',', -11.529644012451172)\n",
      "(Zisserman, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(Progressive, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(search, ',', -9.428295135498047)\n",
      "(space, ',', -9.31265640258789)\n",
      "(reduction, ',', -11.643821716308594)\n",
      "(for, ',', -4.91396951675415)\n",
      "(human, ',', -8.437538146972656)\n",
      "(pose, ',', -11.87548542022705)\n",
      "(estimation, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(In, ',', -7.314908981323242)\n",
      "(CVPR, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(2008, ',', -9.696126937866211)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(Left, ',', -11.449413299560547)\n",
      "(-, ',', -5.202415943145752)\n",
      "(OursRight, ',', -19.579313278198242)\n",
      "(-, ',', -5.202415943145752)\n",
      "(Yang&Ramanan, ',', -19.579313278198242)\n",
      "(\f",
      ", ',', -19.579313278198242)\n",
      "([, ',', -5.266753196716309)\n",
      "(15, ',', -9.510315895080566)\n",
      "(], ',', -5.498423099517822)\n",
      "(G., ',', -19.579313278198242)\n",
      "(Gkioxari, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(P., ',', -19.579313278198242)\n",
      "(Arbel´aez, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(L., ',', -19.579313278198242)\n",
      "(Bourdev, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(and, ',', -4.195279121398926)\n",
      "(J., ',', -19.579313278198242)\n",
      "(Malik, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(Articu-, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(lated, ',', -19.579313278198242)\n",
      "(pose, ',', -11.87548542022705)\n",
      "(estimation, ',', -19.579313278198242)\n",
      "(using, ',', -7.910459041595459)\n",
      "(discriminative, ',', -19.579313278198242)\n",
      "(armlet, ',', -19.579313278198242)\n",
      "(classiﬁers, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(In, ',', -7.314908981323242)\n",
      "(CVPR, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(2013, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "([, ',', -5.266753196716309)\n",
      "(16, ',', -10.35681438446045)\n",
      "(], ',', -5.498423099517822)\n",
      "(I., ',', -11.430323600769043)\n",
      "(Goodfellow, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(H., ',', -19.579313278198242)\n",
      "(Lee, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(Q., ',', -19.579313278198242)\n",
      "(V., ',', -19.579313278198242)\n",
      "(Le, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(A., ',', -11.529644012451172)\n",
      "(Saxe, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(and, ',', -4.195279121398926)\n",
      "(A., ',', -11.529644012451172)\n",
      "(Y., ',', -19.579313278198242)\n",
      "(Ng, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(Measuring, ',', -19.579313278198242)\n",
      "(invariances, ',', -19.579313278198242)\n",
      "(in, ',', -4.585874080657959)\n",
      "(deep, ',', -9.902402877807617)\n",
      "(networks, ',', -11.04748821258545)\n",
      "(., ',', -3.0729479789733887)\n",
      "(In, ',', -7.314908981323242)\n",
      "(NIPS, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(2009, ',', -11.436646461486816)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "([, ',', -5.266753196716309)\n",
      "(17, ',', -10.712736129760742)\n",
      "(], ',', -5.498423099517822)\n",
      "(M., ',', -19.579313278198242)\n",
      "(Guillaumin, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(J., ',', -19.579313278198242)\n",
      "(Verbeek, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(and, ',', -4.195279121398926)\n",
      "(C., ',', -11.6917724609375)\n",
      "(Schmid, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(Multimodal, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(semi, ',', -10.963041305541992)\n",
      "(-, ',', -5.202415943145752)\n",
      "(supervised, ',', -19.579313278198242)\n",
      "(learning, ',', -9.830510139465332)\n",
      "(for, ',', -4.91396951675415)\n",
      "(image, ',', -9.559075355529785)\n",
      "(classiﬁcation, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(In, ',', -7.314908981323242)\n",
      "(CVPR, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(2010, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "([, ',', -5.266753196716309)\n",
      "(18, ',', -10.092371940612793)\n",
      "(], ',', -5.498423099517822)\n",
      "(G., ',', -19.579313278198242)\n",
      "(E., ',', -19.579313278198242)\n",
      "(Hinton, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(S., ',', -19.579313278198242)\n",
      "(Osindero, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(and, ',', -4.195279121398926)\n",
      "(Y., ',', -19.579313278198242)\n",
      "(Teh, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(A, ',', -7.381028175354004)\n",
      "(fast, ',', -9.421085357666016)\n",
      "(learning, ',', -9.830510139465332)\n",
      "(al-, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(gorithm, ',', -19.579313278198242)\n",
      "(for, ',', -4.91396951675415)\n",
      "(deep, ',', -9.902402877807617)\n",
      "(belief, ',', -9.547979354858398)\n",
      "(nets, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(Neural, ',', -19.579313278198242)\n",
      "(Computation, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(18:1527–, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(1554, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(2006, ',', -10.604964256286621)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "([, ',', -5.266753196716309)\n",
      "(19, ',', -10.888833999633789)\n",
      "(], ',', -5.498423099517822)\n",
      "(G., ',', -19.579313278198242)\n",
      "(E., ',', -19.579313278198242)\n",
      "(Hinton, ',', -19.579313278198242)\n",
      "(and, ',', -4.195279121398926)\n",
      "(R., ',', -19.579313278198242)\n",
      "(R., ',', -19.579313278198242)\n",
      "(Salakhutdinov, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(dimensionality, ',', -19.579313278198242)\n",
      "(of, ',', -4.1284637451171875)\n",
      "(data, ',', -9.084769248962402)\n",
      "(with, ',', -5.363764762878418)\n",
      "(neural, ',', -19.579313278198242)\n",
      "(networks, ',', -11.04748821258545)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(313(5786):504, ',', -19.579313278198242)\n",
      "(–, ',', -10.355737686157227)\n",
      "(507, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(July, ',', -11.424039840698242)\n",
      "(2006, ',', -10.604964256286621)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(Reducing, ',', -19.579313278198242)\n",
      "(the, ',', -3.425445795059204)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(Science, ',', -10.600831031799316)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "([, ',', -5.266753196716309)\n",
      "(20, ',', -8.932963371276855)\n",
      "(], ',', -5.498423099517822)\n",
      "(K., ',', -19.579313278198242)\n",
      "(Jarrett, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(K., ',', -19.579313278198242)\n",
      "(Kavukcuoglu, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(M., ',', -19.579313278198242)\n",
      "(Ranzato, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(and, ',', -4.195279121398926)\n",
      "(Y., ',', -19.579313278198242)\n",
      "(LeCun, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(What, ',', -7.083032131195068)\n",
      "(is, ',', -4.3297648429870605)\n",
      "(the, ',', -3.425445795059204)\n",
      "(best, ',', -7.835007667541504)\n",
      "(multi, ',', -10.772948265075684)\n",
      "(-, ',', -5.202415943145752)\n",
      "(stage, ',', -10.263298034667969)\n",
      "(architecture, ',', -11.57960033416748)\n",
      "(for, ',', -4.91396951675415)\n",
      "(object, ',', -10.152877807617188)\n",
      "(recog-, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(nition, ',', -19.579313278198242)\n",
      "(?, ',', -4.931884288787842)\n",
      "(In, ',', -7.314908981323242)\n",
      "(CVPR, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(2009, ',', -11.436646461486816)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "([, ',', -5.266753196716309)\n",
      "(21, ',', -11.032556533813477)\n",
      "(], ',', -5.498423099517822)\n",
      "(S., ',', -19.579313278198242)\n",
      "(Johnson, ',', -11.605546951293945)\n",
      "(and, ',', -4.195279121398926)\n",
      "(M., ',', -19.579313278198242)\n",
      "(Everingham, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(Clustered, ',', -19.579313278198242)\n",
      "(pose, ',', -11.87548542022705)\n",
      "(and, ',', -4.195279121398926)\n",
      "(nonlin-, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(ear, ',', -11.345757484436035)\n",
      "(appearance, ',', -11.022025108337402)\n",
      "(models, ',', -10.62589168548584)\n",
      "(for, ',', -4.91396951675415)\n",
      "(human, ',', -8.437538146972656)\n",
      "(pose, ',', -11.87548542022705)\n",
      "(estimation, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(In, ',', -7.314908981323242)\n",
      "(BMVC, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(2010, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "([, ',', -5.266753196716309)\n",
      "(22, ',', -11.036799430847168)\n",
      "(], ',', -5.498423099517822)\n",
      "(S., ',', -19.579313278198242)\n",
      "(Johnson, ',', -11.605546951293945)\n",
      "(and, ',', -4.195279121398926)\n",
      "(M., ',', -19.579313278198242)\n",
      "(Everingham, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(Learning, ',', -19.579313278198242)\n",
      "(effective, ',', -9.795491218566895)\n",
      "(human, ',', -8.437538146972656)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(pose, ',', -11.87548542022705)\n",
      "(estimation, ',', -19.579313278198242)\n",
      "(from, ',', -6.028810501098633)\n",
      "(inaccurate, ',', -11.275856971740723)\n",
      "(annotation, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(In, ',', -7.314908981323242)\n",
      "(CVPR, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(2011, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "([, ',', -5.266753196716309)\n",
      "(23, ',', -11.210525512695312)\n",
      "(], ',', -5.498423099517822)\n",
      "(A., ',', -11.529644012451172)\n",
      "(Krizhevsky, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(I., ',', -11.430323600769043)\n",
      "(Sutskever, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(and, ',', -4.195279121398926)\n",
      "(G., ',', -19.579313278198242)\n",
      "(Hinton, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(Imagenet, ',', -19.579313278198242)\n",
      "(clas-, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(siﬁcation, ',', -19.579313278198242)\n",
      "(with, ',', -5.363764762878418)\n",
      "(deep, ',', -9.902402877807617)\n",
      "(convolutional, ',', -19.579313278198242)\n",
      "(neural, ',', -19.579313278198242)\n",
      "(networks, ',', -11.04748821258545)\n",
      "(., ',', -3.0729479789733887)\n",
      "(In, ',', -7.314908981323242)\n",
      "(NIPS, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(2012, ',', -11.137375831604004)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "([, ',', -5.266753196716309)\n",
      "(24, ',', -10.54468822479248)\n",
      "(], ',', -5.498423099517822)\n",
      "(F., ',', -19.579313278198242)\n",
      "(R., ',', -19.579313278198242)\n",
      "(Kschischang, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(B., ',', -11.687687873840332)\n",
      "(J., ',', -19.579313278198242)\n",
      "(Frey, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(and, ',', -4.195279121398926)\n",
      "(H, ',', -19.579313278198242)\n",
      "(.-, ',', -19.579313278198242)\n",
      "(A., ',', -11.529644012451172)\n",
      "(Loeliger, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(Factor, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(graphs, ',', -19.579313278198242)\n",
      "(and, ',', -4.195279121398926)\n",
      "(the, ',', -3.425445795059204)\n",
      "(sum, ',', -10.786099433898926)\n",
      "(-, ',', -5.202415943145752)\n",
      "(product, ',', -9.455039978027344)\n",
      "(algorithm, ',', -11.107267379760742)\n",
      "(., ',', -3.0729479789733887)\n",
      "(IEEE, ',', -19.579313278198242)\n",
      "(Trans, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(Inf, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(The-, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(ory, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(47(2):498–519, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(2001, ',', -10.794407844543457)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "([, ',', -5.266753196716309)\n",
      "(25, ',', -10.009870529174805)\n",
      "(], ',', -5.498423099517822)\n",
      "(H., ',', -19.579313278198242)\n",
      "(Larochelle, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(Y., ',', -19.579313278198242)\n",
      "(Bengio, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(J., ',', -19.579313278198242)\n",
      "(Louradour, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(and, ',', -4.195279121398926)\n",
      "(P., ',', -19.579313278198242)\n",
      "(Lamblin, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(Ex-, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(ploring, ',', -19.579313278198242)\n",
      "(strategies, ',', -19.579313278198242)\n",
      "(for, ',', -4.91396951675415)\n",
      "(training, ',', -10.32292366027832)\n",
      "(deep, ',', -9.902402877807617)\n",
      "(neural, ',', -19.579313278198242)\n",
      "(networks, ',', -11.04748821258545)\n",
      "(., ',', -3.0729479789733887)\n",
      "(J., ',', -19.579313278198242)\n",
      "(Ma-, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(chine, ',', -19.579313278198242)\n",
      "(Learning, ',', -19.579313278198242)\n",
      "(Research, ',', -11.832168579101562)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(10:1–40, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(2009, ',', -11.436646461486816)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "([, ',', -5.266753196716309)\n",
      "(26, ',', -11.492071151733398)\n",
      "(], ',', -5.498423099517822)\n",
      "(Q., ',', -19.579313278198242)\n",
      "(V., ',', -19.579313278198242)\n",
      "(Le, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(M., ',', -19.579313278198242)\n",
      "(Ranzato, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(R., ',', -19.579313278198242)\n",
      "(Monga, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(M., ',', -19.579313278198242)\n",
      "(Devin, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(K., ',', -19.579313278198242)\n",
      "(Chen, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(G., ',', -19.579313278198242)\n",
      "(S., ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(Corrado, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(J., ',', -19.579313278198242)\n",
      "(Dean, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(and, ',', -4.195279121398926)\n",
      "(A., ',', -11.529644012451172)\n",
      "(Y., ',', -19.579313278198242)\n",
      "(Ng, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(Building, ',', -19.579313278198242)\n",
      "(high, ',', -8.075313568115234)\n",
      "(-, ',', -5.202415943145752)\n",
      "(level, ',', -8.576519012451172)\n",
      "(features, ',', -10.029866218566895)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(using, ',', -7.910459041595459)\n",
      "(large, ',', -8.6246976852417)\n",
      "(scale, ',', -9.841387748718262)\n",
      "(unsupervised, ',', -19.579313278198242)\n",
      "(learning, ',', -9.830510139465332)\n",
      "(., ',', -3.0729479789733887)\n",
      "(In, ',', -7.314908981323242)\n",
      "(ICML, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(2012, ',', -11.137375831604004)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "([, ',', -5.266753196716309)\n",
      "(27, ',', -11.449413299560547)\n",
      "(], ',', -5.498423099517822)\n",
      "(Y., ',', -19.579313278198242)\n",
      "(LeCun, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(L., ',', -19.579313278198242)\n",
      "(Bottou, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(Y., ',', -19.579313278198242)\n",
      "(Bengio, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(and, ',', -4.195279121398926)\n",
      "(P., ',', -19.579313278198242)\n",
      "(Haffner, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(Gradient-, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(based, ',', -8.318479537963867)\n",
      "(learning, ',', -9.830510139465332)\n",
      "(applied, ',', -10.576382637023926)\n",
      "(to, ',', -3.83851957321167)\n",
      "(document, ',', -10.694451332092285)\n",
      "(recognition, ',', -11.375144958496094)\n",
      "(., ',', -3.0729479789733887)\n",
      "(Proceed-, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(ings, ',', -19.579313278198242)\n",
      "(of, ',', -4.1284637451171875)\n",
      "(the, ',', -3.425445795059204)\n",
      "(IEEE, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(86(11):2278–2324, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(1998, ',', -11.643821716308594)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "([, ',', -5.266753196716309)\n",
      "(28, ',', -11.241353034973145)\n",
      "(], ',', -5.498423099517822)\n",
      "(W., ',', -10.755142211914062)\n",
      "(Li, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(R., ',', -19.579313278198242)\n",
      "(Zhao, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(T., ',', -19.579313278198242)\n",
      "(Xiao, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(and, ',', -4.195279121398926)\n",
      "(X., ',', -11.786141395568848)\n",
      "(Wang, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(Deepreid, ',', -19.579313278198242)\n",
      "(:, ',', -6.052439212799072)\n",
      "(Deep, ',', -19.579313278198242)\n",
      "(ﬁlter, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(pairing, ',', -19.579313278198242)\n",
      "(neural, ',', -19.579313278198242)\n",
      "(network, ',', -9.909953117370605)\n",
      "(for, ',', -4.91396951675415)\n",
      "(person, ',', -7.776208400726318)\n",
      "(re, ',', -9.438133239746094)\n",
      "(-, ',', -5.202415943145752)\n",
      "(identiﬁcation, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(In, ',', -7.314908981323242)\n",
      "(CVPR, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(2014, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "([, ',', -5.266753196716309)\n",
      "(29, ',', -11.733567237854004)\n",
      "(], ',', -5.498423099517822)\n",
      "(P., ',', -19.579313278198242)\n",
      "(Luo, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(Y., ',', -19.579313278198242)\n",
      "(Tian, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(X., ',', -11.786141395568848)\n",
      "(Wang, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(and, ',', -4.195279121398926)\n",
      "(X., ',', -11.786141395568848)\n",
      "(Tang, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(Switchable, ',', -19.579313278198242)\n",
      "(deep, ',', -9.902402877807617)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(network, ',', -9.909953117370605)\n",
      "(for, ',', -4.91396951675415)\n",
      "(pedestrian, ',', -19.579313278198242)\n",
      "(detection, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(In, ',', -7.314908981323242)\n",
      "(CVPR, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(2014, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "([, ',', -5.266753196716309)\n",
      "(30, ',', -9.207648277282715)\n",
      "(], ',', -5.498423099517822)\n",
      "(P., ',', -19.579313278198242)\n",
      "(Luo, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(X., ',', -11.786141395568848)\n",
      "(Wang, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(and, ',', -4.195279121398926)\n",
      "(X., ',', -11.786141395568848)\n",
      "(Tang, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(Hierarchical, ',', -19.579313278198242)\n",
      "(face, ',', -8.649394035339355)\n",
      "(parsing, ',', -19.579313278198242)\n",
      "(via, ',', -9.864821434020996)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(deep, ',', -9.902402877807617)\n",
      "(learning, ',', -9.830510139465332)\n",
      "(., ',', -3.0729479789733887)\n",
      "(In, ',', -7.314908981323242)\n",
      "(CVPR, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(2012, ',', -11.137375831604004)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "([, ',', -5.266753196716309)\n",
      "(31, ',', -11.86569595336914)\n",
      "(], ',', -5.498423099517822)\n",
      "(P., ',', -19.579313278198242)\n",
      "(Luo, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(X., ',', -11.786141395568848)\n",
      "(Wang, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(and, ',', -4.195279121398926)\n",
      "(X., ',', -11.786141395568848)\n",
      "(Tang, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(A, ',', -7.381028175354004)\n",
      "(deep, ',', -9.902402877807617)\n",
      "(sum, ',', -10.786099433898926)\n",
      "(-, ',', -5.202415943145752)\n",
      "(product, ',', -9.455039978027344)\n",
      "(archi-, ',', -19.579313278198242)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(tecture, ',', -19.579313278198242)\n",
      "(for, ',', -4.91396951675415)\n",
      "(robust, ',', -19.579313278198242)\n",
      "(facial, ',', -19.579313278198242)\n",
      "(attributes, ',', -19.579313278198242)\n",
      "(analysis, ',', -10.233338356018066)\n",
      "(., ',', -3.0729479789733887)\n",
      "(In, ',', -7.314908981323242)\n",
      "(ICCV, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(2013, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "([, ',', -5.266753196716309)\n",
      "(32, ',', -11.498798370361328)\n",
      "(], ',', -5.498423099517822)\n",
      "(P., ',', -19.579313278198242)\n",
      "(Luo, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(X., ',', -11.786141395568848)\n",
      "(Wang, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(and, ',', -4.195279121398926)\n",
      "(X., ',', -11.786141395568848)\n",
      "(Tang, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(Pedestrian, ',', -19.579313278198242)\n",
      "(parsing, ',', -19.579313278198242)\n",
      "(via, ',', -9.864821434020996)\n",
      "(deep, ',', -9.902402877807617)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(decompositional, ',', -19.579313278198242)\n",
      "(neural, ',', -19.579313278198242)\n",
      "(network, ',', -9.909953117370605)\n",
      "(., ',', -3.0729479789733887)\n",
      "(In, ',', -7.314908981323242)\n",
      "(ICCV, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(2013, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "([, ',', -5.266753196716309)\n",
      "(33, ',', -11.822792053222656)\n",
      "(], ',', -5.498423099517822)\n",
      "(G., ',', -19.579313278198242)\n",
      "(Mori, ',', -19.579313278198242)\n",
      "(and, ',', -4.195279121398926)\n",
      "(J., ',', -19.579313278198242)\n",
      "(Malik, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(Estimating, ',', -19.579313278198242)\n",
      "(human, ',', -8.437538146972656)\n",
      "(body, ',', -9.267072677612305)\n",
      "(conﬁgurations, ',', -19.579313278198242)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(using, ',', -7.910459041595459)\n",
      "(shape, ',', -10.592615127563477)\n",
      "(context, ',', -9.45460319519043)\n",
      "(matching, ',', -11.78165340423584)\n",
      "(., ',', -3.0729479789733887)\n",
      "(In, ',', -7.314908981323242)\n",
      "(ECCV, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(2002, ',', -11.427176475524902)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "([, ',', -5.266753196716309)\n",
      "(34, ',', -11.746454238891602)\n",
      "(], ',', -5.498423099517822)\n",
      "(G., ',', -19.579313278198242)\n",
      "(Mori, ',', -19.579313278198242)\n",
      "(and, ',', -4.195279121398926)\n",
      "(J., ',', -19.579313278198242)\n",
      "(Malik, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(Recovering, ',', -19.579313278198242)\n",
      "(3D, ',', -11.643821716308594)\n",
      "(human, ',', -8.437538146972656)\n",
      "(body, ',', -9.267072677612305)\n",
      "(conﬁgura-, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(tions, ',', -19.579313278198242)\n",
      "(using, ',', -7.910459041595459)\n",
      "(shape, ',', -10.592615127563477)\n",
      "(contexts, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(IEEE, ',', -19.579313278198242)\n",
      "(Trans, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(PAMI, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(28(7):1052–, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(1062, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(2006, ',', -10.604964256286621)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "([, ',', -5.266753196716309)\n",
      "(35, ',', -11.098182678222656)\n",
      "(], ',', -5.498423099517822)\n",
      "(J., ',', -19.579313278198242)\n",
      "(Ngiam, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(A., ',', -11.529644012451172)\n",
      "(Khosla, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(M., ',', -19.579313278198242)\n",
      "(Kim, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(J., ',', -19.579313278198242)\n",
      "(Nam, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(H., ',', -19.579313278198242)\n",
      "(Lee, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(and, ',', -4.195279121398926)\n",
      "(A., ',', -11.529644012451172)\n",
      "(Ng, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(Multimodal, ',', -19.579313278198242)\n",
      "(deep, ',', -9.902402877807617)\n",
      "(learning, ',', -9.830510139465332)\n",
      "(., ',', -3.0729479789733887)\n",
      "(In, ',', -7.314908981323242)\n",
      "(ICML, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(2011, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "([, ',', -5.266753196716309)\n",
      "(36, ',', -11.679567337036133)\n",
      "(], ',', -5.498423099517822)\n",
      "(M., ',', -19.579313278198242)\n",
      "(Norouzi, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(M., ',', -19.579313278198242)\n",
      "(Ranjbar, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(and, ',', -4.195279121398926)\n",
      "(G., ',', -19.579313278198242)\n",
      "(Mori, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(Stacks, ',', -19.579313278198242)\n",
      "(of, ',', -4.1284637451171875)\n",
      "(convolu-, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(tional, ',', -19.579313278198242)\n",
      "(restricted, ',', -11.86569595336914)\n",
      "(boltzmann, ',', -19.579313278198242)\n",
      "(machines, ',', -9.629410743713379)\n",
      "(for, ',', -4.91396951675415)\n",
      "(shift, ',', -10.481719017028809)\n",
      "(-, ',', -5.202415943145752)\n",
      "(invariant, ',', -19.579313278198242)\n",
      "(fea-, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(ture, ',', -19.579313278198242)\n",
      "(learning, ',', -9.830510139465332)\n",
      "(., ',', -3.0729479789733887)\n",
      "(In, ',', -7.314908981323242)\n",
      "(CVPR, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(2009, ',', -11.436646461486816)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "([, ',', -5.266753196716309)\n",
      "(37, ',', -19.579313278198242)\n",
      "(], ',', -5.498423099517822)\n",
      "(W., ',', -10.755142211914062)\n",
      "(Ouyang, ',', -19.579313278198242)\n",
      "(and, ',', -4.195279121398926)\n",
      "(X., ',', -11.786141395568848)\n",
      "(Wang, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(A, ',', -7.381028175354004)\n",
      "(discriminative, ',', -19.579313278198242)\n",
      "(deep, ',', -9.902402877807617)\n",
      "(model, ',', -9.600680351257324)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(for, ',', -4.91396951675415)\n",
      "(pedestrian, ',', -19.579313278198242)\n",
      "(detection, ',', -19.579313278198242)\n",
      "(with, ',', -5.363764762878418)\n",
      "(occlusion, ',', -19.579313278198242)\n",
      "(handling, ',', -11.462346076965332)\n",
      "(., ',', -3.0729479789733887)\n",
      "(In, ',', -7.314908981323242)\n",
      "(CVPR, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(2012, ',', -11.137375831604004)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "([, ',', -5.266753196716309)\n",
      "(38, ',', -19.579313278198242)\n",
      "(], ',', -5.498423099517822)\n",
      "(W., ',', -10.755142211914062)\n",
      "(Ouyang, ',', -19.579313278198242)\n",
      "(and, ',', -4.195279121398926)\n",
      "(X., ',', -11.786141395568848)\n",
      "(Wang, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(Joint, ',', -19.579313278198242)\n",
      "(deep, ',', -9.902402877807617)\n",
      "(learning, ',', -9.830510139465332)\n",
      "(for, ',', -4.91396951675415)\n",
      "(pedestrian, ',', -19.579313278198242)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(detection, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(In, ',', -7.314908981323242)\n",
      "(ICCV, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(2013, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "([, ',', -5.266753196716309)\n",
      "(39, ',', -19.579313278198242)\n",
      "(], ',', -5.498423099517822)\n",
      "(W., ',', -10.755142211914062)\n",
      "(Ouyang, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(X., ',', -11.786141395568848)\n",
      "(Zeng, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(and, ',', -4.195279121398926)\n",
      "(X., ',', -11.786141395568848)\n",
      "(Wang, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(Modeling, ',', -19.579313278198242)\n",
      "(mutual, ',', -11.547200202941895)\n",
      "(visi-, ',', -19.579313278198242)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(bility, ',', -19.579313278198242)\n",
      "(relationship, ',', -9.987283706665039)\n",
      "(in, ',', -4.585874080657959)\n",
      "(pedestrian, ',', -19.579313278198242)\n",
      "(detection, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(In, ',', -7.314908981323242)\n",
      "(CVPR, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(2013, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "([, ',', -5.266753196716309)\n",
      "(40, ',', -9.825430870056152)\n",
      "(], ',', -5.498423099517822)\n",
      "(L., ',', -19.579313278198242)\n",
      "(Pishchulin, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(M., ',', -19.579313278198242)\n",
      "(Andriluka, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(P., ',', -19.579313278198242)\n",
      "(Gehler, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(and, ',', -4.195279121398926)\n",
      "(B., ',', -11.687687873840332)\n",
      "(Schiele, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(Pose-, ',', -19.579313278198242)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(let, ',', -7.957937717437744)\n",
      "(conditioned, ',', -19.579313278198242)\n",
      "(pictorial, ',', -19.579313278198242)\n",
      "(structures, ',', -11.369197845458984)\n",
      "(., ',', -3.0729479789733887)\n",
      "(In, ',', -7.314908981323242)\n",
      "(CVPR, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(2013, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "([, ',', -5.266753196716309)\n",
      "(41, ',', -19.579313278198242)\n",
      "(], ',', -5.498423099517822)\n",
      "(L., ',', -19.579313278198242)\n",
      "(Pishchulin, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(M., ',', -19.579313278198242)\n",
      "(Andriluka, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(P., ',', -19.579313278198242)\n",
      "(Gehler, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(and, ',', -4.195279121398926)\n",
      "(B., ',', -11.687687873840332)\n",
      "(Schiele, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(Strong, ',', -19.579313278198242)\n",
      "(appearance, ',', -11.022025108337402)\n",
      "(and, ',', -4.195279121398926)\n",
      "(expressive, ',', -19.579313278198242)\n",
      "(spatial, ',', -19.579313278198242)\n",
      "(models, ',', -10.62589168548584)\n",
      "(for, ',', -4.91396951675415)\n",
      "(human, ',', -8.437538146972656)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(pose, ',', -11.87548542022705)\n",
      "(estimation, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(In, ',', -7.314908981323242)\n",
      "(ICCV, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(December, ',', -11.827468872070312)\n",
      "(2013, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "([, ',', -5.266753196716309)\n",
      "(42, ',', -11.540140151977539)\n",
      "(], ',', -5.498423099517822)\n",
      "(L., ',', -19.579313278198242)\n",
      "(Pishchulin, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(A., ',', -11.529644012451172)\n",
      "(Jain, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(M., ',', -19.579313278198242)\n",
      "(Andriluka, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(T., ',', -19.579313278198242)\n",
      "(Thormahlen, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(and, ',', -4.195279121398926)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(B., ',', -11.687687873840332)\n",
      "(Schiele, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(Articulated, ',', -19.579313278198242)\n",
      "(people, ',', -5.853616714477539)\n",
      "(detection, ',', -19.579313278198242)\n",
      "(and, ',', -4.195279121398926)\n",
      "(pose, ',', -11.87548542022705)\n",
      "(estimation, ',', -19.579313278198242)\n",
      "(:, ',', -6.052439212799072)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(Reshaping, ',', -19.579313278198242)\n",
      "(the, ',', -3.425445795059204)\n",
      "(future, ',', -8.836128234863281)\n",
      "(., ',', -3.0729479789733887)\n",
      "(In, ',', -7.314908981323242)\n",
      "(CVPR, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(2012, ',', -11.137375831604004)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "([, ',', -5.266753196716309)\n",
      "(43, ',', -19.579313278198242)\n",
      "(], ',', -5.498423099517822)\n",
      "(H., ',', -19.579313278198242)\n",
      "(Poon, ',', -19.579313278198242)\n",
      "(and, ',', -4.195279121398926)\n",
      "(P., ',', -19.579313278198242)\n",
      "(Domingos, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(Sum, ',', -19.579313278198242)\n",
      "(-, ',', -5.202415943145752)\n",
      "(product, ',', -9.455039978027344)\n",
      "(networks, ',', -11.04748821258545)\n",
      "(:, ',', -6.052439212799072)\n",
      "(A, ',', -7.381028175354004)\n",
      "(new, ',', -7.5666375160217285)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(deep, ',', -9.902402877807617)\n",
      "(architecture, ',', -11.57960033416748)\n",
      "(., ',', -3.0729479789733887)\n",
      "(In, ',', -7.314908981323242)\n",
      "(UAI, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(2011, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "([, ',', -5.266753196716309)\n",
      "(44, ',', -19.579313278198242)\n",
      "(], ',', -5.498423099517822)\n",
      "(D., ',', -19.579313278198242)\n",
      "(Ramanan, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(Learning, ',', -19.579313278198242)\n",
      "(to, ',', -3.83851957321167)\n",
      "(parse, ',', -19.579313278198242)\n",
      "(images, ',', -10.350371360778809)\n",
      "(of, ',', -4.1284637451171875)\n",
      "(articulated, ',', -19.579313278198242)\n",
      "(bodies, ',', -10.928106307983398)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(In, ',', -7.314908981323242)\n",
      "(NIPS, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(2007, ',', -10.725114822387695)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "([, ',', -5.266753196716309)\n",
      "(45, ',', -11.121050834655762)\n",
      "(], ',', -5.498423099517822)\n",
      "(M., ',', -19.579313278198242)\n",
      "(Ranzato, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(F., ',', -19.579313278198242)\n",
      "(J., ',', -19.579313278198242)\n",
      "(Huang, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(Y, ',', -11.468875885009766)\n",
      "(.-, ',', -19.579313278198242)\n",
      "(L., ',', -19.579313278198242)\n",
      "(Boureau, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(and, ',', -4.195279121398926)\n",
      "(Y., ',', -19.579313278198242)\n",
      "(Lecun, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(Un-, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(supervised, ',', -19.579313278198242)\n",
      "(learning, ',', -9.830510139465332)\n",
      "(of, ',', -4.1284637451171875)\n",
      "(invariant, ',', -19.579313278198242)\n",
      "(feature, ',', -9.824798583984375)\n",
      "(hierarchies, ',', -19.579313278198242)\n",
      "(with, ',', -5.363764762878418)\n",
      "(ap-, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(plications, ',', -19.579313278198242)\n",
      "(to, ',', -3.83851957321167)\n",
      "(object, ',', -10.152877807617188)\n",
      "(recognition, ',', -11.375144958496094)\n",
      "(., ',', -3.0729479789733887)\n",
      "(In, ',', -7.314908981323242)\n",
      "(CVPR, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(2007, ',', -10.725114822387695)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "([, ',', -5.266753196716309)\n",
      "(46, ',', -19.579313278198242)\n",
      "(], ',', -5.498423099517822)\n",
      "(B., ',', -11.687687873840332)\n",
      "(Sapp, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(A., ',', -11.529644012451172)\n",
      "(Toshev, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(and, ',', -4.195279121398926)\n",
      "(B., ',', -11.687687873840332)\n",
      "(Taskar, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(Cascaded, ',', -19.579313278198242)\n",
      "(models, ',', -10.62589168548584)\n",
      "(for, ',', -4.91396951675415)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(articulated, ',', -19.579313278198242)\n",
      "(pose, ',', -11.87548542022705)\n",
      "(estimation, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(In, ',', -7.314908981323242)\n",
      "(ECCV, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(2010, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "([, ',', -5.266753196716309)\n",
      "(47, ',', -19.579313278198242)\n",
      "(], ',', -5.498423099517822)\n",
      "(N., ',', -19.579313278198242)\n",
      "(Srivastava, ',', -19.579313278198242)\n",
      "(and, ',', -4.195279121398926)\n",
      "(R., ',', -19.579313278198242)\n",
      "(Salakhutdinov, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(Multimodal, ',', -19.579313278198242)\n",
      "(learning, ',', -9.830510139465332)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(with, ',', -5.363764762878418)\n",
      "(deep, ',', -9.902402877807617)\n",
      "(boltzmann, ',', -19.579313278198242)\n",
      "(machines, ',', -9.629410743713379)\n",
      "(., ',', -3.0729479789733887)\n",
      "(In, ',', -7.314908981323242)\n",
      "(NIPS, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(2012, ',', -11.137375831604004)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "([, ',', -5.266753196716309)\n",
      "(48, ',', -11.763898849487305)\n",
      "(], ',', -5.498423099517822)\n",
      "(M., ',', -19.579313278198242)\n",
      "(Sun, ',', -10.870661735534668)\n",
      "(and, ',', -4.195279121398926)\n",
      "(S., ',', -19.579313278198242)\n",
      "(Savarese, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(Articulated, ',', -19.579313278198242)\n",
      "(part, ',', -7.779394626617432)\n",
      "(-, ',', -5.202415943145752)\n",
      "(based, ',', -8.318479537963867)\n",
      "(model, ',', -9.600680351257324)\n",
      "(for, ',', -4.91396951675415)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(joint, ',', -11.446206092834473)\n",
      "(object, ',', -10.152877807617188)\n",
      "(detection, ',', -19.579313278198242)\n",
      "(and, ',', -4.195279121398926)\n",
      "(pose, ',', -11.87548542022705)\n",
      "(estimation, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(In, ',', -7.314908981323242)\n",
      "(ICCV, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(2011, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "([, ',', -5.266753196716309)\n",
      "(49, ',', -19.579313278198242)\n",
      "(], ',', -5.498423099517822)\n",
      "(Y., ',', -19.579313278198242)\n",
      "(Sun, ',', -10.870661735534668)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(X., ',', -11.786141395568848)\n",
      "(Wang, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(and, ',', -4.195279121398926)\n",
      "(X., ',', -11.786141395568848)\n",
      "(Tang, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(Deep, ',', -19.579313278198242)\n",
      "(convolutional, ',', -19.579313278198242)\n",
      "(network, ',', -9.909953117370605)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(cascade, ',', -19.579313278198242)\n",
      "(for, ',', -4.91396951675415)\n",
      "(facial, ',', -19.579313278198242)\n",
      "(point, ',', -7.199707984924316)\n",
      "(detection, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(In, ',', -7.314908981323242)\n",
      "(CVPR, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(2013, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "([, ',', -5.266753196716309)\n",
      "(50, ',', -9.218977928161621)\n",
      "(], ',', -5.498423099517822)\n",
      "(Y., ',', -19.579313278198242)\n",
      "(Sun, ',', -10.870661735534668)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(X., ',', -11.786141395568848)\n",
      "(Wang, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(and, ',', -4.195279121398926)\n",
      "(X., ',', -11.786141395568848)\n",
      "(Tang, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(Hybrid, ',', -19.579313278198242)\n",
      "(deep, ',', -9.902402877807617)\n",
      "(learning, ',', -9.830510139465332)\n",
      "(for, ',', -4.91396951675415)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(computing, ',', -11.62450122833252)\n",
      "(face, ',', -8.649394035339355)\n",
      "(similarities, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(In, ',', -7.314908981323242)\n",
      "(ICCV, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(2013, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "([, ',', -5.266753196716309)\n",
      "(51, ',', -11.86569595336914)\n",
      "(], ',', -5.498423099517822)\n",
      "(Y., ',', -19.579313278198242)\n",
      "(Sun, ',', -10.870661735534668)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(X., ',', -11.786141395568848)\n",
      "(Wang, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(and, ',', -4.195279121398926)\n",
      "(X., ',', -11.786141395568848)\n",
      "(Tang, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(Deep, ',', -19.579313278198242)\n",
      "(learning, ',', -9.830510139465332)\n",
      "(face, ',', -8.649394035339355)\n",
      "(represen-, ',', -19.579313278198242)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(tation, ',', -19.579313278198242)\n",
      "(from, ',', -6.028810501098633)\n",
      "(predicting, ',', -19.579313278198242)\n",
      "(10,000, ',', -11.007466316223145)\n",
      "(classes, ',', -10.08577823638916)\n",
      "(., ',', -3.0729479789733887)\n",
      "(In, ',', -7.314908981323242)\n",
      "(CVPR, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(2014, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "([, ',', -5.266753196716309)\n",
      "(52, ',', -19.579313278198242)\n",
      "(], ',', -5.498423099517822)\n",
      "(Y., ',', -19.579313278198242)\n",
      "(Tian, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(C., ',', -11.6917724609375)\n",
      "(L., ',', -19.579313278198242)\n",
      "(Zitnick, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(and, ',', -4.195279121398926)\n",
      "(S., ',', -19.579313278198242)\n",
      "(G., ',', -19.579313278198242)\n",
      "(Narasimhan, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(Exploring, ',', -19.579313278198242)\n",
      "(the, ',', -3.425445795059204)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(spatial, ',', -19.579313278198242)\n",
      "(hierarchy, ',', -19.579313278198242)\n",
      "(of, ',', -4.1284637451171875)\n",
      "(mixture, ',', -19.579313278198242)\n",
      "(models, ',', -10.62589168548584)\n",
      "(for, ',', -4.91396951675415)\n",
      "(human, ',', -8.437538146972656)\n",
      "(pose, ',', -11.87548542022705)\n",
      "(estima-, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(tion, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(In, ',', -7.314908981323242)\n",
      "(ECCV, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(2012, ',', -11.137375831604004)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "([, ',', -5.266753196716309)\n",
      "(53, ',', -19.579313278198242)\n",
      "(], ',', -5.498423099517822)\n",
      "(D., ',', -19.579313278198242)\n",
      "(Tran, ',', -19.579313278198242)\n",
      "(and, ',', -4.195279121398926)\n",
      "(D., ',', -19.579313278198242)\n",
      "(Forsyth, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(Improved, ',', -19.579313278198242)\n",
      "(human, ',', -8.437538146972656)\n",
      "(parsing, ',', -19.579313278198242)\n",
      "(with, ',', -5.363764762878418)\n",
      "(a, ',', -3.9830753803253174)\n",
      "(full, ',', -8.511314392089844)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(relational, ',', -19.579313278198242)\n",
      "(model, ',', -9.600680351257324)\n",
      "(., ',', -3.0729479789733887)\n",
      "(In, ',', -7.314908981323242)\n",
      "(ECCV, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(2010, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "([, ',', -5.266753196716309)\n",
      "(54, ',', -19.579313278198242)\n",
      "(], ',', -5.498423099517822)\n",
      "(F., ',', -19.579313278198242)\n",
      "(Wang, ',', -19.579313278198242)\n",
      "(and, ',', -4.195279121398926)\n",
      "(Y., ',', -19.579313278198242)\n",
      "(Li, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(Beyond, ',', -19.579313278198242)\n",
      "(physical, ',', -9.736432075500488)\n",
      "(connections, ',', -11.144454956054688)\n",
      "(:, ',', -6.052439212799072)\n",
      "(Tree, ',', -19.579313278198242)\n",
      "(mod-, ',', -19.579313278198242)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(els, ',', -19.579313278198242)\n",
      "(in, ',', -4.585874080657959)\n",
      "(human, ',', -8.437538146972656)\n",
      "(pose, ',', -11.87548542022705)\n",
      "(estimation, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(In, ',', -7.314908981323242)\n",
      "(CVPR, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(2013, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "([, ',', -5.266753196716309)\n",
      "(55, ',', -11.763898849487305)\n",
      "(], ',', -5.498423099517822)\n",
      "(Y., ',', -19.579313278198242)\n",
      "(Wang, ',', -19.579313278198242)\n",
      "(and, ',', -4.195279121398926)\n",
      "(G., ',', -19.579313278198242)\n",
      "(Mori, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(Multiple, ',', -19.579313278198242)\n",
      "(tree, ',', -10.449295997619629)\n",
      "(models, ',', -10.62589168548584)\n",
      "(for, ',', -4.91396951675415)\n",
      "(occlusion, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(and, ',', -4.195279121398926)\n",
      "(spatial, ',', -19.579313278198242)\n",
      "(constraints, ',', -19.579313278198242)\n",
      "(in, ',', -4.585874080657959)\n",
      "(human, ',', -8.437538146972656)\n",
      "(pose, ',', -11.87548542022705)\n",
      "(estimation, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(In, ',', -7.314908981323242)\n",
      "(ECCV, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(2008, ',', -9.696126937866211)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "([, ',', -5.266753196716309)\n",
      "(56, ',', -19.579313278198242)\n",
      "(], ',', -5.498423099517822)\n",
      "(Y., ',', -19.579313278198242)\n",
      "(Wang, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(D., ',', -19.579313278198242)\n",
      "(Tran, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(and, ',', -4.195279121398926)\n",
      "(Z., ',', -19.579313278198242)\n",
      "(Liao, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(Learning, ',', -19.579313278198242)\n",
      "(hierarchical, ',', -19.579313278198242)\n",
      "(pose-, ',', -19.579313278198242)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(lets, ',', -10.029866218566895)\n",
      "(for, ',', -4.91396951675415)\n",
      "(human, ',', -8.437538146972656)\n",
      "(parsing, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(In, ',', -7.314908981323242)\n",
      "(CVPR, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(2011, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "([, ',', -5.266753196716309)\n",
      "(57, ',', -19.579313278198242)\n",
      "(], ',', -5.498423099517822)\n",
      "(Y., ',', -19.579313278198242)\n",
      "(Yang, ',', -19.579313278198242)\n",
      "(and, ',', -4.195279121398926)\n",
      "(D., ',', -19.579313278198242)\n",
      "(Ramanan, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(Articulated, ',', -19.579313278198242)\n",
      "(pose, ',', -11.87548542022705)\n",
      "(estimation, ',', -19.579313278198242)\n",
      "(with, ',', -5.363764762878418)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(ﬂexible, ',', -19.579313278198242)\n",
      "(mixtures, ',', -19.579313278198242)\n",
      "(-, ',', -5.202415943145752)\n",
      "(of, ',', -4.1284637451171875)\n",
      "(-, ',', -5.202415943145752)\n",
      "(parts, ',', -9.519133567810059)\n",
      "(., ',', -3.0729479789733887)\n",
      "(In, ',', -7.314908981323242)\n",
      "(CVPR, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(2011, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "([, ',', -5.266753196716309)\n",
      "(58, ',', -19.579313278198242)\n",
      "(], ',', -5.498423099517822)\n",
      "(Y., ',', -19.579313278198242)\n",
      "(Yang, ',', -19.579313278198242)\n",
      "(and, ',', -4.195279121398926)\n",
      "(D., ',', -19.579313278198242)\n",
      "(Ramanan, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(Articulated, ',', -19.579313278198242)\n",
      "(human, ',', -8.437538146972656)\n",
      "(detection, ',', -19.579313278198242)\n",
      "(with, ',', -5.363764762878418)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(ﬂexible, ',', -19.579313278198242)\n",
      "(mixtures, ',', -19.579313278198242)\n",
      "(-, ',', -5.202415943145752)\n",
      "(of, ',', -4.1284637451171875)\n",
      "(-, ',', -5.202415943145752)\n",
      "(parts, ',', -9.519133567810059)\n",
      "(., ',', -3.0729479789733887)\n",
      "(IEEE, ',', -19.579313278198242)\n",
      "(Trans, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(PAMI, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(To, ',', -8.49110221862793)\n",
      "(appear, ',', -9.914788246154785)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "([, ',', -5.266753196716309)\n",
      "(59, ',', -19.579313278198242)\n",
      "(], ',', -5.498423099517822)\n",
      "(M., ',', -19.579313278198242)\n",
      "(D., ',', -19.579313278198242)\n",
      "(Zeiler, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(G., ',', -19.579313278198242)\n",
      "(W., ',', -10.755142211914062)\n",
      "(Taylor, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(and, ',', -4.195279121398926)\n",
      "(R., ',', -19.579313278198242)\n",
      "(Fergus, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(Adaptive, ',', -19.579313278198242)\n",
      "(decon-, ',', -19.579313278198242)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(volutional, ',', -19.579313278198242)\n",
      "(networks, ',', -11.04748821258545)\n",
      "(for, ',', -4.91396951675415)\n",
      "(mid, ',', -10.965018272399902)\n",
      "(and, ',', -4.195279121398926)\n",
      "(high, ',', -8.075313568115234)\n",
      "(level, ',', -8.576519012451172)\n",
      "(feature, ',', -9.824798583984375)\n",
      "(learning, ',', -9.830510139465332)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      ", ',', -6.1798787117004395)\n",
      "(In, ',', -7.314908981323242)\n",
      "(ICCV, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(2011, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "([, ',', -5.266753196716309)\n",
      "(60, ',', -10.115797996520996)\n",
      "(], ',', -5.498423099517822)\n",
      "(X., ',', -11.786141395568848)\n",
      "(Zeng, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(W., ',', -10.755142211914062)\n",
      "(Ouyang, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(and, ',', -4.195279121398926)\n",
      "(X., ',', -11.786141395568848)\n",
      "(Wang, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(Multi, ',', -19.579313278198242)\n",
      "(-, ',', -5.202415943145752)\n",
      "(stage, ',', -10.263298034667969)\n",
      "(contextual, ',', -19.579313278198242)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(deep, ',', -9.902402877807617)\n",
      "(learning, ',', -9.830510139465332)\n",
      "(for, ',', -4.91396951675415)\n",
      "(pedestrian, ',', -19.579313278198242)\n",
      "(detection, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(In, ',', -7.314908981323242)\n",
      "(ICCV, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(2013, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "([, ',', -5.266753196716309)\n",
      "(61, ',', -19.579313278198242)\n",
      "(], ',', -5.498423099517822)\n",
      "(Z., ',', -19.579313278198242)\n",
      "(Zhu, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(P., ',', -19.579313278198242)\n",
      "(Luo, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(X., ',', -11.786141395568848)\n",
      "(Wang, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(and, ',', -4.195279121398926)\n",
      "(X., ',', -11.786141395568848)\n",
      "(Tang, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(Deep, ',', -19.579313278198242)\n",
      "(learning, ',', -9.830510139465332)\n",
      "(identity, ',', -10.966999053955078)\n",
      "(\n",
      "\n",
      ", ',', -4.458347320556641)\n",
      "(preserving, ',', -19.579313278198242)\n",
      "(face, ',', -8.649394035339355)\n",
      "(space, ',', -9.31265640258789)\n",
      "(., ',', -3.0729479789733887)\n",
      "(In, ',', -7.314908981323242)\n",
      "(ICCV, ',', -19.579313278198242)\n",
      "(,, ',', -3.3914804458618164)\n",
      "(2013, ',', -19.579313278198242)\n",
      "(., ',', -3.0729479789733887)\n",
      "(\n",
      "\n",
      "\f",
      ", ',', -19.579313278198242)\n"
     ]
    }
   ],
   "source": [
    "# def tokens_to_root(token):\n",
    "#     \"\"\"\n",
    "#     Walk up the syntactic tree, collecting tokens to the root of the given `token`.\n",
    "#     :param token: Spacy token\n",
    "#     :return: list of Spacy tokens\n",
    "#     \"\"\"\n",
    "#     tokens_to_r = []\n",
    "#     while token.head is not token:\n",
    "#         tokens_to_r.append(token)\n",
    "#         token = token.head\n",
    "#         tokens_to_r.append(token)\n",
    "\n",
    "#     return tokens_to_r\n",
    "\n",
    "# # For every token in document, print it's tokens to the root\n",
    "# for token in doc:\n",
    "#     print('{} --> {}'.format(token, tokens_to_root(token)))\n",
    "\n",
    "# # Print dependency labels of the tokens\n",
    "# for token in doc:\n",
    "#     print('-> '.join(['{}-{}'.format(dependent_token, dependent_token.dep_) for dependent_token in tokens_to_root(token)]))\n",
    "\n",
    "# doc = nlp.make_doc(text[0])\n",
    "\n",
    "# tokens_to_root(doc[5])\n",
    "\n",
    "doc = par\n",
    "for token in doc:\n",
    "    print(token, ',', token.prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "parser = English()\n",
    "doc = nlp(text[0])\n",
    "sents = list(doc.sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###\n",
      "Multi-source Deep Learning for Human Pose Estimation\n",
      "\n",
      "Wanli Ouyang Xiao Chu Xiaogang Wang\n",
      "\n",
      "Department of Electronic Engineering, The Chinese University of Hong Kong\n",
      "\n",
      "wlouyang@ee.cuhk.edu.hk, xgwang@ee.cuhk.edu.hk\n",
      "\n",
      "Abstract\n",
      "\n",
      "Visual appearance score, appearance mixture type and\n",
      "deformation are three important information sources for\n",
      "human pose estimation.\n",
      "###\n",
      "This paper proposes to build a\n",
      "multi-source deep model in order to extract non-linear\n",
      "representation from these different aspects of information\n",
      "sources.\n",
      "###\n",
      "With the deep model, the global, high-order hu-\n",
      "man body articulation patterns in these information sources\n",
      "are extracted for pose estimation.\n",
      "###\n",
      "The task for estimat-\n",
      "ing body locations and the task for human detection are\n",
      "jointly learned using a uniﬁed deep model.\n",
      "###\n",
      "The proposed\n",
      "approach can be viewed as a post-processing of pose esti-\n",
      "mation results and can ﬂexibly integrate with existing meth-\n",
      "ods by taking their information sources as input.\n",
      "###\n",
      "By extract-\n",
      "ing the non-linear representation from multiple information\n",
      "sources, the deep model outperforms state-of-the-art by up\n",
      "to 8.6 percent on three public benchmark datasets.\n",
      "\n",
      "1.\n",
      "###\n",
      "Introduction\n",
      "\n",
      "Human pose estimation is the process of determining,\n",
      "from an image, the positions of human body parts such as\n",
      "the head, shoulder, elbow, wrist, hip, knee, and ankle.\n",
      "###\n",
      "It\n",
      "is a fundamental problem in computer vision and has abun-\n",
      "dant important applications such as sports, action recogni-\n",
      "tion, character animation, clinical analysis of gait patholo-\n",
      "gies, content-based video and image retrieval, and intelli-\n",
      "gent video surveillance.\n",
      "###\n",
      "Despite many years of research\n",
      "[52, 54, 2, 40, 6, 57, 56], pose estimation remains a difﬁ-\n",
      "cult problem.\n",
      "###\n",
      "One of the most signiﬁcant challenges in pose\n",
      "estimation is how to model the complex human articulation.\n",
      "\n",
      "###\n",
      "Many approaches have been used to handle the com-\n",
      "plex human articulation by using three information sources:\n",
      "mixture type, appearance score and deformation [57, 52, 54,\n",
      "11, 58].\n",
      "###\n",
      "Inﬂuenced by human body articulation, clothing,\n",
      "occlusion etc., body part appearance varies.\n",
      "###\n",
      "To handle this\n",
      "variation, the appearance of a part is clustered into multiple\n",
      "mixture types as shown in Fig. 1 .\n",
      "###\n",
      "For each mixture type of\n",
      "a part, a part template is learned to capture its appearance.\n",
      "\n",
      "###\n",
      "Then the appearance scores (log-likelihoods) of body parts\n",
      "\n",
      "Figure 1.\n",
      "###\n",
      "The motivation of this paper in using multi-source deep\n",
      "model for constructing the non-linear representation from three in-\n",
      "formation sources: mixture type, appearance score and deforma-\n",
      "tion.\n",
      "###\n",
      "Best viewed in color.\n",
      "\n",
      "###\n",
      "being at different locations are obtained by convolving the\n",
      "part templates with the visual features of the input image,\n",
      "e.g. HOG [7].\n",
      "###\n",
      "The appearance scores are inaccurate for\n",
      "well-locating body parts because the part template is imper-\n",
      "fect.\n",
      "###\n",
      "Therefore, the deformations (relative locations) among\n",
      "body parts are used as for encoding likely pairwise poses;\n",
      "for example, the head should not be far from the neck.\n",
      "\n",
      "\n",
      "###\n",
      "Existing approaches use log-linear models with pairwise\n",
      "potentials of these three information sources [52, 54, 40, 57,\n",
      "56] to determine whether an estimated location is correct.\n",
      "\n",
      "###\n",
      "However, these information sources are not log-linearly cor-\n",
      "related when choosing the correct candidate.\n",
      "###\n",
      "For the exam-\n",
      "ple in Fig. 1, linear models may ﬁnd that the estimated\n",
      "result on the left and the result on the right have the same\n",
      "deformation score because they simply linearly add local\n",
      "deformation cost.\n",
      "###\n",
      "While it is obvious for human to ﬁnd that\n",
      "the result on the left is not reasonable.\n",
      "###\n",
      "Similar situations\n",
      "also occur for mixture type and appearance score.\n",
      "###\n",
      "There-\n",
      "fore, it is desirable to construct the non-linear representation\n",
      "that identiﬁes reasonable conﬁgurations of deformation, ap-\n",
      "pearance score and mixture type.\n",
      "\n",
      "\n",
      "###\n",
      "In order to construct useful representation from multi-\n",
      "ple information sources for pose estimation, a model should\n",
      "\n",
      "1\n",
      "\n",
      "Multi-sourcedeep modelEstimated resultType 1Type 2Non-Linearmodel?YesNoEstimated resultMixture typeHead topNeckDeformationAppearance scoreTemplateLinear model1389\f",
      "satisfy certain properties.\n",
      "###\n",
      "First, the model should capture\n",
      "the global, complex relationships among body parts.\n",
      "###\n",
      "For\n",
      "the example in Fig. 1, the result on the left is unreasonable\n",
      "because of its global conﬁguration in arm, torso, and leg.\n",
      "\n",
      "###\n",
      "Second, since reasonable conﬁguration is a very abstract\n",
      "concept while the information sources are less abstract con-\n",
      "cepts, the model should construct more abstract representa-\n",
      "tion from the less abstract representation.\n",
      "###\n",
      "Third, since dif-\n",
      "ferent information sources describe different aspects of hu-\n",
      "man pose and have different statistical properties, the model\n",
      "should learn useful representation from these sources and\n",
      "fuse them into a joint representation for pose estimation.\n",
      "\n",
      "###\n",
      "The multi-source deep architecture we propose satisﬁes the\n",
      "above requirement.\n",
      "\n",
      "\n",
      "###\n",
      "There are three contributions of this paper.\n",
      "\n",
      "1.\n",
      "###\n",
      "We propose a deep architecture to construct the non-\n",
      "linear representation from different aspects of informa-\n",
      "tion sources.\n",
      "###\n",
      "To the best of our knowledge, this paper is\n",
      "the ﬁrst to use deep model for pose estimation.\n",
      "\n",
      "2.\n",
      "###\n",
      "The body articulation patterns (global and more abstract\n",
      "representations) are captured by the deep model from the\n",
      "information sources (local and less abstract representa-\n",
      "tions).\n",
      "###\n",
      "For each information source, more abstract repre-\n",
      "sentation at the higher layer is composed by the less ab-\n",
      "stract representation of all body parts in the lower layer.\n",
      "\n",
      "###\n",
      "Then representations of all information sources in the\n",
      "higher layer are fused for pose estimation.\n",
      "\n",
      "\n",
      "###\n",
      "3.\n",
      "###\n",
      "Both the task for detecting human and the task for esti-\n",
      "mating body locations are jointly learned using a single\n",
      "deep model.\n",
      "###\n",
      "Joint learning of these tasks with a shared\n",
      "representation improves pose estimation accuracy.\n",
      "\n",
      "2.\n",
      "###\n",
      "Related work\n",
      "\n",
      "Human pose estimation.\n",
      "###\n",
      "Pose estimation is considered\n",
      "as holistic recognition in [15, 33, 34].\n",
      "###\n",
      "On the other hand,\n",
      "many recent works use local body parts [52, 54, 11, 58, 9,\n",
      "13, 48, 40, 2, 42, 21, 46, 55, 41, 1] in order to handle the\n",
      "many degrees of freedom in body part articulation.\n",
      "###\n",
      "Since\n",
      "the ﬁrst work in [57], some approaches [52, 54, 11, 58, 9]\n",
      "have clustered part appearance into mixture types as shown\n",
      "in Fig. 1.\n",
      "###\n",
      "There are also approaches that warp the part\n",
      "template by ﬂexible sizes and orientations [13, 48, 40, 2,\n",
      "42, 21, 46, 55].\n",
      "###\n",
      "The appearance score, rotation, size, and\n",
      "location used in these approaches can be treated as multiple\n",
      "information sources and used by our deep model for pose\n",
      "estimation.\n",
      "\n",
      "\n",
      "###\n",
      "In existing pose estimation approaches, the pair-wise\n",
      "part deformation relationships are arranged in tree models\n",
      "[52, 54, 2, 40, 57], multi-tree model [55], or loopy mod-\n",
      "els [56, 53, 10].\n",
      "###\n",
      "Tree models allow for efﬁcient and exact\n",
      "inference but are insufﬁcient in modeling the complex re-\n",
      "lationships among body parts.\n",
      "###\n",
      "Hence, tree models often\n",
      "suffer from double counting; for example, given the posi-\n",
      "\n",
      "tion of a torso, the positions of two legs are independent\n",
      "and often respond to the same visual cue.\n",
      "###\n",
      "Loopy models\n",
      "allow more complex relationships among parts, but require\n",
      "approximate inference.\n",
      "###\n",
      "Our deep architecture models the\n",
      "complex relationships among parts and is computationally\n",
      "efﬁcient in both training and testing.\n",
      "\n",
      "\n",
      "###\n",
      "Deep learning.\n",
      "###\n",
      "Since the breakthrough in deep learning\n",
      "initiated by G. Hinton in [18, 19], deep learning is gain-\n",
      "ing more and more attention.\n",
      "###\n",
      "Bengio [3] proved that exist-\n",
      "ing commonly used machine learning tools such as SVM\n",
      "and Boosting are shallow models, and they may require\n",
      "many more computational elements, potentially exponen-\n",
      "tially more (with respect to input size), than deep models\n",
      "whose depth is matched to the task.\n",
      "###\n",
      "Deep architecture is\n",
      "found to yield better data representation, for example, in\n",
      "terms of classiﬁcation error [25], invariance to input trans-\n",
      "formations [16], or modeling multi-modal data [35].\n",
      "###\n",
      "Deep\n",
      "learning has achieved spectacular progress in computer vi-\n",
      "sion [45, 20, 26, 36, 23, 59, 12, 43, 39, 50, 49, 60, 38,\n",
      "37, 30, 32, 31, 29, 61, 28, 51].\n",
      "###\n",
      "Recent progress on deep\n",
      "learning is reviewed in [4].\n",
      "###\n",
      "Krizhevsky et al.\n",
      "[23] pro-\n",
      "posed a large-scale deep convolutional network [27] with\n",
      "breakthrough on the large-scale ImageNet object recogni-\n",
      "tion dataset [8], attaining a signiﬁcant gap compared with\n",
      "existing approaches that use shallow models, and bringing\n",
      "high impact to research in computer vision.\n",
      "###\n",
      "Our approaches\n",
      "in [38, 39, 37, 29] learns feature learning, translational de-\n",
      "formation, and occlusion relationship in pedestrian detec-\n",
      "tion; the approach in [50] learns relational ﬁlter pairs in face\n",
      "veriﬁcation.\n",
      "###\n",
      "To the best of our knowledge, however, deep\n",
      "model for human pose estimation has not yet been explored.\n",
      "\n",
      "###\n",
      "Our work is inspired by multi-modality models that learn\n",
      "from multiple modalities such as audio, visual, text data [35,\n",
      "47, 17].\n",
      "###\n",
      "In contrast to these works, we investigate multi-\n",
      "source learning from single modality, which is image data\n",
      "in pose estimation.\n",
      "\n",
      "3. Pictorial structure model for pose estimation\n",
      "\n",
      "###\n",
      "The model introduced in this section is used to provide\n",
      "our deep model with information sources.\n",
      "###\n",
      "Pictorial struc-\n",
      "ture model considers human body parts as nodes tied to-\n",
      "gether in a conditional random ﬁeld.\n",
      "###\n",
      "Let lp for p = 1, . .\n",
      "###\n",
      ". , P\n",
      "be the conﬁguration of the pth part.\n",
      "###\n",
      "The posterior of a con-\n",
      "ﬁguration of parts L = {lp|p = 1 . . .\n",
      "###\n",
      "P} given an image I\n",
      "is:\n",
      "\n",
      "P (L|I) ∝ exp(cid:0) P(cid:88)\n",
      "\n",
      "(cid:88)\n",
      "\n",
      "ψ(lp, lq)(cid:1).\n",
      "\n",
      "\n",
      "###\n",
      "(1)\n",
      "\n",
      "φ(I|lp)) +\n",
      "\n",
      "p=1\n",
      "\n",
      "(p,q)∈E\n",
      "\n",
      "ψ(lp, lq) is the pair-wise term that models the geometric\n",
      "deformation constraint on the pth and qth parts; for exam-\n",
      "ple, head shall not be too far from torso.\n",
      "###\n",
      "The edge set de-\n",
      "noted by E is arranged in tree models [52, 54, 2, 40, 57, 11]\n",
      "\n",
      "\f",
      "or loopy models [56, 10, 53].\n",
      "\n",
      "\n",
      "###\n",
      "φ(I|lp) is the unary term that models the appearance of\n",
      "lp.\n",
      "###\n",
      "The appearance varies as body articulates.\n",
      "###\n",
      "To model\n",
      "this variation, lp = {sp, θp, zp} and φ(I|lp) speciﬁes the\n",
      "part appearance warped by size sp, orientation θp at loca-\n",
      "tion zp in [2, 40, 13].\n",
      "###\n",
      "Alternatively, Yang and Ramanan\n",
      "propose to use appearance mixture type tp for approximat-\n",
      "ing the variation in rotation θp and size sp in [57].\n",
      "###\n",
      "In this\n",
      "model, lp = {tp, zp} and φ(I|lp) speciﬁes the part appear-\n",
      "ance with mixture type tp at location zp.\n",
      "###\n",
      "The appearance of\n",
      "a part is clustered into multiple appearance mixture types as\n",
      "shown in Fig. 1.\n",
      "###\n",
      "The overall model in [57, 58] is as follows:\n",
      "\n",
      "P (L|I) ∝ exp(cid:0)S(I, t, z)(cid:1),\n",
      "(cid:88)\n",
      "(cid:88)\n",
      "\n",
      "(cid:88)\n",
      "\n",
      "p,q\n",
      "\n",
      "Sc(t) =\n",
      "\n",
      "btp\n",
      "p +\n",
      "\n",
      "btp,tq\n",
      "p,q\n",
      "\n",
      "where S(I, t, z) = Sc(t) +\n",
      "\n",
      "Sd(t, z, p, q) +\n",
      "\n",
      "(cid:88)\n",
      "\n",
      "(2)\n",
      "\n",
      "Sa(I, tp, zp),\n",
      "\n",
      "(3)\n",
      "\n",
      "(4)\n",
      "\n",
      "p\n",
      "\n",
      ",\n",
      "\n",
      "p\n",
      "\n",
      "p,q\n",
      "\n",
      "T\n",
      "\n",
      "d(zp − zq),\n",
      "\n",
      "Sd(t, z, p, q) = wtp,tq\n",
      "p,q\n",
      "T\n",
      "Sa(I, tp, zp) = wtp\n",
      "\n",
      "p\n",
      "\n",
      "f (I, zp).\n",
      "\n",
      "compatibility/co-occurrence of mixture types.\n",
      "\n",
      "\n",
      "###\n",
      "(5)\n",
      "•\n",
      "###\n",
      "Sc(t) is the pair-wise compatibility term that models the\n",
      "• Sd(t, z, p, q) is the pair-wise deformation term that mod-\n",
      "els the geometric deformation constraints on the pth and\n",
      "qth parts.\n",
      "###\n",
      "d(zp\n",
      "###\n",
      "− zq) = [dx, dy, dx2dy2]\n",
      "T.\n",
      "• Sa(I, tp, zp) is the unary appearance term that computes\n",
      "p at location zp of the\n",
      "\n",
      "the score of placing a template wtp\n",
      "HOG feature map for image I, denoted by f (I, zp).\n",
      "\n",
      "\n",
      "###\n",
      "p , wtp,tq\n",
      "Linear SVM is used to learn the linear weights wtp\n",
      "p,q\n",
      "and compatibility biases btp\n",
      "p,q .\n",
      "###\n",
      "The model in Eq.(2)-(5)\n",
      "is used in many approaches, with different implementations\n",
      "on edge set, part size, and part locations [52, 54, 57, 10, 11,\n",
      "58].\n",
      "\n",
      "\n",
      "###\n",
      "p , btp,tq\n",
      "\n",
      "4.\n",
      "###\n",
      "The multi-source deep model\n",
      "\n",
      "An overview of our framework in the testing stage is\n",
      "shown in Fig.\n",
      "###\n",
      "2.\n",
      "###\n",
      "In this framework, an existing approach\n",
      "is used to generate candidate body locations with conserva-\n",
      "tive thresholding.\n",
      "###\n",
      "In the experiment, the existing approach\n",
      "is the off-the-shelf approach in [58].\n",
      "###\n",
      "A multi-source deep\n",
      "model is then applied to a candidate of all body locations\n",
      "in order to determine whether its body locations are correct.\n",
      "\n",
      "###\n",
      "Simultaneously, the body locations of this candidate is esti-\n",
      "mated.\n",
      "\n",
      "\n",
      "###\n",
      "One direct approach with which to train a multi-source\n",
      "model is to train a deep model over the concatenated infor-\n",
      "mation sources as shown in Fig.\n",
      "###\n",
      "3(a).\n",
      "###\n",
      "This approach is lim-\n",
      "ited because information sources with different statistical\n",
      "properties are mixed in the ﬁrst hidden layer.\n",
      "###\n",
      "A better so-\n",
      "lution is to have their high-level representations constructed\n",
      "before they are mixed.\n",
      "###\n",
      "Therefore, we use the architecture\n",
      "as shown in Fig.\n",
      "###\n",
      "3(b), in which each information source\n",
      "\n",
      "Figure 2.\n",
      "###\n",
      "Framework in the testing stage.\n",
      "###\n",
      "The existing approach is\n",
      "used to generate multiple candidate locations.\n",
      "###\n",
      "A candidate is used\n",
      "as the input to a deep model to determine whether the candidate is\n",
      "correct and estimate body locations.\n",
      "###\n",
      "Best viewed in color.\n",
      "\n",
      "\n",
      "###\n",
      "Figure 3.\n",
      "###\n",
      "Direct use of deep model (a) and the deep architecture\n",
      "we propose (b) for part score s, deformation d and mixture type t.\n",
      "Best viewed in color.\n",
      "\n",
      "\n",
      "###\n",
      "is connected to two layers for constructing high level rep-\n",
      "resentation individually.\n",
      "###\n",
      "High-level representations of dif-\n",
      "ferent information sources are then fused using other two\n",
      "layers for pose estimation.\n",
      "\n",
      "###\n",
      "4.1.\n",
      "###\n",
      "Inference\n",
      "\n",
      "The mixture type information t in Fig.\n",
      "###\n",
      "3 is taken from\n",
      "the t in (3).\n",
      "###\n",
      "The relative positions among parts, denoted\n",
      "by d, comes from the deformation information d(zp − zq)\n",
      "in (4).\n",
      "###\n",
      "The appearance scores, denoted by s, is obtained\n",
      "from the unary appearance term in (5).\n",
      "###\n",
      "In our experiment,\n",
      "s, t, and d are obtained using the approach in [58].\n",
      "###\n",
      "At the\n",
      "inference stage, the model is as follows:\n",
      "\n",
      "h1,1 = a(sTW1,1 + b1,1),\n",
      "h1,2 = a(dTW1,2 + b1,2),\n",
      "h1,3 = a(tTW1,3 + b1,3),\n",
      "h2,u = a(h1,uT\n",
      "h2 = [h2,1T\n",
      "h3 = a(h2T\n",
      "˜ycls = σ(h3T\n",
      "˜ypst = h3T\n",
      "\n",
      "Wpst + bpst.\n",
      "\n",
      "\n",
      "###\n",
      "wcls + bcls),\n",
      "\n",
      "W2,u + b2,u), u = 1, 2, 3,\n",
      "h2,2T\n",
      "h2,3T\n",
      "W3 + b2),\n",
      "\n",
      "]T,\n",
      "\n",
      "(6)\n",
      "(7)\n",
      "(8)\n",
      "\n",
      "(9)\n",
      "\n",
      "(10)\n",
      "\n",
      "(11)\n",
      "\n",
      "(12)\n",
      "\n",
      "(13)\n",
      "\n",
      "•\n",
      "###\n",
      "σ(x) = (1 + exp(−x))−1 is the sigmoid function.\n",
      "•\n",
      "###\n",
      "a(∗) is the point-wise non-linear activation function, for\n",
      "• W∗ and wcls connect nodes between adjacent layers.\n",
      "•\n",
      "###\n",
      "b∗ and bcls are biases.\n",
      "\n",
      "which sigmoid function can be used.\n",
      "\n",
      "\n",
      "###\n",
      "Our deep modelExisting approachInputCandidatesResultsExisting approachDeep model.........(d)...h3h1,3......sh2,3........................dt......\n",
      "###\n",
      "ypstW1, 1W1, 2W1, 3W2, 1W2, 2W2, 3W3Wpstyclswclsh3h1...\n",
      "###\n",
      "sh2............dt......ypstycls..................(a)(b)h1,1h1,2h2,2h2,1\f",
      "ing non-linear representations from s, d, and t.\n",
      "\n",
      "• h∗ are hidden nodes in different layers used for extract-\n",
      "•\n",
      "###\n",
      "˜ycls is the estimated label indicating whether the candi-\n",
      "date of body locations is correct.\n",
      "###\n",
      "For pose estimatio of\n",
      "single human, the candidate with the largest ˜ycls is used\n",
      "as the ﬁnal output in our experiments.\n",
      "\n",
      "•\n",
      "###\n",
      "˜ypst contains the estimated part locations.\n",
      "\n",
      "###\n",
      "Through the ﬁrst two separate layers in Eq.\n",
      "###\n",
      "(6)-(9), each\n",
      "information source has its individual representation con-\n",
      "structed.\n",
      "###\n",
      "Then all high-order representations are combined\n",
      "by two layers in Eq.\n",
      "###\n",
      "(11)-(13).\n",
      "4.2.\n",
      "###\n",
      "Training method\n",
      "\n",
      "Denote the parameter set for the model in Eq.\n",
      "###\n",
      "(6)-(13) by\n",
      "λ, λ = {W∗, wcls, b∗, bcls}. The objective function J(λ)\n",
      "for backpropagating error derivatives is as follows:\n",
      "\n",
      "(cid:88)\n",
      "\n",
      "(cid:16)\n",
      "\n",
      "J(λ) =\n",
      "\n",
      "J1(ycls\n",
      "\n",
      "n , ˜ycls\n",
      "+ J3(W∗, wcls),\n",
      "\n",
      "n\n",
      "\n",
      "n ) + ycls\n",
      "\n",
      "n J2(ypst\n",
      "\n",
      "n , ˜ypst\n",
      "n )\n",
      "\n",
      "(14)\n",
      "n ) log(1 − ˜ycls\n",
      "n ),\n",
      "\n",
      "n , ˜ycls\n",
      "n , ˜ypst\n",
      "\n",
      "J1(ycls\n",
      "J2(ypst\n",
      "J3(W∗, wcls) =\n",
      "\n",
      "n ) = − ycls\n",
      "n ) − (1 − ycls\n",
      "n log(˜ycls\n",
      "(cid:88)\n",
      "(cid:88)\n",
      "n −\n",
      "###\n",
      "˜ypst\n",
      "n ) =||ypst\n",
      "n ||2,\n",
      "|w∗\n",
      "i,j| +\n",
      "\n",
      "|wcls\n",
      "\n",
      "|,\n",
      "\n",
      "i\n",
      "\n",
      "(cid:17)\n",
      "\n",
      "(15)\n",
      "\n",
      "i,j\n",
      "\n",
      "i\n",
      "\n",
      "where ∗n denotes the nth sample, n = 1, 2, . .\n",
      "###\n",
      ".\n",
      "###\n",
      "N.\n",
      "•\n",
      "###\n",
      "˜ycls\n",
      "n are computed using Eq.\n",
      "###\n",
      "(6)-(13).\n",
      "\n",
      "###\n",
      "• ycls\n",
      "\n",
      "\n",
      "###\n",
      "n and ˜ypst\n",
      "n ∈ {0, 1} is the ground truth classiﬁcation label in-\n",
      "dicating whether the current body location estimation is\n",
      "correct or not.\n",
      "###\n",
      "Positive training samples have their part\n",
      "templates placed around annotated body locations.\n",
      "###\n",
      "As in\n",
      "[58], negative training samples have their part templates\n",
      "placed on images without human.\n",
      "###\n",
      "Therefore, ycls can be\n",
      "used for human detection by considering it as an indi-\n",
      "cator on whether the rectangle covering body locations\n",
      "contains a human.\n",
      "\n",
      "•\n",
      "###\n",
      "J1(ycls\n",
      "• ypst\n",
      "•\n",
      "###\n",
      "J2(ypst\n",
      "\n",
      "n , ˜ycls\n",
      "\n",
      "n ) is the cross-entropy error on classiﬁcation.\n",
      "\n",
      "\n",
      "###\n",
      "n contains the ground truth body locations.\n",
      "\n",
      "\n",
      "###\n",
      "n , ˜ypst\n",
      "\n",
      "n ) is the sum of square error on body loca-\n",
      "tion estimation.\n",
      "###\n",
      "Since negative background samples do\n",
      "not have ground truth body location, the ycls\n",
      "n is multi-\n",
      "plied by J2 in (14) to ensure that only positive samples\n",
      "are used to learn location estimation.\n",
      "•\n",
      "###\n",
      "J3(W∗, wcls) is the L1 norm regularization term.\n",
      "###\n",
      "w∗\n",
      "i,j\n",
      "is the (i, j)th element in W∗ and wcls\n",
      "is the ith and\n",
      "element in wcls.\n",
      "###\n",
      "The information sources and hidden\n",
      "nodes may have different purpose.\n",
      "###\n",
      "For example, a node\n",
      "in h3 may use the information source mixture type.\n",
      "\n",
      "###\n",
      "Hence, J3(W∗, wcls) is used to encourage sparsity in\n",
      "the weights.\n",
      "\n",
      "\n",
      "###\n",
      "i\n",
      "\n",
      "Body part location estimation and human detection are both\n",
      "learned through shared representation in this model.\n",
      "###\n",
      "They\n",
      "are jointly learned because they are dependent tasks.\n",
      "\n",
      "4.3.\n",
      "###\n",
      "Analysis\n",
      "\n",
      "The mixture type t is used as an example for analysis.\n",
      "###\n",
      "In\n",
      "the layer-wise pre-training stage [18], t and hidden vector\n",
      "h1,3 are considered as a restricted Boltzmann machine with\n",
      "the following distribution:\n",
      "p(t, h1,3) ∝ exp(tTW1,3h1,3 + b1,3T\n",
      "\n",
      "h1,3 + cTt).\n",
      "\n",
      "\n",
      "###\n",
      "(16)\n",
      "\n",
      "Denote the jth column of W1,3 by w1,3∗,j . Denote the jth\n",
      "element of b1,3 by bj.\n",
      "###\n",
      "The marginal distribution p(t) can\n",
      "be obtained as follows:\n",
      "\n",
      "p(t) =\n",
      "\n",
      "p(t, h1,3)\n",
      "\n",
      "(cid:88)\n",
      "∝(cid:88)\n",
      "\n",
      "h1,3\n",
      "\n",
      "h1,3\n",
      "\n",
      "∝ exp(cTt)\n",
      "\n",
      "(cid:89)\n",
      "\n",
      "(cid:16)\n",
      "\n",
      "j\n",
      "\n",
      "exp(tTW1,3h1,3 + b1,3T\n",
      "\n",
      "h1,3 + cTt)\n",
      "\n",
      "1 + exp(tTw1,3∗,j + bj)\n",
      "\n",
      "=\n",
      "\n",
      "(cid:17)\n",
      "\n",
      "(17)\n",
      "\n",
      "φj(t),\n",
      "\n",
      "(cid:89)\n",
      "\n",
      "j\n",
      "\n",
      "where φj(t) = 1 + exp(tTw1,3∗,j + bj) and φj(t) is a fully\n",
      "connected graphical model because it cannot be factorized.\n",
      "\n",
      "###\n",
      "φj(t) can be considered as a factor that explains t in fac-\n",
      "tor graph [5, 24].\n",
      "\n",
      "###\n",
      "In pose estimation, φj(t) can be con-\n",
      "sidered as a global pattern explaining the mixture type t\n",
      "for all parts.\n",
      "###\n",
      "In both training and inference stages, every\n",
      "node in h1,3 is connected to the mixture types of all parts.\n",
      "\n",
      "###\n",
      "Therefore, h1,3 nonlinearly extracts the global representa-\n",
      "tion from t1,3.\n",
      "###\n",
      "Similarly, the h2,3 extracts higher-level rep-\n",
      "resentation from h1,3.\n",
      "###\n",
      "Therefore, the stack of hidden layers\n",
      "extracts global, high-level representation from the informa-\n",
      "tion source t. The analysis to mixture type t is applica-\n",
      "ble to deformation and appearance score.\n",
      "###\n",
      "As shown in Fig.\n",
      "4, h2,3 captures the global articulation patterns of human\n",
      "body.\n",
      "###\n",
      "One of the nodes in h2,3 has high response to people\n",
      "squat.\n",
      "###\n",
      "Another node has high response to people standing\n",
      "upright.\n",
      "###\n",
      "Yet another node concisely captures two clusters of\n",
      "pose patterns.\n",
      "\n",
      "\n",
      "###\n",
      "In our deep model, the ﬁrst hidden layer has 200 hidden\n",
      "nodes, the second layer, i.e. h2 in Eq.\n",
      "###\n",
      "(10) has 150 hidden\n",
      "nodes and the third layer, i.e. h3 in Eq. (\n",
      "###\n",
      "11), has 100 hidden\n",
      "nodes.\n",
      "###\n",
      "Since the dimensions of s, d, and t are small, train-\n",
      "ing of the deep model is fast.\n",
      "###\n",
      "Unlike loopy graphical mod-\n",
      "els, the deep model is fast in the inference stage because it\n",
      "does not require loopy belief propagation or sampling.\n",
      "###\n",
      "The\n",
      "extra testing time required by our deep model is less than 10\n",
      "percent of the testing time required by the approach in [58].\n",
      "\n",
      "\n",
      "###\n",
      "5.\n",
      "###\n",
      "Experimental results\n",
      "\n",
      "The proposed approach is evaluated on three datasets:\n",
      "LSP [21], PARSE [44] and UIUC people [53].\n",
      "###\n",
      "The train-\n",
      "ing procedure and training set are the same as [58].\n",
      "###\n",
      "Positive\n",
      "\n",
      "\f",
      "pose estimation for future applications.\n",
      "###\n",
      "For example, in\n",
      "character animation, the rendering of a limb is possible\n",
      "only when both end points of the limb are correct.\n",
      "\n",
      "###\n",
      "We follow [11, 40] and use the observer-centric anno-\n",
      "tations for all approaches when we evaluate on the LSP\n",
      "dataset.\n",
      "5.2.\n",
      "###\n",
      "Overall experimental results\n",
      "\n",
      "Table 1 shows the experimental results from the three\n",
      "\n",
      "datasets.\n",
      "\n",
      "\n",
      "###\n",
      "Pishchulin’s approach in [40] used the LSP+PARSE\n",
      "training set when evaluated on the PARSE dataset and used\n",
      "the UIUC+LSP training set when evaluated on the UIUC\n",
      "dataset.\n",
      "###\n",
      "To evaluate on the PARSE dataset, Pishchulin’s\n",
      "approach [40] + [42] included LSP+PARSE and 2744 ex-\n",
      "tra animated samples for training.\n",
      "###\n",
      "Johnson’s approach in\n",
      "[22] included 10,000 extra training samples when evaluated\n",
      "on the PARSE dataset.\n",
      "###\n",
      "In all experiments, Andriluka’s ap-\n",
      "proach in [2], Yang and Ramanan’s approach in [57, 58] and\n",
      "our approach are trained on the 1000 training images of the\n",
      "LSP dataset [21].\n",
      "\n",
      "\n",
      "###\n",
      "As shown in Table 1, our deep model obviously improves\n",
      "the pose estimation accuracy and outperforms all the state-\n",
      "of-the-art on these three datasets.\n",
      "###\n",
      "Speciﬁcally, our approach\n",
      "is better in detecting legs, arms and head compared with ex-\n",
      "isting approaches.\n",
      "###\n",
      "The approach of Pishchulin et al.\n",
      "###\n",
      "[42] is\n",
      "better than our approach in locating torso, possibly because\n",
      "the torso region is included in many poslets, which helps to\n",
      "increase the accuracy of their approach in locating torso.\n",
      "\n",
      "\n",
      "###\n",
      "Our approach is complementary to existing approaches\n",
      "because the information sources provided by these ap-\n",
      "proaches can be used by our model to improve their re-\n",
      "sults.\n",
      "###\n",
      "Currently, our model uses the approach in [58] to\n",
      "obtain information sources.\n",
      "###\n",
      "Compared with the approach in\n",
      "[58], our approach improves the pose estimation accuracy\n",
      "by 5.8% (62.8% vs. 68.6% PCP), 7.4% (63.6% vs. 71.0%\n",
      "PCP) and 8.6% (57.0% vs. 65.6% PCP) respectively on the\n",
      "LSP, PARSE and UIUC datasets.\n",
      "###\n",
      "Fig.\n",
      "###\n",
      "5 shows the compar-\n",
      "ison between our approach (left) and the approach in [58]\n",
      "(right).\n",
      "5.3.\n",
      "###\n",
      "Results on different designs of deep models\n",
      "\n",
      "\n",
      "###\n",
      "In this section, we evaluate different designs of deep\n",
      "models.\n",
      "###\n",
      "Yang and Ramanan’s approach in [58] is used as\n",
      "the baseline because this approach is used by our model for\n",
      "obtaining information sources.\n",
      "###\n",
      "To be concise, we only refer\n",
      "to the PCP results on the LSP dataset.\n",
      "\n",
      "\n",
      "###\n",
      "Depth of model is investigated in Table 2.\n",
      "###\n",
      "The approach\n",
      "in [58] uses linear-SVM for combining information sources.\n",
      "\n",
      "###\n",
      "We also trained a Kernel-SVM with RBF kernel for learn-\n",
      "ing a non-linear model using the off-the-shelf tool Libsvm.\n",
      "\n",
      "###\n",
      "The difference in PCP between Linear SVM and kernel-\n",
      "SVM is within 2% (62.8% vs. 64.2% on LSP).\n",
      "###\n",
      "Bengio [3]\n",
      "\n",
      "Figure 4.\n",
      "###\n",
      "Visualization of mixture-type patterns extracted by hid-\n",
      "den nodes in h2,3.\n",
      "###\n",
      "We use the approach in [26] and visualize train-\n",
      "ing samples with the largest responses on each hidden node.\n",
      "###\n",
      "Sam-\n",
      "ples with the highest responses are placed at the upper-left corner.\n",
      "Hidden node 1 has high response to people squat.\n",
      "###\n",
      "Node 2 has high\n",
      "response to standing people.\n",
      "###\n",
      "Node 3 has high response to two\n",
      "clusters of pose patterns.\n",
      "###\n",
      "Best viewed in color.\n",
      "\n",
      "\n",
      "###\n",
      "training samples are constrained to have estimated part lo-\n",
      "cations near the ground truth.\n",
      "###\n",
      "Part of the training data is\n",
      "used for validation.\n",
      "\n",
      "###\n",
      "5.1.\n",
      "###\n",
      "Evaluation criteria\n",
      "\n",
      "\n",
      "###\n",
      "In all experiments, we use the most popular criterion,\n",
      "which is the percentage of correctly localized parts (PCP)\n",
      "introduced in [14].\n",
      "###\n",
      "As stated in [42, 58], the PCP scoring\n",
      "metric has been implemented in different ways in different\n",
      "papers.\n",
      "###\n",
      "These differences have two dimensions.\n",
      "1.\n",
      "###\n",
      "There are two ways to compute the ﬁnal PCP score\n",
      "across the dataset.\n",
      "###\n",
      "In the single way, only a single candi-\n",
      "date (given by the maximum scoring candidate of an al-\n",
      "gorithm) for one image is used.\n",
      "###\n",
      "The match way matches\n",
      "multiple candidates without penalizing false positives.\n",
      "\n",
      "2.\n",
      "###\n",
      "There are two deﬁnitions of a correct part localization.\n",
      "\n",
      "###\n",
      "For the deﬁnition both, it requires both end points of a\n",
      "part (for example, end points wrist and elbow for the\n",
      "part lower arm) to be correct.\n",
      "###\n",
      "For the deﬁnition avg, it\n",
      "requires only the average of the endpoints to be correct.\n",
      "\n",
      "###\n",
      "The paper in [54, 57, 52] used ‘\n",
      "###\n",
      "match+avg’.\n",
      "###\n",
      "The paper in\n",
      "[40, 2, 42] used ‘\n",
      "###\n",
      "single+both’, which is the strictest case\n",
      "and generally has lower PCP value.\n",
      "###\n",
      "The paper in [10] pro-\n",
      "vides results for ‘match+both’ and ‘match+avg’. We follow\n",
      "[42, 40] and evaluate all approaches using the strictest ‘\n",
      "###\n",
      "sin-\n",
      "gle+both’ criterion.\n",
      "###\n",
      "This is used because of the following\n",
      "reasons:\n",
      "1.\n",
      "###\n",
      "For ‘single’ and ‘match’, as discussed in [58],\n",
      "\n",
      "the\n",
      "‘match’ way gives unfair advantage to approaches that\n",
      "produce a large number of candidates because mis-\n",
      "matched candidates (false positives) are not penalized.\n",
      "\n",
      "\n",
      "###\n",
      "2.\n",
      "###\n",
      "For ‘both’ and ‘avg’, ‘\n",
      "###\n",
      "both’ is better at describing the\n",
      "orientation of body parts and will facilitate the use of\n",
      "\n",
      "\f",
      "Table 1.\n",
      "###\n",
      "Pose estimation results (PCP) on LSP [21], UIUC people\n",
      "[53] and PARSE [44].\n",
      "\n",
      "\n",
      "###\n",
      "Method\n",
      "\n",
      "Torso U.leg L.leg U.arm L.arm head Total\n",
      "\n",
      "LSP\n",
      "\n",
      "Andriluka et al. [2] 80.9 67.1 60.7 46.5\n",
      "Yang&Ramanan [57] 81.0 69.5 65.9 53.5\n",
      "Yang&Ramanan [58] 82.9 70.3 67.0 56.0\n",
      "Pishchulin et al. [40] 87.5 75.7 68.0 54.2\n",
      "\n",
      "26.4 74.9 55.7\n",
      "35.8 76.8 60.7\n",
      "39.8 79.3 62.8\n",
      "33.9 78.1 62.9\n",
      "\n",
      "Eichner&\n",
      "Ferrari [11]\n",
      "\n",
      "Ours\n",
      "\n",
      "86.2 74.3 69.3 56.5\n",
      "85.8 76.5 72.2 63.3\n",
      "\n",
      "37.4 80.1 64.3\n",
      "46.6 83.1 68.6\n",
      "\n",
      "PARSE\n",
      "\n",
      "Andriluka et al. [2] 86.3 66.3 60.0 54.6\n",
      "Yang&Ramanan [57] 83.4 68.8 60.7 59.8\n",
      "Yang&Ramanan [58] 82.9 68.8 60.5 63.4\n",
      "Pishchulin et al.\n",
      "###\n",
      "[42] 88.8 77.3 67.1 53.7\n",
      "Pishchulin et al.\n",
      "###\n",
      "[40] 92.2 74.6 63.7 54.9\n",
      "90.7 80.0 70.0 59.3\n",
      "\n",
      "[40]+[42]\n",
      "Johnson&\n",
      "\n",
      "35.6 72.7 59.2\n",
      "40.7 83.4 62.7\n",
      "42.4 82.4 63.6\n",
      "36.1 73.7 63.1\n",
      "39.8 70.7 62.9\n",
      "37.1 77.6 66.1\n",
      "\n",
      "Everingham [22]\n",
      "\n",
      "Ours\n",
      "\n",
      "87.6 74.7 67.1 67.3\n",
      "89.3 78.0 72.0 67.8\n",
      "\n",
      "45.8 76.8 67.4\n",
      "47.8 89.3 71.0\n",
      "\n",
      "UIUC People\n",
      "\n",
      "Andriluka et al.\n",
      "###\n",
      "[2] 88.3 64.0 50.6 42.3\n",
      "Yang&Ramanan [57] 78.1 60.9 53.2 41.3\n",
      "Yang&Ramanan [58] 81.8 65.0 55.1 46.8\n",
      "Pishchulin et al.\n",
      "###\n",
      "[40] 91.5 66.8 54.7 38.3\n",
      "86.8 56.3 50.2 30.8\n",
      "89.1 72.9 62.4 56.3\n",
      "\n",
      "Wang et al.\n",
      "###\n",
      "[56]\n",
      "\n",
      "Ours\n",
      "\n",
      "21.3 81.8 52.6\n",
      "32.2 76.1 53.0\n",
      "37.7 79.8 57.0\n",
      "23.9 85.0 54.4\n",
      "20.3 68.8 47.0\n",
      "47.6 89.1 65.6\n",
      "\n",
      "Table 2.\n",
      "###\n",
      "Results (PCP) on investigating model depth.\n",
      "\n",
      "\n",
      "###\n",
      "Method\n",
      "\n",
      "Torso U.leg L.leg U.arm L.arm Head Total\n",
      "\n",
      "LSP\n",
      "[58]\n",
      "82.9 70.3 67.0\n",
      "Kernel SVM 81.9 72.2 67.6\n",
      "1 hidden layer 84.9 73.9 69.5\n",
      "2 hidden layers 85.0 74.6 70.7\n",
      "Ours\n",
      "85.8 76.5 72.2\n",
      "\n",
      "PARSE\n",
      "\n",
      "[58]\n",
      "82.9 68.8 60.5\n",
      "Kernel SVM 81.0 67.8 61.2\n",
      "1 hidden layer 84.4 71.2 63.2\n",
      "2 hidden layers 85.9 74.4 68.3\n",
      "Ours\n",
      "89.3 78.0 72.0\n",
      "\n",
      "UIUC\n",
      "\n",
      "[58]\n",
      "81.8 65.0 55.1\n",
      "Kernel SVM 82.2 65.0 54.9\n",
      "1 hidden layer 83.0 65.6 55.9\n",
      "2 hidden layers 84.2 68.4 59.3\n",
      "Ours\n",
      "89.1 72.9 62.3\n",
      "\n",
      "56\n",
      "58.8\n",
      "57.5\n",
      "61.2\n",
      "63.3\n",
      "\n",
      "63.4\n",
      "63.2\n",
      "62.4\n",
      "64.6\n",
      "67.8\n",
      "\n",
      "46.8\n",
      "50.2\n",
      "50.6\n",
      "53.0\n",
      "56.3\n",
      "\n",
      "39.8 79.3 62.8\n",
      "42.8 77.5 64.2\n",
      "42.9 50.7 62.3\n",
      "45.2 82.2 67.1\n",
      "46.6 83.1 68.6\n",
      "\n",
      "42.4 82.4 63.6\n",
      "44.1 78.0 63.2\n",
      "44.4 70.2 63.7\n",
      "46.3 85.4 67.9\n",
      "47.8 89.3 71.0\n",
      "\n",
      "37.7 79.8 57.0\n",
      "43.1 80.6 58.9\n",
      "42.3 79.8 59.2\n",
      "45.3\n",
      "###\n",
      "83.4 62.0\n",
      "47.6 89.1 65.6\n",
      "\n",
      "proved that linear-SVM and kernel-SVM are shallow mod-\n",
      "els.\n",
      "###\n",
      "With the deep model, our approach performs better.\n",
      "###\n",
      "As\n",
      "the number of hidden layers increases from 1 hidden layer\n",
      "to 2 hidden layers, the estimation accuracy increases from\n",
      "62.3% to 67.1%.\n",
      "###\n",
      "With PCP 68.6%, our ﬁnal model in Fig.\n",
      "\n",
      "###\n",
      "3(b) uses three hidden layers and is better than SVM and\n",
      "deep models with fewer layers.\n",
      "\n",
      "\n",
      "###\n",
      "Table 3.\n",
      "###\n",
      "Results (PCP) on investigating deep model structures.\n",
      "\n",
      "###\n",
      "Method\n",
      "\n",
      "Torso U.leg L.leg U.arm L.arm Head Total\n",
      "\n",
      "LSP\n",
      "DBN in Fig.\n",
      "###\n",
      "3(a) 82.9 73.2 69.5\n",
      "Ours\n",
      "85.8 76.5 72.2\n",
      "\n",
      "PARSE\n",
      "\n",
      "DBN in Fig.\n",
      "###\n",
      "3(a) 82.0 70.0 64.6\n",
      "\n",
      "###\n",
      "Ours\n",
      "89.3 78.0 72.0\n",
      "\n",
      "DBN in Fig.\n",
      "###\n",
      "3(a) 87.4 68.4 58.3\n",
      "Ours\n",
      "89.1 72.9 62.3\n",
      "\n",
      "UIUC\n",
      "\n",
      "59.8\n",
      "63.3\n",
      "\n",
      "62.9\n",
      "67.8\n",
      "\n",
      "52.2\n",
      "56.3\n",
      "\n",
      "43.8 79.2 65.5\n",
      "46.6 83.1 68.6\n",
      "\n",
      "46.3 80.5 65.0\n",
      "47.8 89.3 71.0\n",
      "\n",
      "44.3 84.6 61.8\n",
      "47.6 89.1 65.6\n",
      "\n",
      "Deep model structure design is investigated in Table 3.\n",
      "\n",
      "###\n",
      "The DBN in Fig.\n",
      "###\n",
      "3(a) trains a three-layer deep model over\n",
      "the concatenated informations with three hidden layers.\n",
      "###\n",
      "The\n",
      "model in 3(b) learns high-order representations individu-\n",
      "ally.\n",
      "###\n",
      "The model in 3(b) with PCP 68.6% is better in con-\n",
      "structing the high-order representations and therefore has\n",
      "higher estimation accuracy compared with the DBN in Fig.\n",
      "\n",
      "###\n",
      "3(a) with PCP 65.5%.\n",
      "\n",
      "\n",
      "###\n",
      "Classiﬁcation label and location learning is investigated\n",
      "in Table 4.\n",
      "###\n",
      "There are two sets of labels to be estimated\n",
      "in our deep model: classiﬁcation label ycls and part posi-\n",
      "tions ypst.\n",
      "###\n",
      "In the experiments, we evaluate different ways\n",
      "of estimating these labels.\n",
      "###\n",
      "The Only ycls in Table 4, with\n",
      "PCP 63.7%, only estimates class label, with part location\n",
      "directly obtained by the approach in [58].\n",
      "###\n",
      "The Only ypst,\n",
      "with PCP 64.1%, only reﬁnes the part location, with class\n",
      "label directly obtained by the approach in [58].\n",
      "###\n",
      "Separate\n",
      "ycls+ypst, with PCP 64.7%, uses two deep models for esti-\n",
      "mating ycls and ypst separately.\n",
      "###\n",
      "It can be seen that both ycls\n",
      "and ypst are helpful for improving accuracy.\n",
      "###\n",
      "Our model\n",
      "uses the single deep model to jointly learn both ycls and\n",
      "ypst (PCP 68.6%) and performs better than using two mod-\n",
      "els to learn them separately (PCP 64.7%) because body lo-\n",
      "cation and the correctness of candidate body location are\n",
      "dependent.\n",
      "\n",
      "\n",
      "###\n",
      "Analysis.\n",
      "###\n",
      "Our model extracts high-order representations\n",
      "of appearance, deformation and mixture types and better\n",
      "models their dependence at the top layer.\n",
      "###\n",
      "For example, if\n",
      "the mixture types are upright upper- and lower-arms, the\n",
      "weighted combination of the locations of wrist and shoul-\n",
      "der is a good estimation on the location of elbow.\n",
      "\n",
      "###\n",
      "If the\n",
      "mixture types change, such estimation should change cor-\n",
      "respondingly.\n",
      "###\n",
      "Such complex dependence cannot be mod-\n",
      "eled linearly and deep model is a better solution.\n",
      "###\n",
      "When\n",
      "different information sources are extracted separately with\n",
      "the ﬁrst several layers, the connections across sources are\n",
      "removed and the number of parameters is reduced.\n",
      "###\n",
      "It helps\n",
      "to regularize optimization when training samples are lim-\n",
      "ited.\n",
      "###\n",
      "Existing methods only use ycls for supervision, while\n",
      "we use both ypts and ycls.\n",
      "###\n",
      "As shown in Fig. 4, reﬁning\n",
      "ypts does help to rectify incorrect part locations based on\n",
      "the high order prior model of body pose. Jointly learning\n",
      "\n",
      "\f",
      "Table 4.\n",
      "###\n",
      "PCP results on classiﬁcation label and location learning.\n",
      "\n",
      "\n",
      "###\n",
      "Method\n",
      "\n",
      "Torso U.leg L.leg U.arm L.arm Head Total\n",
      "\n",
      "LSP\n",
      "[58]\n",
      "82.9 70.3 67.0\n",
      "Only ycls 82.0 71.5 68.0\n",
      "Only ypst 80.4 72.0 68.0\n",
      "Separate\n",
      "ycls+ypst 81.1 72.8 69.0\n",
      "Ours\n",
      "85.8 76.5 72.2\n",
      "\n",
      "PARSE\n",
      "\n",
      "[58]\n",
      "82.9 68.8 60.5\n",
      "Only ycls 81.0 69.8 66.1\n",
      "\n",
      "###\n",
      "Only ypst 80.5 71.2 65.4\n",
      "Separate\n",
      "ycls+ypst 83.4 73.7 67.6\n",
      "89.3 78.0 72.0\n",
      "Ours\n",
      "\n",
      "UIUC\n",
      "\n",
      "[58]\n",
      "81.8 65.0 55.1\n",
      "Only ycls 85.4 68.8 59.3\n",
      "Only ypst 82.6 66.6 58.3\n",
      "Separate\n",
      "\n",
      "###\n",
      "ycls+ypst 87.9 69.6 60.3\n",
      "Ours\n",
      "89.1 72.9 62.3\n",
      "\n",
      "56.0\n",
      "57.6\n",
      "59.2\n",
      "\n",
      "59.5\n",
      "63.3\n",
      "\n",
      "63.4\n",
      "60.5\n",
      "62.2\n",
      "\n",
      "64.4\n",
      "67.8\n",
      "\n",
      "46.8\n",
      "49.2\n",
      "52.2\n",
      "\n",
      "53.0\n",
      "56.3\n",
      "\n",
      "39.8 79.3 62.8\n",
      "42.0 77.2 63.7\n",
      "42.8 76.8 64.1\n",
      "\n",
      "43.0 77.7 64.7\n",
      "46.6 83.1 68.6\n",
      "\n",
      "42.4 82.4 63.6\n",
      "43.9 76.1 63.8\n",
      "44.4 79.5 64.6\n",
      "\n",
      "47.1 82.0 67.1\n",
      "47.8 89.3 71.0\n",
      "\n",
      "37.7 79.8 57.0\n",
      "40.5 83.4 60.4\n",
      "44.7 81.8 60.8\n",
      "\n",
      "44.3 85.4 62.8\n",
      "47.6 89.1 65.6\n",
      "\n",
      "ypst and ycls helps to ﬁnd their shared representation under\n",
      "a multi-task learning framework, for which deep model is\n",
      "an ideal choice.\n",
      "\n",
      "\n",
      "###\n",
      "6.\n",
      "###\n",
      "Conclusion\n",
      "\n",
      "This paper has proposed a multi-source deep model for\n",
      "pose estimation.\n",
      "###\n",
      "It non-linearly integrates three information\n",
      "sources: appearance score, deformation and appearance\n",
      "mixture type.\n",
      "###\n",
      "These information sources are used for de-\n",
      "scribing different aspects of the single modality data, which\n",
      "is the image data in our pose estimation approach.\n",
      "###\n",
      "Exten-\n",
      "\n",
      "###\n",
      "sive experimental comparisons on three public benchmark\n",
      "datasets show that the proposed model obviously improves\n",
      "the pose estimation accuracy and outperforms the state of\n",
      "the art.\n",
      "###\n",
      "Since this model is a post-processing of informa-\n",
      "tion sources, it is very ﬂexible in terms of integrating with\n",
      "existing approaches that use different information sources,\n",
      "features, or articulation models.\n",
      "###\n",
      "Learning deep model from\n",
      "pixels for pose estimation and analyzing the inﬂuence of\n",
      "training data number will be the future work.\n",
      "\n",
      "\n",
      "###\n",
      "7.\n",
      "###\n",
      "Acknowledgement\n",
      "\n",
      "This work is supported by the General Research\n",
      "Fund sponsored by the Research Grants Council of\n",
      "Hong Kong (Project No.\n",
      "\n",
      "###\n",
      "CUHK 417110, CUHK\n",
      "417011, CUHK 429412), National Natural Science Foun-\n",
      "dation of China (91320101), Shenzhen Basic Research\n",
      "Program (JC201005270350A, JCYJ20120903092050890,\n",
      "JCYJ20120617114614438), and Guangdong Innovative\n",
      "Research Team Program (No.201001D0104648280).\n",
      "\n",
      "\n",
      "###\n",
      "Figure 5.\n",
      "###\n",
      "Comparison between our method (left) and the approach\n",
      "in [58] (right) on the LSP, PARSE and UIUC dataset.\n",
      "###\n",
      "Our ap-\n",
      "proach obtains more reasonable articulation patterns and is better\n",
      "in solving the double counting problem.\n",
      "###\n",
      "Best viewed in color.\n",
      "\n",
      "\n",
      "###\n",
      "References\n",
      "[1] M. Andriluka, L. Pishchulin, P. Gehler, and B. Schiele.\n",
      "###\n",
      "2d\n",
      "human pose estimation: New benchmark and state of the art\n",
      "analysis.\n",
      "###\n",
      "In CVPR, 2014.\n",
      "\n",
      "\n",
      "###\n",
      "[2] M. Andriluka, S. Roth, and B. Schiele.\n",
      "###\n",
      "Pictorial structures\n",
      "revisited: people detection and articulated pose estimation.\n",
      "\n",
      "###\n",
      "In CVPR, 2009.\n",
      "\n",
      "\n",
      "###\n",
      "[3] Y. Bengio.\n",
      "###\n",
      "Learning deep architectures for AI.\n",
      "###\n",
      "Foundations\n",
      "\n",
      "and Trends in Machine Learning, 2(1):1–127, 2009.\n",
      "\n",
      "[4] Y. Bengio, A. Courville, and P. Vincent.\n",
      "###\n",
      "Representation\n",
      "learning: A review and new perspectives.\n",
      "###\n",
      "IEEE Trans.\n",
      "###\n",
      "PAMI,\n",
      "35(8):1798–1828, 2013.\n",
      "\n",
      "[5] C. M. Bishop and N. M. Nasrabadi.\n",
      "###\n",
      "Pattern recognition and\n",
      "\n",
      "machine learning.\n",
      "###\n",
      "springer, 2006.\n",
      "\n",
      "\n",
      "###\n",
      "[6] L. Bourdev and J. Malik.\n",
      "###\n",
      "Poselets: body part detectors\n",
      "\n",
      "trained using 3D human pose annotations.\n",
      "###\n",
      "In ICCV, 2009.\n",
      "\n",
      "[7] N. Dalal and B. Triggs\n",
      "###\n",
      ". Histograms of oriented gradients for\n",
      "\n",
      "human detection.\n",
      "###\n",
      "In CVPR, 2005.\n",
      "\n",
      "[8] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-\n",
      "Fei.\n",
      "###\n",
      "Imagenet: a large-scale hierarchical image database.\n",
      "###\n",
      "In\n",
      "CVPR, 2009.\n",
      "\n",
      "[9] C. Desai and D. Ramanan.\n",
      "###\n",
      "Detecting actions, poses, and\n",
      "\n",
      "objects with relational phraselets.\n",
      "###\n",
      "In ECCV, 2012.\n",
      "\n",
      "\n",
      "###\n",
      "[10] K. Duan, D. Batra, and D. J. Crandall.\n",
      "###\n",
      "A multi-layer com-\n",
      "\n",
      "posite model for human pose estimation.\n",
      "###\n",
      "In BMVC, 2012.\n",
      "\n",
      "\n",
      "###\n",
      "[11] M. Eichner and V. Ferrari.\n",
      "###\n",
      "Appearance sharing for collective\n",
      "\n",
      "human pose estimation.\n",
      "###\n",
      "In ACCV, 2012.\n",
      "\n",
      "\n",
      "###\n",
      "[12] C. Farabet, C. Couprie, L. Najman, and Y. LeCun.\n",
      "###\n",
      "Learning\n",
      "hierarchical features for scene labeling.\n",
      "###\n",
      "IEEE Trans.\n",
      "###\n",
      "PAMI,\n",
      "30:1915–1929, 2013.\n",
      "\n",
      "[13] P. F. Felzenszwalb and D. P. Huttenlocher.\n",
      "###\n",
      "Pictorial struc-\n",
      "\n",
      "tures for object recognition.\n",
      "###\n",
      "IJCV, 61:55–79, 2005.\n",
      "\n",
      "\n",
      "###\n",
      "[14] V. Ferrari, M. Marin-Jimenez, and A. Zisserman.\n",
      "###\n",
      "Progressive\n",
      "search space reduction for human pose estimation.\n",
      "###\n",
      "In CVPR,\n",
      "2008.\n",
      "\n",
      "\n",
      "###\n",
      "Left-OursRight-Yang&Ramanan\f",
      "[15] G. Gkioxari, P. Arbel´aez, L. Bourdev, and J. Malik.\n",
      "###\n",
      "Articu-\n",
      "lated pose estimation using discriminative armlet classiﬁers.\n",
      "\n",
      "###\n",
      "In CVPR, 2013.\n",
      "\n",
      "\n",
      "###\n",
      "[16] I. Goodfellow, H. Lee, Q. V. Le, A. Saxe, and A. Y. Ng.\n",
      "\n",
      "\n",
      "###\n",
      "Measuring invariances in deep networks.\n",
      "###\n",
      "In NIPS, 2009.\n",
      "\n",
      "[17] M. Guillaumin, J. Verbeek, and C. Schmid.\n",
      "###\n",
      "Multimodal\n",
      "semi-supervised learning for image classiﬁcation.\n",
      "###\n",
      "In CVPR,\n",
      "2010.\n",
      "\n",
      "\n",
      "###\n",
      "[18] G. E. Hinton, S. Osindero, and Y. Teh.\n",
      "###\n",
      "A fast learning al-\n",
      "gorithm for deep belief nets.\n",
      "###\n",
      "Neural Computation, 18:1527–\n",
      "1554, 2006.\n",
      "\n",
      "\n",
      "###\n",
      "[19] G. E. Hinton and R. R. Salakhutdinov.\n",
      "\n",
      "\n",
      "###\n",
      "dimensionality of data with neural networks.\n",
      "\n",
      "###\n",
      "313(5786):504 – 507, July 2006.\n",
      "\n",
      "\n",
      "###\n",
      "Reducing the\n",
      "Science,\n",
      "\n",
      "[20] K. Jarrett, K. Kavukcuoglu, M. Ranzato, and Y. LeCun.\n",
      "\n",
      "###\n",
      "What is the best multi-stage architecture for object recog-\n",
      "nition?\n",
      "###\n",
      "In CVPR, 2009.\n",
      "\n",
      "\n",
      "###\n",
      "[21] S. Johnson and M. Everingham.\n",
      "###\n",
      "Clustered pose and nonlin-\n",
      "ear appearance models for human pose estimation.\n",
      "###\n",
      "In BMVC,\n",
      "2010.\n",
      "\n",
      "\n",
      "###\n",
      "[22] S. Johnson and M. Everingham.\n",
      "###\n",
      "Learning effective human\n",
      "pose estimation from inaccurate annotation.\n",
      "###\n",
      "In CVPR, 2011.\n",
      "\n",
      "###\n",
      "[23] A. Krizhevsky, I. Sutskever, and G. Hinton.\n",
      "###\n",
      "Imagenet clas-\n",
      "siﬁcation with deep convolutional neural networks.\n",
      "###\n",
      "In NIPS,\n",
      "2012.\n",
      "\n",
      "[24] F. R. Kschischang, B. J. Frey, and H.-A. Loeliger.\n",
      "###\n",
      "Factor\n",
      "graphs and the sum-product algorithm.\n",
      "###\n",
      "IEEE Trans.\n",
      "###\n",
      "Inf.\n",
      "###\n",
      "The-\n",
      "ory, 47(2):498–519, 2001.\n",
      "\n",
      "\n",
      "###\n",
      "[25] H. Larochelle, Y. Bengio, J. Louradour, and P. Lamblin.\n",
      "###\n",
      "Ex-\n",
      "ploring strategies for training deep neural networks.\n",
      "###\n",
      "J. Ma-\n",
      "chine Learning Research, 10:1–40, 2009.\n",
      "\n",
      "\n",
      "###\n",
      "[26] Q. V. Le, M. Ranzato, R. Monga, M. Devin, K. Chen, G. S.\n",
      "Corrado, J. Dean, and A. Y. Ng.\n",
      "###\n",
      "Building high-level features\n",
      "using large scale unsupervised learning.\n",
      "###\n",
      "In ICML, 2012.\n",
      "\n",
      "\n",
      "###\n",
      "[27] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner.\n",
      "###\n",
      "Gradient-\n",
      "based learning applied to document recognition.\n",
      "###\n",
      "Proceed-\n",
      "ings of the IEEE, 86(11):2278–2324, 1998.\n",
      "\n",
      "\n",
      "###\n",
      "[28] W. Li, R. Zhao, T. Xiao, and X. Wang.\n",
      "###\n",
      "Deepreid: Deep ﬁlter\n",
      "pairing neural network for person re-identiﬁcation.\n",
      "###\n",
      "In CVPR,\n",
      "2014.\n",
      "\n",
      "\n",
      "###\n",
      "[29] P. Luo, Y. Tian, X. Wang, and X. Tang.\n",
      "###\n",
      "Switchable deep\n",
      "\n",
      "network for pedestrian detection.\n",
      "###\n",
      "In CVPR, 2014.\n",
      "\n",
      "\n",
      "###\n",
      "[30] P. Luo, X. Wang, and X. Tang.\n",
      "###\n",
      "Hierarchical face parsing via\n",
      "\n",
      "deep learning.\n",
      "###\n",
      "In CVPR, 2012.\n",
      "\n",
      "\n",
      "###\n",
      "[31] P. Luo, X. Wang, and X. Tang.\n",
      "###\n",
      "A deep sum-product archi-\n",
      "\n",
      "tecture for robust facial attributes analysis.\n",
      "###\n",
      "In ICCV, 2013.\n",
      "\n",
      "\n",
      "###\n",
      "[32] P. Luo, X. Wang, and X. Tang.\n",
      "###\n",
      "Pedestrian parsing via deep\n",
      "\n",
      "decompositional neural network.\n",
      "###\n",
      "In ICCV, 2013.\n",
      "\n",
      "\n",
      "###\n",
      "[33] G. Mori and J. Malik.\n",
      "###\n",
      "Estimating human body conﬁgurations\n",
      "\n",
      "using shape context matching.\n",
      "###\n",
      "In ECCV, 2002.\n",
      "\n",
      "\n",
      "###\n",
      "[34] G. Mori and J. Malik.\n",
      "###\n",
      "Recovering 3D human body conﬁgura-\n",
      "tions using shape contexts.\n",
      "###\n",
      "IEEE Trans.\n",
      "###\n",
      "PAMI, 28(7):1052–\n",
      "1062, 2006.\n",
      "\n",
      "[35] J. Ngiam, A. Khosla, M. Kim, J. Nam, H. Lee, and A. Ng.\n",
      "\n",
      "\n",
      "###\n",
      "Multimodal deep learning.\n",
      "###\n",
      "In ICML, 2011.\n",
      "\n",
      "\n",
      "###\n",
      "[36] M. Norouzi, M. Ranjbar, and G. Mori.\n",
      "###\n",
      "Stacks of convolu-\n",
      "tional restricted boltzmann machines for shift-invariant fea-\n",
      "ture learning.\n",
      "###\n",
      "In CVPR, 2009.\n",
      "\n",
      "\n",
      "###\n",
      "[37] W. Ouyang and X. Wang.\n",
      "###\n",
      "A discriminative deep model\n",
      "for pedestrian detection with occlusion handling.\n",
      "###\n",
      "In CVPR,\n",
      "2012.\n",
      "\n",
      "\n",
      "###\n",
      "[38] W. Ouyang and X. Wang.\n",
      "###\n",
      "Joint deep learning for pedestrian\n",
      "\n",
      "detection.\n",
      "###\n",
      "In ICCV, 2013.\n",
      "\n",
      "\n",
      "###\n",
      "[39] W. Ouyang, X. Zeng, and X. Wang.\n",
      "###\n",
      "Modeling mutual visi-\n",
      "\n",
      "bility relationship in pedestrian detection.\n",
      "###\n",
      "In CVPR, 2013.\n",
      "\n",
      "\n",
      "###\n",
      "[40] L. Pishchulin, M. Andriluka, P. Gehler, and B. Schiele.\n",
      "###\n",
      "Pose-\n",
      "\n",
      "let conditioned pictorial structures.\n",
      "###\n",
      "In CVPR, 2013.\n",
      "\n",
      "\n",
      "###\n",
      "[41] L. Pishchulin, M. Andriluka, P. Gehler, and B. Schiele.\n",
      "\n",
      "###\n",
      "Strong appearance and expressive spatial models for human\n",
      "pose estimation.\n",
      "###\n",
      "In ICCV, December 2013.\n",
      "\n",
      "\n",
      "###\n",
      "[42] L. Pishchulin, A. Jain, M. Andriluka, T. Thormahlen, and\n",
      "B. Schiele.\n",
      "###\n",
      "Articulated people detection and pose estimation:\n",
      "Reshaping the future.\n",
      "###\n",
      "In CVPR, 2012.\n",
      "\n",
      "\n",
      "###\n",
      "[43] H. Poon and P. Domingos.\n",
      "###\n",
      "Sum-product networks: A new\n",
      "\n",
      "deep architecture.\n",
      "###\n",
      "In UAI, 2011.\n",
      "\n",
      "\n",
      "###\n",
      "[44] D. Ramanan.\n",
      "###\n",
      "Learning to parse images of articulated bodies.\n",
      "\n",
      "\n",
      "###\n",
      "In NIPS, 2007.\n",
      "\n",
      "[45] M. Ranzato, F. J. Huang, Y.-L. Boureau, and Y. Lecun.\n",
      "###\n",
      "Un-\n",
      "supervised learning of invariant feature hierarchies with ap-\n",
      "plications to object recognition.\n",
      "###\n",
      "In CVPR, 2007.\n",
      "\n",
      "\n",
      "###\n",
      "[46] B. Sapp, A. Toshev, and B. Taskar\n",
      "###\n",
      ". Cascaded models for\n",
      "\n",
      "articulated pose estimation.\n",
      "###\n",
      "In ECCV, 2010.\n",
      "\n",
      "\n",
      "###\n",
      "[47] N. Srivastava and R. Salakhutdinov.\n",
      "###\n",
      "Multimodal learning\n",
      "\n",
      "with deep boltzmann machines.\n",
      "###\n",
      "In NIPS, 2012.\n",
      "\n",
      "[48] M. Sun and S. Savarese.\n",
      "###\n",
      "Articulated part-based model for\n",
      "\n",
      "joint object detection and pose estimation.\n",
      "###\n",
      "In ICCV, 2011.\n",
      "\n",
      "\n",
      "###\n",
      "[49] Y. Sun, X. Wang, and X. Tang.\n",
      "###\n",
      "Deep convolutional network\n",
      "\n",
      "cascade for facial point detection.\n",
      "###\n",
      "In CVPR, 2013.\n",
      "\n",
      "\n",
      "###\n",
      "[50] Y. Sun, X. Wang, and X. Tang.\n",
      "###\n",
      "Hybrid deep learning for\n",
      "\n",
      "computing face similarities.\n",
      "###\n",
      "In ICCV, 2013.\n",
      "\n",
      "\n",
      "###\n",
      "[51] Y. Sun, X. Wang, and X. Tang.\n",
      "###\n",
      "Deep learning face represen-\n",
      "\n",
      "tation from predicting 10,000 classes.\n",
      "###\n",
      "In CVPR, 2014.\n",
      "\n",
      "\n",
      "###\n",
      "[52] Y. Tian, C. L. Zitnick, and S. G. Narasimhan. Exploring the\n",
      "spatial hierarchy of mixture models for human pose estima-\n",
      "tion.\n",
      "###\n",
      "In ECCV, 2012.\n",
      "\n",
      "\n",
      "###\n",
      "[53] D. Tran and D. Forsyth.\n",
      "###\n",
      "Improved human parsing with a full\n",
      "\n",
      "relational model.\n",
      "###\n",
      "In ECCV, 2010.\n",
      "\n",
      "\n",
      "###\n",
      "[54] F. Wang and Y. Li.\n",
      "###\n",
      "Beyond physical connections: Tree mod-\n",
      "\n",
      "els in human pose estimation.\n",
      "###\n",
      "In CVPR, 2013.\n",
      "\n",
      "\n",
      "###\n",
      "[55] Y. Wang and G. Mori.\n",
      "###\n",
      "Multiple tree models for occlusion\n",
      "and spatial constraints in human pose estimation.\n",
      "###\n",
      "In ECCV,\n",
      "2008.\n",
      "\n",
      "\n",
      "###\n",
      "[56] Y. Wang, D. Tran, and Z. Liao.\n",
      "###\n",
      "Learning hierarchical pose-\n",
      "\n",
      "lets for human parsing.\n",
      "###\n",
      "In CVPR, 2011.\n",
      "\n",
      "\n",
      "###\n",
      "[57] Y. Yang and D. Ramanan.\n",
      "###\n",
      "Articulated pose estimation with\n",
      "\n",
      "ﬂexible mixtures-of-parts.\n",
      "###\n",
      "In CVPR, 2011.\n",
      "\n",
      "\n",
      "###\n",
      "[58] Y. Yang and D. Ramanan.\n",
      "###\n",
      "Articulated human detection with\n",
      "\n",
      "ﬂexible mixtures-of-parts.\n",
      "###\n",
      "IEEE Trans.\n",
      "###\n",
      "PAMI, To appear.\n",
      "\n",
      "\n",
      "###\n",
      "[59] M. D. Zeiler, G. W. Taylor, and R. Fergus.\n",
      "###\n",
      "Adaptive decon-\n",
      "volutional networks for mid and high level feature learning.\n",
      "\n",
      "###\n",
      "In ICCV, 2011.\n",
      "\n",
      "\n",
      "###\n",
      "[60] X. Zeng, W. Ouyang, and X. Wang.\n",
      "###\n",
      "Multi-stage contextual\n",
      "\n",
      "deep learning for pedestrian detection.\n",
      "###\n",
      "In ICCV, 2013.\n",
      "\n",
      "\n",
      "###\n",
      "[61] Z. Zhu, P. Luo, X. Wang, and X. Tang.\n",
      "###\n",
      "Deep learning identity\n",
      "\n",
      "preserving face space.\n",
      "###\n",
      "In ICCV, 2013.\n",
      "\n",
      "\f",
      "\n"
     ]
    }
   ],
   "source": [
    "for sent in doc.sents:\n",
    "    print \"###\"\n",
    "    print sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk import ngrams\n",
    "def runTextAnalysisSpacey(txt):\n",
    "    def show_ents(ents):\n",
    "        \"\"\"Return an entity array for a document\"\"\"\n",
    "        entitiesArray = []\n",
    "#         print (\"Print first entity ........\")\n",
    "        entities = list(ents)\n",
    "        for entity in entities:\n",
    "            entitiesArray.append((entity.label_,' '.join(t.orth_ for t in entity)))\n",
    "#         if entities:\n",
    "# #             print(entities[0].label_,' '.join(t.orth_ for t in entities[0]))\n",
    "        return entitiesArray\n",
    "    def parse_ents(txtAnalysis):\n",
    "        \"\"\"Parses entities in text into entity name\"\"\"\n",
    "        all_ents = []\n",
    "        parsed_tokens = txtAnalysis['tokens'][:]\n",
    "        for entities in txtAnalysis['entities']:\n",
    "            all_ents.append(entities[0])\n",
    "        all_ents = set(all_ents)\n",
    "\n",
    "        for eachEntType in all_ents:\n",
    "            entType = set([entity[1] for entity in txtAnalysis['entities'] if entity[0] == eachEntType])\n",
    "#             print eachEntType\n",
    "#             print entType\n",
    "            for index, token in enumerate(txtAnalysis['tokens']):\n",
    "                if token in entType:\n",
    "                    parsed_tokens[index] = entType\n",
    "                else:\n",
    "                    parsed_tokens[index] = token\n",
    "        return parsed_tokens\n",
    "    def is_float(s):\n",
    "        \"\"\"Custom function needed for detecting float numbers\"\"\"\n",
    "        try:\n",
    "            float(s)\n",
    "            return True\n",
    "        except ValueError:\n",
    "            return False\n",
    "            \n",
    "    # create English stop words list\n",
    "    en_stop = get_stop_words('en')\n",
    "#     # Create p_stemmer of class PorterStemmer\n",
    "#     p_stemmer = SnowballStemmer('english')\n",
    "    \n",
    "#     wnl = nltk.WordNetLemmatizer()\n",
    "#     sent_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "#     txtAnalysis = {\n",
    "#         'rawText':txt,\n",
    "#         'abstract': txt.lower().split('introduction')[0].split('abstract'),\n",
    "#         'tokens': [i for i in nltk.word_tokenize(txt) if not i.isdigit() if len(i)>2],\n",
    "#         'stopped_tokens':[i for i in nltk.word_tokenize(txt) if not i.isdigit() if not i in en_stop],\n",
    "#         'stemmed_tokens':0,\n",
    "#         'num_words': 0,\n",
    "#         'words': 0,\n",
    "#         'vocab': 0,\n",
    "#         'lemmatizedVocab': 0,\n",
    "#         'senteces': 0\n",
    "#     }  \n",
    "#     txtAnalysis['stemmed_tokens'] = [p_stemmer.stem(i) for i in txtAnalysis['stopped_tokens']]\n",
    "#     txtAnalysis['num_words'] = len(txtAnalysis['tokens'])\n",
    "#     txtAnalysis['words'] = [w for w in txtAnalysis['stopped_tokens']]\n",
    "#     txtAnalysis['vocab'] = sorted(set(txtAnalysis['words']))\n",
    "#     txtAnalysis['lemmatizedVocab'] = [wnl.lemmatize(t) for t in txtAnalysis['vocab']]\n",
    "#     txtAnalysis['sentences'] = sent_tokenizer.tokenize(txt)\n",
    "#     #convert to nltk Text\n",
    "#     text = nltk.Text(txtAnalysis['tokens'])\n",
    "#     #Collocations are very interesting but just prints text to screen need to retrieve this somehow.\n",
    "#     #collocations = text.collocations()\n",
    "    \n",
    "    parser = English()\n",
    "    #Convert to SPACEY text\n",
    "    doc = nlp(txt)\n",
    "    tokens = parser(txt)\n",
    "    txtAnalysis = {\n",
    "        'tokens': [token.orth_.lower() for token in tokens if not token.orth_.isspace() if not token.orth_.lower() in en_stop],\n",
    "        'sents' : list(doc.sents),\n",
    "        'lemma' : [token.lemma_.lower() for token in tokens if not token.orth_.isspace()],\n",
    "        'entities': show_ents(doc.ents),\n",
    "        'parsed_entities' : False\n",
    "    }\n",
    "    txtAnalysis['parsed_entities'] = parse_ents(txtAnalysis)\n",
    "    #We delete the numbers from the tokens\n",
    "    txtAnalysis['tokens'] = [token for token in txtAnalysis['tokens'] if not token.isdigit() if not is_float(token) if not token in [',','.','[',']','-',';','(',')',':','=']]\n",
    "    return txtAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a=runTextAnalysisSpacey(text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'multi',\n",
       " u'source',\n",
       " u'deep',\n",
       " u'learning',\n",
       " u'human',\n",
       " u'pose',\n",
       " u'estimation',\n",
       " u'wanli',\n",
       " u'ouyang',\n",
       " u'xiao',\n",
       " u'chu',\n",
       " u'xiaogang',\n",
       " u'wang',\n",
       " u'department',\n",
       " u'electronic',\n",
       " u'engineering',\n",
       " u'chinese',\n",
       " u'university',\n",
       " u'hong',\n",
       " u'kong',\n",
       " u'wlouyang@ee.cuhk.edu.hk',\n",
       " u'xgwang@ee.cuhk.edu.hk',\n",
       " u'abstract',\n",
       " u'visual',\n",
       " u'appearance',\n",
       " u'score',\n",
       " u'appearance',\n",
       " u'mixture',\n",
       " u'type',\n",
       " u'deformation',\n",
       " u'three',\n",
       " u'important',\n",
       " u'information',\n",
       " u'sources',\n",
       " u'human',\n",
       " u'pose',\n",
       " u'estimation',\n",
       " u'paper',\n",
       " u'proposes',\n",
       " u'build',\n",
       " u'multi',\n",
       " u'source',\n",
       " u'deep',\n",
       " u'model',\n",
       " u'order',\n",
       " u'extract',\n",
       " u'non',\n",
       " u'linear',\n",
       " u'representation',\n",
       " u'different',\n",
       " u'aspects',\n",
       " u'information',\n",
       " u'sources',\n",
       " u'deep',\n",
       " u'model',\n",
       " u'global',\n",
       " u'high',\n",
       " u'order',\n",
       " u'hu-',\n",
       " u'man',\n",
       " u'body',\n",
       " u'articulation',\n",
       " u'patterns',\n",
       " u'information',\n",
       " u'sources',\n",
       " u'extracted',\n",
       " u'pose',\n",
       " u'estimation',\n",
       " u'task',\n",
       " u'estimat-',\n",
       " u'ing',\n",
       " u'body',\n",
       " u'locations',\n",
       " u'task',\n",
       " u'human',\n",
       " u'detection',\n",
       " u'jointly',\n",
       " u'learned',\n",
       " u'using',\n",
       " u'uni\\ufb01ed',\n",
       " u'deep',\n",
       " u'model',\n",
       " u'proposed',\n",
       " u'approach',\n",
       " u'can',\n",
       " u'viewed',\n",
       " u'post',\n",
       " u'processing',\n",
       " u'pose',\n",
       " u'esti-',\n",
       " u'mation',\n",
       " u'results',\n",
       " u'can',\n",
       " u'\\ufb02exibly',\n",
       " u'integrate',\n",
       " u'existing',\n",
       " u'meth-',\n",
       " u'ods',\n",
       " u'taking',\n",
       " u'information',\n",
       " u'sources',\n",
       " u'input',\n",
       " u'extract-',\n",
       " u'ing',\n",
       " u'non',\n",
       " u'linear',\n",
       " u'representation',\n",
       " u'multiple',\n",
       " u'information',\n",
       " u'sources',\n",
       " u'deep',\n",
       " u'model',\n",
       " u'outperforms',\n",
       " u'state',\n",
       " u'art',\n",
       " u'percent',\n",
       " u'three',\n",
       " u'public',\n",
       " u'benchmark',\n",
       " u'datasets',\n",
       " u'introduction',\n",
       " u'human',\n",
       " u'pose',\n",
       " u'estimation',\n",
       " u'process',\n",
       " u'determining',\n",
       " u'image',\n",
       " u'positions',\n",
       " u'human',\n",
       " u'body',\n",
       " u'parts',\n",
       " u'head',\n",
       " u'shoulder',\n",
       " u'elbow',\n",
       " u'wrist',\n",
       " u'hip',\n",
       " u'knee',\n",
       " u'ankle',\n",
       " u'fundamental',\n",
       " u'problem',\n",
       " u'computer',\n",
       " u'vision',\n",
       " u'abun-',\n",
       " u'dant',\n",
       " u'important',\n",
       " u'applications',\n",
       " u'sports',\n",
       " u'action',\n",
       " u'recogni-',\n",
       " u'tion',\n",
       " u'character',\n",
       " u'animation',\n",
       " u'clinical',\n",
       " u'analysis',\n",
       " u'gait',\n",
       " u'patholo-',\n",
       " u'gies',\n",
       " u'content',\n",
       " u'based',\n",
       " u'video',\n",
       " u'image',\n",
       " u'retrieval',\n",
       " u'intelli-',\n",
       " u'gent',\n",
       " u'video',\n",
       " u'surveillance',\n",
       " u'despite',\n",
       " u'many',\n",
       " u'years',\n",
       " u'research',\n",
       " u'pose',\n",
       " u'estimation',\n",
       " u'remains',\n",
       " u'dif\\ufb01-',\n",
       " u'cult',\n",
       " u'problem',\n",
       " u'one',\n",
       " u'signi\\ufb01cant',\n",
       " u'challenges',\n",
       " u'pose',\n",
       " u'estimation',\n",
       " u'model',\n",
       " u'complex',\n",
       " u'human',\n",
       " u'articulation',\n",
       " u'many',\n",
       " u'approaches',\n",
       " u'used',\n",
       " u'handle',\n",
       " u'com-',\n",
       " u'plex',\n",
       " u'human',\n",
       " u'articulation',\n",
       " u'using',\n",
       " u'three',\n",
       " u'information',\n",
       " u'sources',\n",
       " u'mixture',\n",
       " u'type',\n",
       " u'appearance',\n",
       " u'score',\n",
       " u'deformation',\n",
       " u'in\\ufb02uenced',\n",
       " u'human',\n",
       " u'body',\n",
       " u'articulation',\n",
       " u'clothing',\n",
       " u'occlusion',\n",
       " u'etc',\n",
       " u'body',\n",
       " u'part',\n",
       " u'appearance',\n",
       " u'varies',\n",
       " u'handle',\n",
       " u'variation',\n",
       " u'appearance',\n",
       " u'part',\n",
       " u'clustered',\n",
       " u'multiple',\n",
       " u'mixture',\n",
       " u'types',\n",
       " u'shown',\n",
       " u'fig',\n",
       " u'mixture',\n",
       " u'type',\n",
       " u'part',\n",
       " u'part',\n",
       " u'template',\n",
       " u'learned',\n",
       " u'capture',\n",
       " u'appearance',\n",
       " u'appearance',\n",
       " u'scores',\n",
       " u'log',\n",
       " u'likelihoods',\n",
       " u'body',\n",
       " u'parts',\n",
       " u'figure',\n",
       " u'motivation',\n",
       " u'paper',\n",
       " u'using',\n",
       " u'multi',\n",
       " u'source',\n",
       " u'deep',\n",
       " u'model',\n",
       " u'constructing',\n",
       " u'non',\n",
       " u'linear',\n",
       " u'representation',\n",
       " u'three',\n",
       " u'in-',\n",
       " u'formation',\n",
       " u'sources',\n",
       " u'mixture',\n",
       " u'type',\n",
       " u'appearance',\n",
       " u'score',\n",
       " u'deforma-',\n",
       " u'tion',\n",
       " u'best',\n",
       " u'viewed',\n",
       " u'color',\n",
       " u'different',\n",
       " u'locations',\n",
       " u'obtained',\n",
       " u'convolving',\n",
       " u'part',\n",
       " u'templates',\n",
       " u'visual',\n",
       " u'features',\n",
       " u'input',\n",
       " u'image',\n",
       " u'e.g.',\n",
       " u'hog',\n",
       " u'appearance',\n",
       " u'scores',\n",
       " u'inaccurate',\n",
       " u'well',\n",
       " u'locating',\n",
       " u'body',\n",
       " u'parts',\n",
       " u'part',\n",
       " u'template',\n",
       " u'imper-',\n",
       " u'fect',\n",
       " u'therefore',\n",
       " u'deformations',\n",
       " u'relative',\n",
       " u'locations',\n",
       " u'among',\n",
       " u'body',\n",
       " u'parts',\n",
       " u'used',\n",
       " u'encoding',\n",
       " u'likely',\n",
       " u'pairwise',\n",
       " u'poses',\n",
       " u'example',\n",
       " u'head',\n",
       " u'far',\n",
       " u'neck',\n",
       " u'existing',\n",
       " u'approaches',\n",
       " u'use',\n",
       " u'log',\n",
       " u'linear',\n",
       " u'models',\n",
       " u'pairwise',\n",
       " u'potentials',\n",
       " u'three',\n",
       " u'information',\n",
       " u'sources',\n",
       " u'determine',\n",
       " u'whether',\n",
       " u'estimated',\n",
       " u'location',\n",
       " u'correct',\n",
       " u'however',\n",
       " u'information',\n",
       " u'sources',\n",
       " u'log',\n",
       " u'linearly',\n",
       " u'cor-',\n",
       " u'related',\n",
       " u'choosing',\n",
       " u'correct',\n",
       " u'candidate',\n",
       " u'exam-',\n",
       " u'ple',\n",
       " u'fig',\n",
       " u'linear',\n",
       " u'models',\n",
       " u'may',\n",
       " u'\\ufb01nd',\n",
       " u'estimated',\n",
       " u'result',\n",
       " u'left',\n",
       " u'result',\n",
       " u'right',\n",
       " u'deformation',\n",
       " u'score',\n",
       " u'simply',\n",
       " u'linearly',\n",
       " u'add',\n",
       " u'local',\n",
       " u'deformation',\n",
       " u'cost',\n",
       " u'obvious',\n",
       " u'human',\n",
       " u'\\ufb01nd',\n",
       " u'result',\n",
       " u'left',\n",
       " u'reasonable',\n",
       " u'similar',\n",
       " u'situations',\n",
       " u'also',\n",
       " u'occur',\n",
       " u'mixture',\n",
       " u'type',\n",
       " u'appearance',\n",
       " u'score',\n",
       " u'there-',\n",
       " u'fore',\n",
       " u'desirable',\n",
       " u'construct',\n",
       " u'non',\n",
       " u'linear',\n",
       " u'representation',\n",
       " u'identi\\ufb01es',\n",
       " u'reasonable',\n",
       " u'con\\ufb01gurations',\n",
       " u'deformation',\n",
       " u'ap-',\n",
       " u'pearance',\n",
       " u'score',\n",
       " u'mixture',\n",
       " u'type',\n",
       " u'order',\n",
       " u'construct',\n",
       " u'useful',\n",
       " u'representation',\n",
       " u'multi-',\n",
       " u'ple',\n",
       " u'information',\n",
       " u'sources',\n",
       " u'pose',\n",
       " u'estimation',\n",
       " u'model',\n",
       " u'multi',\n",
       " u'sourcedeep',\n",
       " u'modelestimated',\n",
       " u'resulttype',\n",
       " u'1type',\n",
       " u'2non',\n",
       " u'linearmodel?yesnoestimated',\n",
       " u'resultmixture',\n",
       " u'typehead',\n",
       " u'topneckdeformationappearance',\n",
       " u'scoretemplatelinear',\n",
       " u'model1389',\n",
       " u'satisfy',\n",
       " u'certain',\n",
       " u'properties',\n",
       " u'first',\n",
       " u'model',\n",
       " u'capture',\n",
       " u'global',\n",
       " u'complex',\n",
       " u'relationships',\n",
       " u'among',\n",
       " u'body',\n",
       " u'parts',\n",
       " u'example',\n",
       " u'fig',\n",
       " u'result',\n",
       " u'left',\n",
       " u'unreasonable',\n",
       " u'global',\n",
       " u'con\\ufb01guration',\n",
       " u'arm',\n",
       " u'torso',\n",
       " u'leg',\n",
       " u'second',\n",
       " u'since',\n",
       " u'reasonable',\n",
       " u'con\\ufb01guration',\n",
       " u'abstract',\n",
       " u'concept',\n",
       " u'information',\n",
       " u'sources',\n",
       " u'less',\n",
       " u'abstract',\n",
       " u'con-',\n",
       " u'cepts',\n",
       " u'model',\n",
       " u'construct',\n",
       " u'abstract',\n",
       " u'representa-',\n",
       " u'tion',\n",
       " u'less',\n",
       " u'abstract',\n",
       " u'representation',\n",
       " u'third',\n",
       " u'since',\n",
       " u'dif-',\n",
       " u'ferent',\n",
       " u'information',\n",
       " u'sources',\n",
       " u'describe',\n",
       " u'different',\n",
       " u'aspects',\n",
       " u'hu-',\n",
       " u'man',\n",
       " u'pose',\n",
       " u'different',\n",
       " u'statistical',\n",
       " u'properties',\n",
       " u'model',\n",
       " u'learn',\n",
       " u'useful',\n",
       " u'representation',\n",
       " u'sources',\n",
       " u'fuse',\n",
       " u'joint',\n",
       " u'representation',\n",
       " u'pose',\n",
       " u'estimation',\n",
       " u'multi',\n",
       " u'source',\n",
       " u'deep',\n",
       " u'architecture',\n",
       " u'propose',\n",
       " u'satis\\ufb01es',\n",
       " u'requirement',\n",
       " u'three',\n",
       " u'contributions',\n",
       " u'paper',\n",
       " u'propose',\n",
       " u'deep',\n",
       " u'architecture',\n",
       " u'construct',\n",
       " u'non-',\n",
       " u'linear',\n",
       " u'representation',\n",
       " u'different',\n",
       " u'aspects',\n",
       " u'informa-',\n",
       " u'tion',\n",
       " u'sources',\n",
       " u'best',\n",
       " u'knowledge',\n",
       " u'paper',\n",
       " u'\\ufb01rst',\n",
       " u'use',\n",
       " u'deep',\n",
       " u'model',\n",
       " u'pose',\n",
       " u'estimation',\n",
       " u'body',\n",
       " u'articulation',\n",
       " u'patterns',\n",
       " u'global',\n",
       " u'abstract',\n",
       " u'representations',\n",
       " u'captured',\n",
       " u'deep',\n",
       " u'model',\n",
       " u'information',\n",
       " u'sources',\n",
       " u'local',\n",
       " u'less',\n",
       " u'abstract',\n",
       " u'representa-',\n",
       " u'tions',\n",
       " u'information',\n",
       " u'source',\n",
       " u'abstract',\n",
       " u'repre-',\n",
       " u'sentation',\n",
       " u'higher',\n",
       " u'layer',\n",
       " u'composed',\n",
       " u'less',\n",
       " u'ab-',\n",
       " u'stract',\n",
       " u'representation',\n",
       " u'body',\n",
       " u'parts',\n",
       " u'lower',\n",
       " u'layer',\n",
       " u'representations',\n",
       " u'information',\n",
       " u'sources',\n",
       " u'higher',\n",
       " u'layer',\n",
       " u'fused',\n",
       " u'pose',\n",
       " u'estimation',\n",
       " u'task',\n",
       " u'detecting',\n",
       " u'human',\n",
       " u'task',\n",
       " u'esti-',\n",
       " u'mating',\n",
       " u'body',\n",
       " u'locations',\n",
       " u'jointly',\n",
       " u'learned',\n",
       " u'using',\n",
       " u'single',\n",
       " u'deep',\n",
       " u'model',\n",
       " u'joint',\n",
       " u'learning',\n",
       " u'tasks',\n",
       " u'shared',\n",
       " u'representation',\n",
       " u'improves',\n",
       " u'pose',\n",
       " u'estimation',\n",
       " u'accuracy',\n",
       " u'related',\n",
       " u'work',\n",
       " u'human',\n",
       " u'pose',\n",
       " u'estimation',\n",
       " u'pose',\n",
       " u'estimation',\n",
       " u'considered',\n",
       " u'holistic',\n",
       " u'recognition',\n",
       " u'hand',\n",
       " u'many',\n",
       " u'recent',\n",
       " u'works',\n",
       " u'use',\n",
       " u'local',\n",
       " u'body',\n",
       " u'parts',\n",
       " u'order',\n",
       " u'handle',\n",
       " u'many',\n",
       " u'degrees',\n",
       " u'freedom',\n",
       " u'body',\n",
       " u'part',\n",
       " u'articulation',\n",
       " u'since',\n",
       " u'\\ufb01rst',\n",
       " u'work',\n",
       " u'approaches',\n",
       " u'clustered',\n",
       " u'part',\n",
       " u'appearance',\n",
       " u'mixture',\n",
       " u'types',\n",
       " u'shown',\n",
       " u'fig',\n",
       " u'also',\n",
       " u'approaches',\n",
       " u'warp',\n",
       " u'part',\n",
       " u'template',\n",
       " u'\\ufb02exible',\n",
       " u'sizes',\n",
       " u'orientations',\n",
       " u'appearance',\n",
       " u'score',\n",
       " u'rotation',\n",
       " u'size',\n",
       " u'location',\n",
       " u'used',\n",
       " u'approaches',\n",
       " u'can',\n",
       " u'treated',\n",
       " u'multiple',\n",
       " u'information',\n",
       " u'sources',\n",
       " u'used',\n",
       " u'deep',\n",
       " u'model',\n",
       " u'pose',\n",
       " u'estimation',\n",
       " u'existing',\n",
       " u'pose',\n",
       " u'estimation',\n",
       " u'approaches',\n",
       " u'pair',\n",
       " u'wise',\n",
       " u'part',\n",
       " u'deformation',\n",
       " u'relationships',\n",
       " u'arranged',\n",
       " u'tree',\n",
       " u'models',\n",
       " u'multi',\n",
       " u'tree',\n",
       " u'model',\n",
       " u'loopy',\n",
       " u'mod-',\n",
       " u'els',\n",
       " u'tree',\n",
       " u'models',\n",
       " u'allow',\n",
       " u'ef\\ufb01cient',\n",
       " u'exact',\n",
       " u'inference',\n",
       " u'insuf\\ufb01cient',\n",
       " u'modeling',\n",
       " u'complex',\n",
       " u're-',\n",
       " u'lationships',\n",
       " u'among',\n",
       " u'body',\n",
       " u'parts',\n",
       " u'hence',\n",
       " u'tree',\n",
       " u'models',\n",
       " u'often',\n",
       " u'suffer',\n",
       " u'double',\n",
       " u'counting',\n",
       " u'example',\n",
       " u'given',\n",
       " u'posi-',\n",
       " u'tion',\n",
       " u'torso',\n",
       " u'positions',\n",
       " u'two',\n",
       " u'legs',\n",
       " u'independent',\n",
       " u'often',\n",
       " u'respond',\n",
       " u'visual',\n",
       " u'cue',\n",
       " u'loopy',\n",
       " u'models',\n",
       " u'allow',\n",
       " u'complex',\n",
       " u'relationships',\n",
       " u'among',\n",
       " u'parts',\n",
       " u'require',\n",
       " u'approximate',\n",
       " u'inference',\n",
       " u'deep',\n",
       " u'architecture',\n",
       " u'models',\n",
       " u'complex',\n",
       " u'relationships',\n",
       " u'among',\n",
       " u'parts',\n",
       " u'computationally',\n",
       " u'ef\\ufb01cient',\n",
       " u'training',\n",
       " u'testing',\n",
       " u'deep',\n",
       " u'learning',\n",
       " u'since',\n",
       " u'breakthrough',\n",
       " u'deep',\n",
       " u'learning',\n",
       " u'initiated',\n",
       " u'g.',\n",
       " u'hinton',\n",
       " u'deep',\n",
       " u'learning',\n",
       " u'gain-',\n",
       " u'ing',\n",
       " u'attention',\n",
       " u'bengio',\n",
       " u'proved',\n",
       " u'exist-',\n",
       " u'ing',\n",
       " u'commonly',\n",
       " u'used',\n",
       " u'machine',\n",
       " u'learning',\n",
       " u'tools',\n",
       " u'svm',\n",
       " u'boosting',\n",
       " u'shallow',\n",
       " u'models',\n",
       " u'may',\n",
       " u'require',\n",
       " u'many',\n",
       " u'computational',\n",
       " u'elements',\n",
       " u'potentially',\n",
       " u'exponen-',\n",
       " u'tially',\n",
       " u'respect',\n",
       " u'input',\n",
       " u'size',\n",
       " u'deep',\n",
       " u'models',\n",
       " u'whose',\n",
       " u'depth',\n",
       " u'matched',\n",
       " u'task',\n",
       " u'deep',\n",
       " u'architecture',\n",
       " u'found',\n",
       " u'yield',\n",
       " u'better',\n",
       " u'data',\n",
       " u'representation',\n",
       " u'example',\n",
       " u'terms',\n",
       " u'classi\\ufb01cation',\n",
       " u'error',\n",
       " u'invariance',\n",
       " u'input',\n",
       " u'trans-',\n",
       " u'formations',\n",
       " u'modeling',\n",
       " u'multi',\n",
       " u'modal',\n",
       " u'data',\n",
       " u'deep',\n",
       " u'learning',\n",
       " u'achieved',\n",
       " u'spectacular',\n",
       " u'progress',\n",
       " u'computer',\n",
       " u'vi-',\n",
       " u'sion',\n",
       " u'recent',\n",
       " u'progress',\n",
       " u'deep',\n",
       " u'learning',\n",
       " u'reviewed',\n",
       " u'krizhevsky',\n",
       " u'et',\n",
       " u'al',\n",
       " u'pro-',\n",
       " u'posed',\n",
       " u'large',\n",
       " u'scale',\n",
       " u'deep',\n",
       " u'convolutional',\n",
       " u'network',\n",
       " u'breakthrough',\n",
       " u'large',\n",
       " u'scale',\n",
       " u'imagenet',\n",
       " u'object',\n",
       " u'recogni-',\n",
       " u'tion',\n",
       " u'dataset',\n",
       " u'attaining',\n",
       " u'signi\\ufb01cant',\n",
       " u'gap',\n",
       " u'compared',\n",
       " u'existing',\n",
       " u'approaches',\n",
       " u'use',\n",
       " u'shallow',\n",
       " u'models',\n",
       " u'bringing',\n",
       " u'high',\n",
       " u'impact',\n",
       " u'research',\n",
       " u'computer',\n",
       " u'vision',\n",
       " u'approaches',\n",
       " u'learns',\n",
       " u'feature',\n",
       " u'learning',\n",
       " u'translational',\n",
       " u'de-',\n",
       " u'formation',\n",
       " u'occlusion',\n",
       " u'relationship',\n",
       " u'pedestrian',\n",
       " u'detec-',\n",
       " u'tion',\n",
       " u'approach',\n",
       " u'learns',\n",
       " u'relational',\n",
       " u'\\ufb01lter',\n",
       " u'pairs',\n",
       " u'face',\n",
       " u'veri\\ufb01cation',\n",
       " u'best',\n",
       " u'knowledge',\n",
       " u'however',\n",
       " u'deep',\n",
       " u'model',\n",
       " u'human',\n",
       " u'pose',\n",
       " u'estimation',\n",
       " u'yet',\n",
       " u'explored',\n",
       " u'work',\n",
       " u'inspired',\n",
       " u'multi',\n",
       " u'modality',\n",
       " u'models',\n",
       " u'learn',\n",
       " u'multiple',\n",
       " u'modalities',\n",
       " u'audio',\n",
       " u'visual',\n",
       " u'text',\n",
       " u'data',\n",
       " u'contrast',\n",
       " u'works',\n",
       " u'investigate',\n",
       " u'multi-',\n",
       " u'source',\n",
       " u'learning',\n",
       " u'single',\n",
       " u'modality',\n",
       " u'image',\n",
       " u'data',\n",
       " u'pose',\n",
       " u'estimation',\n",
       " u'pictorial',\n",
       " u'structure',\n",
       " u'model',\n",
       " u'pose',\n",
       " u'estimation',\n",
       " u'model',\n",
       " u'introduced',\n",
       " u'section',\n",
       " u'used',\n",
       " u'provide',\n",
       " u'deep',\n",
       " u'model',\n",
       " u'information',\n",
       " u'sources',\n",
       " u'pictorial',\n",
       " u'struc-',\n",
       " u'ture',\n",
       " u'model',\n",
       " u'considers',\n",
       " u'human',\n",
       " u'body',\n",
       " u'parts',\n",
       " u'nodes',\n",
       " u'tied',\n",
       " u'to-',\n",
       " u'gether',\n",
       " u'conditional',\n",
       " u'random',\n",
       " u'\\ufb01eld',\n",
       " u'let',\n",
       " u'lp',\n",
       " u'p',\n",
       " u'p',\n",
       " u'con\\ufb01guration',\n",
       " u'pth',\n",
       " u'part',\n",
       " u'posterior',\n",
       " u'con-',\n",
       " u'\\ufb01guration',\n",
       " u'parts',\n",
       " u'l',\n",
       " u'{',\n",
       " u'lp|p',\n",
       " u'p',\n",
       " u'}',\n",
       " u'given',\n",
       " u'image',\n",
       " u'p',\n",
       " u'l|i',\n",
       " u'\\u221d',\n",
       " u'exp(cid:0',\n",
       " u'p(cid:88',\n",
       " u'cid:88',\n",
       " u'\\u03c8(lp',\n",
       " u'lq)(cid:1',\n",
       " u'\\u03c6(i|lp',\n",
       " u'+',\n",
       " u'p=1',\n",
       " u'p',\n",
       " u'q)\\u2208e',\n",
       " u'\\u03c8(lp',\n",
       " u'lq',\n",
       " u'pair',\n",
       " u'wise',\n",
       " u'term',\n",
       " u'models',\n",
       " u'geometric',\n",
       " u'deformation',\n",
       " u'constraint',\n",
       " u'pth',\n",
       " u'qth',\n",
       " u'parts',\n",
       " u'exam-',\n",
       " u'ple',\n",
       " u'head',\n",
       " u'shall',\n",
       " u'far',\n",
       " u'torso',\n",
       " u'edge',\n",
       " u'set',\n",
       " u'de-',\n",
       " u'noted',\n",
       " u'e',\n",
       " u'arranged',\n",
       " u'tree',\n",
       " u'models',\n",
       " u'loopy',\n",
       " u'models',\n",
       " u'\\u03c6(i|lp',\n",
       " u'unary',\n",
       " u'term',\n",
       " u'models',\n",
       " u'appearance',\n",
       " u'lp',\n",
       " u'appearance',\n",
       " u'varies',\n",
       " u'body',\n",
       " u'articulates',\n",
       " u'model',\n",
       " u'variation',\n",
       " u'lp',\n",
       " u'{',\n",
       " u'sp',\n",
       " u'\\u03b8p',\n",
       " u'zp',\n",
       " u'}',\n",
       " u'\\u03c6(i|lp',\n",
       " u'speci\\ufb01es',\n",
       " u'part',\n",
       " u'appearance',\n",
       " u'warped',\n",
       " u'size',\n",
       " u'sp',\n",
       " u'orientation',\n",
       " u'\\u03b8p',\n",
       " u'loca-',\n",
       " u'tion',\n",
       " u'zp',\n",
       " u'alternatively',\n",
       " u'yang',\n",
       " u'ramanan',\n",
       " u'propose',\n",
       " u'use',\n",
       " u'appearance',\n",
       " u'mixture',\n",
       " u'type',\n",
       " u'tp',\n",
       " u'approximat-',\n",
       " u'ing',\n",
       " u'variation',\n",
       " u'rotation',\n",
       " u'\\u03b8p',\n",
       " u'size',\n",
       " u'sp',\n",
       " u'model',\n",
       " u'lp',\n",
       " u'{',\n",
       " u'tp',\n",
       " u'zp',\n",
       " u'}',\n",
       " u'\\u03c6(i|lp',\n",
       " u'speci\\ufb01es',\n",
       " ...]"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a['tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"03.523\".isdigit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'a', u'about', u'above', u'after', u'again', u'against', u'all', u'am', u'an', u'and', u'any', u'are', u\"aren't\", u'as', u'at', u'be', u'because', u'been', u'before', u'being', u'below', u'between', u'both', u'but', u'by', u\"can't\", u'cannot', u'could', u\"couldn't\", u'did', u\"didn't\", u'do', u'does', u\"doesn't\", u'doing', u\"don't\", u'down', u'during', u'each', u'few', u'for', u'from', u'further', u'had', u\"hadn't\", u'has', u\"hasn't\", u'have', u\"haven't\", u'having', u'he', u\"he'd\", u\"he'll\", u\"he's\", u'her', u'here', u\"here's\", u'hers', u'herself', u'him', u'himself', u'his', u'how', u\"how's\", u'i', u\"i'd\", u\"i'll\", u\"i'm\", u\"i've\", u'if', u'in', u'into', u'is', u\"isn't\", u'it', u\"it's\", u'its', u'itself', u\"let's\", u'me', u'more', u'most', u\"mustn't\", u'my', u'myself', u'no', u'nor', u'not', u'of', u'off', u'on', u'once', u'only', u'or', u'other', u'ought', u'our', u'ours', u'ourselves', u'out', u'over', u'own', u'same', u\"shan't\", u'she', u\"she'd\", u\"she'll\", u\"she's\", u'should', u\"shouldn't\", u'so', u'some', u'such', u'than', u'that', u\"that's\", u'the', u'their', u'theirs', u'them', u'themselves', u'then', u'there', u\"there's\", u'these', u'they', u\"they'd\", u\"they'll\", u\"they're\", u\"they've\", u'this', u'those', u'through', u'to', u'too', u'under', u'until', u'up', u'very', u'was', u\"wasn't\", u'we', u\"we'd\", u\"we'll\", u\"we're\", u\"we've\", u'were', u\"weren't\", u'what', u\"what's\", u'when', u\"when's\", u'where', u\"where's\", u'which', u'while', u'who', u\"who's\", u'whom', u'why', u\"why's\", u'with', u\"won't\", u'would', u\"wouldn't\", u'you', u\"you'd\", u\"you'll\", u\"you're\", u\"you've\", u'your', u'yours', u'yourself', u'yourselves']\n"
     ]
    }
   ],
   "source": [
    "print get_stop_words('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
